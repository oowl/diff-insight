---
date: '2025-05-23'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:aedda49...MicrosoftDocs:2512fb2
summary: 此次更新主要包含了新增和更新若干图像文件，以及对搜索和导入向量文档的改进，旨在提升文档的视觉支持、用户操作体验，并提供更详尽的使用指导。更新中增加了一些新图像，以加强文档的视觉效果，同时对“搜索入门：门户中的图像搜索”的内容进行了重大修订，提升了用户理解和使用的便利性。此外，轻微更新了一些已有的图像文件并对“导入向量”文档进行了小幅修改。这些更改有助于确保文档的准确性和可读性，从而改善用户在使用Azure服务时的整体体验。
title: '[zh_CN] Diff Insight Report - search'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:aedda49...MicrosoftDocs:2512fb2){target="_blank"}

# Highlights
本次更新主要包括图像文件的新增、更新以及搜索和导入向量的文档改进。这些更改旨在增强文档的视觉支持、改善用户操作体验，并提供更详细和准确的使用指导。

## New features
- 添加了若干新的图像文件，比如 "图像可视化标签"、"查询选项" 和 "文本向量化选项卡" 等。
- 增强了文档整体的功能说明和视觉效果。

## Breaking changes
- 对“搜索入门：门户中的图像搜索”文档进行重大更新，包括大量内容的新增和删除。

## Other updates
- 一些图像文件如 "存储图像"、"命令栏" 等的轻微更新。
- 对“导入向量”文档进行小幅修改，以改善指导和统一格式。

# Insights
此次代码差异显示了一系列对项目中文档和图像资源的更新和优化。以下是每部分修改的技术细节解读和其影响：

1. **图像资源的新增和更新**：本次修改中，多个新的图像文件被添加到项目中，这主要是为了改善文档的视觉效果和信息传达能力。新增的图像，诸如 "图像可视化标签" 和 "查询选项"，为用户在理解和使用搜索功能时提供了更直观的帮助。而更新的一些图像文件，如 "存储图像" 和"命令栏"，则通过更高的清晰度或更新的内容，确保文档展示的信息与当前的系统状态保持一致。

2. **文档的重大和小规模更新**：最突出的是对“搜索入门：门户中的图像搜索”文档的重大更新，这次更新涉及了描述内容的改进、步骤流程的优化、结构的增强等方面，使得用户可以更容易地使用Azure AI Search的多模态功能。此外，对“导入向量”文档的一些更新则主要集中在细节和格式上，旨在提高可读性和操作指导的准确性。这些文档改进，是为了确保文档不仅信息齐全准确，同时通过改良的引导和流程，帮助用户更有效地使用Azure的功能。

3. **旧图像文件的移除**：移除旧的 "向量化文本" 图像文件，可能是因为功能的改变使其不再相关，或因为有更好的替代品。这一措施不仅有助于保持文档的整洁性，还能防止用户受到过时或错误信息的影响。

总体来看，这次更新是为了提高用户体验，通过增强文件中的视觉内容，提高文档的可读性和实用性，帮助用户在使用Azure服务时拥有更顺畅的体验。

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [connect-to-your-data.png](#item-20d7ac) | minor update | 更新连接到您的数据图像 | modified | 0 | 0 | 0 | 
| [extract-your-content.png](#item-8531c0) | minor update | 更新提取您的内容图像 | modified | 0 | 0 | 0 | 
| [image-verbalization-tab.png](#item-93404c) | new feature | 添加图像可视化标签图像 | added | 0 | 0 | 0 | 
| [image-verbalization-tile.png](#item-e0825f) | new feature | 添加图像可视化块图像 | added | 0 | 0 | 0 | 
| [query-options.png](#item-480d4c) | new feature | 添加查询选项图像 | added | 0 | 0 | 0 | 
| [search-button.png](#item-3fb619) | new feature | 添加搜索按钮图像 | added | 0 | 0 | 0 | 
| [select-data-source.png](#item-b45b33) | new feature | 添加选择数据源图像 | added | 0 | 0 | 0 | 
| [store-images.png](#item-ecd246) | minor update | 更新存储图像 | modified | 0 | 0 | 0 | 
| [text-vectorization-tab.png](#item-da78c7) | new feature | 添加文本向量化选项卡图像 | added | 0 | 0 | 0 | 
| [vectorize-your-text.png](#item-249870) | bug fix | 移除向量化文本图像 | removed | 0 | 0 | 0 | 
| [wizard-scenarios-multimodal-rag.png](#item-78df78) | minor update | 更新多模态RAG向导场景图像 | modified | 0 | 0 | 0 | 
| [command-bar.png](#item-99b377) | minor update | 更新命令栏图像 | modified | 0 | 0 | 0 | 
| [wizard-scenarios-rag.png](#item-2d3082) | minor update | 更新RAG向导场景图像 | modified | 0 | 0 | 0 | 
| [search-get-started-portal-image-search.md](#item-438b9b) | breaking change | 更新图像搜索入门文档 | modified | 165 | 67 | 232 | 
| [search-get-started-portal-import-vectors.md](#item-7dae77) | minor update | 更新导入向量文档 | modified | 27 | 15 | 42 | 


# Modified Contents
## articles/search/media/search-get-started-portal-images/connect-to-your-data.png{#item-20d7ac}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新连接到您的数据图像"
}
```

### Explanation
此代码差异表明，一个名为“连接到您的数据”的图像文件 (`connect-to-your-data.png`) 已进行了一些更新。虽然没有检测到任何新增、删除或修改的内容，但文件的状态标记为“已修改”，这可能意味着相关文档或元数据已经更新。文件目前仍可通过提供的 Blob URL 和原始 URL 进行访问。此更新不涉及任何重大变化，仅为轻微更新。

## articles/search/media/search-get-started-portal-images/extract-your-content.png{#item-8531c0}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新提取您的内容图像"
}
```

### Explanation
此代码差异表明，一个名为“提取您的内容”的图像文件 (`extract-your-content.png`) 已经被修改。虽然没有记录任何新增、删除或其它更改，但其状态标注为“已修改”，这可能意味着文件的相关信息或文档内容进行了更新。该文件仍然可通过提供的 Blob URL 和原始 URL 进行访问。此次更新属于轻微更新，并未引入任何显著的变化。

## articles/search/media/search-get-started-portal-images/image-verbalization-tab.png{#item-93404c}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加图像可视化标签图像"
}
```

### Explanation
此代码差异显示一个名为“图像可视化标签”的新图像文件 (`image-verbalization-tab.png`) 被添加到项目中。该文件状态标识为“已添加”，意味着这是一个全新的文件，并未涉及到任何的删除或更改。用户可以通过提供的 Blob URL 和原始 URL 访问这个新图像，此次更新带来了一个新的特性，增强了文档的视觉效果和信息传递能力。

## articles/search/media/search-get-started-portal-images/image-verbalization-tile.png{#item-e0825f}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加图像可视化块图像"
}
```

### Explanation
此代码差异显示一个名为“图像可视化块”的新图像文件 (`image-verbalization-tile.png`) 被添加到项目中。该文件的状态为“已添加”，表示这是一个新增的资源，不涉及删除或更改现有文件。用户可以通过提供的 Blob URL 和原始 URL 访问这个新图像，此次更新为文档增加了新的视觉内容，提升了用户的体验和信息传达效果。

## articles/search/media/search-get-started-portal-images/query-options.png{#item-480d4c}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加查询选项图像"
}
```

### Explanation
此代码差异显示一个新图像文件 (`query-options.png`) 被添加到项目中，文件名指向与查询选项相关的内容。该文件的状态标识为“已添加”，表示此图像是全新引入的，没有任何删除或修改现有文件的操作。用户可以通过提供的 Blob URL 和原始 URL 来访问这个新图像，此次更新为文档提供了额外的视觉支持，加强了对查询选项的理解和展示效果。

## articles/search/media/search-get-started-portal-images/search-button.png{#item-3fb619}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加搜索按钮图像"
}
```

### Explanation
此代码差异表明一个名为“搜索按钮”的新图像文件 (`search-button.png`) 被添加到项目中。文件状态为“已添加”，这意味着该图像是新引入的，没有执行任何删除或修改现有文件的操作。提供的 Blob URL 和原始 URL 允许用户访问这个新图像，此次更新为文档增加了一个重要的视觉元素，有助于用户对搜索功能的理解和使用。

## articles/search/media/search-get-started-portal-images/select-data-source.png{#item-b45b33}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加选择数据源图像"
}
```

### Explanation
此代码差异显示了一个新的图像文件 (`select-data-source.png`) 被添加到项目中，文件名与选择数据源的过程相关。文件的状态为“已添加”，意味着这是一次全新的引入，没有进行任何删除或修改已存在的内容。用户可以通过提供的 Blob URL 及原始 URL 访问这个新图像，此次更新为文档添加了有价值的视觉支持，帮助用户更好地理解选择数据源的操作。

## articles/search/media/search-get-started-portal-images/store-images.png{#item-ecd246}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新存储图像"
}
```

### Explanation
此代码差异表明名为“存储图像”的文件 (`store-images.png`) 已被修改。文件状态为“已修改”，但没有具体的添加或删除动作，表明可能是细微的调整或修正。提供的 Blob URL 和原始 URL 使用户可以访问更新后的图像，此次更新可能旨在提高文档的清晰度或视觉质量，以更好地呈现关于存储图像的说明。

## articles/search/media/search-get-started-portal-images/text-vectorization-tab.png{#item-da78c7}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加文本向量化选项卡图像"
}
```

### Explanation
此代码差异显示名为“文本向量化选项卡”的新图像文件 (`text-vectorization-tab.png`) 被添加到项目中。文件状态为“已添加”，意味着这是一次新的引入，且没有删除或修改已存在的内容。提供的 Blob URL 和原始 URL 允许用户访问此新图像，更新为文档增添了视觉元素，以帮助用户更好地理解文本向量化相关的操作，这是一个重要的功能介绍。

## articles/search/media/search-get-started-portal-images/vectorize-your-text.png{#item-249870}

### Summary

```json
{
    "modification_type": "bug fix",
    "modification_title": "移除向量化文本图像"
}
```

### Explanation
此代码差异表明名为“向量化文本”的图像文件 (`vectorize-your-text.png`) 已被移除。文件状态为“已删除”，这意味着之前存在的文件已经被去除。提供的 Blob URL 和原始 URL 允许用户查看被删除的内容，此次删除可能是因为图像不再相关、需要更新，或者存在技术问题。移除此图像的决定或许是为了清理文档，使其更加准确和简洁。

## articles/search/media/search-get-started-portal-images/wizard-scenarios-multimodal-rag.png{#item-78df78}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新多模态RAG向导场景图像"
}
```

### Explanation
此代码差异显示名为“多模态RAG向导场景”的图像文件 (`wizard-scenarios-multimodal-rag.png`) 已被修改。文件状态为“已修改”，表明对该图像进行了更新，但没有添加或删除内容。提供的 Blob URL 和原始 URL 允许用户访问此图像的最新版本。此次修改可能是为了改善图像的清晰度、信息的准确性或与更新的内容保持一致性，从而提升用户体验。

## articles/search/media/search-get-started-portal-import-vectors/command-bar.png{#item-99b377}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新命令栏图像"
}
```

### Explanation
此代码差异表明名为“命令栏”的图像文件 (`command-bar.png`) 已被修改。文件状态为“已修改”，这意味着对该图像进行了更新，而没有添加或删除内容。通过提供的 Blob URL 和原始 URL，用户可以查看该图像的最新版本。这次修改可能是为了增强图像的视觉效果或使其与最新的用户界面一致，从而确保文档的准确性和用户友好性。

## articles/search/media/search-get-started-portal-import-vectors/wizard-scenarios-rag.png{#item-2d3082}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新RAG向导场景图像"
}
```

### Explanation
此代码差异显示名为“RAG向导场景”的图像文件 (`wizard-scenarios-rag.png`) 已被修改。文件的状态标示为“已修改”，表明该图像进行了更新，但实际没有添加或删除任何内容。提供的 Blob URL 和原始 URL 允许用户访问更新后的图像。这一修改可能是为了提升图像质量、确保与当前文档内容的一致性或改善用户体验，从而使得文档的相关性和准确性得以加强。

## articles/search/search-get-started-portal-image-search.md{#item-438b9b}

<details>
<summary>Diff</summary>
````diff
@@ -1,33 +1,33 @@
 ---
 title: "Quickstart: Multimodal Search in the Azure portal"
 titleSuffix: Azure AI Search
-description: Learn how to search for multimodal content on an Azure AI Search index in the Azure portal. Run a wizard to vectorize text and images, and then use Search Explorer to provide multimodal content as your query input.
+description: Learn how to search for multimodal content on an Azure AI Search index in the Azure portal. Run a wizard to generate natural-language descriptions of images and vectorize both text and images, and then use Search Explorer to query your multimodal index.
 author: haileytap
 ms.author: haileytapia
 ms.service: azure-ai-search
 ms.topic: quickstart
-ms.date: 05/12/2025
+ms.date: 05/22/2025
 ms.custom:
   - references_regions
 ---
 
 # Quickstart: Search for multimodal content in the Azure portal
 
-In this quickstart, you use the **Import and vectorize data wizard** in the Azure portal to get started with [multimodal search](multimodal-search-overview.md). Multimodality refers to the ability to process and query over multiple types of data, such as text and images.
+In this quickstart, you use the **Import and vectorize data** wizard in the Azure portal to get started with [multimodal search](multimodal-search-overview.md). The wizard simplifies the process of extracting page text and inline images from documents, describing images in natural language, vectorizing image descriptions and text, and storing images for later retrieval.
 
 The sample data consists of a multimodal PDF in the [azure-search-sample-data](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/sustainable-ai-pdf) repo, but you can use different files and still follow this quickstart.
 
 ## Prerequisites
 
 + An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ An [Azure Storage account](/azure/storage/common/storage-account-create) to store files as blobs. Use Azure Blob Storage or Azure Data Lake Storage Gen2 (a storage account with a hierarchical namespace) on a standard performance (general-purpose v2) account. Access tiers can be hot, cool, or cold.
++ An [Azure Storage account](/azure/storage/common/storage-account-create). Use Azure Blob Storage or Azure Data Lake Storage Gen2 (storage account with a hierarchical namespace) on a standard performance (general-purpose v2) account. Access tiers can be hot, cool, or cold.
 
-+ An [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) for image vectorization, which requires the Azure AI Vision multimodal embeddings. For regional availability, see the [Azure AI Vision documentation](/azure/ai-services/computer-vision/overview-image-analysis#region-availability).
++ An [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) in East US, West Europe, or North Central US.
 
-+ An [Azure AI Search service](search-create-service-portal.md) for indexing and queries. Your service can be on any tier, but it must be in the [same region as your Azure AI multi-service account](search-create-service-portal.md#regions-with-the-most-overlap).
++ An [Azure AI Search service](search-create-service-portal.md) in the same region as your Azure AI multi-service account.
 
-  + The pricing tier determines how many blobs you can index. We used the free tier to create this quickstart and limited the content to one PDF.
++ An [Azure OpenAI resource](/azure/ai-services/openai/how-to/create-resource).
 
 + Familiarity with the wizard. See [Import data wizards in the Azure portal](search-import-data-portal.md).
 
@@ -37,106 +37,204 @@ All of the preceding resources must have public access enabled so that the Azure
 
 If private endpoints are already present and you can't disable them, the alternative is to run the respective end-to-end flow from a script or program on a virtual machine. The virtual machine must be on the same virtual network as the private endpoint. [Here's a Python code sample](https://github.com/Azure/azure-search-vector-samples/tree/main/demo-python/code/integrated-vectorization) for integrated vectorization. The same [GitHub repo](https://github.com/Azure/azure-search-vector-samples/tree/main) has samples in other programming languages.
 
-### Role-based access
+### Check for space
 
-A free search service supports role-based access control on connections to Azure AI Search, but it doesn't support managed identities on outbound connections to Azure Storage or Azure AI Vision. This level of support means you must use key-based authentication on connections between a free search service and other Azure services. For more secure connections:
+If you're starting with the free service, you're limited to three indexes, three data sources, three skillsets, and three indexers. Make sure you have room for extra items before you begin. This quickstart creates one of each object.
 
-+ Use the Basic tier or higher.
+## Configure access
 
-+ [Configure a system-assigned managed identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity) and role assignments to admit requests from Azure AI Search on other Azure services.
+Before you begin, make sure you have permissions to access content and operations. We recommend Microsoft Entra ID authentication and role-based access for authorization. You must be an **Owner** or **User Access Administrator** to assign roles. If roles aren't feasible, you can use [key-based authentication](search-security-api-keys.md) instead.
 
-### Check for space
+Configure access to each resource identified in this section.
 
-If you're starting with the free service, you're limited to three indexes, three data sources, three skillsets, and three indexers. Make sure you have room for extra items before you begin. This quickstart creates one of each object.
+### [**Azure AI Search**](#tab/search-perms)
+
+Azure AI Search provides the multimodal pipeline. Configure access for yourself and your search service to read data, run the pipeline, and interact with other Azure resources.
+
+On your Azure AI Search service:
+
+1. [Enable role-based access](search-security-enable-roles.md).
+
+1. [Configure a system-assigned managed identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
+
+1. [Assign the following roles](search-security-rbac.md) to yourself:
+
+   + **Search Service Contributor**
+
+   + **Search Index Data Contributor**
+
+   + **Search Index Data Reader**
+
+### [**Azure Storage**](#tab/storage-perms)
+
+Azure Storage is both the data source for your documents and the destination for extracted images. Your search service requires access to these storage containers, which you create in the next section of this quickstart.
+
+On your Azure Storage account:
+
++ Assign **Storage Blob Data Contributor** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
+
+### [**Azure AI services**](#tab/ai-services-perms)
+
+An Azure AI multi-service account provides multiple Azure AI services, including [Azure AI Document Intelligence](/azure/ai-services/document-intelligence/overview) for content extraction and semantic chunking. Your search service requires access to call the [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md).
+
+On your Azure AI multi-service account:
+
++ Assign **Cognitive Services User** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
+
+### [**Azure OpenAI**](#tab/openai-perms)
+
+Azure OpenAI provides large language models (LLMs) for image verbalization and embedding models for text and image vectorization. Your search service requires access to call the [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) and [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md).
+
+On your Azure OpenAI resource:
+
++ Assign **Cognitive Services OpenAI User** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
+
+---
 
 ## Prepare sample data
 
 This quickstart uses a sample multimodal PDF, but you can also use your own files. If you're on a free search service, use fewer than 20 files to stay within the free quota for enrichment processing.
 
 To prepare the sample data for this quickstart:
 
-1. Sign in to the [Azure portal](https://portal.azure.com/) and go to your Azure Storage account.
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your Azure Storage account.
 
 1. From the left pane, select **Data storage** > **Containers**.
 
 1. Create a container, and then upload the [sample PDF](https://github.com/Azure-Samples/azure-search-sample-data/blob/main/sustainable-ai-pdf/Accelerating-Sustainability-with-AI-2025.pdf) to the container.
 
 1. Create another container to store images extracted from the PDF.
 
-## Start the wizard
+## Deploy models
+
+The wizard requires an LLM to verbalize images and an embedding model to generate vector representations of text and verbalized text content. Both models are available through Azure OpenAI.
+
+To deploy the models for this quickstart:
+
+1. Sign in to the [Azure AI Foundry portal](https://ai.azure.com) and select your Azure OpenAI resource.
+
+1. From the left pane, select **Model catalog**.
 
-If your Azure AI Search service and Azure AI multi-service account are in the [same region](/azure/ai-services/computer-vision/how-to/image-retrieval) and tenant, and if your Azure Storage blob container has the default configuration, you're ready to proceed.
+1. Deploy one of the following LLMs:
+
+   + gpt-4o
+
+   + gpt-4o-mini
+
+1. Deploy one of the following embedding models:
+
+   + text-embedding-ada-002
+
+   + text-embedding-3-small
+
+   + text-embedding-3-large
+
+## Start the wizard
 
 To start the wizard for multimodal search:
 
-1. Sign in to the [Azure portal](https://portal.azure.com/) and go to your Azure AI Search service.
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your Azure AI Search service.
 
 1. On the **Overview** page, select **Import and vectorize data**.
 
    :::image type="content" source="media/search-get-started-portal-import-vectors/command-bar.png" alt-text="Screenshot of the command to open the wizard for importing and vectorizing data.":::
 
 1. Select your data source: **Azure Blob Storage** or **Azure Data Lake Storage Gen2**.
 
-1. Select the **Multimodal RAG** tile.
+   :::image type="content" source="media/search-get-started-portal-images/select-data-source.png" alt-text="Screenshot of the options for selecting a data source in the wizard." border="true" lightbox="media/search-get-started-portal-images/select-data-source.png":::
+
+1. Select **Multimodal RAG**.
 
    :::image type="content" source="media/search-get-started-portal-images/wizard-scenarios-multimodal-rag.png" alt-text="Screenshot of the Multimodal RAG tile in the wizard." border="true" lightbox="media/search-get-started-portal-images/wizard-scenarios-multimodal-rag.png":::
 
 ## Connect to your data
 
-Azure AI Search requires a connection to the data source that contains the sample data. In this case, the data source is an Azure Storage account.
+Azure AI Search requires a connection to a data source for content ingestion and indexing. In this case, the data source is your Azure Storage account.
+
+To connect to your data:
 
-To connect to your data source:
+1. On the **Connect to your data** page, specify your Azure subscription.
 
-1. On the **Connect to your data** page, specify the Azure subscription.
+1. Select the storage account and container to which you uploaded the sample data.
 
-1. Select the storage account and container that provide the data. Use the default values for the remaining boxes.
+1. Select the **Authenticate using managed identity** checkbox. Leave the identity type as **System-assigned**.
 
    :::image type="content" source="media/search-get-started-portal-images/connect-to-your-data.png" alt-text="Screenshot of the wizard page for setting up a data connection." border="true" lightbox="media/search-get-started-portal-images/connect-to-your-data.png":::
 
 1. Select **Next**.
 
 ## Extract your content
 
-The next step is to select a method for document cracking and chunking. The default method uses the [Document Extraction skill](cognitive-search-skill-document-extraction.md) to extract content and metadata from documents, which includes generating normalized images. The [Text Split skill](cognitive-search-skill-textsplit.md) is then used to split the extracted content into pages.
+The next step is to select a method for document cracking and chunking.
+
+Your Azure AI multi-service account provides access to the [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md), which extracts page numbers, bounding polygons, and other location metadata from both text and images. The Document Layout skill also breaks documents into smaller, more manageable chunks.
+
+To use the Document Layout skill:
 
-To use the default extraction method:
+1. On the **Content extraction** page, select **AI Document Intelligence**.
 
-1. On the **Content extraction** page, select the **Default** tile.
+1. Specify your Azure subscription and Azure AI multi-service account.
+
+1. For the authentication type, select **System assigned identity**.
+
+1. Select the checkbox that acknowledges the billing effects of using these resources.
 
    :::image type="content" source="media/search-get-started-portal-images/extract-your-content.png" alt-text="Screenshot of the wizard page for selecting a content extraction method." border="true" lightbox="media/search-get-started-portal-images/extract-your-content.png":::
 
 1. Select **Next**.
 
 ## Embed your content
 
-If the raw content includes text, or if the Document Extraction skill produces text, the wizard calls the [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) to vectorize the text. The same embedding skill is used to generate vector representations of images.
+During this step, the wizard calls two skills to generate descriptive text for images (image verbalization) and vector embeddings for text and images.
+
+For image verbalization, the [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) uses the LLM you deployed to analyze each extracted image and produce a natural-language description.
 
-The wizard also calls the [Shaper skill](cognitive-search-skill-shaper.md) to enrich the output with metadata, such as page numbers. This metadata is useful for associating vectorized content with its original context in the document.
+For text and image embeddings, the [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) uses the embedding model you deployed to convert the text chunks and verbalized descriptions into high-dimensional vectors. These vectors enable similarity and hybrid retrieval.
 
-To generate embeddings for your text and images:
+To use the GenAI Prompt skill and Azure OpenAI Embedding skill:
 
-1. On the **Content embedding** page, select the **Multimodal Embedding** tile.
+1. On the **Content embedding** page, select **Image Verbalization**.
 
-1. Select **AI Vision vectorization** for the embedding kind. If it's unavailable, make sure your Azure AI Search service and Azure AI multi-service account are both in a region that [supports the AI Vision multimodal APIs](/azure/ai-services/computer-vision/how-to/image-retrieval).
+   :::image type="content" source="media/search-get-started-portal-images/image-verbalization-tile.png" alt-text="Screenshot of the Image Verbalization tile in the wizard." border="true" lightbox="media/search-get-started-portal-images/image-verbalization-tile.png":::
 
-1. Specify the subscription, multi-service account, and authentication type.
+1. On the **Image Verbalization** tab:
 
-1. Select the checkbox that acknowledges the billing effects of using this resource.
+   1. For the kind, select **Azure OpenAI**.
 
-   :::image type="content" source="media/search-get-started-portal-images/vectorize-your-text.png" alt-text="Screenshot of the wizard page for vectorizing text and images." border="true" lightbox="media/search-get-started-portal-images/vectorize-your-text.png":::
+   1. Specify your Azure subscription, Azure OpenAI resource, and LLM deployment.
+
+   1. For the authentication type, select **System assigned identity**.
+
+   1. Select the checkbox that acknowledges the billing effects of using these resources.
+
+      :::image type="content" source="media/search-get-started-portal-images/image-verbalization-tab.png" alt-text="Screenshot of the wizard page for verbalizing images." border="true" lightbox="media/search-get-started-portal-images/image-verbalization-tab.png":::
+
+1. On the **Text Vectorization** tab:
+
+   1. For the kind, select **Azure OpenAI**.
+
+   1. Specify your Azure subscription, Azure OpenAI resource, and embedding model deployment.
+
+   1. For the authentication type, select **System assigned identity**.
+
+   1. Select the checkbox that acknowledges the billing effects of using these resources.
+
+      :::image type="content" source="media/search-get-started-portal-images/text-vectorization-tab.png" alt-text="Screenshot of the wizard page for vectorizing text and images." border="true" lightbox="media/search-get-started-portal-images/text-vectorization-tab.png":::
 
 1. Select **Next**.
 
 ## Store the extracted images
 
-The next step is to save any images extracted from your documents in Azure Storage. In Azure AI Search, this is known as a knowledge store.
+The next step is to send images extracted from your documents to Azure Storage. In Azure AI Search, this secondary storage is known as a [knowledge store](knowledge-store-concept-intro.md).
 
 To store the extracted images:
 
-1. On the **Image output** page, specify the subscription.
+1. On the **Image output** page, specify your Azure subscription.
 
 1. Select the storage account and blob container you created to store the images.
 
+1. Select the **Authenticate using managed identity** checkbox. Leave the identity type as **System-assigned**.
+
    :::image type="content" source="media/search-get-started-portal-images/store-images.png" alt-text="Screenshot of the wizard page for storing the extracting images." border="true" lightbox="media/search-get-started-portal-images/store-images.png":::
 
 1. Select **Next**.
@@ -150,39 +248,42 @@ On the **Advanced settings** page, you can optionally add fields to the index sc
 | content_id | Text and image vectors | String field. Document key for the index. | Searchable, retrievable, sortable, filterable, and facetable. |
 | document_title | Text and image vectors | String field. Human-readable document title, page title, or page number. | Searchable, retrievable, sortable, filterable, and facetable. |
 | text_document_id | Text vectors | String field. Identifies the parent document from which the text chunk originates. | Retrievable and filterable. |
-| image_document_id | Image vectors | String field. Identifies the parent document from which the image chunk originates. | Searchable, retrievable, sortable, filterable, and facetable. |
+| image_document_id | Image vectors | String field. Identifies the parent document from which the image originates. | Searchable, retrievable, sortable, filterable, and facetable. |
 | content_text | Text vectors | String field. Human-readable version of the text chunk. | Searchable, retrievable, sortable, filterable, and facetable. |
-| content_embedding | Image vectors | Collection(Edm.Single). Vector representation of the image chunk. | Searchable and retrievable. |
+| content_embedding | Image vectors | Collection(Edm.Single). Vector representation of the image verbalization. | Searchable and retrievable. |
 | content_path | Text and image vectors | String field. Path to the content in the storage container. | Retrievable, sortable, filterable, and facetable. |
 | locationMetadata | Text and image vectors | Edm.ComplexType. Contains metadata about the content's location. | Varies by field. |
 
 You can't modify the generated fields or their attributes, but you can add fields if your data source provides them. For example, Azure Blob Storage provides a collection of metadata fields.
 
 To add fields to the index schema:
 
-1. Select **Add new**.
+1. Under **Index fields**, select **Preview and edit**.
 
-1. Select a source field from the list of available fields, provide a field name for the index, and accept the default data type or override as needed.
+1. Select **Add field**.
 
-   + Metadata fields are searchable but not retrievable, filterable, facetable, or sortable.
+1. Select a source field from the available fields, enter a field name for the index, and accept (or override) the default data type.
 
-1. Select **Reset** if you want to restore the schema to its original version.
+   > [!NOTE]
+   > Metadata fields are searchable but not retrievable, filterable, facetable, or sortable.
+
+1. If you want to restore the schema to its original version, select **Reset**.
 
 ## Schedule indexing
 
-For data sources where the underlying data is volatile, you can schedule indexing to capture changes at specific intervals or specific dates and times.
+For data sources where the underlying data is volatile, you can [schedule indexing](search-howto-schedule-indexers.md) to capture changes at specific intervals or specific dates and times.
 
 To schedule indexing:
 
-1. On the **Advanced settings** page, under **Schedule indexing**, specify a [run schedule](search-howto-schedule-indexers.md) for the indexer. We recommend **Once** for this quickstart.
+1. On the **Advanced settings** page, under **Schedule indexing**, specify a run schedule for the indexer. We recommend **Once** for this quickstart.
 
    :::image type="content" source="media/search-get-started-portal-images/run-once.png" alt-text="Screenshot of the wizard page for scheduling indexing." border="true" lightbox="media/search-get-started-portal-images/run-once.png":::
 
 1. Select **Next**.
 
 ## Finish the wizard
 
-The final step is to review your configuration and create the objects required for multimodal search. If necessary, return to previous pages in the wizard to adjust your configuration.
+The final step is to review your configuration and create the necessary objects for multimodal search. If necessary, return to the previous pages in the wizard to adjust your configuration.
 
 To finish the wizard:
 
@@ -202,52 +303,49 @@ When the wizard completes the configuration, it creates the following objects:
 
 + A skillset with the following skills:
 
-  + The [Document Extraction skill](cognitive-search-skill-document-extraction.md) extracts both text and images from the documents.
+  + The [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) splits documents into text chunks and extracts images with location data.
 
-  + The [Text Split skill](cognitive-search-skill-textsplit.md) adds data chunking.
+  + The [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) generates natural-language descriptions (verbalizations) of images.
 
-  + The [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) vectorizes text produced by the Document Extraction skill.
+  + The [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) vectorizes each text chunk.
 
-  + The [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) is called again to vectorize images.
+  + The [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) is called again to vectorize each image verbalization.
 
   + The [Shaper skill](cognitive-search-skill-shaper.md) enriches the output with metadata and creates new images with contextual information.
 
-## Check results
+> [!TIP]
+> Wizard-created objects have configurable JSON definitions. To view or modify these definitions, select **Search management** from the left pane, where you can view your indexes, indexers, data sources, and skillsets.
 
-Search Explorer accepts text, images, and vectors as query inputs. For images, Search Explorer vectorizes the image and sends the vector as a query input to the search engine. Image vectorization assumes that your index has a vectorizer definition, which the **Import and vectorize data wizard** creates based on your embedding model inputs.
-
-The following steps assume that you're searching for images. For the other two query types, see [Quickstart: Keyword search](search-get-started-portal.md#query-with-search-explorer) and [Quickstart: Vector search](search-get-started-portal-import-vectors.md#check-results).
-
-To use Search Explorer for image search:
+## Check results
 
-1. Sign in to the [Azure portal](https://portal.azure.com/) and go to your Azure AI Search service.
+This quickstart creates a multimodal index that supports [hybrid search](hybrid-search-overview.md) over both text and verbalized images. However, it doesn't support images as query inputs, which requires integrated vectorization using an embedding skill and an equivalent vectorizer. For more information, see [Query with Search explorer](search-explorer.md).
 
-1. From the left pane, select **Search management** > **Indexes**, and then select the index you created.
+Hybrid search is a combination of full-text queries and vector queries. When you issue a hybrid query, the search engine computes the semantic similarity between your query and the indexed vectors and ranks the results accordingly. For the index created in this quickstart, the results surface content from the `content_text` field that closely aligns with your query.
 
-1. Select the **Search explorer** tab.
+To query your multimodal index:
 
-1. From the **View** menu, select **Image view**.
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your Azure AI Search service.
 
-   :::image type="content" source="media/search-get-started-portal-images/select-image-view.png" alt-text="Screenshot of the command for selecting image view." border="true" lightbox="media/search-get-started-portal-images/select-image-view.png":::
+1. From the left pane, select **Search management** > **Indexes**.
 
-1. Drag or select a [sample PNG](https://github.com/Azure-Samples/azure-search-sample-data/blob/main/sustainable-ai-pdf) from your local folder. The PNGs come directly from the sample PDF used in this quickstart.
+1. Select your index.
 
-1. Select **Search** to run the query.
+1. Select **Query options**, and then select **Hide vector values in search results**. This step makes the results more readable.
 
-   The top match should be the image for which you searched. Because a [vector search](vector-search-overview.md) matches on similar vectors, the search engine returns any document that's sufficiently similar to the query input, up to the `k` number of results. For more advanced queries that include relevance tuning, switch to the JSON view.
+   :::image type="content" source="media/search-get-started-portal-images/query-options.png" alt-text="Screenshot of the Query Options menu in Search Explorer." border="true" lightbox="media/search-get-started-portal-images/query-options.png":::
 
-   :::image type="content" source="media/search-get-started-portal-images/image-search.png" alt-text="Screenshot of the search results for image search." border="true" lightbox="media/search-get-started-portal-images/image-search.png":::
+1. Enter text for which you want to search. Our example uses `energy`.
 
-1. Try other query options to compare search outcomes:
+1. To run the query, select **Search**.
 
-   + (Recommended) Hide vectors for more readable results.
+   :::image type="content" source="media/search-get-started-portal-images/search-button.png" alt-text="Screenshot of the Search button in Search Explorer." border="true" lightbox="media/search-get-started-portal-images/search-button.png":::
 
-   + Select a vector field to query over. The default is text vectors, but you can specify the image vector to exclude text vectors from query execution.
+   The results should include text and image content related to `energy` in your index. Highlights from relevant passages and image verbalizations appear in `@search.captions`, helping you quickly identify matches to your query.
 
 ## Clean up resources
 
 This quickstart uses billable Azure resources. If you no longer need the resources, delete them from your subscription to avoid charges.
 
 ## Next step
 
-This quickstart introduced you to the **Import and vectorize data wizard**, which creates all of the necessary objects for multimodal search. To explore each step in detail, see [Tutorial: Index mixed content using multimodal embeddings and the Document Extraction skill](tutorial-multimodal-indexing-with-embedding-and-doc-extraction.md).
+This quickstart introduced you to the **Import and vectorize data wizard**, which creates all of the necessary objects for multimodal search. To explore each step in detail, see [Tutorial: Index mixed content using image verbalizations and the Document Layout skill](tutorial-multimodal-index-image-verbalization-skill.md).
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "更新图像搜索入门文档"
}
```

### Explanation
此代码差异显示了对文件“搜索入门：门户中的图像搜索” (`search-get-started-portal-image-search.md`) 的重大更新。此次修改包含165处新增内容和67处删除内容，总变更达到232处。这些改动的主要内容包括：

1. **描述性改进**：更新了文件的描述，更加详细地说明了如何在Azure门户中搜索多模态内容，包括生成图像的自然语言描述。
   
2. **流程优化**：对使用向导的步骤进行了改进，使得用户理解和操作更加便捷。增加了有关模型部署和内容提取过程的详细信息，以帮助用户更好地使用这些功能。

3. **教学更新**：更新了使用的各种Azure服务的权利要求和配置步骤，确保用户能够正确设置必需的Azure资源。

4. **结构增强**：引入了一些新节，例如“配置访问”和“模型部署”，以优化文档的结构，使其更具可读性和实用性。

这些修改将为用户提供更加流畅和详细的多模态搜索入门体验，确保在使用Azure AI Search时，用户能够获得全面的指导，同时提升文档的整体质量。

## articles/search/search-get-started-portal-import-vectors.md{#item-7dae77}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: "Quickstart: Vector Search in the Azure Portal"
+title: "Quickstart: Vector Search in the Azure portal"
 titleSuffix: Azure AI Search
 description: Learn how to use a wizard to automate data chunking and vectorization in a search index.
 author: haileytap
@@ -9,12 +9,12 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: quickstart
-ms.date: 05/12/2025
+ms.date: 05/22/2025
 ---
 
 # Quickstart: Vectorize text in the Azure portal
 
-In this quickstart, you use the **Import and vectorize data wizard** in the Azure portal to get started with [integrated vectorization](vector-search-integrated-vectorization.md). The wizard chunks your content and calls an embedding model to vectorize content during indexing and for queries.
+In this quickstart, you use the **Import and vectorize data** wizard in the Azure portal to get started with [integrated vectorization](vector-search-integrated-vectorization.md). The wizard chunks your content and calls an embedding model to vectorize content during indexing and for queries.
 
 The sample data for this quickstart consists of text-based PDFs, but you can also use images and follow this quickstart to vectorize them.
 
@@ -47,7 +47,7 @@ For integrated vectorization, you must use one of the following embedding models
 | Provider | Supported models |
 |--|--|
 | [Azure OpenAI in Azure AI Foundry Models](/azure/ai-services/openai/how-to/create-resource) <sup>1, 2</sup> | text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large |
-| [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) <sup>3</sup> | For text and images: [Azure AI Vision multimodal](/azure/ai-services/computer-vision/how-to/image-retrieval) <sup>4</sup></li> |
+| [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> | For text and images: [Azure AI Vision multimodal](/azure/ai-services/computer-vision/how-to/image-retrieval) <sup>4</sup></li> |
 | [Azure AI Foundry model catalog](/azure/ai-foundry/what-is-azure-ai-foundry) | For text:<br>Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br><br>For images:<br>Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant |
 
 <sup>1</sup> The endpoint of your Azure OpenAI resource must have a [custom subdomain](/azure/ai-services/cognitive-services-custom-subdomains), such as `https://my-unique-name.openai.azure.com`. If you created your resource in the [Azure portal](https://portal.azure.com/), this subdomain was automatically generated during resource setup.
@@ -74,6 +74,14 @@ To configure the recommended role-based access:
 
 1. On your search service, [enable roles](search-security-enable-roles.md) and [configure a system-assigned managed identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
 
+1. [Assign the following roles](search-security-rbac.md) to yourself:
+
+   + **Search Service Contributor**
+
+   + **Search Index Data Contributor**
+
+   + **Search Index Data Reader**
+
 1. On your data source platform and embedding model provider, create role assignments that allow your search service to access data and models. See [Prepare sample data](#prepare-sample-data) and [Prepare embedding models](#prepare-embedding-model).
 
 > [!NOTE]
@@ -101,7 +109,7 @@ This section points you to the content that works for this quickstart. Before yo
 
    1. Select **Add** > **Add role assignment**.
 
-   1. Under **Job function roles**, select **[Storage Blob Data Reader](search-howto-managed-identities-data-sources.md#assign-a-role)**, and then select **Next**.
+   1. Under **Job function roles**, select **Storage Blob Data Reader**, and then select **Next**.
 
    1. Under **Members**, select **Managed identity**, and then select **Select members**.
 
@@ -127,7 +135,7 @@ This section points you to the content that works for this quickstart. Before yo
 
    1. Select **Add** > **Add role assignment**.
 
-   1. Under **Job function roles**, select **[Storage Blob Data Reader](search-howto-managed-identities-data-sources.md#assign-a-role)**, and then select **Next**.
+   1. Under **Job function roles**, select **Storage Blob Data Reader**, and then select **Next**.
 
    1. Under **Members**, select **Managed identity**, and then select **Select members**.
 
@@ -188,7 +196,7 @@ The wizard supports text-embedding-ada-002, text-embedding-3-large, and text-emb
 
    1. Select **Add** > **Add role assignment**.
 
-   1. Under **Job function roles**, select **[Cognitive Services OpenAI User](/azure/ai-services/openai/how-to/role-based-access-control#azure-openai-roles)**, and then select **Next**.
+   1. Under **Job function roles**, select **Cognitive Services OpenAI User**, and then select **Next**.
 
    1. Under **Members**, select **Managed identity**, and then select **Select members**.
 
@@ -257,7 +265,7 @@ For the model catalog, you should have an [Azure AI Foundry project](/azure/ai-f
 
 ## Start the wizard
 
-1. Sign in to the [Azure portal](https://portal.azure.com/) and go to your Azure AI Search service.
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your Azure AI Search service.
 
 1. On the **Overview** page, select **Import and vectorize data**.
 
@@ -271,7 +279,7 @@ For the model catalog, you should have an [Azure AI Foundry project](/azure/ai-f
 
    + OneLake
 
-1. Select the **RAG** tile.
+1. Select **RAG**.
 
    :::image type="content" source="media/search-get-started-portal-import-vectors/wizard-scenarios-rag.png" alt-text="Screenshot of the RAG tile in the wizard." border="true" lightbox="media/search-get-started-portal-import-vectors/wizard-scenarios-rag.png":::
 
@@ -371,7 +379,7 @@ In this step, you specify an embedding model to vectorize chunked data. Chunking
 
    + Azure AI Foundry model catalog
 
-   + An Azure AI Vision multimodal resource in the same region as Azure AI Search. If there's no [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) in the same region, this option isn't available.
+   + An Azure AI Vision multimodal resource in the same region as Azure AI Search. If there's no [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) in the same region, this option isn't available.
 
 1. Specify the Azure subscription.
 
@@ -447,11 +455,12 @@ You can't modify the generated fields or their attributes, but you can add new f
 
 1. Select **Add field**.
 
-1. Select a source field from the list of available fields, provide a field name for the index, and accept the default data type or override as needed.
+1. Select a source field from the available fields, enter a field name for the index, and accept (or override) the default data type.
 
-   + Metadata fields are searchable but not retrievable, filterable, facetable, or sortable.
+   > [!NOTE]
+   > Metadata fields are searchable but not retrievable, filterable, facetable, or sortable.
 
-1. Select **Reset** if you want to restore the schema to its original version.
+1. If you want to restore the schema to its original version, select **Reset**.
 
 ## Schedule indexing
 
@@ -473,13 +482,16 @@ When the wizard completes the configuration, it creates the following objects:
 
 + An indexer with field mappings and output field mappings (if applicable).
 
+> [!TIP]
+> Wizard-created objects have configurable JSON definitions. To view or modify these definitions, select **Search management** from the left pane, where you can view your indexes, indexers, data sources, and skillsets.
+
 ## Check results
 
 Search Explorer accepts text strings as input and then vectorizes the text for vector query execution.
 
-1. In the Azure portal, go to **Search Management** > **Indexes**, and then select the index that you created.
+1. In the Azure portal, go to **Search Management** > **Indexes**, and then select your index.
 
-1. Select **Query options** and hide vector values in search results. This step makes your search results easier to read.
+1. Select **Query options**, and then select **Hide vector values in search results**. This step makes the results more readable.
 
    :::image type="content" source="media/search-get-started-portal-import-vectors/query-options.png" alt-text="Screenshot of the button for query options.":::
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新导入向量文档"
}
```

### Explanation
此代码差异显示了对“导入向量”入门指南 (`search-get-started-portal-import-vectors.md`) 的更新。该文档的修改包括27处新增内容和15处删除内容，共计42处更改。主要更新内容如下：

1. **标题和格式**：将标题中的“门户”更改为小写，并统一了其他标题格式，使文档的格式更加一致。

2. **时间戳更新**：文档中的日期已更新为2025年5月22日。

3. **内容描述改进**：增强了对使用“导入和向量化数据向导”的描述，明确了可以处理哪些类型的样本数据。

4. **角色和权限**：添加了新的步骤以指导用户如何配置角色和访问权限，确保用户能够正确设置对数据和模型的访问。

5. **具体细节更新**：在特定步骤中，更新了与角色绑定相关的术语和说明，确保用户在执行步骤时能够获得清晰的指导。

6. **新内容提示**：在完成向导的步骤后，增加了提示，指导用户如何查看和修改生成的对象的JSON定义，提升了用户体验。

这些更改旨在提高文档的可读性和易用性，确保用户在使用向导进行向量化时能够获得更全面的支持和指导。


