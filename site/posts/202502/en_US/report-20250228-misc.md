---
date: '2025-02-28'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:42a07e4...MicrosoftDocs:63a40ca
summary: |-
  The recent code updates in the AI Studio documentation focus on enhancing user experience by refining hyperlinks, improving document clarity, and updating content to connect users more effectively with Azure AI Foundry resources. Key features include the addition of hyperlinks for easier access to the Azure AI Foundry portal and SDK, as well as the introduction of a new model called "Phi-4-multimodal-instruct" in the model catalogs.

  There are no breaking changes identified in these updates; all changes are intended to enhance accessibility and clarity while maintaining the original functionality of the documents. Additionally, updates have been made to document review information and content organization to improve readability and accuracy.

  Overall, these modifications emphasize making the Azure AI Studio documentation more user-friendly, allowing users to efficiently navigate resources and utilize Azure's latest AI capabilities.
title: '[en_US] Diff Insight Report - misc'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:42a07e4...MicrosoftDocs:63a40ca){target="_blank"}

# Highlights

The recent code updates across multiple articles in the AI Studio documentation primarily involve minor enhancements that focus on refining hyperlinks, improving document clarity, and updating content to better connect users with Azure AI Foundry resources. These changes ensure that users can easily navigate to pertinent information and leverage the latest Azure AI capabilities.

## New features
- Addition of hyperlinks to streamline access to the Azure AI Foundry portal and SDK.
- Inclusion of a new model, "Phi-4-multimodal-instruct," in model catalogs and region availability documentation.

## Breaking changes
- None identified in these minor updates; all adjustments are designed to enhance accessibility and clarity without altering fundamental document functionality or user guidance.

## Other updates
- Updates to document review information and content restructuring to improve readability and align with current information.
- Content revisions in articles to include updated dates and reviewer details, ensuring accurate metadata.
  
# Insights

### Overview

The recent documentation enhancements address minor but crucial refinements that aim to improve user interaction with the Azure AI Studio resources. These updates mainly introduce hyperlinks to allow seamless navigation to the Azure AI Foundry portal and SDK, a critical step in aligning document content with user expectations for quick resource access.

### Detailed Explanation

One of the most repeated and relevant changes is the inclusion of hyperlinks within the various documentation articles. With the Azure AI Foundry SDK and the main portal as central elements in these documents, providing direct links simplifies the process for users who need to access these resources quickly. This is especially crucial given the nature of technical documentation where users seek immediate guidance and accessible tools.

Moreover, the introduction of the "Phi-4-multimodal-instruct" model across several documents demonstrates Azure's commitment to expanding its AI capabilities. By providing comprehensive information on new models, Azure ensures users are well-equipped to leverage these models effectively. This includes detailing out deployment capabilities and regions where these models can be utilized, thus expanding the functionality and versatility of their AI solutions.

The updates also entail revisions to enhance document clarity, as seen with content restructuring in guides like the SDK overview. The refining of section headers and other textual elements serves to better organize the documents and thus improves user comprehension. This restructuring aligns with best practices for technical writing, which emphasize clarity, coherence, and ease of navigation across complex documentation topics.

Overall, these updates reflect a focus on enhancing usability and accessibility within the Azure AI Studio documentation. By continuously updating content with direct access points and improving clarity, Azure allows users to more effectively harness their platform's capabilities, fostering a seamless user experience. These incremental improvements collectively bolster the coherence and utility of Azure's documentation library.

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [azure-openai-in-ai-studio.md](#item-07639b) | minor update | Update link to Azure AI Foundry portal in documentation | modified | 1 | 1 | 2 | 
| [connections.md](#item-01b26a) | minor update | Revise connections documentation with updated content and review information | modified | 9 | 9 | 18 | 
| [content-filtering.md](#item-91b372) | minor update | Update link for Azure OpenAI Code of Conduct in content filtering documentation | modified | 1 | 1 | 2 | 
| [fine-tuning-overview.md](#item-31b07b) | minor update | Enhancements to fine-tuning overview documentation | modified | 25 | 21 | 46 | 
| [faq.yml](#item-e7baa2) | minor update | Update FAQ summary to include Azure AI Foundry reference | modified | 1 | 1 | 2 | 
| [create-manage-compute-session.md](#item-6ed743) | minor update | Add hyperlink to Azure AI Foundry in compute session documentation | modified | 1 | 1 | 2 | 
| [create-projects.md](#item-cb10b3) | minor update | Add hyperlink to Azure AI Foundry in project creation guide | modified | 1 | 1 | 2 | 
| [deploy-models-phi-4.md](#item-c40212) | minor update | Add details and references for the Phi-4-multimodal-instruct model | modified | 63 | 15 | 78 | 
| [ai-template-get-started.md](#item-d71b59) | minor update | Revise AI template get started guide with updated instructions and content | modified | 31 | 30 | 61 | 
| [create-hub-project-sdk.md](#item-8c3e99) | minor update | Update link to Azure AI Foundry in the hub project SDK guide | modified | 1 | 1 | 2 | 
| [sdk-overview.md](#item-d3ab19) | minor update | Enhance SDK overview with updated content and links | modified | 31 | 29 | 60 | 
| [vscode.md](#item-24bd97) | minor update | Add hyperlink to Azure AI Foundry in VS Code development guide | modified | 1 | 1 | 2 | 
| [model-catalog-overview.md](#item-278001) | minor update | Add new model to Microsoft Phi family in the model catalog | modified | 1 | 1 | 2 | 
| [region-availability-maas.md](#item-35d79c) | minor update | Add new model to the Phi family in region availability document | modified | 1 | 1 | 2 | 
| [get-started-code.md](#item-8a5082) | minor update | Update hyperlink for Azure AI Foundry SDK in quickstart guide | modified | 1 | 1 | 2 | 
| [copilot-sdk-build-rag.md](#item-b77dba) | minor update | Include hyperlink for Azure AI Foundry SDK in RAG tutorial | modified | 1 | 1 | 2 | 
| [copilot-sdk-create-resources.md](#item-552960) | minor update | Add hyperlink for Azure AI Foundry SDK in resource creation tutorial | modified | 1 | 1 | 2 | 
| [copilot-sdk-evaluate.md](#item-bb5754) | minor update | Add hyperlink for Azure AI Foundry SDK in evaluation tutorial | modified | 1 | 1 | 2 | 
| [screen-reader.md](#item-4dc029) | minor update | Add hyperlink for Azure AI Foundry in screen reader article | modified | 1 | 1 | 2 | 


# Modified Contents
## articles/ai-studio/azure-openai-in-ai-studio.md{#item-07639b}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ ms.custom: ignite-2023, build-2024, ignite-2024
 
 # What is Azure OpenAI in Azure AI Foundry portal?
 
-Azure OpenAI Service provides REST API access to OpenAI's powerful language models. Azure OpenAI Studio was previously where you went to access and work with the Azure OpenAI Service. This studio is now integrated into Azure AI Foundry portal. 
+Azure OpenAI Service provides REST API access to OpenAI's powerful language models. Azure OpenAI Studio was previously where you went to access and work with the Azure OpenAI Service. This studio is now integrated into [Azure AI Foundry portal](https://ai.azure.com). 
 
 ## Access Azure OpenAI Service in Azure AI Foundry portal
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link to Azure AI Foundry portal in documentation"
}
```

### Explanation
The code diff reflects a minor update to the document titled "azure-openai-in-ai-studio.md". In this modification, one line of content was added and one line was deleted, resulting in a total of two changes. The previous text describing the Azure OpenAI Studio has been updated to include a hyperlink to the Azure AI Foundry portal. This enhancement provides users with direct access to the portal, improving the documentation's usability and relevance. The change signifies a small but useful adjustment to ensure that readers can easily navigate to the appropriate resources.

## articles/ai-studio/concepts/connections.md{#item-01b26a}

<details>
<summary>Diff</summary>
````diff
@@ -9,8 +9,8 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 11/21/2024
-ms.reviewer: sgilley
+ms.date: 02/21/2025
+ms.reviewer: meerakurup
 ms.author: sgilley
 author: sdgilley
 ---
@@ -31,30 +31,30 @@ As another example, you can [create a connection](../how-to/connections-add.md)
 
 ## Connections to non-Microsoft services
 
-Azure AI Foundry supports connections to non-Microsoft services, including the following:
-- The [API key connection](../how-to/connections-add.md) handles authentication to your specified target on an individual basis. This is the most common non-Microsoft connection type.
-- The [custom connection](../how-to/connections-add.md) allows you to securely store and access keys while storing related properties, such as targets and versions. Custom connections are useful when you have many targets that or cases where you wouldn't need a credential to access. LangChain scenarios are a good example where you would use custom service connections. Custom connections don't manage authentication, so you'll have to manage authentication on your own.
+Azure AI Foundry supports connections to non-Microsoft services, including:
+- The [API key connection](../how-to/connections-add.md) handles authentication to your specified target on an individual basis. API key is the most common non-Microsoft connection type.
+- The [custom connection](../how-to/connections-add.md) allows you to securely store and access keys while storing related properties, such as targets and versions. Custom connections are useful when you have many targets that or cases where you wouldn't need a credential to access. LangChain scenarios are a good example where you would use custom service connections. Custom connections don't manage authentication, so you have to manage authentication on your own.
 
 ## Connections to datastores
 
 > [!IMPORTANT]
-> Data connections cannot be shared across projects. They are created exclusively in the context of one project. 
+> Data connections can't be shared across projects. They're created exclusively in the context of one project. 
 
 Creating a data connection allows you to access external data without copying it to your project. Instead, the connection provides a reference to the data source.
 
 A data connection offers these benefits:
 
 - A common, easy-to-use API that interacts with different storage types including Microsoft OneLake, Azure Blob, and Azure Data Lake Gen2.
 - Easier discovery of useful connections in team operations.
-- For credential-based access (service principal/SAS/key), Azure AI Foundry connection secures credential information. This way, you won't need to place that information in your scripts.
+- Credential-based access (service principal/SAS/key). Azure AI Foundry connection secures credential information so you don't need to place that information in your scripts.
 
 When you create a connection with an existing Azure storage account, you can choose between two different authentication methods:
 
 - **Credential-based**: Authenticate data access with a service principal, shared access signature (SAS) token, or account key. Users with *Reader* project permissions can access the credentials.
 - **Identity-based**: Use your Microsoft Entra ID or managed identity to authenticate data access.
 
     > [!TIP]
-    > When using an identity-based connection, Azure role-based access control (Azure RBAC) is used to determine who can access the connection. You must assign the correct Azure RBAC roles to your developers before they can use the connection. For more information, see [Scenario: Connections using Microsoft Entra ID](rbac-ai-studio.md#scenario-connections-using-microsoft-entra-id-authentication).
+    > When you use an identity-based connection, Azure role-based access control (Azure RBAC) determines who can access the connection. You must assign the correct Azure RBAC roles to your developers before they can use the connection. For more information, see [Scenario: Connections using Microsoft Entra ID](rbac-ai-studio.md#scenario-connections-using-microsoft-entra-id-authentication).
 
 
 The following table shows the supported Azure cloud-based storage services and authentication methods:
@@ -82,7 +82,7 @@ A Uniform Resource Identifier (URI) represents a storage location on your local
 
 ## Key vaults and secrets
 
-Connections allow you to securely store credentials, authenticate access, and consume data and information.  Secrets associated with connections are securely persisted in the corresponding Azure Key Vault, adhering to robust security and compliance standards. As an administrator, you can audit both shared and project-scoped connections on a hub level (link to connection rbac). 
+Connections allow you to securely store credentials, authenticate access, and consume data and information.  Secrets associated with connections are securely persisted in the corresponding Azure Key Vault, adhering to robust security and compliance standards. As an administrator, you can audit both shared and project-scoped connections on a hub level. 
 
 Azure connections serve as key vault proxies, and interactions with connections are direct interactions with an Azure key vault. Azure AI Foundry connections store API keys securely, as secrets, in a key vault. The key vault [Azure role-based access control (Azure RBAC)](./rbac-ai-studio.md) controls access to these connection resources. A connection references the credentials from the key vault storage location for further use. You won't need to directly deal with the credentials after they're stored in the hub's key vault. You have the option to store the credentials in the YAML file. A CLI command or SDK can override them. We recommend that you avoid credential storage in a YAML file, because a security breach could lead to a credential leak.  
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise connections documentation with updated content and review information"
}
```

### Explanation
The code diff indicates a minor update to the "connections.md" file, which is part of the AI Studio documentation. The changes involve 9 additions and 9 deletions, totaling 18 overall changes. Key updates include revisions to the text for improved clarity and consistency. 

Notably, the date has been updated from November 21, 2024, to February 21, 2025, and the reviewer has changed from "sgilley" to "meerakurup". The text has been refined to enhance readability, such as changing "can't" from "cannot" in certain sentences, and clarifying descriptions around non-Microsoft service connections. Additionally, some sentences were restructured to streamline the information presented, ensuring that essential concepts—like credential-based access and the use of Azure Key Vault—are effectively conveyed. 

Overall, this modification serves to improve both the accuracy and clarity of the documentation, making it more helpful for users seeking to understand connections within Azure AI Foundry.

## articles/ai-studio/concepts/content-filtering.md{#item-91b372}

<details>
<summary>Diff</summary>
````diff
@@ -86,7 +86,7 @@ The configurability feature allows customers to adjust the settings, separately
 
 <sup>1</sup> For Azure OpenAI models, only customers who have been approved for modified content filtering have full content filtering control, including configuring content filters at severity level high only or turning off content filters. Apply for modified content filters via these forms: [Azure OpenAI Limited Access Review: Modified Content Filters](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUMlBQNkZMR0lFRldORTdVQzQ0TEI5Q1ExOSQlQCN0PWcu), and [Modified Abuse Monitoring](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOE9MUTFMUlpBNk5IQlZWWkcyUEpWWEhGOCQlQCN0PWcu).
 
-Customers are responsible for ensuring that applications integrating Azure OpenAI comply with the [Code of Conduct](/legal/cognitive-services/openai/code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext). 
+Customers are responsible for ensuring that applications integrating Azure OpenAI comply with the [Code of Conduct](/legal/ai-code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext). 
 
 
 ## Next steps
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link for Azure OpenAI Code of Conduct in content filtering documentation"
}
```

### Explanation
The code diff indicates a minor update to the "content-filtering.md" document in the AI Studio documentation. The changes consist of 1 addition and 1 deletion, resulting in a total of 2 modifications. 

The primary change involves updating the hyperlink for the Azure OpenAI Code of Conduct. The previous link has been modified to direct users to the new URL: `/legal/ai-code-of-conduct`. This adjustment is significant as it ensures that users can access the most current version of the Code of Conduct, which is crucial for compliance when integrating Azure OpenAI services into their applications.

Overall, this update enhances the accuracy of the documentation, providing users with clear and relevant access to important compliance information.

## articles/ai-studio/concepts/fine-tuning-overview.md{#item-31b07b}

<details>
<summary>Diff</summary>
````diff
@@ -1,24 +1,25 @@
 ---
 title: Fine-tuning in Azure AI Foundry portal
 titleSuffix: Azure AI Foundry
-description: This article introduces fine-tuning of models in Azure AI Foundry portal.
+description: This article explains what fine-tuning is and under what circumstances you should consider doing it.
 manager: scottpolly
 ms.service: azure-ai-foundry
 ms.custom:
   - build-2024
   - code01
-ms.topic: conceptual
-ms.date: 10/31/2024
-ms.reviewer: sgilley
+ms.topic: concept-article
+ms.date: 02/21/2025
+ms.reviewer: keli19
 ms.author: sgilley
 author: sdgilley
+#customer intent: As a developer, I want to learn what it means to fine-tune a model.
 ---
 
 # Fine-tune models with Azure AI Foundry
 
-[!INCLUDE [feature-preview](../includes/feature-preview.md)]
+Fine-tuning customizes a pretrained AI model with additional training on a specific task or dataset to improve performance, add new skills, or enhance accuracy. The result is a new, optimized GenAI model based on the provided examples.
 
-Fine-tuning refers to customizing a pre-trained generative AI model with additional training on a specific task or new dataset for enhanced performance, new skills, or improved accuracy. The result is a new, custom GenAI model that's optimized based on the provided examples.
+[!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
 Consider fine-tuning GenAI models to:
 - Scale and adapt to specific enterprise needs
@@ -27,19 +28,20 @@ Consider fine-tuning GenAI models to:
 - Save time and resources with faster and more precise results
 - Get more relevant and context-aware outcomes as models are fine-tuned for specific use cases
 
-[Azure AI Foundry](https://ai.azure.com) offers several models across model providers enabling you to get access to the latest and greatest in the market. You can discover supported models for fine-tuning through our model catalog by using the **Fine-tuning tasks** filter and selecting the model card to learn detailed information about each model. Specific models may be subjected to regional constraints, [view this list for more details](#supported-models-for-fine-tuning). 
+[Azure AI Foundry](https://ai.azure.com) offers several models across model providers enabling you to get access to the latest and greatest in the market. You can discover supported models for fine-tuning through our model catalog by using the **Fine-tuning tasks** filter and selecting the model card to learn detailed information about each model. Specific models might be subjected to regional constraints. [View this list for more details](#supported-models-for-fine-tuning). 
 
 :::image type="content" source="../media/concepts/model-catalog-fine-tuning.png" alt-text="Screenshot of Azure AI Foundry model catalog and filtering by Fine-tuning tasks." lightbox="../media/concepts/model-catalog-fine-tuning.png":::
 
-This article will walk you through use-cases for fine-tuning and how this can help you in your GenAI journey.
+This article walks you through use-cases for fine-tuning and how it helps you in your GenAI journey.
 
 ## Getting started with fine-tuning
 
 When starting out on your generative AI journey, we recommend you begin with prompt engineering and RAG to familiarize yourself with base models and its capabilities. 
 - [Prompt engineering](../../ai-services/openai/concepts/prompt-engineering.md) is a technique that involves designing prompts using tone and style details, example responses, and intent mapping for natural language processing models. This process improves accuracy and relevancy in responses, to optimize the performance of the model.
 - [Retrieval-augmented generation (RAG)](../concepts/retrieval-augmented-generation.md) improves LLM performance by retrieving data from external sources and incorporating it into a prompt. RAG can help businesses achieve customized solutions while maintaining data relevance and optimizing costs.
 
-As you get comfortable and begin building your solution, it's important to understand where prompt engineering falls short and that will help you realize if you should try fine-tuning. 
+As you get comfortable and begin building your solution, it's important to understand where prompt engineering falls short and when you should try fine-tuning.
+
 - Is the base model failing on edge cases or exceptions? 
 - Is the base model not consistently providing output in the right format?
 - Is it difficult to fit enough examples in the context window to steer the model?
@@ -53,26 +55,29 @@ _A customer wants to use GPT-3.5 Turbo to turn natural language questions into q
 
 ### Use cases
 
-Base models are already pre-trained on vast amounts of data and most times you'll add instructions and examples to the prompt to get the quality responses that you're looking for - this process is called "few-shot learning". Fine-tuning allows you to train a model with many more examples that you can tailor to meet your specific use-case, thus improving on few-shot learning. This can reduce the number of tokens in the prompt leading to potential cost savings and requests with lower latency. 
+Base models are already pretrained on vast amounts of data. Most times you add instructions and examples to the prompt to get the quality responses that you're looking for - this process is called "few-shot learning." Fine-tuning allows you to train a model with many more examples that you can tailor to meet your specific use-case, thus improving on few-shot learning. Fine-tuning can reduce the number of tokens in the prompt leading to potential cost savings and requests with lower latency. 
+
+Turning natural language into a query language is just one use case where you can  "_show not tell_" the model how to behave. Here are some other use cases:
 
-Turning natural language into a query language is just one use case where you can  _show not tell_ the model how to behave. Here are some additional use cases:
 - Improve the model's handling of retrieved data
 - Steer model to output content in a specific style, tone, or format
 - Improve the accuracy when you look up information
 - Reduce the length of your prompt
-- Teach new skills (i.e. natural language to code)
+- Teach new skills (that is, natural language to code)
 
-If you identify cost as your primary motivator, proceed with caution. Fine-tuning might reduce costs for certain use cases by shortening prompts or allowing you to use a smaller model. But there may be a higher upfront cost to training, and you have to pay for hosting your own custom model. 
+If you identify cost as your primary motivator, proceed with caution. Fine-tuning might reduce costs for certain use cases by shortening prompts or allowing you to use a smaller model. But there might be a higher upfront cost to training, and you have to pay for hosting your own custom model. 
 
 ### Steps to fine-tune a model
+
 Here are the general steps to fine-tune a model:
-1. Based on your use case, choose a model that supports your task
-2. Prepare and upload training data
-3. (Optional) Prepare and upload validation data
-4. (Optional) Configure task parameters
-5. Train your model. 
-6. Once completed, review metrics and evaluate model. If the results don't meet your benchmark, then go back to step 2.
-7. Use your fine-tuned model
+
+1. Choose a model that supports your task.
+1. Prepare and upload training data.
+1. (Optional) Prepare and upload validation data.
+1. (Optional) Configure task parameters.
+1. Train your model. 
+1. Once completed, review metrics and evaluate model. If the results don't meet your benchmark, then go back to step 2.
+1. Use your fine-tuned model.
 
 It's important to call out that fine-tuning is heavily dependent on the quality of data that you can provide. It's best practice to provide hundreds, if not thousands, of training examples to be successful and get your desired results.
 
@@ -87,7 +92,6 @@ For more information on fine-tuning using a managed compute (preview), see [Fine
 
 For details about Azure OpenAI models that are available for fine-tuning, see the [Azure OpenAI Service models documentation](../../ai-services/openai/concepts/models.md#fine-tuning-models) or the [Azure OpenAI models table](#fine-tuning-azure-openai-models) later in this guide.
 
-
 For the Azure OpenAI  Service models that you can fine tune, supported regions for fine-tuning include North Central US, Sweden Central, and more.
 
 ### Fine-tuning Azure OpenAI models
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to fine-tuning overview documentation"
}
```

### Explanation
The code diff shows a minor update made to the "fine-tuning-overview.md" document, comprising 25 additions and 21 deletions, resulting in a total of 46 changes. The modifications enhance the clarity, correctness, and comprehensiveness of the content.

Key updates include a refined description that provides a clearer understanding of what fine-tuning entails and the conditions under which it should be considered. The document's metadata has also been updated, including changing the topic from "conceptual" to "concept-article," adjusting the review date to February 21, 2025, and changing the reviewer from "sgilley" to "keli19". 

The content has been restructured for improved readability, with updates to bullet points and explanatory text. Additionally, the article now explicitly mentions customer intent as a developer wanting to understand fine-tuning, which helps orient the audience.

The inclusion of graphical content, clarifications on the benefits and use cases for fine-tuning, and the emphasis on the importance of high-quality training data all serve to enhance the user experience. The updated steps for fine-tuning a model are also presented in a clearer format, making it easier for users to follow the process. 

Overall, these changes aim to make the documentation more accessible and useful for users looking to implement fine-tuning within Azure AI Foundry.

## articles/ai-studio/faq.yml{#item-e7baa2}

<details>
<summary>Diff</summary>
````diff
@@ -14,7 +14,7 @@ metadata:
   author: sdgilley
 title: Azure AI Foundry frequently asked questions
 summary: |
-  If you can't find answers to your questions in this document, and still need help check the [Azure AI services support options guide](../ai-services/cognitive-services-support-options.md?context=/azure/ai-services/openai/context/context). Azure OpenAI is part of Azure AI services.
+  FAQ for [Azure AI Foundry](https://ai.azure.com). If you can't find answers to your questions in this document, and still need help check the [Azure AI services support options guide](../ai-services/cognitive-services-support-options.md?context=/azure/ai-services/openai/context/context). Azure OpenAI is part of Azure AI services.
 sections:
   - name: General questions
     questions:
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update FAQ summary to include Azure AI Foundry reference"
}
```

### Explanation
The code diff reflects a minor update made to the "faq.yml" file, which is part of the documentation for Azure AI Foundry. This update consists of 1 addition and 1 deletion, resulting in a total of 2 changes.

The main alteration involves expanding the summary section of the FAQ document to explicitly reference "Azure AI Foundry" at the beginning. The updated summary now states, "FAQ for [Azure AI Foundry](https://ai.azure.com)," creating a more direct connection to the platform and enhancing the clarity of the document's purpose.

This change improves the user experience by immediately informing readers that the FAQ pertains specifically to Azure AI Foundry and invites them to explore the linked support options if their questions remain unanswered. Overall, the modification aims to ensure that users have a better contextual understanding when searching for information within the document.

## articles/ai-studio/how-to/create-manage-compute-session.md{#item-6ed743}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
-You need a compute session to run [prompt flows](prompt-flow.md). Use Azure AI Foundry to create and manage prompt flow compute sessions.
+You need a compute session to run [prompt flows](prompt-flow.md). Use [Azure AI Foundry](https://ai.azure.com) to create and manage prompt flow compute sessions.
 
 A prompt flow compute session has computing resources that are required for the application to run, including a Docker image that contains all necessary dependency packages. In addition to flow execution, Azure AI Foundry uses the compute session to ensure the accuracy and functionality of the tools incorporated within the flow when you make updates to the prompt or code content.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add hyperlink to Azure AI Foundry in compute session documentation"
}
```

### Explanation
The code diff indicates a minor update made to the "create-manage-compute-session.md" file. This change involves 1 addition and 1 deletion, leading to a total of 2 changes. 

The primary modification is the inclusion of a hyperlink to "Azure AI Foundry" within the text. The updated sentence now reads: "Use [Azure AI Foundry](https://ai.azure.com) to create and manage prompt flow compute sessions." This adjustment enhances the document by providing users a direct link to the Azure AI Foundry platform, improving accessibility to relevant resources.

This minor update contributes to a better user experience, as it enables users to easily navigate to the Azure AI Foundry site for more information, facilitating the process of creating and managing compute sessions necessary for running prompt flows. Overall, the alteration clarifies the instructions and enriches the documentation's utility.

## articles/ai-studio/how-to/create-projects.md{#item-cb10b3}

<details>
<summary>Diff</summary>
````diff
@@ -18,7 +18,7 @@ author: sdgilley
 
 # Create a project in Azure AI Foundry portal
 
-This article describes how to create an Azure AI Foundry project. A project is used to organize your work and save state while building customized AI apps. 
+This article describes how to create an [Azure AI Foundry](https://ai.azure.com) project. A project is used to organize your work and save state while building customized AI apps. 
 
 Projects are hosted by an Azure AI Foundry hub. If your company has an administrative team that has created a hub for you, you can create a project from that hub. If you are working on your own, you can create a project and a default hub will automatically be created for you.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add hyperlink to Azure AI Foundry in project creation guide"
}
```

### Explanation
The code diff shows a minor update made to the "create-projects.md" file. This modification consists of 1 addition and 1 deletion, resulting in 2 changes overall.

The key change involves incorporating a hyperlink to "Azure AI Foundry" within the introductory statement of the article. The updated text now reads: "This article describes how to create an [Azure AI Foundry](https://ai.azure.com) project." By adding this link, the documentation enhances its informative quality, allowing users to directly access the Azure AI Foundry platform for further details.

This adjustment not only improves access to relevant resources but also enriches the context for users who are learning how to create projects within Azure AI Foundry. The clarity provided by this minor update supports users in organizing their work and managing their AI applications more effectively.

## articles/ai-studio/how-to/deploy-models-phi-4.md{#item-c40212}

<details>
<summary>Diff</summary>
````diff
@@ -29,6 +29,18 @@ The Phi-4 family of small language models (SLMs) is a collection of instruction-
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -155,7 +167,7 @@ print("Model provider name:", model_info.model_provider_name)
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -176,7 +188,7 @@ response = client.complete(
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -192,7 +204,7 @@ print("\tCompletion tokens:", response.usage.completion_tokens)
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -341,6 +353,18 @@ except HttpResponseError as ex:
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -465,7 +489,7 @@ console.log("Model provider name: ", model_info.body.model_provider_name)
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -488,7 +512,7 @@ var response = await client.path("/chat/completions").post({
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -508,7 +532,7 @@ console.log("\tCompletion tokens:", response.body.usage.completion_tokens);
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -676,6 +700,18 @@ catch (error) {
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -815,7 +851,7 @@ Console.WriteLine($"Model provider name: {modelInfo.Value.ModelProviderName}");
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -837,7 +873,7 @@ Response<ChatCompletions> response = client.Complete(requestOptions);
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -853,7 +889,7 @@ Console.WriteLine($"\tCompletion tokens: {response.Value.Usage.CompletionTokens}
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -1023,6 +1059,18 @@ catch (RequestFailedException ex)
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -1113,7 +1161,7 @@ The response is as follows:
 
 ```json
 {
-    "model_name": "Phi-4-mini-instruct",
+    "model_name": "Phi-4-multimodal-instruct",
     "model_type": "chat-completions",
     "model_provider_name": "Microsoft"
 }
@@ -1139,7 +1187,7 @@ The following example shows how you can create a basic chat completions request
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -1149,7 +1197,7 @@ The response is as follows, where you can see the model's usage statistics:
     "id": "0a1234b5de6789f01gh2i345j6789klm",
     "object": "chat.completion",
     "created": 1718726686,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1206,7 +1254,7 @@ You can visualize how streaming generates content:
     "id": "23b54589eba14564ad8a2e6978775a39",
     "object": "chat.completion.chunk",
     "created": 1718726371,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1229,7 +1277,7 @@ The last message in the stream has `finish_reason` set, indicating the reason fo
     "id": "23b54589eba14564ad8a2e6978775a39",
     "object": "chat.completion.chunk",
     "created": 1718726371,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1280,7 +1328,7 @@ Explore other parameters that you can specify in the inference client. For a ful
     "id": "0a1234b5de6789f01gh2i345j6789klm",
     "object": "chat.completion",
     "created": 1718726686,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add details and references for the Phi-4-multimodal-instruct model"
}
```

### Explanation
The code diff reflects a significant update to the "deploy-models-phi-4.md" file, featuring 63 additions and 15 deletions, resulting in 78 total changes. 

This modification enhances the documentation by introducing detailed information about the "Phi-4-multimodal-instruct" model. New sections have been added to describe the capabilities of this lightweight multimodal foundation model, which integrates language, vision, and speech processing. The text now elaborates on how this model handles various input types, emphasizes its development enhancements such as supervised fine-tuning, and its ability to generate text outputs from diverse inputs.

Moreover, there's the inclusion of a direct hyperlink to the model's landing page, which further assists users in accessing pertinent resources. The documentation has also updated references across various sections to reflect the transition from the previous "Phi-4-mini-instruct" model to the newly introduced "Phi-4-multimodal-instruct."

These changes not only provide clearer guidance on how to leverage the new model but also ensure users are informed about the distinctions between the different Phi-4 models. Overall, the update significantly enriches the content of the document and improves user understanding of utilizing the Phi-4 series in their AI projects.

## articles/ai-studio/how-to/develop/ai-template-get-started.md{#item-d71b59}

<details>
<summary>Diff</summary>
````diff
@@ -1,58 +1,59 @@
 ---
 title: How to get started with an AI template
 titleSuffix: Azure AI Foundry
-description: This article provides instructions on how to get started with an AI template.
+description: This article provides instructions on how to use an AI template to get started with Azure AI Foundry.
 manager: scottpolly
 ms.service: azure-ai-foundry
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 01/02/2025
-ms.reviewer: dantaylo
+ms.date: 02/20/2025
+ms.reviewer: varundua
 ms.author: sgilley
 author: sdgilley
+#customer intent: As a developer, I want to jump start my journey with an AI template.
 ---
 
-# How to get started with an AI template
+# Get started with an AI template
+
+Streamline your code-first development with prebuilt, task-specific Azure AI templates. Benefit from using the latest features and best practices from Microsoft Azure AI, with popular frameworks like LangChain, prompt flow, and Semantic Kernel in multiple languages.
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-Streamline your code-first development with prebuilt, task-specific Azure AI templates. Benefit from using the latest features and best practices from Microsoft Azure AI, with popular frameworks like LangChain, prompt flow, and Semantic Kernel in multiple languages.
+## Prerequisites
 
-> [!TIP]
-> Discover the latest templates in our curated [AZD templates collection](https://aka.ms/azd-ai-templates). Deploy them with a single command ```azd up``` using the [Azure Developer CLI](/azure/developer/azure-developer-cli/). 
+- [Azure subscription](https://azure.microsoft.com/free)
+- An [Azure AI Foundry project](../create-projects.md).
 
 ## Start with a sample application
 
-Start with our sample applications! Choose the right template for your needs, then refer to the README in any of the following Azure Developer CLI enabled templates for more instructions and information.
+1. Go to [Azure AI Foundry portal](https://ai.azure.com).
+1. Open your project in Azure AI Foundry portal.
+1. On the left menu, select **Code** (preview).
+1. Find the solution template you want to use.
+1. Select **Open in Github** to view the entire sample application.
+1. Or, clone the repository to your local machine with the provided command.
+1. In some cases, you can also view a step-by-step tutorial that explains the AI code.
 
-### [Python](#tab/python)
+## Explore the sample application
 
-| Template      | App host | Tech stack | Description |
-| ----------- | ----------| ----------- | ------------|
-| [Azure AI Basic Template with Python](https://github.com/azure-samples/azureai-basic-python) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep | The app serves as a straightforward example of integrating Azure AI Services within a basic prompt-based application. This template walks you through building a simple chat app that utilizes models and prompts. The template also covers setting up the necessary infrastructure for the app, including creating an Azure AI Foundry Hub, configuring projects, and provisioning essential resources such as Azure AI Service, Azure Container Apps, Cognitive Search, and more. <br>You can build, deploy, and test it with a single command.  |
-| [Contoso Chat Retail copilot with Azure AI Foundry](https://github.com/Azure-Samples/contoso-chat) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Cosmos DB](/azure/cosmos-db/index-overview), [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI Search](/azure/search/search-what-is-azure-search), Bicep  | A retailer conversation agent that can answer questions grounded in your product catalog and customer order history. This template uses a retrieval augmented generation architecture with cutting-edge models for chat completion, chat evaluation, and embeddings. Build, evaluate, and deploy, an end-to-end solution with a single command. | 
-| [Process Automation: speech to text and summarization with Azure AI Foundry](https://github.com/Azure-Samples/summarization-openai-python-prompflow) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI speech to text service](../../../ai-services/speech-service/index-speech-to-text.yml), Bicep  | An app for workers to report issues via text or speech, translating audio to text, summarizing it, and specify the relevant department. | 
-| [Multi-Modal Creative Writing copilot with Dalle](https://github.com/Azure-Samples/agent-openai-python-prompty) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep | demonstrates how to create and work with AI agents. The app takes a topic and instruction input and then calls a research agent, writer agent, and editor agent. |  
-| [Assistant API Analytics Copilot with Python and Azure AI Foundry](https://github.com/Azure-Samples/assistant-data-openai-python-promptflow) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) |  [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep| A data analytics chatbot based on the Assistants API. The chatbot can answer questions in natural language, and interpret them as queries on an example sales dataset. |
-<!-- remove for now
-| Function Calling with Prompty, LangChain, and Pinecone | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction), [Pinecone](https://www.pinecone.io/), Bicep  | Utilize the new Prompty tool, LangChain, and Pinecone to build a large language model (LLM) search agent. This agent with Retrieval-Augmented Generation (RAG) technology is capable of answering user questions based on the provided data by integrating real-time information retrieval with generative responses. | 
-| Function Calling with Prompty, LangChain, and Elastic Search | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Elastic Search](https://www.elastic.co/elasticsearch), [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction) , Bicep  | Utilize the new Prompty tool, LangChain, and Elasticsearch to build a large language model (LLM) search agent. This agent with Retrieval-Augmented Generation (RAG) technology is capable of answering user questions based on the provided data by integrating real-time information retrieval with generative responses |
--->
+Once you're looking at the GitHub repository for your sample, refer to the README for more instructions and information on how to deploy your own version of the application.
 
+Instructions vary by sample, but most include how to:
 
-### [C#](#tab/csharp)
+* Open the solution in the location of your choice:
+  * GitHub Codespaces
+  * VS Code Dev Containers
+  * Your local IDE
+* Deploy the application to Azure
+* How to test the app
 
-| Template      | App host | Tech stack | Description |
-| ----------- | ----------| ----------- | -------------- |
-| [Contoso Chat Retail copilot with .NET and Semantic Kernel](https://github.com/Azure-Samples/contoso-chat-csharp-prompty) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Cosmos DB](/azure/cosmos-db/index-overview), [Azure Monitor](/azure/azure-monitor/overview), [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure Container Apps](/azure/container-apps/overview), [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Services](../../../ai-services/openai/overview.md), [Semantic Kernel](/semantic-kernel/overview/?tabs=Csharp), Bicep | A retailer conversation agent that can answer questions grounded in your product catalog and customer order history. This template uses a retrieval augmented generation architecture with cutting-edge models for chat completion, chat evaluation, and embeddings. Build, evaluate, and deploy, an end-to-end solution with a single command. |
-| [Process Automation: speech to text and summarization with .NET and GPT 3.5 Turbo](https://github.com/Azure-Samples/summarization-openai-csharp-prompty) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI speech to text service](../../../ai-services/speech-service/index-speech-to-text.yml), Bicep | An app for workers to report issues via text or speech, translating audio to text, summarizing it, and specify the relevant department. |
-
----
+The README also includes information about the application, such as the use case, architecture, and pricing information.
 
+> [!TIP]
+> Discover more templates in our curated [AZD templates collection](https://azure.github.io/ai-app-templates). Deploy them with a single command ```azd up``` using the [Azure Developer CLI](/azure/developer/azure-developer-cli/).
 
 ## Related content
 
-- [Get started building a chat app using the prompt flow SDK](../../quickstarts/get-started-code.md)
-- [Work with projects in VS Code](vscode.md)
-- [Connections in Azure AI Foundry portal](../../concepts/connections.md)
+- [Get started building a chat app using the Azure AI Foundry SDK](../../quickstarts/get-started-code.md)
+- [Develop AI apps using Azure AI services](/azure/developer/ai/)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise AI template get started guide with updated instructions and content"
}
```

### Explanation
The code diff showcases a comprehensive update to the "ai-template-get-started.md" file, resulting in 31 additions and 30 deletions, leading to a total of 61 changes. 

This update refines the instructional content on how to get started with AI templates in Azure AI Foundry. Notably, the description has been expanded to specify that the article provides guidance on using an AI template to initiate projects with Azure AI Foundry. The title has also been adjusted for greater clarity.

New sections have been introduced to outline the prerequisites for using an AI template, including the requirement for an Azure subscription and a project in Azure AI Foundry. The structure of the document has been enhanced, and detailed step-by-step instructions have been added for navigating the Azure AI Foundry portal, selecting solution templates, and deploying sample applications.

Additionally, the documentation now encourages users to explore more templates through a curated collection, and it emphasizes the functionality of deploying them effortlessly with the Azure Developer CLI.

The changes made improve the clarity and usability of the document, offering developers better guidance and resources to effectively start their projects using AI templates in Azure AI Foundry. The update also ensures that all referenced content is current, which assists users in utilizing the latest features available in the service.

## articles/ai-studio/how-to/develop/create-hub-project-sdk.md{#item-8c3e99}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-In this article, you learn how to create the following Azure AI Foundry resources using the Azure Machine Learning SDK and Azure CLI (with machine learning extension):
+In this article, you learn how to create the following [Azure AI Foundry](https://ai.azure.com) resources using the Azure Machine Learning SDK and Azure CLI (with machine learning extension):
 - An Azure AI Foundry hub
 - An Azure AI Services connection
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link to Azure AI Foundry in the hub project SDK guide"
}
```

### Explanation
The code diff indicates a minor update to the "create-hub-project-sdk.md" file, involving one addition and one deletion, resulting in two total changes. 

The primary modification in this document is the inclusion of a hyperlink to the Azure AI Foundry website within the introductory sentence. Specifically, the phrase "Azure AI Foundry" is now linked to its official website, providing readers direct access to further information about the platform.

The context of the article remains unchanged, as it continues to instruct users on how to create various Azure AI Foundry resources using the Azure Machine Learning SDK and Azure CLI (with the machine learning extension). The list of resources to be created, including an Azure AI Foundry hub and an Azure AI Services connection, also remains intact.

Overall, this modification enhances user experience by facilitating easier access to relevant information about Azure AI Foundry, helping to support users as they follow the guide to implement the specified resources.

## articles/ai-studio/how-to/develop/sdk-overview.md{#item-d3ab19}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: overview
-ms.date: 11/25/2024
+ms.date: 02/27/2025
 ms.reviewer: dantaylo
 ms.author: sgilley
 author: sdgilley
@@ -18,7 +18,7 @@ zone_pivot_groups: programming-languages-sdk-overview
 
 # The Azure AI Foundry SDK
 
-The Azure AI Foundry SDK is a comprehensive toolchain designed to simplify the development of AI applications on Azure. It enables developers to:
+The [Azure AI Foundry](https://ai.azure.com) SDK is a comprehensive toolchain designed to simplify the development of AI applications on Azure. It enables developers to:
 
 - Access popular models from various model providers through a single interface
 - Easily combine together models, data, and AI services to build AI-powered applications
@@ -111,7 +111,7 @@ Not yet available in C#.
 
 Copy the **Project connection string** from the **Overview** page of the project and update the connections string value above.
 
-Once you have created the project client, you can use the client for the capabilities in the following sections.
+Once you create the project client, you can use the client for the capabilities in the following sections.
 
 ::: zone pivot="programming-language-python"
 
@@ -127,7 +127,7 @@ Be sure to check out the [reference](https://aka.ms/aifoundrysdk/reference) and
 
 ## Azure OpenAI Service
 
-The [Azure OpenAI Service](../../../ai-services/openai/overview.md) provides access to OpenAI's models including the GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, DALLE-3, Whisper, and Embeddings model series with the data residency, scalability, safety, security and enterprise capabilities of Azure.
+The [Azure OpenAI Service](../../../ai-services/openai/overview.md) provides access to OpenAI's models including the GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, DALLE-3, Whisper, and Embeddings model series with the data residency, scalability, safety, security, and enterprise capabilities of Azure.
 
 If you have code that uses the OpenAI SDK, you can easily target your code to use the Azure OpenAI service. First, install the OpenAI SDK:
 
@@ -233,9 +233,9 @@ To learn more about using the Azure AI inferencing client, check out the [Azure
 
 ::: zone pivot="programming-language-python"
 
-## Prompt Templates
+## Prompt templates
 
-The inferencing client supports for creating prompt messages from templates.  The template allows you to dynamically generate prompts using inputs that are available at runtime.
+The inferencing client supports creating prompt messages from templates. The template allows you to dynamically generate prompts using inputs that are available at runtime.
 
 To use prompt templates, install the `azure-ai-inference` package:
 
@@ -356,7 +356,7 @@ To learn more about using Azure AI Search, check out [Azure AI Search documentat
 
 ## Azure AI Agent Service
 
-Azure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents. Using an extensive ecosystem of models, tools and capabilities from OpenAI, Microsoft, and third-party providers, [Azure AI Agent Service](/azure/ai-services/agents) enables building agents for a wide range of generative AI use cases.
+Azure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents. To enable building agents for a wide range of generative AI use cases, [Azure AI Agent Service](/azure/ai-services/agents) uses an extensive ecosystem of models, tools and capabilities from OpenAI, Microsoft, and third-party providers.
 
 ## Evaluation
 
@@ -393,7 +393,7 @@ To learn more, check out [Evaluation using the SDK](evaluate-sdk.md).
 
 ::: zone pivot="programming-language-csharp"
 
-An Azure AI evaluation package is not yet available for C#. For a sample on how to use Prompty and Semantic Kernel for evaluation, see the [contoso-chat-csharp-prompty](https://github.com/Azure-Samples/contoso-chat-csharp-prompty/blob/main/src/ContosoChatAPI/ContosoChat.Evaluation.Tests/Evalutate.cs) sample.
+An Azure AI evaluation package isn't yet available for C#. For a sample on how to use Prompty and Semantic Kernel for evaluation, see the [contoso-chat-csharp-prompty](https://github.com/Azure-Samples/contoso-chat-csharp-prompty/blob/main/src/ContosoChatAPI/ContosoChat.Evaluation.Tests/Evalutate.cs) sample.
 
 ::: zone-end
 
@@ -428,41 +428,43 @@ if application_insights_connection_string:
 
 ::: zone pivot="programming-language-csharp"
 
-Tracing is not yet integrated into the projects package. For instructions on how to instrument and log traces from the Azure AI Inferencing package, see [azure-sdk-for-dotnet](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Inference/samples/Sample8_ChatCompletionsWithOpenTelemetry.md).
+Tracing isn't yet integrated into the projects package. For instructions on how to instrument and log traces from the Azure AI Inferencing package, see [azure-sdk-for-dotnet](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Inference/samples/Sample8_ChatCompletionsWithOpenTelemetry.md).
 
 ::: zone-end
 
-## Related content
+## Other services and frameworks
 
-Below are some helpful links to other services and frameworks that you can use with the Azure AI Foundry SDK.
+The following sections provide helpful links to other services and frameworks that you can use with the Azure AI Foundry SDK.
 
 ### Azure AI Services
 
-Client libraries:
+* Client libraries:
 
-* [Azure AI services SDKs](../../../ai-services/reference/sdk-package-resources.md?context=/azure/ai-studio/context/context)
-* [Azure AI services REST APIs](../../../ai-services/reference/rest-api-resources.md?context=/azure/ai-studio/context/context) 
+    * [Azure AI services SDKs](../../../ai-services/reference/sdk-package-resources.md?context=/azure/ai-studio/context/context)
+    * [Azure AI services REST APIs](../../../ai-services/reference/rest-api-resources.md?context=/azure/ai-studio/context/context) 
 
-Management libraries:
-* [Azure AI Services Python Management Library](/python/api/overview/azure/mgmt-cognitiveservices-readme)
-* [Azure AI Search Python Management Library](/python/api/azure-mgmt-search/azure.mgmt.search)
+* Management libraries:
+
+    * [Azure AI Services Python Management Library](/python/api/overview/azure/mgmt-cognitiveservices-readme)
+    * [Azure AI Search Python Management Library](/python/api/azure-mgmt-search/azure.mgmt.search)
 
 ### Frameworks
 
-Azure Machine Learning
+* Azure Machine Learning
+
+    * [Azure Machine Learning Python SDK (v2)](/python/api/overview/azure/ai-ml-readme)
+    * [Azure Machine Learning CLI (v2)](/azure/machine-learning/how-to-configure-cli)
+    * [Azure Machine Learning REST API](/rest/api/azureml) 
 
-* [Azure Machine Learning Python SDK (v2)](/python/api/overview/azure/ai-ml-readme)
-* [Azure Machine Learning CLI (v2)](/azure/machine-learning/how-to-configure-cli)
-* [Azure Machine Learning REST API](/rest/api/azureml) 
+* Prompt flow
 
-Prompt flow
+    * [Prompt flow SDK](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html)
+    * [pfazure CLI](https://microsoft.github.io/promptflow/reference/pfazure-command-reference.html)
+    * [pfazure Python library](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow-azure/promptflow.azure.html)
 
-* [Prompt flow SDK](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html)
-* [pfazure CLI](https://microsoft.github.io/promptflow/reference/pfazure-command-reference.html)
-* [pfazure Python library](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow-azure/promptflow.azure.html)
+* Semantic Kernel
+    * [Semantic Kernel Overview](/semantic-kernel/overview/)
 
-Semantic Kernel
- * [Semantic Kernel Overview](/semantic-kernel/overview/)
-Agentic frameworks
+* Agentic frameworks
 
-* [LlamaIndex](llama-index.md)
+    * [LlamaIndex](llama-index.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhance SDK overview with updated content and links"
}
```

### Explanation
The code diff reflects a significant update to the "sdk-overview.md" file, involving 31 additions and 29 deletions—resulting in a total of 60 changes. 

This update primarily focuses on enhancing the clarity and accessibility of the content related to the Azure AI Foundry SDK. One of the key modifications includes a hyperlink that directs readers to the official Azure AI Foundry website, improving access to relevant information. 

Other changes involve minor grammatical adjustments for better readability, such as changes from "Once you have created the project client" to "Once you create the project client," making the writing slightly more concise.

Additionally, the document's structure has been improved through clearer headers and bullet points, which assist in organizing information into more accessible segments. For example, the section headers have been refined from "Related content" to "Other services and frameworks" to better reflect the guiding content provided. 

The informative nature of the document is preserved, as it continues to detail the capabilities of the Azure AI Foundry SDK, including accessing models, connecting to AI services, and utilizing the Azure OpenAI Service. Enhanced references to other SDKs and libraries have been retained while formatting changes help outline them more clearly.

Overall, these modifications contribute to an updated, user-friendly overview that effectively supports developers in utilizing the SDK for building AI applications on Azure. The incorporation of new links and structural adjustments serves to enhance the users' comprehension and navigability of the content.

## articles/ai-studio/how-to/develop/vscode.md{#item-24bd97}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-Azure AI Foundry supports developing in VS Code - Desktop and Web. In each scenario, your VS Code instance is remotely connected to a prebuilt custom container running on a virtual machine, also known as a compute instance.
+[Azure AI Foundry](https://ai.azure.com) supports developing in VS Code - Desktop and Web. In each scenario, your VS Code instance is remotely connected to a prebuilt custom container running on a virtual machine, also known as a compute instance.
 
 ## Launch VS Code from Azure AI Foundry
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add hyperlink to Azure AI Foundry in VS Code development guide"
}
```

### Explanation
The code diff signifies a minor update to the "vscode.md" file, which includes one addition and one deletion, resulting in two overall changes.

The key modification in this document is the addition of a hyperlink to the term "Azure AI Foundry" in the introductory sentence. This change allows readers to directly access the Azure AI Foundry website by clicking the link, enhancing the ease of navigation for users seeking further information on the platform.

The primary content of the guide remains unchanged, as it continues to describe how the Azure AI Foundry supports software development within Visual Studio Code (both Desktop and Web versions). It explains that in this scenario, the user's VS Code instance connects remotely to a prebuilt custom container hosted on a virtual machine, referred to as a compute instance.

Overall, this update improves user experience by making relevant resources more accessible while maintaining the informative nature of the guide concerning the use of VS Code in the Azure AI Foundry ecosystem.

## articles/ai-studio/how-to/model-catalog-overview.md{#item-278001}

<details>
<summary>Diff</summary>
````diff
@@ -84,7 +84,7 @@ Gretel | Not available | Gretel-Navigator
 Healthcare AI family Models | MedImageParse<BR>  MedImageInsight<BR>  CxrReportGen<BR>  Virchow<BR>  Virchow2<BR>  Prism<BR>  BiomedCLIP-PubMedBERT<BR>  microsoft-llava-med-v1.5<BR>  m42-health-llama3-med4<BR>  biomistral-biomistral-7b<BR>  microsoft-biogpt-large-pub<BR>  microsoft-biomednlp-pub<BR>  stanford-crfm-biomedlm<BR>  medicalai-clinicalbert<BR>  microsoft-biogpt<BR>  microsoft-biogpt-large<BR>  microsoft-biomednlp-pub<BR> | Not Available
 JAIS | Not available | jais-30b-chat
 Meta Llama family models | Llama-3.3-70B-Instruct<BR> Llama-3.2-3B-Instruct<BR>  Llama-3.2-1B-Instruct<BR>  Llama-3.2-1B<BR>  Llama-3.2-90B-Vision-Instruct<BR>  Llama-3.2-11B-Vision-Instruct<BR>  Llama-3.1-8B-Instruct<BR>  Llama-3.1-8B<BR>  Llama-3.1-70B-Instruct<BR>  Llama-3.1-70B<BR>  Llama-3-8B-Instruct<BR>  Llama-3-70B<BR>  Llama-3-8B<BR>  Llama-Guard-3-1B<BR>  Llama-Guard-3-8B<BR>  Llama-Guard-3-11B-Vision<BR>  Llama-2-7b<BR>  Llama-2-70b<BR>  Llama-2-7b-chat<BR>  Llama-2-13b-chat<BR>  CodeLlama-7b-hf<BR>  CodeLlama-7b-Instruct-hf<BR>  CodeLlama-34b-hf<BR>  CodeLlama-34b-Python-hf<BR>  CodeLlama-34b-Instruct-hf<BR>  CodeLlama-13b-Instruct-hf<BR>  CodeLlama-13b-Python-hf<BR>  Prompt-Guard-86M<BR>  CodeLlama-70b-hf<BR> | Llama-3.3-70B-Instruct<BR> Llama-3.2-90B-Vision-Instruct<br>  Llama-3.2-11B-Vision-Instruct<br>  Llama-3.1-8B-Instruct<br>  Llama-3.1-70B-Instruct<br>  Llama-3.1-405B-Instruct<br>  Llama-3-8B-Instruct<br>  Llama-3-70B-Instruct<br>  Llama-2-7b<br>  Llama-2-7b-chat<br>  Llama-2-70b<br>  Llama-2-70b-chat<br>  Llama-2-13b<br>  Llama-2-13b-chat<br>
-Microsoft Phi family models | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> Phi-3-vision-128k-Instruct <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct
+Microsoft Phi family models | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> Phi-3-vision-128k-Instruct <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct <br> Phi-4-multimodal-instruct | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct <br> Phi-4-multimodal-instruct
 Mistral family models | mistralai-Mixtral-8x22B-v0-1 <br> mistralai-Mixtral-8x22B-Instruct-v0-1 <br> mistral-community-Mixtral-8x22B-v0-1 <br> mistralai-Mixtral-8x7B-v01 <br> mistralai-Mistral-7B-Instruct-v0-2 <br> mistralai-Mistral-7B-v01 <br> mistralai-Mixtral-8x7B-Instruct-v01 <br> mistralai-Mistral-7B-Instruct-v01 | Mistral-large (2402) <br> Mistral-large (2407) <br> Mistral-small <br> Ministral-3B <br> Mistral-NeMo
 Nixtla | Not available | TimeGEN-1
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add new model to Microsoft Phi family in the model catalog"
}
```

### Explanation
The code diff represents a minor update to the "model-catalog-overview.md" file, which includes one addition and one deletion, leading to a total of two changes.

The key change in this update is the inclusion of a new model, "Phi-4-multimodal-instruct," within the Microsoft Phi family models section. This addition expands the list of available models, thereby enhancing the breadth of resources available to users interested in utilizing Microsoft Phi models for AI applications. 

While the rest of the content related to various model families remains intact, this modification underscores the ongoing development and enhancement of the AI model catalog, providing users with access to more advanced functionalities. 

Overall, this update not only keeps the content current but also assists users by presenting them with a more comprehensive overview of the available AI models in the Microsoft ecosystem, encouraging exploration and deployment of newly added models.

## articles/ai-studio/includes/region-availability-maas.md{#item-35d79c}

<details>
<summary>Diff</summary>
````diff
@@ -61,7 +61,7 @@ Llama 3.1 405B Instruct  | [Microsoft Managed countries/regions](/partner-center
 
 | Model | Offer Availability Region  | Hub/Project Region for Deployment  | Hub/Project Region for Fine tuning  |
 |---------|---------|---------|---------|
-Phi-4 <br>  Phi-4-mini-instruct    | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
+Phi-4 <br>  Phi-4-mini-instruct <br>  Phi-4-multimodal-instruct    | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
 Phi-3.5-vision-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
 Phi-3.5-MoE-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | East US 2       |
 Phi-3.5-Mini-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | East US 2  | East US 2       |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add new model to the Phi family in region availability document"
}
```

### Explanation
The code diff represents a minor update to the "region-availability-maas.md" file, comprising one addition and one deletion, culminating in a total of two changes.

In this update, the new model "Phi-4-multimodal-instruct" has been added to the section detailing availability for various model offerings within the Phi family. This addition ensures that users are aware of the new model's availability and capabilities alongside existing models, enhancing the comprehensive nature of the information provided.

The other details regarding the availability regions for deployment and fine-tuning for the Phi family models remain unchanged. However, the inclusion of this new model serves to inform users about the latest developments and options available for utilizing Phi models in different regions, which can significantly impact users' deployment strategies and project planning.

Overall, this update contributes to maintaining the accuracy and relevance of the model availability documentation, ensuring that users have access to the most current information about the models offered within the Microsoft ecosystem.

## articles/ai-studio/quickstarts/get-started-code.md{#item-8a5082}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
-In this quickstart, we walk you through setting up your local development environment with the Azure AI Foundry SDK. We write a prompt, run it as part of your app code, trace the LLM calls being made, and run a basic evaluation on the outputs of the LLM.
+In this quickstart, we walk you through setting up your local development environment with the [Azure AI Foundry](https://ai.azure.com) SDK. We write a prompt, run it as part of your app code, trace the LLM calls being made, and run a basic evaluation on the outputs of the LLM.
 
 ## Prerequisites
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update hyperlink for Azure AI Foundry SDK in quickstart guide"
}
```

### Explanation
The code diff illustrates a minor update in the "get-started-code.md" file, involving one addition and one deletion, resulting in a total of two changes.

This specific change enhances the clarity and usability of the quickstart guide by providing a direct hyperlink to the Azure AI Foundry SDK. The phrase "Azure AI Foundry" was modified from plain text to a clickable link, making it easier for users to access relevant resources immediately.

By incorporating this link, the documentation improves user experience and accessibility, enabling new users to quickly navigate to the SDK's webpage for more information. This minor adjustment is particularly beneficial in a quickstart context, where users typically look for straightforward guidance and resources to facilitate their initial setup and development process. 

Overall, this update contributes to a more user-friendly documentation experience, ensuring that users have easy access to critical information needed to begin their journey with the Azure AI Foundry SDK.

## articles/ai-studio/tutorials/copilot-sdk-build-rag.md{#item-b77dba}

<details>
<summary>Diff</summary>
````diff
@@ -15,7 +15,7 @@ ms.custom: copilot-learning-hub, ignite-2024
 
 # Tutorial:  Part 2 - Build a custom knowledge retrieval (RAG) app with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI Foundry SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
 
 This part two shows you how to enhance a basic chat application by adding [retrieval augmented generation (RAG)](../concepts/retrieval-augmented-generation.md) to ground the responses in your custom data. Retrieval Augmented Generation (RAG) is a pattern that uses your data with a large language model (LLM) to generate answers specific to your data. In this part two, you learn how to:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Include hyperlink for Azure AI Foundry SDK in RAG tutorial"
}
```

### Explanation
The code diff reflects a minor update in the "copilot-sdk-build-rag.md" file, featuring one addition and one deletion, resulting in a total of two changes.

This specific change enhances the tutorial by converting the mention of the "Azure AI Foundry SDK" into a clickable hyperlink. By linking to the SDK's webpage, users can easily access detailed information and resources directly related to the SDK, improving the overall user experience.

The tutorial itself guides users in building a chat application for a retail company, Contoso Trek, focusing on configuring and evaluating the app to respond to product-related inquiries. The introduction of the hyperlink not only facilitates quick navigation for users but also enriches the instructional content by connecting them with essential tools needed for practical implementation.

Overall, this update serves to streamline the learning process and assists users in easily locating the resources necessary for successfully utilizing the Azure AI Foundry SDK in their projects.

## articles/ai-studio/tutorials/copilot-sdk-create-resources.md{#item-552960}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 # Tutorial:  Part 1 - Set up project and development environment to build a custom knowledge retrieval (RAG) app with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI Foundry SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
 
 This tutorial is part one of a three-part tutorial.  This part one gets you ready to write code in part two and evaluate your chat app in part three. In this part, you:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add hyperlink for Azure AI Foundry SDK in resource creation tutorial"
}
```

### Explanation
The code diff presents a minor update in the "copilot-sdk-create-resources.md" file, showcasing one addition and one deletion, culminating in a total of two changes.

This modification enhances the tutorial by embedding a clickable hyperlink for the "Azure AI Foundry SDK," directing users to the relevant webpage. By adding this link, users gain immediate access to important information and resources associated with the SDK, facilitating their understanding and usage as they work through the tutorial.

The tutorial focuses on setting up the project and development environment for creating a chat application tailored for Contoso Trek, a retail company specializing in outdoor gear. It guides users through the process of building, configuring, and evaluating the application, addressing common customer queries.

By incorporating the hyperlink, the documentation not only improves navigation but also enriches the tutorial's usability, ensuring users can easily find supporting materials necessary for effective implementation. This small yet significant update streamlines the learning process, empowering users to effectively harness the capabilities of the Azure AI Foundry SDK in their projects.

## articles/ai-studio/tutorials/copilot-sdk-evaluate.md{#item-bb5754}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 # Tutorial: Part 3 - Evaluate a custom chat application with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI SDK (and other libraries) to  evaluate the chat app you built in [Part 2 of the tutorial series](copilot-sdk-build-rag.md). In this part three, you learn how to:
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to  evaluate the chat app you built in [Part 2 of the tutorial series](copilot-sdk-build-rag.md). In this part three, you learn how to:
 
 > [!div class="checklist"]
 > - Create an evaluation dataset
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add hyperlink for Azure AI Foundry SDK in evaluation tutorial"
}
```

### Explanation
The code diff indicates a minor update in the "copilot-sdk-evaluate.md" file, which includes one addition and one deletion, resulting in two changes.

In this modification, the reference to the "Azure AI SDK" is replaced with a hyperlink to the "Azure AI Foundry" SDK. By making this change, users are provided with direct access to the Azure AI Foundry webpage, enhancing the tutorial's usability. This allows users to quickly gather more information about the SDK as they proceed with the tutorial.

The tutorial itself is designed to guide users in evaluating a chat application created in the earlier parts of the tutorial series. This section (Part 3) focuses on leveraging the capabilities of the Azure AI Foundry SDK to assess the performance and effectiveness of the chat app that was built.

The addition of the hyperlink enriches the tutorial's content, ensuring that users can easily find pertinent resources related to the Azure AI Foundry SDK. This update enhances the learning experience by facilitating access to essential information while the users engage in the process of evaluating their application.

## articles/ai-studio/tutorials/screen-reader.md{#item-4dc029}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ This article is for people who use screen readers such as [Microsoft's Narrator]
 
 ## Getting oriented in Azure AI Foundry portal 
 
-Most Azure AI Foundry pages are composed of the following landmark structure: 
+Most [Azure AI Foundry](https://ai.azure.com) pages are composed of the following landmark structure: 
 
 - Banner (contains Azure AI Foundry app title, settings, and profile information)
     - Might sometimes contain a breadcrumb navigation element 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Add hyperlink for Azure AI Foundry in screen reader article"
}
```

### Explanation
The code diff reflects a minor update in the "screen-reader.md" file, with one addition and one deletion, resulting in a total of two changes.

This update modifies a sentence to include a hyperlink for the "Azure AI Foundry" within the article. By linking directly to the Azure AI Foundry portal, users who utilize screen readers can now easily access the platform while reading the tutorial, enhancing their overall experience and accessibility.

The article serves as a guide for individuals who rely on screen readers, providing an overview of how to navigate the Azure AI Foundry portal. The updated text clarifies that most pages within the portal are structured with specific landmarks for easier navigation, which is crucial for screen reader users.

By enhancing the content with a hyperlink, the tutorial not only improves usability but also empowers screen reader users to access related resources quickly. This minor update significantly contributes to making the tutorial more informative and user-friendly, ensuring that all users can effectively engage with the Azure AI Foundry platform.


