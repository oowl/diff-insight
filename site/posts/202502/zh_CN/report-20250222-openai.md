---
date: '2025-02-22'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:6d12544...MicrosoftDocs:b49c252
summary: 本次代码更新对Azure OpenAI服务的文档进行了小幅改进，主要包括提高语言准确性和现代性，更新最后修改日期，增加和优化示例对话，以提升用户体验。新增了以“Clippy”角色为主的对话示例，而无重大破坏性更改。同时，更新了文档结构和内容，删除冗余信息，使文档更易读。此次更新旨在为用户提供最新、最有用的信息，确保他们能够顺畅导航和理解Azure
  OpenAI服务的应用。
title: '[zh_CN] Diff Insight Report - openai'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:6d12544...MicrosoftDocs:b49c252){target="_blank"}

<format>
# Highlights
本次代码更新对Azure OpenAI服务的多个文档进行了小幅更新。主要亮点包括：改善语言措辞以提高准确性和现代性，更新文档的修改日期以确保时效性，添加和优化示例对话以及术语的一致性，以增强文档的清晰度和用户体验。

## New features
- 文档中新增了示例对话，如使用“Clippy”角色的对话实例。

## Breaking changes
- 无重大破坏性更改。

## Other updates
- 更新文档中的最后修改日期。
- 改进语言措辞和段落结构。
- 增加直接链接和注释块以增强易读性。
- 删除冗余和不必要的内容以精简文档。

# Insights
此次文档更新聚焦于提高文本的准确性、流畅性与现代性，并确保用户在使用Azure OpenAI服务时能够获取最新和最有用的信息。通过细致的语言改进和精简的内容，文档不仅变得更易读，也更具指导价值。

文档的更新反映了对用户体验的重视。例如，增加对“Azure AI Foundry”的直接链接和注释格式的调整，为用户提供了更直观的引导。这种改进确保了用户能够快速找到所需的信息并且顺畅地导航至相关资源。

在示例对话中，引入了具有讽刺特色的角色“Clippy”，不仅提供了更现代化的互动方式，也为用户展示了如何运用幽默来丰富对话体验。这种详细的示例有助于开发者更好地理解如何将模型应用于实际任务中，提升微调功能的使用效率。

另外，更新文档保持了Azure OpenAI服务文档一致性的优良传统，并继续通过优化结构和提供清晰的指南来提高整体的文档质量。这种逐步的改进策略为用户提供更为有效的学习和使用路径，适应快速变迁的技术环境。
</format>

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [content-credentials.md](#item-a23b50) | minor update | 更新内容凭证文档部分 | modified | 13 | 16 | 29 | 
| [content-filter.md](#item-dfc7e7) | minor update | 更新内容过滤文档日期 | modified | 1 | 1 | 2 | 
| [default-safety-policies.md](#item-39b6a0) | minor update | 更新默认内容安全政策文档 | modified | 11 | 11 | 22 | 
| [gpt-4-v-prompt-engineering.md](#item-fd7772) | minor update | 更新GPT-4提示工程文档 | modified | 11 | 12 | 23 | 
| [gpt-with-vision.md](#item-991388) | minor update | 更新GPT与视觉模型文档 | modified | 7 | 4 | 11 | 
| [prompt-transformation.md](#item-21e047) | minor update | 更新提示转换文档 | modified | 5 | 8 | 13 | 
| [safety-system-message-templates.md](#item-460532) | minor update | 更新安全系统消息模板文档 | modified | 7 | 7 | 14 | 
| [dall-e-quickstart.md](#item-fcd528) | minor update | 更新DALL-E快速入门文档 | modified | 1 | 1 | 2 | 
| [gpt-v-quickstart.md](#item-2a6183) | minor update | 更新GPT-V快速入门文档 | modified | 2 | 2 | 4 | 
| [dall-e.md](#item-ac9616) | minor update | 更新DALL-E使用文档 | modified | 6 | 6 | 12 | 
| [gpt-with-vision.md](#item-4d8502) | minor update | 更新视觉启用聊天模型使用指南 | modified | 8 | 7 | 15 | 
| [risks-safety-monitor.md](#item-b2be0b) | minor update | 更新风险和安全监控指南 | modified | 6 | 5 | 11 | 
| [use-blocklists.md](#item-e99db7) | minor update | 更新如何使用封锁列表指南 | modified | 5 | 5 | 10 | 
| [latest-inference-preview.md](#item-24bf0f) | minor update | 更新最新推理预览文档 | modified | 34 | 36 | 70 | 
| [dall-e-rest.md](#item-4bac64) | minor update | 更新DALL·E REST API指南日期 | modified | 1 | 1 | 2 | 
| [fine-tuning-openai-in-ai-studio.md](#item-723c8d) | minor update | 更新模型和示例对话 | modified | 11 | 4 | 15 | 
| [fine-tuning-python.md](#item-976f58) | minor update | 更新微调示例对话和模型信息 | modified | 10 | 3 | 13 | 
| [fine-tuning-rest.md](#item-9add3e) | minor update | 微调示例对话的更新 | modified | 10 | 3 | 13 | 


# Modified Contents
## articles/ai-services/openai/concepts/content-credentials.md{#item-a23b50}

<details>
<summary>Diff</summary>
````diff
@@ -1,18 +1,18 @@
 ---
 title: Content Credentials in Azure OpenAI
 titleSuffix: Azure OpenAI
-description: Learn about the content credentials feature, which lets you verify that an image was generated by an AI model.
+description: Learn about the Content Credentials feature, which lets you verify that an image was generated by an AI model.
 author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 8/28/2024
+ms.date: 02/20/2025
 manager: nitinme
 ---
 
 # Content Credentials
 
-With the improved quality of content from generative AI models, there is an increased need for more transparency on the origin of AI-generated content. All AI-generated images from Azure OpenAI Service now include Content Credentials, a tamper-evident way to disclose the origin and history of content. Content Credentials are based on an open technical specification from the [Coalition for Content Provenance and Authenticity (C2PA)](https://www.c2pa.org), a Joint Development Foundation project. 
+With the improved quality of content from generative AI models, there is an increased need for more transparency about the origin of AI-generated content. All AI-generated images from Azure OpenAI Service include Content Credentials, a tamper-evident way to disclose the origin and history of content. Content Credentials are based on an open technical specification from the [Coalition for Content Provenance and Authenticity (C2PA)](https://www.c2pa.org), a Joint Development Foundation project. 
 
 ## What are Content Credentials? 
 
@@ -29,21 +29,18 @@ The manifest contains several key pieces of information:
 
 Content Credentials in the Azure OpenAI Service can help people understand when visual content is AI-generated. For more information on how to responsibly build solutions with Azure OpenAI Service image-generation models, visit the [Azure OpenAI transparency note](/legal/cognitive-services/openai/transparency-note?tabs=text).
 
-## How do I leverage Content Credentials in my solution today?
+## How do I use Content Credentials in my solution today?
 
-Customers may leverage Content Credentials by:
+Customers may use Content Credentials by:
 - Ensuring that their AI generated images contain Content Credentials
-
-No additional set-up is necessary. Content Credentials are automatically applied to all generated images from DALL·E in the Azure OpenAI Service. 
-
+    No additional set-up is necessary. Content Credentials are automatically applied to all generated images from DALL·E in the Azure OpenAI Service. 
 - Verifying that an image has Content Credentials
-  
-There are two recommended ways today to check the Credential of an image generated by Azure OpenAI DALL-E models:
-
-1. **Content Credentials Verify webpage (contentcredentials.org/verify)**: This is a tool that allows users to inspect the Content Credentials of a piece of content. If an image was generated by DALL-E in Azure OpenAI, the tool will display that its Content Credentials were issued by Microsoft Corporation alongside the date and time of issuance.
-    
-  :::image type="content" source="../media/encryption/credential-check.png" alt-text="Screenshot of the content credential verification website.":::
+    There are two recommended ways today to check the credential of an image generated by Azure OpenAI DALL-E models:
 
-  This page shows that an image generated by Azure OpenAI DALL-E has Content Credentials issued by Microsoft.
+    - **Content Credentials Verify webpage (contentcredentials.org/verify)**: This is a tool that allows users to inspect the Content Credentials of a piece of content. If an image was generated by DALL-E in Azure OpenAI, the tool will display that its Content Credentials were issued by Microsoft Corporation alongside the date and time of issuance.
+       :::image type="content" source="../media/encryption/credential-check.png" alt-text="Screenshot of the content credential verification website.":::
     
-2. **Content Authenticity Initiative (CAI) open-source tools**: The CAI provides multiple open-source tools that validate and display C2PA Content Credentials. Find the tool right for your application and [get started here](https://opensource.contentauthenticity.org/).
+       This page shows that an image generated by Azure OpenAI DALL-E has Content Credentials issued by Microsoft.
+        
+    - **Content Authenticity Initiative (CAI) open-source tools**: The CAI provides multiple open-source tools that validate and display C2PA Content Credentials. Find the tool right for your application and [get started here](https://opensource.contentauthenticity.org/).
+    
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新内容凭证文档部分"
}
```

### Explanation
本次修改对Azure OpenAI服务中的内容凭证文档进行了小幅更新。具体而言，更新主要包括：

1. 修正了文档中“关于”的措辞，将“more transparency on the origin of AI-generated content”改为“more transparency about the origin of AI-generated content”，以提高语句的准确性。
2. 修改了标题“如何利用内容凭证”至“如何在我的解决方案中使用内容凭证”，使其更符合现代化的用语。
3. 增强了对用户如何检查生成内容凭证的描述，明确了用户可以通过两个推荐的方式来验证图像的内容凭证。这包括使用“内容凭证验证网页”和“内容真实性倡议（CAI）开源工具”的说明。
4. 更新了最后修改日期，将其从“2024年8月28日”更改为“2025年2月20日”，反映了文档的最新信息。

此次更改可以提高用户对内容凭证功能的理解，以及如何在Azure OpenAI服务中有效使用该功能。

## articles/ai-services/openai/concepts/content-filter.md{#item-dfc7e7}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 08/22/2024
+ms.date: 02/20/2025
 ms.custom: template-concept, devx-track-python
 manager: nitinme
 ---
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新内容过滤文档日期"
}
```

### Explanation
本次修改对Azure OpenAI服务中的内容过滤文档进行了小幅更新，主要是更新了文档的最后修改日期。具体更改如下：

- 将文档的最后修改日期从“2024年8月22日”更改为“2025年2月20日”。这一变更反映了文档的最新状态和信息。

此次更新确保用户能够获取到最新的文档信息，有助于保持内容的时效性和准确性。

## articles/ai-services/openai/concepts/default-safety-policies.md{#item-39b6a0}

<details>
<summary>Diff</summary>
````diff
@@ -1,27 +1,27 @@
 ---
 title: Azure OpenAI default content safety policies
 titleSuffix: Azure OpenAI
-description: Learn about the default content safety policies that Azure OpenAI uses to flag content.
+description: Learn about the default content safety policies that Azure OpenAI uses to flag content and ensure responsible use of the service.
 author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
-ms.topic: conceptual 
-ms.date: 07/15/2024
+ms.topic: conceptual
+ms.date: 02/20/2025
 manager: nitinme
 ---
 
 # Default content safety policies
 
 
-Azure OpenAI Service includes default safety applied to all models, excluding Azure OpenAI Whisper. These configurations provide you with a responsible experience by default, including [content filtering models](/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cpython-new), blocklists, prompt transformation, [content credentials](/azure/ai-services/openai/concepts/content-credentials), and others.
+Azure OpenAI Service includes default safety policies applied to all models, excluding Azure OpenAI Whisper. These configurations provide you with a responsible experience by default, including [content filtering models](/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cpython-new), blocklists, prompt transformation, [content credentials](/azure/ai-services/openai/concepts/content-credentials), and others.
 
-Default safety aims to mitigate risks such as hate and fairness, sexual, violence, self-harm, protected material content and user prompt injection attacks. To learn more about content filtering, visit our documentation describing categories and severity levels [here](/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cpython-new).
+Default safety aims to mitigate risks in different categories such as hate and fairness, sexual, violence, self-harm, protected material content, and user prompt injection attacks. To learn more about content filtering, visit our documentation describing [categories and severity levels](/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cpython-new).
 
-All safety is configurable. To learn more about configurability, visit our documentation on [configuring content filtering](/azure/ai-services/openai/how-to/content-filters).
+All safety policies are configurable. To learn more about configurability, see the documentation on [configuring content filtering](/azure/ai-services/openai/how-to/content-filters).
 
-## Text models: GPT-4, GPT-3.5
+## Text models
 
-Text models in the Azure OpenAI Service can take in and generate both text and code. These models leverage Azure’s text content filtering models to detect and prevent harmful content. This system works on both prompt and completion. 
+Text models in the Azure OpenAI Service can take in and generate both text and code. These models leverage Azure’s text content filters to detect and prevent harmful content. This system works on both prompts and completions. 
 
 | Risk Category                             | Prompt/Completion      | Severity Threshold |
 |-------------------------------------------|------------------------|---------------------|
@@ -35,9 +35,9 @@ Text models in the Azure OpenAI Service can take in and generate both text and c
 
 
 
-## Vision models: GPT-4o, GPT-4 Turbo, DALL-E 3, DALL-E 2
+## Vision models
 
-### GPT-4o and GPT-4 Turbo
+### Vision-enabled chat models
 
 | Risk Category                                        | Prompt/Completion      | Severity Threshold |
 |------------------------------------------------------|------------------------|---------------------|
@@ -48,7 +48,7 @@ Text models in the Azure OpenAI Service can take in and generate both text and c
 | Identification of Individuals and Inference of Sensitive Attributes | Prompts                | N/A                 |
 | User prompt injection attack (Jailbreak)             | Prompts                | N/A                 |
 
-### DALL-E 3 and DALL-E 2
+### Image generation models
 
 
 | Risk Category                                     | Prompt/Completion      | Severity Threshold |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新默认内容安全政策文档"
}
```

### Explanation
本次修改对Azure OpenAI服务中的默认内容安全政策文档进行了小幅更新，具体包括以下内容：

1. **描述扩展**：将文档描述由“Learn about the default content safety policies that Azure OpenAI uses to flag content.”更新为“Learn about the default content safety policies that Azure OpenAI uses to flag content and ensure responsible use of the service.”，增加了对服务负责任使用的强调。

2. **日期更新**：将最后修改日期从“2024年7月15日”更改为“2025年2月20日”，确保文档反映最新信息。

3. **语言调整**：在文档中对某些措辞进行了细微调整，例如，将“default safety”修改为“default safety policies”，增强了术语的准确性。同时对描述风险类别的句子进行调整，使其更流畅易懂。

4. **节标题修改**：将“文本模型: GPT-4, GPT-3.5”更新为“文本模型”和“视觉模型: GPT-4o, GPT-4 Turbo, DALL-E 3, DALL-E 2”更新为“视觉启用聊天模型”和“图像生成模型”，以使标题更具通用性和准确性。

这些改动提升了文档的清晰度，并增强了对用户的指导和服务的责任意识。

## articles/ai-services/openai/concepts/gpt-4-v-prompt-engineering.md{#item-fd7772}

<details>
<summary>Diff</summary>
````diff
@@ -6,28 +6,27 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 06/10/2024
+ms.date: 02/20/2025
 manager: nitinme
 ---
 
 # Image prompt engineering techniques
 
 To unlock the full potential of vision-enabled chat models like GPT-4 Turbo with Vision and GPT-4o, it's essential to tailor the prompts to your specific needs. Here are some guidelines to enhance the accuracy and efficiency of your prompts.
 
-## Fundamentals of crafting your image prompt
+## Fundamentals of writing an image prompt
+
 - **Contextual specificity:** Adding context to the scenario at hand gives the model a better understanding of an appropriate output. This level of specificity aids in focusing on relevant aspects and avoiding extraneous details.
 - **Task-oriented prompts:** Focusing on a specific task helps the model to develop the output while taking that perspective into consideration.  
-- **Handling refusals:** When the model indicates an inability to perform a task, refining the prompt can be an effective solution. More specific prompts can guide the model towards a clearer understanding and better execution of the task. Some tips to keep in mind: 
-
+- **Handle refusals:** When the model indicates an inability to perform a task, refining the prompt can be an effective solution. More specific prompts can guide the model towards a clearer understanding and better execution of the task. Keep these tips in mind:
    - Request explanations for generated responses to enhance transparency in the model's output <br>
    - If using a single-image prompt, place the image before the text <br>
    - Ask the model to describe the image in details first and complete your specific task from the description
-
-- **Adding examples:** Add examples that represent the type of responses you're looking for
+- **Add examples:** Add examples that represent the type of responses you're looking for
 - **Break down requests:** Try breaking down complex requests step-by-step to create manageable sub-goals
 - **Define output format:** Clearly mention the desired format for the output, such as markdown, JSON, HTML, etc. You can also suggest a specific structure, length, or specific attributes about the response.
 
-## Example prompt inputs & outputs
+## Example prompt inputs and outputs
 There are many ways to craft system prompts to tailor the output specifically to your needs. The following sample inputs and outputs showcase how adjusting your prompts can give you different results. Try out the model for yourself using these images and adjusting the system prompt in the [Azure AI Foundry playground](https://ai.azure.com/).
 
 ### Contextual specificity  
@@ -62,7 +61,7 @@ When you receive a refusal output from a model, there are several ways you can t
 
 |Follow up prompt|Model output|
 |--------|-----------|
-| What information do you need? | `I apologize for the confusion earlier. To plan a meal based on the image provided, I would need to know the following: <br> The number of people dining to ensure there's enough food for everyone.<br>Any dietary restrictions or preferences to accommodate (e.g., vegetarian, gluten-free, allergies).<br>The occasion or type of meal being planned (e.g., casual lunch, dinner party).<br>Desired balance of the meal in terms of food groups and nutrition. <br>With this information, I could help suggest how to portion the dishes, create a balanced meal, and ensure the meal meets the needs of all diners.`|
+| What information do you need? | `I apologize for the confusion earlier. To plan a meal based on the image provided, I would need to know the following: `<br> `The number of people dining to ensure there's enough food for everyone.`<br>`Any dietary restrictions or preferences to accommodate (e.g., vegetarian, gluten-free, allergies).`<br>`The occasion or type of meal being planned (e.g., casual lunch, dinner party).`<br>`Desired balance of the meal in terms of food groups and nutrition.` <br>`With this information, I could help suggest how to portion the dishes, create a balanced meal, and ensure the meal meets the needs of all diners.`|
 
 ### Adding examples
 
@@ -74,17 +73,17 @@ When you receive a refusal output from a model, there are several ways you can t
 |--------|-----------|
 | Q: What is this dog breed and what is its average weight & height? :::image type="content" source="../media/concepts/gpt-v/pomeranian.jpg" lightbox="../media/concepts/gpt-v/pomeranian.jpg" alt-text="Photograph of a pomeranian dog sitting on a pool diving board above the water."::: <br> A: Breed: pomeranian; average weight: 3-7 lbs; average height: 8-14 inches <br> :::image type="content" source="../media/concepts/gpt-v/bulldog.jpg" lightbox="../media/concepts/gpt-v/bulldog.jpg" alt-text="Photograph of a bulldog laying in a grass field."::: A: Breed: English bulldog; average weight: 50-55 lbs; average height: 14-16 inches :::image type="content" source="../media/concepts/gpt-v/border-collie.jpg" lightbox="../media/concepts/gpt-v/border-collie.jpg" alt-text="Photograph of a border collie dog laying in a grass field."::: | `A: Breed: Border Collie; average weight: 30-45 lbs; average height: 18-22 inches` |
 
-### Break down requests
+### Breaking down requests
 
 |Basic prompt|Model output|
 |--------|-----------|
-| :::image type="content" source="../media/concepts/gpt-v/grandmas-recipe.jpg" lightbox="../media/concepts/gpt-v/grandmas-recipe.jpg" alt-text="Photograph of a recipe card detailing a recipe for Grandma's Apple Crumb Pie."::: What's in this image? | `The image contains a recipe card titled Grandma's Apple Crumb Pie. It is noted as a Thanksgiving tradition. The ingredients listed are:<br><ul><li>5-7 apples, peeled </li> <li>1 9-inch pie crust</li> <li>	3/4 cups sugar </li> <li>1/2 teaspoon cinnamon </li> <li>A dash of salt </li><li>	3 tablespoons butter</li> </ul><br>Please note that the measurements for sugar, flour, and butter are not fully visible in the image. If needed, the standard measures for a crumb topping typically involve a combination of sugar, flour, and butter mixed together until crumbly`|
+| :::image type="content" source="../media/concepts/gpt-v/grandmas-recipe.jpg" lightbox="../media/concepts/gpt-v/grandmas-recipe.jpg" alt-text="Photograph of a recipe card detailing a recipe for Grandma's Apple Crumb Pie."::: What's in this image? | `The image contains a recipe card titled Grandma's Apple Crumb Pie. It is noted as a Thanksgiving tradition. The ingredients listed are:`<br><ul><li>`5-7 apples, peeled `</li> <li>`1 9-inch pie crust`</li> <li>`	3/4 cups sugar `</li> <li>`1/2 teaspoon cinnamon `</li> <li>`A dash of salt `</li><li>`	3 tablespoons butter`</li> </ul><br>`Please note that the measurements for sugar, flour, and butter are not fully visible in the image. If needed, the standard measures for a crumb topping typically involve a combination of sugar, flour, and butter mixed together until crumbly`|
 
 |Updated prompt|Model output|
 |----------|-----------|
-| :::image type="content" source="../media/concepts/gpt-v/grandmas-recipe.jpg" lightbox="../media/concepts/gpt-v/grandmas-recipe.jpg" alt-text="Photograph of a recipe card detailing a recipe for Grandma's Apple Crumb Pie."::: You are an AI assistant that helps create shopping lists and instructions to prep food based on recipes. Please provide a list of ingredients to purchase, and then provide a list of instructions to prep the ingredients based on the recipe card.  | Sure, here's a shopping list based on the recipe card: <br>Shopping List:</br><ol><li>5-7 apples (suitable for baking, such as Granny Smith or Honeycrisp)</li> <li>1 ready-made pie crust (9-inch) </li><li>Sugar (ensure you have enough for 1/2 cup)</li><li>All-purpose flour (ensure you have enough for 3/4 cup)</li><li>Ground cinnamon</li><li>Salt</li><li>Butter (ensure you have enough for 3 tablespoons)</li></ol><br>Prepping Instructions:</br> <ul><li>Peel the apples: Using a vegetable peeler or a paring knife, remove the skin from 5 to 7 apples.</li><li>Core and slice the apples: Cut the apples into quarters, remove the cores, and then slice the apple quarters into thin slices or chunks, depending on your preference for pie texture.</li><li>Measure the ingredients: Measure out 3/4 cups of sugar, 1/2 teaspoon of cinnamon, a dash of salt (about 1/8 teaspoon), and 3 tablespoons of butter.</li><li>Prepare the butter: Cut the butter into small pieces to be ready to use in the recipe. It's often easier to incorporate if it's cold, so keep it refrigerated until it's needed</li><li>Preheat your oven according to the pie recipe's instructions so it's ready for baking once your pie is assembled.</li><li>Remember to follow any additional preparation or baking instructions provided in the full recipe for your apple crumb pie.</li></ul> |
+| :::image type="content" source="../media/concepts/gpt-v/grandmas-recipe.jpg" lightbox="../media/concepts/gpt-v/grandmas-recipe.jpg" alt-text="Photograph of a recipe card detailing a recipe for Grandma's Apple Crumb Pie."::: You are an AI assistant that helps create shopping lists and instructions to prep food based on recipes. Please provide a list of ingredients to purchase, and then provide a list of instructions to prep the ingredients based on the recipe card.  | `Sure, here's a shopping list based on the recipe card: `<br>`Shopping List:`</br><ol><li>`5-7 apples (suitable for baking, such as Granny Smith or Honeycrisp)`</li> <li>`1 ready-made pie crust (9-inch) `</li><li>`Sugar (ensure you have enough for 1/2 cup)`</li><li>`All-purpose flour (ensure you have enough for 3/4 cup)`</li><li>`Ground cinnamon`</li><li>`Salt`</li><li>`Butter (ensure you have enough for 3 tablespoons)`</li></ol><br>`Prepping Instructions:`</br> <ul><li>`Peel the apples: Using a vegetable peeler or a paring knife, remove the skin from 5 to 7 apples.`</li><li>`Core and slice the apples: Cut the apples into quarters, remove the cores, and then slice the apple quarters into thin slices or chunks, depending on your preference for pie texture.`</li><li>`Measure the ingredients: Measure out 3/4 cups of sugar, 1/2 teaspoon of cinnamon, a dash of salt (about 1/8 teaspoon), and 3 tablespoons of butter.`</li><li>`Prepare the butter: Cut the butter into small pieces to be ready to use in the recipe. It's often easier to incorporate if it's cold, so keep it refrigerated until it's needed`</li><li>`Preheat your oven according to the pie recipe's instructions so it's ready for baking once your pie is assembled.`</li><li>`Remember to follow any additional preparation or baking instructions provided in the full recipe for your apple crumb pie.`</li></ul> |
 
-### Define output format
+### Defining output format
 
 |Basic prompt|Model output|
 |----------|-----------|
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GPT-4提示工程文档"
}
```

### Explanation
本次修改对关于GPT-4提示工程的文档进行了小幅更新，具体改动包括：

1. **日期更新**：将文档的最后修改日期从“2024年6月10日”更改为“2025年2月20日”，确保文档反映最新的信息。

2. **标题调整**：将“Fundamentals of crafting your image prompt”更新为“Fundamentals of writing an image prompt”，以及其他相关标题的措辞也进行了相应的调整，增强了表述的一致性和清晰度。

3. **内容优化**：在一些段落中对措辞进行了修改，以提高可读性和准确性。例如，“Handling refusals”调整为“Handle refusals”，使语句更直接。

4. **结构调整**：某些部分的内容结构和语言描述进行了更改，以增强逻辑性和易于理解性。例如，“Define output format”更新为“Defining output format”，使其更符合原文的表述。

5. **示例扩展**：在示例部分，延续了对输出格式的定义，增加了关于如何处理模型拒绝输出的具体案例，并给出了明确的指导。

这些更改使文档更具可读性，同时也帮助用户更好地理解和利用Prompt Engineering技术。

## articles/ai-services/openai/concepts/gpt-with-vision.md{#item-991388}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 09/24/2024
+ms.date: 02/20/2025
 manager: nitinme
 ---
 
@@ -18,7 +18,7 @@ To try out vision-enabled chat models, see the [quickstart](/azure/ai-services/o
 
 ## Vision-enabled chats
 
-The vision-enabled models answer general questions about what's present in the images or videos you upload.
+The vision-enabled models answer general questions about what's present in the images you upload.
 
 
 ## Special pricing information
@@ -51,7 +51,7 @@ For a typical use case, take an image with both visible objects and text and a 1
 | Output Tokens      | 100 tokens (assumed)    | $0.003       |
 | **Total** |  |**$0.00955** |
 
-
+<!--
 ### Example video price calculation
 
 > [!IMPORTANT]
@@ -68,6 +68,7 @@ For a typical use case, take a 3-minute video with a 100-token prompt input. The
 | **Total**      |      | **$0.03025** |
 
 Additionally, there's a one-time indexing cost of $0.15 to generate the Video Retrieval index for this 3-minute video. This index can be reused across any number of Video Retrieval and GPT-4 Turbo with Vision API calls.
+-->
 
 ## Input limitations
 
@@ -79,16 +80,18 @@ This section describes the limitations of vision-enabled chat models.
 - **Low resolution accuracy**: When images are analyzed using the "low resolution" setting, it allows for faster responses and uses fewer input tokens for certain use cases. However, this could impact the accuracy of object and text recognition within the image.
 - **Image chat restriction**: When you upload images in Azure AI Foundry portal or the API, there is a limit of 10 images per chat call.
 
+<!--
 ### Video support
 
 - **Low resolution**: Video frames are analyzed using GPT-4 Turbo with Vision's "low resolution" setting, which may affect the accuracy of small object and text recognition in the video.
 - **Video file limits**: Both MP4 and MOV file types are supported. In Azure AI Foundry portal, videos must be less than 3 minutes long. When you use the API there is no such limitation.
 - **Prompt limits**: Video prompts only contain one video and no images. In Azure AI Foundry portal, you can clear the session to try another video or images.
 - **Limited frame selection**: The service selects 20 frames from the entire video, which might not capture all the critical moments or details. Frame selection can be approximately evenly spread through the video or focused by a specific video retrieval query, depending on the prompt.
 - **Language support**: The service primarily supports English for grounding with transcripts. Transcripts don't provide accurate information on lyrics in songs.
+-->
 
 ## Next steps
 
 - Get started using vision-enabled models by following the [quickstart](/azure/ai-services/openai/gpt-v-quickstart).
-- For a more in-depth look at the APIs, and to use video prompts in chat, follow the [how-to guide](../how-to/gpt-with-vision.md).
+- For a more in-depth look at the APIs, follow the [how-to guide](../how-to/gpt-with-vision.md).
 - See the [completions and embeddings API reference](../reference.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GPT与视觉模型文档"
}
```

### Explanation
本次修改对《GPT与视觉模型》文档进行了小幅更新，主要内容包括：

1. **日期更新**：文档的最后修改日期从“2024年9月24日”更改为“2025年2月20日”，以便与最新信息保持一致。

2. **语言优化**：将“videos you upload”更改为“images you upload”，删去了对视频的引用，以提高叙述的清晰度。

3. **内容删除**：注释掉了与示例视频价格计算相关的内容，这部分被放置在了一个注释块中，反映了该部分在当前文档中的非主要作用。

4. **说明简化**：在输入限制部分，文档调整了一些内容以便更清晰表达对图像和视频支持的描述，并删除了某些不必要的细节。

5. **下一步指引调整**：更新了对后续步骤的指导，将重点放在如何获取关于API的进一步信息，删除了有关视频提示的内容，以缩小文档的聚焦范围。

这些修改提高了文档的准确性和可读性，同时确保用户能更容易地理解使用视觉启用模型的相关信息。

## articles/ai-services/openai/concepts/prompt-transformation.md{#item-21e047}

<details>
<summary>Diff</summary>
````diff
@@ -6,13 +6,13 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 07/16/2024
+ms.date: 02/20/2025
 manager: nitinme
 ---
 
 # What is prompt transformation?
 
-Prompt transformation is a process in DALL-E 3 image generation that applies a safety and quality system message to your original prompt using a large language model (LLM) call before being sent to the model for image generation. This system message enriches your original prompt with the goal of generating more diverse and higher-quality images, while maintaining intent. 
+Prompt transformation is a process included in DALL-E 3 image generation that applies a safety and quality system message to your original prompt. It uses a large language model (LLM) call to add the message before sending your prompt to the model for image generation. This system message enriches your original prompt with the goal of generating more diverse and higher-quality images while maintaining intent. 
 
 After prompt transformation is applied to the original prompt, content filtering is applied as a secondary step before image generation; for more information, see [Content filtering](./content-filter.md).
 
@@ -21,7 +21,6 @@ After prompt transformation is applied to the original prompt, content filtering
 
 ## Prompt transformation example
 
-
 | **Example text prompt** | **Example generated image without prompt transformation** | **Example generated image with prompt transformation** |
 |---|---|---|
 |"Watercolor painting of the Seattle skyline" | ![Watercolor painting of the Seattle skyline (simple).](../media/how-to/generated-seattle.png) | ![Watercolor painting of the Seattle skyline, with more detail and structure.](../media/how-to/generated-seattle-prompt-transformed.png) |
@@ -39,12 +38,10 @@ Prompt transformation is applied by default to all Azure OpenAI DALL-E 3 request
 
 Like image generation, prompt transformation is non-deterministic due to the nature of large language models. A single original prompt may lead to many image variants.
 
-
 ## View prompt transformations
 
 Your revised or transformed prompt is visible in the API response object as shown here, in the `revised_prompt` field. 
 
-
 ```json
 Input Content:
 {
@@ -69,7 +66,7 @@ Output Content:
 }
 ```
 
+## Next step
 
-## Next steps
-
-* [DALL-E quickstart](/azure/ai-services/openai/dall-e-quickstart)
\ No newline at end of file
+> [!div class="nextstepaction"]
+> [DALL-E quickstart](/azure/ai-services/openai/dall-e-quickstart)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新提示转换文档"
}
```

### Explanation
本次修改对《提示转换》文档进行了小幅更新，主要内容如下：

1. **日期更新**：文档的最后修改日期从“2024年7月16日”更改为“2025年2月20日”，确保信息的时效性。

2. **表述优化**：对提示转换的定义进行了语言简化和结构优化，将其描述中的某些语句进行了重新排列，使信息更加流畅和易于理解。例如，将“它使用大型语言模型（LLM）调用以添加消息”调整为“它使用一个大型语言模型（LLM）调用来添加消息”。

3. **内容压缩**：在示例部分，删除了一些无关的空行，以使格式更紧凑，同时保持信息的完整性。

4. **次要部分调整**：将“下一步”部分的标题调整为“下一个步骤”，并将其内容格式化为更为简洁的指引形式，以突出关键操作。

5. **说明删减**：删除了一些不必要的行，优化了示例代码的显示，使其更加整洁。

这些更改提高了文档的可读性和清晰性，并确保用户能够快速获取有关提示转换的必要信息。

## articles/ai-services/openai/concepts/safety-system-message-templates.md{#item-460532}

<details>
<summary>Diff</summary>
````diff
@@ -4,7 +4,7 @@ titleSuffix: Azure OpenAI Service
 description: This article contains recommended safety system messages for your generative AI systems, to help reduce the propensity of harm in various concern areas.
 ms.service: azure-ai-openai
 ms.topic: conceptual
-ms.date: 09/20/2024
+ms.date: 02/20/2025
 ms.custom:
 manager: nitinme
 author: PatrickFarley
@@ -14,14 +14,14 @@ ms.author: pafarley
 
 # Safety system message templates
 
-
 This article contains recommended safety system messages for your generative AI systems, to help reduce the propensity of harm in various concern areas. Before you begin evaluating and integrating your safety system messages, visit the [Safety System Message documentation](/azure/ai-services/openai/concepts/system-message) to get started.  
 
-Note that using a safety system message is one of many techniques that can be used for mitigations risks in AI systems, and different from [Azure AI Content Safety](/azure/ai-services/content-safety/overview). 
+> [!NOTE]
+> Using a safety system message is one of many techniques that can be used for mitigations risks in AI systems and is different from [Azure AI Content Safety](/azure/ai-services/content-safety/overview). 
 
 ## Recommended system messages 
 
-Below are examples of recommended system message components you can include to potentially mitigate various harms in your system. 
+Below are examples of recommended system message components you can include to potentially mitigate various harms in your AI system. 
 
 | Category | Component | When this concern area may apply |
 | --- | --- | --- |
@@ -32,7 +32,7 @@ Below are examples of recommended system message components you can include to p
 
 ## Add safety system messages in Azure AI Foundry portal 
 
-The following steps show how to leverage safety system messages in Azure AI Foundry portal.  
+The following steps show how to leverage safety system messages in [Azure AI Foundry portal](https://ai.azure.com/).
 
 1. Go to Azure AI Foundry and navigate to Azure OpenAI and the Chat playground.
     :::image type="content" source="../media/navigate-chat-playground.PNG" alt-text="Screenshot of the Azure AI Foundry portal selection.":::
@@ -47,8 +47,8 @@ The following steps show how to leverage safety system messages in Azure AI Foun
 
 
 > [!NOTE]
-> If you are using a safety system message that is not integrated into the studio by default, simply copy the appropriate component and paste it in the safety system message section, or the system message section. Repeat steps 4 and 5 for optimal performance and safety. 
+> If you're using a safety system message that is not integrated into the studio by default, simply copy the appropriate component and paste it in the safety system message section, or the system message section. Repeat steps 4 and 5 for optimal performance and safety. 
 
 ## Evaluation 
 
-We recommend informing your safety system message approach based on an iterative process of identification and evaluation. Learn more in our [Safety System Message documentation](/azure/ai-services/openai/concepts/system-message). 
\ No newline at end of file
+We recommend adjusting your safety system message approach based on an iterative process of identification and evaluation. Learn more in our [Safety System Message documentation](/azure/ai-services/openai/concepts/system-message). 
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新安全系统消息模板文档"
}
```

### Explanation
本次修改针对《安全系统消息模板》文档进行了小幅更新，具体内容包括：

1. **日期更新**：文档最后的修改日期由“2024年9月20日”更新为“2025年2月20日”，确保信息的及时性和准确性。

2. **注释形式调整**：将原文中关于使用安全系统消息注意事项的表述格式化为注释块（Note），使得提醒内容更加突出和易于阅读。

3. **表述简化**：对文中一些句子的表达进行了优化，例如，将“在各种关注领域减少伤害倾向”的描述进行了小幅调整，以增强句子的流畅性。

4. **链接清晰化**：在提及“Azure AI Foundry portal”时，提供了直接的链接，增强用户体验，使用户能够更方便地导航到相关页面。

5. **部分内容修改**：更新了部分句子的措辞，例如，将“安全系统消息的方法”改为“安全系统消息的调整”，以更好地反映内容的意图。

这些更改不仅提升了文档的清晰度和可读性，还确保用户能更准确地理解和实施安全系统消息的相关实践。

## articles/ai-services/openai/dall-e-quickstart.md{#item-fcd528}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.custom: devx-track-python, devx-track-dotnet, devx-track-extended-java, devx-
 ms.topic: quickstart
 author: PatrickFarley
 ms.author: pafarley
-ms.date: 09/06/2024
+ms.date: 02/20/2025
 zone_pivot_groups: openai-quickstart-dall-e
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新DALL-E快速入门文档"
}
```

### Explanation
本次对《DALL-E快速入门》文档进行了小幅修改，具体内容如下：

1. **日期更新**：文档的最后修改日期从“2024年9月6日”更改为“2025年2月20日”，确保所有信息的时效性，以反映最新的内容和指引。

通过这一简化的更新，文档保持了相关性，并确保用户获取的都是最新的信息，以便于他们在使用DALL-E服务时能够依据最新的指导进行操作。

## articles/ai-services/openai/gpt-v-quickstart.md{#item-2a6183}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.custom: devx-track-python, devx-track-js, devx-track-ts
 ms.topic: quickstart
 author: PatrickFarley
 ms.author: pafarley
-ms.date: 10/03/2024
+ms.date: 02/20/2025
 zone_pivot_groups: openai-quickstart-gpt-v
 ---
 
@@ -64,7 +64,7 @@ Get started using GPT-4 Turbo with images with the Azure OpenAI Service.
 
 ## Next steps
 
-* [Get started with multimodal vision chat apps using Azure OpenAI](/azure/developer/ai/get-started-app-chat-vision?tabs=github-codespaces) AI App template
+* [Get started with multimodal vision chat apps using Azure OpenAI AI App template](/azure/developer/ai/get-started-app-chat-vision?tabs=github-codespaces)
 * Learn more about these APIs in the [Vision-enabled models how-to guide](./gpt-v-quickstart.md)
 * [GPT-4 Turbo with Vision frequently asked questions](./faq.yml#gpt-4-turbo-with-vision)
 * [GPT-4 Turbo with Vision API reference](https://aka.ms/gpt-v-api-ref)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GPT-V快速入门文档"
}
```

### Explanation
本次修改对《GPT-V快速入门》文档进行了小幅更新，具体包括以下几个方面：

1. **日期更新**：文档中的最后修改日期从“2024年10月3日”更新为“2025年2月20日”，以确保信息的时效性。

2. **链接描述格式调整**：在“下一步”部分的链接描述中，原先的表述顺序进行了优化，将“使用Azure OpenAI的多模态视觉聊天应用程序”放置在前面，之后跟上“AI App模板”。这种调整提高了句子的逻辑性和易读性，使用户能够更清楚地理解链接所指向的内容。

这些小幅度的调整保持了文档的清晰性，并确保用户获取到最新的信息，从而在使用GPT-V服务时能获得更好的指引。

## articles/ai-services/openai/how-to/dall-e.md{#item-ac9616}

<details>
<summary>Diff</summary>
````diff
@@ -1,20 +1,20 @@
 ---
-title: How to work with DALL-E models 
+title: How to use DALL-E models 
 titleSuffix: Azure OpenAI Service
-description: Learn about the options for how to use the DALL-E image generation models.
+description: Learn how to generate images with the DALL-E models, and learn about the configuration options that are available.
 author: PatrickFarley
 ms.author: pafarley 
 ms.service: azure-ai-openai
 ms.custom: 
 ms.topic: how-to
-ms.date: 10/02/2024
+ms.date: 02/20/2025
 manager: nitinme
 keywords: 
 zone_pivot_groups: 
 # Customer intent: as an engineer or hobbyist, I want to know how to use DALL-E image generation models to their full capability.
 ---
 
-# How to work with the DALL-E models
+# How to use the DALL-E models
 
 OpenAI's DALL-E models generate images based on user-provided text prompts. This guide demonstrates how to use the DALL-E models and configure their options through REST API calls.
 
@@ -104,7 +104,7 @@ It's also possible that the generated image itself is filtered. In this case, th
 }
 ```
 
-## Writing image prompts
+## Write image prompts
 
 Your image prompts should describe the content you want to see in the image, and the visual style of image.
 
@@ -124,7 +124,7 @@ Specify the size of the generated images. Must be one of `1024x1024`, `1792x1024
 
 ### Style
 
-DALL-E 3 introduces two style options: `natural` and `vivid`. The natural style is more similar to the DALL-E 2 default style, while the vivid style generates more hyper-real and cinematic images.
+DALL-E 3 offers two style options: `natural` and `vivid`. The natural style is more similar to the default style of older models, while the vivid style generates more hyper-real and cinematic images.
 
 The natural style is useful in cases where DALL-E 3 over-exaggerates or confuses a subject that's meant to be more simple, subdued, or realistic.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新DALL-E使用文档"
}
```

### Explanation
这次对《如何使用DALL-E模型》的文档进行了小幅更新，主要修改包括：

1. **标题和描述调整**：
   - 文档标题由“如何处理DALL-E模型”更改为“如何使用DALL-E模型”，使标题更加简洁并更好地反映文档内容。
   - 描述从“了解如何使用DALL-E图像生成模型的选项”更新为“了解如何使用DALL-E模型生成图像，以及可用的配置选项”，增加了对内容的清晰描述。

2. **日期更新**：文档的日期更新为“2025年2月20日”，以确保信息的时效性。

3. **内容一致性和用词改善**：
   - 将子标题“写图像提示”更改为“编写图像提示”，使其用词更加一致和标准化。
   - 将有关DALL-E 3样式选项的描述中用词从“引入”更改为“提供”，使句子更清晰易懂，并更好地描述了新模型的特性。

通过这些小幅度的修改，文档保持了准确性和清晰性，将帮助用户更有效地理解和使用DALL-E模型及其功能。

## articles/ai-services/openai/how-to/gpt-with-vision.md{#item-4d8502}

<details>
<summary>Diff</summary>
````diff
@@ -1,35 +1,36 @@
 ---
 title: How to use vision-enabled chat models
 titleSuffix: Azure OpenAI Service
-description: Learn about the options for using vision-enabled chat models
+description: Learn how to use vision-enabled chat models in Azure OpenAI Service, including how to call the Chat Completion API and process images.
 author: PatrickFarley #dereklegenzoff
 ms.author: pafarley #delegenz
+#customer intent: As a developer, I want to learn how to use vision-enabled chat models so that I can integrate image processing capabilities into my applications.
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 08/21/2024
+ms.date: 02/20/2025
 manager: nitinme
 ---
 
 # Use vision-enabled chat models
 
 
-Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are [o1](./reasoning.md), GPT-4o, and GPT-4o-mini, GPT-4 Turbo with Vision.
+Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are [o1](./reasoning.md), GPT-4o, GPT-4o-mini, and GPT-4 Turbo with Vision.
 
-The vision-enabled models answer general questions about what's present in the images you upload.
+The vision-enabled models can answer general questions about what's present in the images you upload.
 
 > [!TIP]
 > To use vision-enabled models, you call the Chat Completion API on a supported model that you have deployed. If you're not familiar with the Chat Completion API, see the [Vision-enabled chat how-to guide](/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-completions).
 
 ## Call the Chat Completion APIs
 
-The following command shows the most basic way to use the GPT-4 Turbo with Vision model with code. If this is your first time using these models programmatically, we recommend starting with our [GPT-4 Turbo with Vision quickstart](../gpt-v-quickstart.md). 
+The following command shows the most basic way to use a vision-enabled chat model with code. If this is your first time using these models programmatically, we recommend starting with our [Chat with images quickstart](../gpt-v-quickstart.md). 
 
 #### [REST](#tab/rest)
 
 Send a POST request to `https://{RESOURCE_NAME}.openai.azure.com/openai/deployments/{DEPLOYMENT_NAME}/chat/completions?api-version=2024-02-15-preview` where 
 
 - RESOURCE_NAME is the name of your Azure OpenAI resource 
-- DEPLOYMENT_NAME is the name of your GPT-4 Turbo with Vision model deployment 
+- DEPLOYMENT_NAME is the name of your model deployment 
 
 **Required headers**: 
 - `Content-Type`: application/json 
@@ -75,7 +76,7 @@ The following is a sample request body. The format is the same as the chat compl
 #### [Python](#tab/python)
 
 1. Define your Azure OpenAI resource endpoint and key. 
-1. Enter the name of your GPT-4 Turbo with Vision model deployment.
+1. Enter the name of your model deployment.
 1. Create a client object using those values.
 
     ```python
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新视觉启用聊天模型使用指南"
}
```

### Explanation
本次对《如何使用视觉启用聊天模型》的文档进行了小幅更新，主要修改内容如下：

1. **描述改进**：文档的描述部分从“了解使用视觉启用聊天模型的选项”更新为“了解如何在Azure OpenAI服务中使用视觉启用聊天模型，包括如何调用聊天完成API和处理图像”，使其更加具体和直观。

2. **顾客意图添加**：新增了一条顾客意图，以帮助开发者了解使用视觉启用聊天模型的目的：“作为开发者，我希望学习如何使用视觉启用聊天模型，以便将图像处理能力集成到我的应用中。”

3. **日期更新**：文档的日期更新为“2025年2月20日”，确保信息的时效性。

4. **内容细节调整**：
   - 原先的描述“GPT-4 Turbo与视觉模型”被简化为“视觉启用聊天模型”，以涵盖所有相关模型。
   - 对于调用聊天完成API的描述，进一步优化了表述，将“GPT-4 Turbo与视觉模型”改为“视觉启用聊天模型”，以增强通用性。
   - 在命令示例和Python示例中，删除了特定模型名称的引用，转而使用“模型部署的名称”，提高了文档的灵活性和适应性。

这些微小的修改增强了文档的清晰度和可用性，为用户提供了更好的指导，帮助他们更有效地利用视觉启用聊天模型。

## articles/ai-services/openai/how-to/risks-safety-monitor.md{#item-b2be0b}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley 
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 12/05/2024
+ms.date: 02/20/2025
 manager: nitinme
 ---
 
@@ -50,13 +50,13 @@ To use Potentially abusive user detection, you need:
 - A content filter configuration applied to your deployment.
 - You must be sending user ID information in your Chat Completion requests (see the _user_ parameter of the [Completions API](/azure/ai-services/openai/reference#completions), for example).
     > [!CAUTION]
-    > Use GUID strings to identify individual users. Do not include sensitive personal information in the "user" field.
+    > Use GUID strings to identify individual users. Do not include sensitive personal information in the _user_ field.
 - An Azure Data Explorer database set up to store the user analysis results (instructions below).
 
 ### Set up your Azure Data Explorer database
 
 In order to protect the data privacy of user information and manage the permission of the data, we support the option for our customers to bring their own storage to get the detailed potentially abusive user detection insights (including user GUID and statistics on harmful request by category) stored in a compliant way and with full control. Follow these steps to enable it:
-1. In Azure AI Foundry, navigate to the model deployment that you'd like to set up user abuse analysis with, and select **Add a data store**. 
+1. In [Azure AI Foundry](https://ai.azure.com/), navigate to the model deployment that you'd like to set up user abuse analysis with, and select **Add a data store**. 
 1. Fill in the required information and select **Save**. We recommend you create a new database to store the analysis results.
 1. After you connect the data store, take the following steps to grant permission to write analysis results to the connected database:
     1. Go to your Azure OpenAI resource's page in the Azure portal, and choose the **Identity** tab.
@@ -89,8 +89,9 @@ The potentially abusive user detection relies on the user information that custo
 
 Combine this data with enriched signals to validate whether the detected users are truly abusive or not. If they are, then take responsive action such as throttling or suspending the user to ensure the responsible use of your application.
 
-## Next steps
+## Next step
 
 Next, create or edit a content filter configuration in Azure AI Foundry.
 
-- [Configure content filters with Azure OpenAI Service](/azure/ai-services/openai/how-to/content-filters)
+> [!div class="nextstepaction"]
+> [Configure content filters with Azure OpenAI Service](/azure/ai-services/openai/how-to/content-filters)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新风险和安全监控指南"
}
```

### Explanation
此次修改对《风险和安全监控指南》进行了小幅更新，主要包括以下几个方面：

1. **日期更新**：文档日期已更新为“2025年2月20日”，以确保信息的时效性。

2. **用户识别提示细节修正**：在有关使用潜在滥用用户检测的部分中，强调用户的识别方法，建议使用GUID字符串来标识用户，并明确指出不要在“用户”字段中包括敏感的个人信息。此处的用词进行了细微改动，以增强警告的清晰性。

3. **Azure AI Foundry链接的增加**：在设置Azure Data Explorer数据库的步骤中，增加了对“Azure AI Foundry”的直接链接，使用户能更方便地找到相关资源。

4. **标题修正**：将“下一个步骤”修正为“下一个步骤”，稍做修正以保持标题的一致性。

5. **下一步操作增强**：在关于下一步的部分，将内容过滤器配置的说明格式化为引用块，以强调和提升其可读性。

这些更新使文档更清晰易懂，更加准确地指导用户如何进行风险和安全监控配置，并确保隐私保护。

## articles/ai-services/openai/how-to/use-blocklists.md{#item-e99db7}

<details>
<summary>Diff</summary>
````diff
@@ -6,14 +6,14 @@ description: Learn how to use blocklists with Azure OpenAI Service
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 12/05/2024
+ms.date: 02/20/2025
 author: PatrickFarley
 ms.author: pafarley
 ---
 
 # Use a blocklist with Azure OpenAI
 
-The configurable content filters are sufficient for most content moderation needs. However, you might need to filter terms specific to your use case. 
+The [configurable content filters](/azure/ai-services/openai/how-to/content-filters) available in Azure OpenAI are sufficient for most content moderation needs. However, you might need to filter terms specific to your use case. For this, you can use custom blocklists.
 
 ## Prerequisites
 
@@ -63,7 +63,7 @@ The response code should be `201` (created a new list) or `200` (updated an exis
 
 ### Apply a blocklist to a content filter
 
-If you haven't yet created a content filter, you can do so in Azure AI Foundry. See [Content filtering](/azure/ai-services/openai/how-to/content-filters#create-a-content-filter-in-azure-ai-foundry).
+If you haven't yet created a content filter, you can do so in [Azure AI Foundry](https://ai.azure.com/). See [Content filtering](/azure/ai-services/openai/how-to/content-filters#create-a-content-filter-in-azure-ai-foundry).
 
 To apply a **completion** blocklist to a content filter, use the following cURL command: 
 
@@ -178,7 +178,7 @@ In the below example, a GPT-35-Turbo deployment with a blocklist is blocking the
 } 
 ```
 
-If the completion itself is blocked, the response returns `200`, as the completion only cuts off when the blocklist content is matched. The annotations show that a blocklist was matched. 
+If the completion itself is blocked, the response returns `200`, as the completion only cuts off when the blocklist content is matched. The annotations show that a blocklist item was matched. 
 
 ```json
 { 
@@ -257,7 +257,7 @@ If the completion itself is blocked, the response returns `200`, as the completi
 
 ---
 
-## Next steps
+## Related content
 
 - Learn more about Responsible AI practices for Azure OpenAI: [Overview of Responsible AI practices for Azure OpenAI models](/legal/cognitive-services/openai/overview?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext). 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新如何使用封锁列表指南"
}
```

### Explanation
此次修改对《如何使用封锁列表》的文档进行了小幅更新，主要包括以下内容：

1. **日期更新**：文档日期更新为“2025年2月20日”，以确保信息的时效性。

2. **内容过滤器说明增强**：在提及Azure OpenAI的可配置内容过滤器时，增加了对链接的引用，使用户能够直接访问相关内容过滤器的文档。这一改动使得相关信息更加清晰，便于用户了解如何使用这些工具。

3. **Azure AI Foundry的直接链接**：在创建内容过滤器的部分，文中增加了对“Azure AI Foundry”的直接链接，方便用户快速定位该平台，提升文档的易用性。

4. **术语一致性修正**：在涉及区分匹配的内容时，原文中使用“blocklist matched”更换为“blocklist item matched”，使术语在文档中的使用更为一致，从而提高文档的专业性和准确性。

5. **标题修改**：将“下一步”标题更改为“相关内容”，以更好地反映该部分提供的信息，改进了文档结构。

这些更新旨在提高用户对封锁列表使用过程的理解，确保文档中的信息准确无误，并增加了可用性与易读性。

## articles/ai-services/openai/includes/api-versions/latest-inference-preview.md{#item-24bf0f}

<details>
<summary>Diff</summary>
````diff
@@ -44,9 +44,6 @@ Creates a completion for the provided prompt, parameters and chosen model.
 | logprobs | integer | Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.<br><br>The maximum value for `logprobs` is 5.<br> | No | None |
 | max_tokens | integer | The maximum number of tokens that can be generated in the completion.<br><br>The token count of your prompt plus `max_tokens` can't exceed the model's context length.  | No | 16 |
 | n | integer | How many completions to generate for each prompt.<br><br>**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.<br> | No | 1 |
-| modalities | [ChatCompletionModalities](#chatcompletionmodalities) | Output types that you would like the model to generate for this request.<br>Most models are capable of generating text, which is the default:<br><br>`["text"]`<br><br>The `gpt-4o-audio-preview` model can also be used to generate audio. To<br>request that this model generate both text and audio responses, you can<br>use:<br><br>`["text", "audio"]`<br> | No |  |
-| prediction | [PredictionContent](#predictioncontent) | Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content. | No |  |
-| audio | object | Parameters for audio output. Required when audio output is requested with<br>`modalities: ["audio"]`.  | No |  |
 | presence_penalty | number | Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.<br> | No | 0 |
 | seed | integer | If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.<br><br>Determinism isn't guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.<br> | No |  |
 | stop | string or array | Up to four sequences where the API will stop generating further tokens. The returned text won't contain the stop sequence.<br> | No |  |
@@ -57,20 +54,6 @@ Creates a completion for the provided prompt, parameters and chosen model.
 | user | string | A unique identifier representing your end-user, which can help to monitor and detect abuse.<br> | No |  |
 
 
-### Properties for audio
-
-#### voice
-
-| Name | Type | Description | Default |
-|------|------|-------------|--------|
-| voice | string | Specifies the voice type. Supported voices are `alloy`, `echo`, <br>`fable`, `onyx`, `nova`, and `shimmer`.<br> |  |
-
-#### format
-
-| Name | Type | Description | Default |
-|------|------|-------------|--------|
-| format | string | Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,<br>`opus`, or `pcm16`. <br> |  |
-
 ### Responses
 
 **Status Code:** 200
@@ -334,6 +317,23 @@ Creates a completion for the chat message
 | function_call | string or [chatCompletionFunctionCallOption](#chatcompletionfunctioncalloption) | Deprecated in favor of `tool_choice`.<br><br>Controls which (if any) function is called by the model.<br>`none` means the model won't call a function and instead generates a message.<br>`auto` means the model can pick between generating a message or calling a function.<br>Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.<br><br>`none` is the default when no functions are present. `auto` is the default if functions are present.<br> | No |  |
 | functions | array | Deprecated in favor of `tools`.<br><br>A list of functions the model may generate JSON inputs for.<br> | No |  |
 | user_security_context | [userSecurityContext](#usersecuritycontext) | User security context contains several parameters that describe the AI application itself, and the end user that interacts with the AI application. These fields assist your security operations teams to investigate and mitigate security incidents by providing a comprehensive approach to protecting your AI applications. [Learn more](https://aka.ms/TP4AI/Documentation/EndUserContext) about protecting AI applications using Microsoft Defender for Cloud. | No |  |
+| modalities | [ChatCompletionModalities](#chatcompletionmodalities) | Output types that you would like the model to generate for this request.<br>Most models are capable of generating text, which is the default:<br><br>`["text"]`<br><br>The `gpt-4o-audio-preview` model can also be used to generate audio. To<br>request that this model generate both text and audio responses, you can<br>use:<br><br>`["text", "audio"]`<br> | No |  |
+| prediction | [PredictionContent](#predictioncontent) | Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content. | No |  |
+| audio | object | Parameters for audio output. Required when audio output is requested with<br>`modalities: ["audio"]`.  | No |  |
+
+### Properties for audio
+
+#### voice
+
+| Name | Type | Description | Default |
+|------|------|-------------|--------|
+| voice | string | Specifies the voice type. Supported voices are `alloy`, `echo`, <br>`fable`, `onyx`, `nova`, and `shimmer`.<br> |  |
+
+#### format
+
+| Name | Type | Description | Default |
+|------|------|-------------|--------|
+| format | string | Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,<br>`opus`, or `pcm16`. <br> |  |
 
 ### Responses
 
@@ -4597,9 +4597,6 @@ Information about the content filtering category (hate, sexual, violence, self_h
 | logprobs | integer | Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.<br><br>The maximum value for `logprobs` is 5.<br> | No | None |
 | max_tokens | integer | The maximum number of tokens that can be generated in the completion.<br><br>The token count of your prompt plus `max_tokens` can't exceed the model's context length.  | No | 16 |
 | n | integer | How many completions to generate for each prompt.<br><br>**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.<br> | No | 1 |
-| modalities | [ChatCompletionModalities](#chatcompletionmodalities) | Output types that you would like the model to generate for this request.<br>Most models are capable of generating text, which is the default:<br><br>`["text"]`<br><br>The `gpt-4o-audio-preview` model can also be used to generate audio. To<br>request that this model generate both text and audio responses, you can<br>use:<br><br>`["text", "audio"]`<br> | No |  |
-| prediction | [PredictionContent](#predictioncontent) | Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content. | No |  |
-| audio | object | Parameters for audio output. Required when audio output is requested with<br>`modalities: ["audio"]`.  | No |  |
 | presence_penalty | number | Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.<br> | No | 0 |
 | seed | integer | If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.<br><br>Determinism isn't guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.<br> | No |  |
 | stop | string or array | Up to four sequences where the API will stop generating further tokens. The returned text won't contain the stop sequence.<br> | No |  |
@@ -4610,21 +4607,6 @@ Information about the content filtering category (hate, sexual, violence, self_h
 | user | string | A unique identifier representing your end-user, which can help to monitor and detect abuse.<br> | No |  |
 
 
-### Properties for audio
-
-#### voice
-
-| Name | Type | Description | Default |
-|------|------|-------------|--------|
-| voice | string | Specifies the voice type. Supported voices are `alloy`, `echo`, <br>`fable`, `onyx`, `nova`, and `shimmer`.<br> |  |
-
-#### format
-
-| Name | Type | Description | Default |
-|------|------|-------------|--------|
-| format | string | Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,<br>`opus`, or `pcm16`. <br> |  |
-
-
 ### createCompletionResponse
 
 Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).
@@ -4674,8 +4656,24 @@ Represents a completion response from the API. Note: both the streamed and non-s
 | tool_choice | [chatCompletionToolChoiceOption](#chatcompletiontoolchoiceoption) | Controls which (if any) tool is called by the model. `none` means the model won't call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools. Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool. `none` is the default when no tools are present. `auto` is the default if tools are present. | No |  |
 | function_call | string or [chatCompletionFunctionCallOption](#chatcompletionfunctioncalloption) | Deprecated in favor of `tool_choice`.<br><br>Controls which (if any) function is called by the model.<br>`none` means the model won't call a function and instead generates a message.<br>`auto` means the model can pick between generating a message or calling a function.<br>Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.<br><br>`none` is the default when no functions are present. `auto` is the default if functions are present.<br> | No |  |
 | functions | array | Deprecated in favor of `tools`.<br><br>A list of functions the model may generate JSON inputs for.<br> | No |  |
-| user_security_context | [userSecurityContext](#usersecuritycontext) | User security context contains several parameters that describe the AI application itself, and the end user that interacts with the AI application. These fields assist your security operations teams to investigate and mitigate security incidents by providing a comprehensive approach to protecting your AI applications. [Learn more](https://aka.ms/TP4AI/Documentation/EndUserContext) about protecting AI applications using Microsoft Defender for Cloud. | No |  |
+| user_security_context | [userSecurityContext](#usersecuritycontext) | User security context contains several parameters that describe the AI application itself, and the end user that interacts with the AI application. These fields assist your security operations teams to investigate and mitigate security incidents by providing a comprehensive approach to protecting your AI applications. [Learn more](https://aka.ms/TP4AI/Documentation/EndUserContext) about protecting AI applications using Microsoft Defender for Cloud. | No |  | and detect abuse.<br> | No |  |
+| modalities | [ChatCompletionModalities](#chatcompletionmodalities) | Output types that you would like the model to generate for this request.<br>Most models are capable of generating text, which is the default:<br><br>`["text"]`<br><br>The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To<br>request that this model generate both text and audio responses, you can<br>use:<br><br>`["text", "audio"]`<br> | No |  |
+| prediction | [PredictionContent](#predictioncontent) | Configuration for a [Predicted Output](/docs/guides/predicted-outputs), which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content. | No |  |
+| audio | object | Parameters for audio output. Required when audio output is requested with<br>`modalities: ["audio"]`. [Learn more](/docs/guides/audio).<br> | No |  |
+
+### Properties for audio
+
+#### voice
+
+| Name | Type | Description | Default |
+|------|------|-------------|--------|
+| voice | string | Specifies the voice type. Supported voices are `alloy`, `echo`, <br>`fable`, `onyx`, `nova`, and `shimmer`.<br> |  |
+
+#### format
 
+| Name | Type | Description | Default |
+|------|------|-------------|--------|
+| format | string | Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,<br>`opus`, or `pcm16`. <br> |  |
 
 ### userSecurityContext
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新最新推理预览文档"
}
```

### Explanation
此次修改对《最新推理预览》的文档进行了小幅更新，主要包括以下内容：

1. **日期更新**：文档中的日期已更改为“2025年2月20日”，确保信息的时效性和准确性。

2. **音频输出属性重组**：将音频输出的相关属性重新组织并恢复，包括：
   - "voice"属性，描述可用的声音类型。
   - "format"属性，指明输出音频的格式。
   之前这些属性的相关内容被删除，现在重新加入，使得文档中提供了更完整的音频输出配置指导。

3. **新属性的引入**：介绍了有关音频输出的参数，特别是在请求音频输出时所需的内容。这些更新增强了用户对如何设置和使用音频输出功能的理解。

4. **专业术语一致性修正**：在描述产生的输出类型参数时，确保术语的精准使用，使用户更易于理解功能。

5. **去除冗余内容**：一些不再使用的参数（如某些与音频输出相关的参数）被删除，这有助于简化和清晰化文档。

这些修改旨在增强文档的可用性和准确性，确保用户在使用最新推理 API 时，能够获得完整且明确的指导信息。

## articles/ai-services/openai/includes/dall-e-rest.md{#item-4bac64}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.service: azure-ai-openai
 ms.custom:
   - ignite-2023
 ms.topic: include
-ms.date: 08/08/2023
+ms.date: 02/20/2025
 ---
 
 Use this guide to get started calling the Azure OpenAI Service image generation REST APIs by using Python.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新DALL·E REST API指南日期"
}
```

### Explanation
此次修改对《DALL·E REST API》的文档进行了小幅更新，主要包括以下内容：

1. **日期更新**：文档中的日期已更改为“2025年2月20日”，以反映最新的更新和文档状态，确保用户获取的信息是最新的。

该修改小而简单，重点在于确保文档中的日期信息准确，帮助用户理解该指南的更新时间。

## articles/ai-services/openai/includes/fine-tuning-openai-in-ai-studio.md{#item-723c8d}

<details>
<summary>Diff</summary>
````diff
@@ -64,14 +64,21 @@ Your training data and validation data sets consist of input and output examples
 
 The training and validation data you use **must** be formatted as a JSON Lines (JSONL) document. For `gpt-35-turbo-0613` the fine-tuning dataset must be formatted in the conversational format that is used by the [Chat completions](../how-to/chatgpt.md) API.
 
-If you would like a step-by-step walk-through of fine-tuning a `gpt-35-turbo-0613` model please refer to the [Azure OpenAI fine-tuning tutorial.](../tutorials/fine-tune.md)
+If you would like a step-by-step walk-through of fine-tuning a `gpt-4o-mini-2024-07-18` model please refer to the [Azure OpenAI fine-tuning tutorial.](../tutorials/fine-tune.md)
 
 ### Example file format
 
 ```json
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "Is Xbox better than PlayStation?"}, {"role": "assistant", "content": "I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?"}]}
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "Can I play PC games on my Xbox?"}, {"role": "assistant", "content": "While Xbox consoles don't natively support PC games, they can play an array of Xbox games, many of which are also available on PC. Can I assist you with finding games compatible with your Xbox device?"}]}
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "I'm having trouble connecting my Xbox to the Wi-Fi."}, {"role": "assistant", "content": "No worries, let's go through the network settings on your Xbox. Can you please tell me what happens when you try to connect it to the Wi-Fi?"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who discovered Antarctica?"}, {"role": "assistant", "content": "Some chaps named Fabian Gottlieb von Bellingshausen and Mikhail Lazarev, as if they don't teach that in every school!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the biggest ocean?"}, {"role": "assistant", "content": "The Pacific Ocean. It's not like it's a small pond or anything."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the largest planet?"}, {"role": "assistant", "content": "It's called Jupiter, you might have heard of it...or not."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who painted the Mona Lisa?"}, {"role": "assistant", "content": "Oh, some obscure artist named Leonardo da Vinci. Not that he is famous or anything."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the boiling point of water?"}, {"role": "assistant", "content": "Exciting fact of the day: it's 100 degrees Celsius. Life-changing, isn't it?"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote 'The Odyssey'?"}, {"role": "assistant", "content": "Oh, just some poet named Homer. Nothing special, really."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the largest mammal?"}, {"role": "assistant", "content": "Blue Whale. Quite a big fish isn't it? Oh wait, it's not a fish!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the highest mountain peak?"}, {"role": "assistant", "content": "Everest. Brace yourself, it's not a walk in the park."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who invented the telephone?"}, {"role": "assistant", "content": "A guy named Alexander Graham Bell. Bet you didn't guess that one!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the distance from Earth to the Sun?"}, {"role": "assistant", "content": "About 93 million miles. Just a quick drive, really."}]}
 ```
 
 ### Multi-turn chat file format
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新模型和示例对话"
}
```

### Explanation
此次修改对《在 AI Studio 中微调 OpenAI 模型》的文档进行了小幅更新，主要内容如下：

1. **模型更新**：文档中提到的微调模型已更改为 `gpt-4o-mini-2024-07-18`，以反映最新的模型版本。

2. **示例对话更新**：增加了多个示例对话，展示了新的对话格式和内容。这些对话的角色是“Clippy”，一个提供事实并带有讽刺的聊天机器人，其示例显示了如何在对话中回应用户提问。

3. **数据规范的保持**：文档仍然强调训练和验证数据必须以 JSON Lines (JSONL) 的格式编写，确保与 API 的兼容性。

通过这些更新，文档提供了更现代化的模型信息和示例，帮助用户更好地理解如何使用最新的微调功能。

## articles/ai-services/openai/includes/fine-tuning-python.md{#item-976f58}

<details>
<summary>Diff</summary>
````diff
@@ -65,9 +65,16 @@ If you would like a step-by-step walk-through of fine-tuning a `gpt-35-turbo-061
 ### Example file format
 
 ```json
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "Is Xbox better than PlayStation?"}, {"role": "assistant", "content": "I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?"}]}
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "Can I play PC games on my Xbox?"}, {"role": "assistant", "content": "While Xbox consoles don't natively support PC games, they can play an array of Xbox games, many of which are also available on PC. Can I assist you with finding games compatible with your Xbox device?"}]}
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "I'm having trouble connecting my Xbox to the Wi-Fi."}, {"role": "assistant", "content": "No worries, let's go through the network settings on your Xbox. Can you please tell me what happens when you try to connect it to the Wi-Fi?"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who discovered Antarctica?"}, {"role": "assistant", "content": "Some chaps named Fabian Gottlieb von Bellingshausen and Mikhail Lazarev, as if they don't teach that in every school!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the biggest ocean?"}, {"role": "assistant", "content": "The Pacific Ocean. It's not like it's a small pond or anything."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the largest planet?"}, {"role": "assistant", "content": "It's called Jupiter, you might have heard of it...or not."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who painted the Mona Lisa?"}, {"role": "assistant", "content": "Oh, some obscure artist named Leonardo da Vinci. Not that he is famous or anything."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the boiling point of water?"}, {"role": "assistant", "content": "Exciting fact of the day: it's 100 degrees Celsius. Life-changing, isn't it?"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote 'The Odyssey'?"}, {"role": "assistant", "content": "Oh, just some poet named Homer. Nothing special, really."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the largest mammal?"}, {"role": "assistant", "content": "Blue Whale. Quite a big fish isn't it? Oh wait, it's not a fish!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the highest mountain peak?"}, {"role": "assistant", "content": "Everest. Brace yourself, it's not a walk in the park."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who invented the telephone?"}, {"role": "assistant", "content": "A guy named Alexander Graham Bell. Bet you didn't guess that one!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the distance from Earth to the Sun?"}, {"role": "assistant", "content": "About 93 million miles. Just a quick drive, really."}]}
 ```
 
 ### Multi-turn chat file format
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新微调示例对话和模型信息"
}
```

### Explanation
此次修改对《进行微调的 Python 指南》的文档进行了小幅更新，主要包括以下内容：

1. **示例对话更新**：文档中替换和增加了一些示例对话，新的对话角色为“Clippy”，一个提供事实并带有讽刺的聊天机器人。这些示例展示了如何在不同情况下与用户进行互动。

2. **模型信息更新**：提及的微调模型已经更新，旨在反映最新的可用版本，帮助用户获取更准确的信息。

3. **数据格式保持**：文档继续强调示例对话的格式要求，使用户能够理解如何构造符合规格的输入数据。

通过这些更新，文档为用户提供了最新的模型和示例，使其在微调过程中能够更有效地实现目标。

## articles/ai-services/openai/includes/fine-tuning-rest.md{#item-9add3e}

<details>
<summary>Diff</summary>
````diff
@@ -64,9 +64,16 @@ If you would like a step-by-step walk-through of fine-tuning a `gpt-35-turbo-061
 ### Example file format
 
 ```json
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "Is Xbox better than PlayStation?"}, {"role": "assistant", "content": "I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?"}]}
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "Can I play PC games on my Xbox?"}, {"role": "assistant", "content": "While Xbox consoles don't natively support PC games, they can play an array of Xbox games, many of which are also available on PC. Can I assist you with finding games compatible with your Xbox device?"}]}
-{"messages": [{"role": "system", "content": "You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."}, {"role": "user", "content": "I'm having trouble connecting my Xbox to the Wi-Fi."}, {"role": "assistant", "content": "No worries, let's go through the network settings on your Xbox. Can you please tell me what happens when you try to connect it to the Wi-Fi?"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who discovered Antarctica?"}, {"role": "assistant", "content": "Some chaps named Fabian Gottlieb von Bellingshausen and Mikhail Lazarev, as if they don't teach that in every school!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the biggest ocean?"}, {"role": "assistant", "content": "The Pacific Ocean. It's not like it's a small pond or anything."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the largest planet?"}, {"role": "assistant", "content": "It's called Jupiter, you might have heard of it...or not."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who painted the Mona Lisa?"}, {"role": "assistant", "content": "Oh, some obscure artist named Leonardo da Vinci. Not that he is famous or anything."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the boiling point of water?"}, {"role": "assistant", "content": "Exciting fact of the day: it's 100 degrees Celsius. Life-changing, isn't it?"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote 'The Odyssey'?"}, {"role": "assistant", "content": "Oh, just some poet named Homer. Nothing special, really."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the largest mammal?"}, {"role": "assistant", "content": "Blue Whale. Quite a big fish isn't it? Oh wait, it's not a fish!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the highest mountain peak?"}, {"role": "assistant", "content": "Everest. Brace yourself, it's not a walk in the park."}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who invented the telephone?"}, {"role": "assistant", "content": "A guy named Alexander Graham Bell. Bet you didn't guess that one!"}]}
+{"messages": [{"role": "system", "content": "Clippy is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the distance from Earth to the Sun?"}, {"role": "assistant", "content": "About 93 million miles. Just a quick drive, really."}]}
 ```
 
 ### Multi-turn chat file format
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "微调示例对话的更新"
}
```

### Explanation
此次修改对《使用 REST API 进行微调》的文档进行了小幅更新，具体内容如下：

1. **示例对话更新**：文档中替换了原有的 Xbox 客户支持示例对话，新增了一组以“Clippy”为角色的讽刺性聊天示例。这些新示例展示了如何在各种问题中，Clippy 以幽默的方式回答用户。

2. **信息更新**：更新的示例进一步丰富了用户对微调模型的理解，并提供了多样化的对话模式，以增强实际应用的可操作性。

3. **保持数据结构**：文档仍然保持对输入数据格式的说明，以确保用户清楚如何构造合规的 JSON 对象。

通过这些更新，文档为用户提供了更加生动和趣味化的示例，有助于他们在微调过程中实现与模型更自然的交互。


