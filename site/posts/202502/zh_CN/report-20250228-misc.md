---
date: '2025-02-28'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:42a07e4...MicrosoftDocs:63a40ca
summary: 此次代码更新对多个文档进行了小范围的修改和优化，重点在于更新 Azure OpenAI Studio 的集成信息、日期和审阅者信息、行为规范及区域可用性链接，此外还补充了
  Phi-4 模型的多模态指令功能描述。同时，对 SDK 和开发工具的文档结构进行了优化，更新了超链接，以便为用户在 Azure AI Foundry 的开发和项目管理提供更多指导和便捷。
title: '[zh_CN] Diff Insight Report - misc'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:42a07e4...MicrosoftDocs:63a40ca){target="_blank"}

<format>
# 突出

这次代码更新对多个文档进行了轻微的修改和优化，主要集中在以下几个方面：增加和更新了 Azure OpenAI Studio 相关的集成信息与链接、日期和审阅者信息的更新、行为规范和区域可用性的链接更新、模型描述的补充特别是针对 Phi-4 模型的多模态指令功能的添加。此外，对 SDK 和开发工具的文档进行了结构优化和超链接更新，为用户在 Azure AI Foundry 中的开发和项目管理提供了更多便利性和指引。

## 新特性

- 新增 Phi-4 多模态指令模型的描述和其在文本、图像、音频输入处理方面的能力。
- 为用户提供了新的序列化模板和框架，简化代码开发流程。

## 重大变化

- 更改多个文档的超链接，使用户更容易获取和访问所需的信息。
- 对使用 Phi 系列模型的文档进行了更新，添加了一款新模型并相应地调整文档信息。

## 其他更新

- 更新日期和文档审阅者的信息以保证内容的时效性。
- 轻微优化了多个文档的格式和表述，提升了可读性。
- 增加了对 SDK 和相关平台的超链接，引导用户直接访问相关资源。

# 洞察

这次更新显示出对 Azure OpenAI Studio 文档的多方面优化，旨在通过改善文字表述、超链接的更新以及加入最新的模型信息等手段提升用户的导航体验和获取信息的效率。更新后的文档更具可读性和实用性，特别适合开发者快速查找和利用 Azure 的多种服务。

通过为文档引入直观的超链接和最新模型的信息，避免了用户在导航和信息查找上的障碍，使得使用文档的过程更加简单和高效。此外，新内容尤其针对新用户进行了优化，例如在文档中添加详细的操作步骤、新用户可以快速上手 Azure AI Foundry，而不需要花时间去摸索配置和了解细节。这项更新不仅在文档的访问性上大有改观，同时也能显著提升新旧用户的综合体验。

另外，加入的 Phi-4 多模态模型的详细说明为用户提供了新的技术选项，帮助用户更好地设计和实现高效、集成性强的 AI 解决方案。这体现了 Azure 对技术前沿发展的重视，以及其为开发者提供先进工具和资源的承诺。
</format>

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [azure-openai-in-ai-studio.md](#item-07639b) | minor update | 更新 Azure OpenAI Studio 集成信息 | modified | 1 | 1 | 2 | 
| [connections.md](#item-01b26a) | minor update | 更新连接文档中的日期和审阅者 | modified | 9 | 9 | 18 | 
| [content-filtering.md](#item-91b372) | minor update | 更新代码行为规范链接 | modified | 1 | 1 | 2 | 
| [fine-tuning-overview.md](#item-31b07b) | minor update | 更新模型微调概述文档 | modified | 25 | 21 | 46 | 
| [faq.yml](#item-e7baa2) | minor update | 更新常见问题文档的摘要 | modified | 1 | 1 | 2 | 
| [create-manage-compute-session.md](#item-6ed743) | minor update | 更新计算会话文档中的链接 | modified | 1 | 1 | 2 | 
| [create-projects.md](#item-cb10b3) | minor update | 更新项目创建文档中的链接 | modified | 1 | 1 | 2 | 
| [deploy-models-phi-4.md](#item-c40212) | minor update | 增加关于Phi-4多模态指令模型的详细信息 | modified | 63 | 15 | 78 | 
| [ai-template-get-started.md](#item-d71b59) | minor update | 更新AI模板入门指南 | modified | 31 | 30 | 61 | 
| [create-hub-project-sdk.md](#item-8c3e99) | minor update | 更新创建Hub项目SDK的文档链接 | modified | 1 | 1 | 2 | 
| [sdk-overview.md](#item-d3ab19) | minor update | 更新SDK概述文档 | modified | 31 | 29 | 60 | 
| [vscode.md](#item-24bd97) | minor update | 更新VS Code开发文档 | modified | 1 | 1 | 2 | 
| [model-catalog-overview.md](#item-278001) | minor update | 更新模型目录概述文档 | modified | 1 | 1 | 2 | 
| [region-availability-maas.md](#item-35d79c) | minor update | 更新区域可用性文档 | modified | 1 | 1 | 2 | 
| [get-started-code.md](#item-8a5082) | minor update | 更新快速入门文档中的链接 | modified | 1 | 1 | 2 | 
| [copilot-sdk-build-rag.md](#item-b77dba) | minor update | 更新教程文档中的链接 | modified | 1 | 1 | 2 | 
| [copilot-sdk-create-resources.md](#item-552960) | minor update | 更新教程文档中的链接 | modified | 1 | 1 | 2 | 
| [copilot-sdk-evaluate.md](#item-bb5754) | minor update | 更新教程文档中的链接 | modified | 1 | 1 | 2 | 
| [screen-reader.md](#item-4dc029) | minor update | 更新屏幕阅读器教程中的链接 | modified | 1 | 1 | 2 | 


# Modified Contents
## articles/ai-studio/azure-openai-in-ai-studio.md{#item-07639b}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ ms.custom: ignite-2023, build-2024, ignite-2024
 
 # What is Azure OpenAI in Azure AI Foundry portal?
 
-Azure OpenAI Service provides REST API access to OpenAI's powerful language models. Azure OpenAI Studio was previously where you went to access and work with the Azure OpenAI Service. This studio is now integrated into Azure AI Foundry portal. 
+Azure OpenAI Service provides REST API access to OpenAI's powerful language models. Azure OpenAI Studio was previously where you went to access and work with the Azure OpenAI Service. This studio is now integrated into [Azure AI Foundry portal](https://ai.azure.com). 
 
 ## Access Azure OpenAI Service in Azure AI Foundry portal
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure OpenAI Studio 集成信息"
}
```

### Explanation
此代码差异展示了对文档的轻微更新，主要修改了关于 Azure OpenAI Studio 的描述。具体而言，原文中提到 Azure OpenAI Studio 集成到 Azure AI Foundry 门户的内容，已更新为包含一个指向 Azure AI Foundry 门户的超链接。这一改动有助于用户更方便地访问相关资源，同时保持文档的准确性和实用性。

## articles/ai-studio/concepts/connections.md{#item-01b26a}

<details>
<summary>Diff</summary>
````diff
@@ -9,8 +9,8 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 11/21/2024
-ms.reviewer: sgilley
+ms.date: 02/21/2025
+ms.reviewer: meerakurup
 ms.author: sgilley
 author: sdgilley
 ---
@@ -31,30 +31,30 @@ As another example, you can [create a connection](../how-to/connections-add.md)
 
 ## Connections to non-Microsoft services
 
-Azure AI Foundry supports connections to non-Microsoft services, including the following:
-- The [API key connection](../how-to/connections-add.md) handles authentication to your specified target on an individual basis. This is the most common non-Microsoft connection type.
-- The [custom connection](../how-to/connections-add.md) allows you to securely store and access keys while storing related properties, such as targets and versions. Custom connections are useful when you have many targets that or cases where you wouldn't need a credential to access. LangChain scenarios are a good example where you would use custom service connections. Custom connections don't manage authentication, so you'll have to manage authentication on your own.
+Azure AI Foundry supports connections to non-Microsoft services, including:
+- The [API key connection](../how-to/connections-add.md) handles authentication to your specified target on an individual basis. API key is the most common non-Microsoft connection type.
+- The [custom connection](../how-to/connections-add.md) allows you to securely store and access keys while storing related properties, such as targets and versions. Custom connections are useful when you have many targets that or cases where you wouldn't need a credential to access. LangChain scenarios are a good example where you would use custom service connections. Custom connections don't manage authentication, so you have to manage authentication on your own.
 
 ## Connections to datastores
 
 > [!IMPORTANT]
-> Data connections cannot be shared across projects. They are created exclusively in the context of one project. 
+> Data connections can't be shared across projects. They're created exclusively in the context of one project. 
 
 Creating a data connection allows you to access external data without copying it to your project. Instead, the connection provides a reference to the data source.
 
 A data connection offers these benefits:
 
 - A common, easy-to-use API that interacts with different storage types including Microsoft OneLake, Azure Blob, and Azure Data Lake Gen2.
 - Easier discovery of useful connections in team operations.
-- For credential-based access (service principal/SAS/key), Azure AI Foundry connection secures credential information. This way, you won't need to place that information in your scripts.
+- Credential-based access (service principal/SAS/key). Azure AI Foundry connection secures credential information so you don't need to place that information in your scripts.
 
 When you create a connection with an existing Azure storage account, you can choose between two different authentication methods:
 
 - **Credential-based**: Authenticate data access with a service principal, shared access signature (SAS) token, or account key. Users with *Reader* project permissions can access the credentials.
 - **Identity-based**: Use your Microsoft Entra ID or managed identity to authenticate data access.
 
     > [!TIP]
-    > When using an identity-based connection, Azure role-based access control (Azure RBAC) is used to determine who can access the connection. You must assign the correct Azure RBAC roles to your developers before they can use the connection. For more information, see [Scenario: Connections using Microsoft Entra ID](rbac-ai-studio.md#scenario-connections-using-microsoft-entra-id-authentication).
+    > When you use an identity-based connection, Azure role-based access control (Azure RBAC) determines who can access the connection. You must assign the correct Azure RBAC roles to your developers before they can use the connection. For more information, see [Scenario: Connections using Microsoft Entra ID](rbac-ai-studio.md#scenario-connections-using-microsoft-entra-id-authentication).
 
 
 The following table shows the supported Azure cloud-based storage services and authentication methods:
@@ -82,7 +82,7 @@ A Uniform Resource Identifier (URI) represents a storage location on your local
 
 ## Key vaults and secrets
 
-Connections allow you to securely store credentials, authenticate access, and consume data and information.  Secrets associated with connections are securely persisted in the corresponding Azure Key Vault, adhering to robust security and compliance standards. As an administrator, you can audit both shared and project-scoped connections on a hub level (link to connection rbac). 
+Connections allow you to securely store credentials, authenticate access, and consume data and information.  Secrets associated with connections are securely persisted in the corresponding Azure Key Vault, adhering to robust security and compliance standards. As an administrator, you can audit both shared and project-scoped connections on a hub level. 
 
 Azure connections serve as key vault proxies, and interactions with connections are direct interactions with an Azure key vault. Azure AI Foundry connections store API keys securely, as secrets, in a key vault. The key vault [Azure role-based access control (Azure RBAC)](./rbac-ai-studio.md) controls access to these connection resources. A connection references the credentials from the key vault storage location for further use. You won't need to directly deal with the credentials after they're stored in the hub's key vault. You have the option to store the credentials in the YAML file. A CLI command or SDK can override them. We recommend that you avoid credential storage in a YAML file, because a security breach could lead to a credential leak.  
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新连接文档中的日期和审阅者"
}
```

### Explanation
该代码差异涉及对文档 `connections.md` 的一系列轻微更新。此次修改主要包括两个方面：一是更新了文档的日期，从 2024 年 11 月 21 日更改为 2025 年 2 月 21 日，二是更改了文档审阅者的名字。从原来的 sgilley 更改为 meerakurup。此外，文档的某些文本内容也进行了轻微的格式优化，以增强可读性。这些变化旨在确保文档的信息保持最新，并提高用户的阅读体验。

## articles/ai-studio/concepts/content-filtering.md{#item-91b372}

<details>
<summary>Diff</summary>
````diff
@@ -86,7 +86,7 @@ The configurability feature allows customers to adjust the settings, separately
 
 <sup>1</sup> For Azure OpenAI models, only customers who have been approved for modified content filtering have full content filtering control, including configuring content filters at severity level high only or turning off content filters. Apply for modified content filters via these forms: [Azure OpenAI Limited Access Review: Modified Content Filters](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUMlBQNkZMR0lFRldORTdVQzQ0TEI5Q1ExOSQlQCN0PWcu), and [Modified Abuse Monitoring](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOE9MUTFMUlpBNk5IQlZWWkcyUEpWWEhGOCQlQCN0PWcu).
 
-Customers are responsible for ensuring that applications integrating Azure OpenAI comply with the [Code of Conduct](/legal/cognitive-services/openai/code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext). 
+Customers are responsible for ensuring that applications integrating Azure OpenAI comply with the [Code of Conduct](/legal/ai-code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext). 
 
 
 ## Next steps
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新代码行为规范链接"
}
```

### Explanation
此次代码差异显示了对文档 `content-filtering.md` 的小幅修改，主要是对包含的链接进行了更新。具体来说，文中提到的关于 Azure OpenAI 的《行为规范》链接已从原来的 `/legal/cognitive-services/openai/code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext` 更改为新的链接 `/legal/ai-code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext`。这一更新旨在确保用户获得最新和有效的指导文档，维护内容的准确性和相关性。

## articles/ai-studio/concepts/fine-tuning-overview.md{#item-31b07b}

<details>
<summary>Diff</summary>
````diff
@@ -1,24 +1,25 @@
 ---
 title: Fine-tuning in Azure AI Foundry portal
 titleSuffix: Azure AI Foundry
-description: This article introduces fine-tuning of models in Azure AI Foundry portal.
+description: This article explains what fine-tuning is and under what circumstances you should consider doing it.
 manager: scottpolly
 ms.service: azure-ai-foundry
 ms.custom:
   - build-2024
   - code01
-ms.topic: conceptual
-ms.date: 10/31/2024
-ms.reviewer: sgilley
+ms.topic: concept-article
+ms.date: 02/21/2025
+ms.reviewer: keli19
 ms.author: sgilley
 author: sdgilley
+#customer intent: As a developer, I want to learn what it means to fine-tune a model.
 ---
 
 # Fine-tune models with Azure AI Foundry
 
-[!INCLUDE [feature-preview](../includes/feature-preview.md)]
+Fine-tuning customizes a pretrained AI model with additional training on a specific task or dataset to improve performance, add new skills, or enhance accuracy. The result is a new, optimized GenAI model based on the provided examples.
 
-Fine-tuning refers to customizing a pre-trained generative AI model with additional training on a specific task or new dataset for enhanced performance, new skills, or improved accuracy. The result is a new, custom GenAI model that's optimized based on the provided examples.
+[!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
 Consider fine-tuning GenAI models to:
 - Scale and adapt to specific enterprise needs
@@ -27,19 +28,20 @@ Consider fine-tuning GenAI models to:
 - Save time and resources with faster and more precise results
 - Get more relevant and context-aware outcomes as models are fine-tuned for specific use cases
 
-[Azure AI Foundry](https://ai.azure.com) offers several models across model providers enabling you to get access to the latest and greatest in the market. You can discover supported models for fine-tuning through our model catalog by using the **Fine-tuning tasks** filter and selecting the model card to learn detailed information about each model. Specific models may be subjected to regional constraints, [view this list for more details](#supported-models-for-fine-tuning). 
+[Azure AI Foundry](https://ai.azure.com) offers several models across model providers enabling you to get access to the latest and greatest in the market. You can discover supported models for fine-tuning through our model catalog by using the **Fine-tuning tasks** filter and selecting the model card to learn detailed information about each model. Specific models might be subjected to regional constraints. [View this list for more details](#supported-models-for-fine-tuning). 
 
 :::image type="content" source="../media/concepts/model-catalog-fine-tuning.png" alt-text="Screenshot of Azure AI Foundry model catalog and filtering by Fine-tuning tasks." lightbox="../media/concepts/model-catalog-fine-tuning.png":::
 
-This article will walk you through use-cases for fine-tuning and how this can help you in your GenAI journey.
+This article walks you through use-cases for fine-tuning and how it helps you in your GenAI journey.
 
 ## Getting started with fine-tuning
 
 When starting out on your generative AI journey, we recommend you begin with prompt engineering and RAG to familiarize yourself with base models and its capabilities. 
 - [Prompt engineering](../../ai-services/openai/concepts/prompt-engineering.md) is a technique that involves designing prompts using tone and style details, example responses, and intent mapping for natural language processing models. This process improves accuracy and relevancy in responses, to optimize the performance of the model.
 - [Retrieval-augmented generation (RAG)](../concepts/retrieval-augmented-generation.md) improves LLM performance by retrieving data from external sources and incorporating it into a prompt. RAG can help businesses achieve customized solutions while maintaining data relevance and optimizing costs.
 
-As you get comfortable and begin building your solution, it's important to understand where prompt engineering falls short and that will help you realize if you should try fine-tuning. 
+As you get comfortable and begin building your solution, it's important to understand where prompt engineering falls short and when you should try fine-tuning.
+
 - Is the base model failing on edge cases or exceptions? 
 - Is the base model not consistently providing output in the right format?
 - Is it difficult to fit enough examples in the context window to steer the model?
@@ -53,26 +55,29 @@ _A customer wants to use GPT-3.5 Turbo to turn natural language questions into q
 
 ### Use cases
 
-Base models are already pre-trained on vast amounts of data and most times you'll add instructions and examples to the prompt to get the quality responses that you're looking for - this process is called "few-shot learning". Fine-tuning allows you to train a model with many more examples that you can tailor to meet your specific use-case, thus improving on few-shot learning. This can reduce the number of tokens in the prompt leading to potential cost savings and requests with lower latency. 
+Base models are already pretrained on vast amounts of data. Most times you add instructions and examples to the prompt to get the quality responses that you're looking for - this process is called "few-shot learning." Fine-tuning allows you to train a model with many more examples that you can tailor to meet your specific use-case, thus improving on few-shot learning. Fine-tuning can reduce the number of tokens in the prompt leading to potential cost savings and requests with lower latency. 
+
+Turning natural language into a query language is just one use case where you can  "_show not tell_" the model how to behave. Here are some other use cases:
 
-Turning natural language into a query language is just one use case where you can  _show not tell_ the model how to behave. Here are some additional use cases:
 - Improve the model's handling of retrieved data
 - Steer model to output content in a specific style, tone, or format
 - Improve the accuracy when you look up information
 - Reduce the length of your prompt
-- Teach new skills (i.e. natural language to code)
+- Teach new skills (that is, natural language to code)
 
-If you identify cost as your primary motivator, proceed with caution. Fine-tuning might reduce costs for certain use cases by shortening prompts or allowing you to use a smaller model. But there may be a higher upfront cost to training, and you have to pay for hosting your own custom model. 
+If you identify cost as your primary motivator, proceed with caution. Fine-tuning might reduce costs for certain use cases by shortening prompts or allowing you to use a smaller model. But there might be a higher upfront cost to training, and you have to pay for hosting your own custom model. 
 
 ### Steps to fine-tune a model
+
 Here are the general steps to fine-tune a model:
-1. Based on your use case, choose a model that supports your task
-2. Prepare and upload training data
-3. (Optional) Prepare and upload validation data
-4. (Optional) Configure task parameters
-5. Train your model. 
-6. Once completed, review metrics and evaluate model. If the results don't meet your benchmark, then go back to step 2.
-7. Use your fine-tuned model
+
+1. Choose a model that supports your task.
+1. Prepare and upload training data.
+1. (Optional) Prepare and upload validation data.
+1. (Optional) Configure task parameters.
+1. Train your model. 
+1. Once completed, review metrics and evaluate model. If the results don't meet your benchmark, then go back to step 2.
+1. Use your fine-tuned model.
 
 It's important to call out that fine-tuning is heavily dependent on the quality of data that you can provide. It's best practice to provide hundreds, if not thousands, of training examples to be successful and get your desired results.
 
@@ -87,7 +92,6 @@ For more information on fine-tuning using a managed compute (preview), see [Fine
 
 For details about Azure OpenAI models that are available for fine-tuning, see the [Azure OpenAI Service models documentation](../../ai-services/openai/concepts/models.md#fine-tuning-models) or the [Azure OpenAI models table](#fine-tuning-azure-openai-models) later in this guide.
 
-
 For the Azure OpenAI  Service models that you can fine tune, supported regions for fine-tuning include North Central US, Sweden Central, and more.
 
 ### Fine-tuning Azure OpenAI models
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新模型微调概述文档"
}
```

### Explanation
此次代码差异涉及对文档 `fine-tuning-overview.md` 的多项修改，整体上对内容进行了更新和调整。文档的描述部分被修正为更加清晰地解释了微调的概念及其适用场景。此外，文档的元数据（如日期和审阅者）也被更新为新的信息。同时，内容的结构得到了优化，部分句子更加简洁流畅，增加了一些细节，以增强用户的理解和体验，比如重新组织了微调模型的步骤和相关用例。

这一更新还有助于用户更好地理解微调技术的应用，提高模型在特定任务上的表现，同时提供了更符合最新情况的参考资料。这些修改旨在提升文档的实用性和可读性，并确保信息的准确性。

## articles/ai-studio/faq.yml{#item-e7baa2}

<details>
<summary>Diff</summary>
````diff
@@ -14,7 +14,7 @@ metadata:
   author: sdgilley
 title: Azure AI Foundry frequently asked questions
 summary: |
-  If you can't find answers to your questions in this document, and still need help check the [Azure AI services support options guide](../ai-services/cognitive-services-support-options.md?context=/azure/ai-services/openai/context/context). Azure OpenAI is part of Azure AI services.
+  FAQ for [Azure AI Foundry](https://ai.azure.com). If you can't find answers to your questions in this document, and still need help check the [Azure AI services support options guide](../ai-services/cognitive-services-support-options.md?context=/azure/ai-services/openai/context/context). Azure OpenAI is part of Azure AI services.
 sections:
   - name: General questions
     questions:
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新常见问题文档的摘要"
}
```

### Explanation
此次代码差异对 `faq.yml` 文档中的摘要部分进行了小幅调整。具体而言，摘要前添加了“FAQ for [Azure AI Foundry](https://ai.azure.com)”这一短语，以便更明确地指明该文档的主题。此修改有助于用户在阅读摘要时迅速了解内容的焦点，同时保持了对现有支持选项指南的引用，确保用户在找不到答案时能够获得帮助。

整体而言，这一更新提升了文档的清晰度和可读性，使用户能够更快地定位相关信息，同时维护了对 Azure AI 服务的相关性。

## articles/ai-studio/how-to/create-manage-compute-session.md{#item-6ed743}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
-You need a compute session to run [prompt flows](prompt-flow.md). Use Azure AI Foundry to create and manage prompt flow compute sessions.
+You need a compute session to run [prompt flows](prompt-flow.md). Use [Azure AI Foundry](https://ai.azure.com) to create and manage prompt flow compute sessions.
 
 A prompt flow compute session has computing resources that are required for the application to run, including a Docker image that contains all necessary dependency packages. In addition to flow execution, Azure AI Foundry uses the compute session to ensure the accuracy and functionality of the tools incorporated within the flow when you make updates to the prompt or code content.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新计算会话文档中的链接"
}
```

### Explanation
此次代码差异对文档 `create-manage-compute-session.md` 中的一句话进行了小幅修改。具体而言，原句中的 “Azure AI Foundry” 被更新为包含完整超链接的形式，即“[Azure AI Foundry](https://ai.azure.com)”，这样用户可以直接通过点击链接访问相关网站。

这一进行中的细小更新旨在增强文档的可用性，方便用户更快找到所需的在线资源，从而提升用户体验。此外，文档的其他部分仍保持不变，继续详述计算会话在运行提示流中的重要性及其资源要求。整体而言，这一修改提升了信息的可达性，使得对新用户尤其友好。

## articles/ai-studio/how-to/create-projects.md{#item-cb10b3}

<details>
<summary>Diff</summary>
````diff
@@ -18,7 +18,7 @@ author: sdgilley
 
 # Create a project in Azure AI Foundry portal
 
-This article describes how to create an Azure AI Foundry project. A project is used to organize your work and save state while building customized AI apps. 
+This article describes how to create an [Azure AI Foundry](https://ai.azure.com) project. A project is used to organize your work and save state while building customized AI apps. 
 
 Projects are hosted by an Azure AI Foundry hub. If your company has an administrative team that has created a hub for you, you can create a project from that hub. If you are working on your own, you can create a project and a default hub will automatically be created for you.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新项目创建文档中的链接"
}
```

### Explanation
此次代码差异对 `create-projects.md` 文档中的描述进行了小幅修改。文章中提到的“Azure AI Foundry”现在被更新为带有超链接的格式，即“[Azure AI Foundry](https://ai.azure.com)”。这一更改的目的是为用户提供直接的访问途径，以便方便他们获取更多信息或资源。

这一修改旨在增强文档的可用性和互动性，确保用户能够迅速访问相关平台，从而提升整体使用体验。文章其余部分仍然保持不变，继续清晰地阐述了如何在 Azure AI Foundry 中创建项目的步骤和重要性。此更新对于新用户尤其有帮助，使他们更方便地开始使用 Azure AI Foundry 做项目管理。

## articles/ai-studio/how-to/deploy-models-phi-4.md{#item-c40212}

<details>
<summary>Diff</summary>
````diff
@@ -29,6 +29,18 @@ The Phi-4 family of small language models (SLMs) is a collection of instruction-
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -155,7 +167,7 @@ print("Model provider name:", model_info.model_provider_name)
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -176,7 +188,7 @@ response = client.complete(
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -192,7 +204,7 @@ print("\tCompletion tokens:", response.usage.completion_tokens)
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -341,6 +353,18 @@ except HttpResponseError as ex:
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -465,7 +489,7 @@ console.log("Model provider name: ", model_info.body.model_provider_name)
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -488,7 +512,7 @@ var response = await client.path("/chat/completions").post({
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -508,7 +532,7 @@ console.log("\tCompletion tokens:", response.body.usage.completion_tokens);
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -676,6 +700,18 @@ catch (error) {
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -815,7 +851,7 @@ Console.WriteLine($"Model provider name: {modelInfo.Value.ModelProviderName}");
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -837,7 +873,7 @@ Response<ChatCompletions> response = client.Complete(requestOptions);
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -853,7 +889,7 @@ Console.WriteLine($"\tCompletion tokens: {response.Value.Usage.CompletionTokens}
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -1023,6 +1059,18 @@ catch (RequestFailedException ex)
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -1113,7 +1161,7 @@ The response is as follows:
 
 ```json
 {
-    "model_name": "Phi-4-mini-instruct",
+    "model_name": "Phi-4-multimodal-instruct",
     "model_type": "chat-completions",
     "model_provider_name": "Microsoft"
 }
@@ -1139,7 +1187,7 @@ The following example shows how you can create a basic chat completions request
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -1149,7 +1197,7 @@ The response is as follows, where you can see the model's usage statistics:
     "id": "0a1234b5de6789f01gh2i345j6789klm",
     "object": "chat.completion",
     "created": 1718726686,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1206,7 +1254,7 @@ You can visualize how streaming generates content:
     "id": "23b54589eba14564ad8a2e6978775a39",
     "object": "chat.completion.chunk",
     "created": 1718726371,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1229,7 +1277,7 @@ The last message in the stream has `finish_reason` set, indicating the reason fo
     "id": "23b54589eba14564ad8a2e6978775a39",
     "object": "chat.completion.chunk",
     "created": 1718726371,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1280,7 +1328,7 @@ Explore other parameters that you can specify in the inference client. For a ful
     "id": "0a1234b5de6789f01gh2i345j6789klm",
     "object": "chat.completion",
     "created": 1718726686,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "增加关于Phi-4多模态指令模型的详细信息"
}
```

### Explanation
此次代码差异对文档 `deploy-models-phi-4.md` 进行了重大的修改，特别是针对 Phi-4 多模态指令模型新增了多条信息。这些信息介绍了 Phi-4-multimodal-instruct 模型的特性、用途及其优势，如其在文本、图像和音频输入处理方面的能力，以及其经过增强的优化过程，包括监督微调和直接偏好优化，以确保模型在执行指令时的准确性和安全性。

文档中还明确列出了多模态指令模型所支持的128K标记长度的变体，并提供了便捷的链接，以便用户能快速访问该模型的相关信息。此外，原文中提到的模型名称也相应地更新为 “Phi-4-multimodal-instruct”，替换了先前的 “Phi-4-mini-instruct”。

最后，关于系统消息（`role="system"`）的说明也进行了更新，列举了新的模型并说明了它们的行为。这些修改旨在为用户提供更准确的信息，使他们在使用模型时能够更有效地理解其功能和限制，从而提升用户体验和操作的便利性。

## articles/ai-studio/how-to/develop/ai-template-get-started.md{#item-d71b59}

<details>
<summary>Diff</summary>
````diff
@@ -1,58 +1,59 @@
 ---
 title: How to get started with an AI template
 titleSuffix: Azure AI Foundry
-description: This article provides instructions on how to get started with an AI template.
+description: This article provides instructions on how to use an AI template to get started with Azure AI Foundry.
 manager: scottpolly
 ms.service: azure-ai-foundry
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 01/02/2025
-ms.reviewer: dantaylo
+ms.date: 02/20/2025
+ms.reviewer: varundua
 ms.author: sgilley
 author: sdgilley
+#customer intent: As a developer, I want to jump start my journey with an AI template.
 ---
 
-# How to get started with an AI template
+# Get started with an AI template
+
+Streamline your code-first development with prebuilt, task-specific Azure AI templates. Benefit from using the latest features and best practices from Microsoft Azure AI, with popular frameworks like LangChain, prompt flow, and Semantic Kernel in multiple languages.
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-Streamline your code-first development with prebuilt, task-specific Azure AI templates. Benefit from using the latest features and best practices from Microsoft Azure AI, with popular frameworks like LangChain, prompt flow, and Semantic Kernel in multiple languages.
+## Prerequisites
 
-> [!TIP]
-> Discover the latest templates in our curated [AZD templates collection](https://aka.ms/azd-ai-templates). Deploy them with a single command ```azd up``` using the [Azure Developer CLI](/azure/developer/azure-developer-cli/). 
+- [Azure subscription](https://azure.microsoft.com/free)
+- An [Azure AI Foundry project](../create-projects.md).
 
 ## Start with a sample application
 
-Start with our sample applications! Choose the right template for your needs, then refer to the README in any of the following Azure Developer CLI enabled templates for more instructions and information.
+1. Go to [Azure AI Foundry portal](https://ai.azure.com).
+1. Open your project in Azure AI Foundry portal.
+1. On the left menu, select **Code** (preview).
+1. Find the solution template you want to use.
+1. Select **Open in Github** to view the entire sample application.
+1. Or, clone the repository to your local machine with the provided command.
+1. In some cases, you can also view a step-by-step tutorial that explains the AI code.
 
-### [Python](#tab/python)
+## Explore the sample application
 
-| Template      | App host | Tech stack | Description |
-| ----------- | ----------| ----------- | ------------|
-| [Azure AI Basic Template with Python](https://github.com/azure-samples/azureai-basic-python) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep | The app serves as a straightforward example of integrating Azure AI Services within a basic prompt-based application. This template walks you through building a simple chat app that utilizes models and prompts. The template also covers setting up the necessary infrastructure for the app, including creating an Azure AI Foundry Hub, configuring projects, and provisioning essential resources such as Azure AI Service, Azure Container Apps, Cognitive Search, and more. <br>You can build, deploy, and test it with a single command.  |
-| [Contoso Chat Retail copilot with Azure AI Foundry](https://github.com/Azure-Samples/contoso-chat) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Cosmos DB](/azure/cosmos-db/index-overview), [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI Search](/azure/search/search-what-is-azure-search), Bicep  | A retailer conversation agent that can answer questions grounded in your product catalog and customer order history. This template uses a retrieval augmented generation architecture with cutting-edge models for chat completion, chat evaluation, and embeddings. Build, evaluate, and deploy, an end-to-end solution with a single command. | 
-| [Process Automation: speech to text and summarization with Azure AI Foundry](https://github.com/Azure-Samples/summarization-openai-python-prompflow) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI speech to text service](../../../ai-services/speech-service/index-speech-to-text.yml), Bicep  | An app for workers to report issues via text or speech, translating audio to text, summarizing it, and specify the relevant department. | 
-| [Multi-Modal Creative Writing copilot with Dalle](https://github.com/Azure-Samples/agent-openai-python-prompty) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep | demonstrates how to create and work with AI agents. The app takes a topic and instruction input and then calls a research agent, writer agent, and editor agent. |  
-| [Assistant API Analytics Copilot with Python and Azure AI Foundry](https://github.com/Azure-Samples/assistant-data-openai-python-promptflow) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) |  [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep| A data analytics chatbot based on the Assistants API. The chatbot can answer questions in natural language, and interpret them as queries on an example sales dataset. |
-<!-- remove for now
-| Function Calling with Prompty, LangChain, and Pinecone | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction), [Pinecone](https://www.pinecone.io/), Bicep  | Utilize the new Prompty tool, LangChain, and Pinecone to build a large language model (LLM) search agent. This agent with Retrieval-Augmented Generation (RAG) technology is capable of answering user questions based on the provided data by integrating real-time information retrieval with generative responses. | 
-| Function Calling with Prompty, LangChain, and Elastic Search | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Elastic Search](https://www.elastic.co/elasticsearch), [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction) , Bicep  | Utilize the new Prompty tool, LangChain, and Elasticsearch to build a large language model (LLM) search agent. This agent with Retrieval-Augmented Generation (RAG) technology is capable of answering user questions based on the provided data by integrating real-time information retrieval with generative responses |
--->
+Once you're looking at the GitHub repository for your sample, refer to the README for more instructions and information on how to deploy your own version of the application.
 
+Instructions vary by sample, but most include how to:
 
-### [C#](#tab/csharp)
+* Open the solution in the location of your choice:
+  * GitHub Codespaces
+  * VS Code Dev Containers
+  * Your local IDE
+* Deploy the application to Azure
+* How to test the app
 
-| Template      | App host | Tech stack | Description |
-| ----------- | ----------| ----------- | -------------- |
-| [Contoso Chat Retail copilot with .NET and Semantic Kernel](https://github.com/Azure-Samples/contoso-chat-csharp-prompty) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Cosmos DB](/azure/cosmos-db/index-overview), [Azure Monitor](/azure/azure-monitor/overview), [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure Container Apps](/azure/container-apps/overview), [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Services](../../../ai-services/openai/overview.md), [Semantic Kernel](/semantic-kernel/overview/?tabs=Csharp), Bicep | A retailer conversation agent that can answer questions grounded in your product catalog and customer order history. This template uses a retrieval augmented generation architecture with cutting-edge models for chat completion, chat evaluation, and embeddings. Build, evaluate, and deploy, an end-to-end solution with a single command. |
-| [Process Automation: speech to text and summarization with .NET and GPT 3.5 Turbo](https://github.com/Azure-Samples/summarization-openai-csharp-prompty) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI speech to text service](../../../ai-services/speech-service/index-speech-to-text.yml), Bicep | An app for workers to report issues via text or speech, translating audio to text, summarizing it, and specify the relevant department. |
-
----
+The README also includes information about the application, such as the use case, architecture, and pricing information.
 
+> [!TIP]
+> Discover more templates in our curated [AZD templates collection](https://azure.github.io/ai-app-templates). Deploy them with a single command ```azd up``` using the [Azure Developer CLI](/azure/developer/azure-developer-cli/).
 
 ## Related content
 
-- [Get started building a chat app using the prompt flow SDK](../../quickstarts/get-started-code.md)
-- [Work with projects in VS Code](vscode.md)
-- [Connections in Azure AI Foundry portal](../../concepts/connections.md)
+- [Get started building a chat app using the Azure AI Foundry SDK](../../quickstarts/get-started-code.md)
+- [Develop AI apps using Azure AI services](/azure/developer/ai/)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新AI模板入门指南"
}
```

### Explanation
此次代码差异对 `ai-template-get-started.md` 文档进行了相应的更新和修改，包含了对文档内容的整体调整和优化。首先，文档的描述部分被扩展，明确指出了使用AI模板开始Azure AI Foundry的步骤。此外，管理者和审阅者的信息也进行了更新，确保文档的准确性和时效性。

文档标题由“如何开始使用AI模板”更改为“开始使用AI模板”，并引入了新的序言段落，强调使用预构建的Azure AI模板能简化代码开发流程。这些模板可以借助最新特性和最佳实践来加速开发，并适用于多种流行框架。

在“前提条件”部分中，添加了有关Azure订阅和项目创建的具体要求。这为用户提供了更加清晰的步骤指导。同时，在关于示例应用程序的部分，文档内容得到了显著详实化，增加了针对如何选择和使用各种解决方案模板的具体操作步骤。

此外，更新中还增加了指向 GitHub 仓库的链接，鼓励用户参考 README 获取更多使用指引。整体而言，这些修改旨在提升用户在使用Azure AI模板时的体验，使得新手开发者能够更快捷地获取所需信息并进行实际操作。

## articles/ai-studio/how-to/develop/create-hub-project-sdk.md{#item-8c3e99}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-In this article, you learn how to create the following Azure AI Foundry resources using the Azure Machine Learning SDK and Azure CLI (with machine learning extension):
+In this article, you learn how to create the following [Azure AI Foundry](https://ai.azure.com) resources using the Azure Machine Learning SDK and Azure CLI (with machine learning extension):
 - An Azure AI Foundry hub
 - An Azure AI Services connection
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新创建Hub项目SDK的文档链接"
}
```

### Explanation
此次对文档 `create-hub-project-sdk.md` 的修改主要体现在对 Azure AI Foundry 资源描述的更新。文中明确添加了Azure AI Foundry的链接，使得用户在阅读时能够更方便地访问相关的在线资源。具体来说，文档中说明了如何使用 Azure 机器学习 SDK 和 Azure CLI（配合机器学习扩展）来创建 Azure AI Foundry 的相关资源，包括 Azure AI Foundry hub 和 Azure AI 服务连接。

这一微小但重要的更新旨在增强用户体验，使其能够更迅速地获取所需信息并加深对Azure AI Foundry的理解。通过添加直接的链接，用户可以直达 Azure AI Foundry 的官方网站，从而更方便地访问相关文档和资料。

## articles/ai-studio/how-to/develop/sdk-overview.md{#item-d3ab19}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: overview
-ms.date: 11/25/2024
+ms.date: 02/27/2025
 ms.reviewer: dantaylo
 ms.author: sgilley
 author: sdgilley
@@ -18,7 +18,7 @@ zone_pivot_groups: programming-languages-sdk-overview
 
 # The Azure AI Foundry SDK
 
-The Azure AI Foundry SDK is a comprehensive toolchain designed to simplify the development of AI applications on Azure. It enables developers to:
+The [Azure AI Foundry](https://ai.azure.com) SDK is a comprehensive toolchain designed to simplify the development of AI applications on Azure. It enables developers to:
 
 - Access popular models from various model providers through a single interface
 - Easily combine together models, data, and AI services to build AI-powered applications
@@ -111,7 +111,7 @@ Not yet available in C#.
 
 Copy the **Project connection string** from the **Overview** page of the project and update the connections string value above.
 
-Once you have created the project client, you can use the client for the capabilities in the following sections.
+Once you create the project client, you can use the client for the capabilities in the following sections.
 
 ::: zone pivot="programming-language-python"
 
@@ -127,7 +127,7 @@ Be sure to check out the [reference](https://aka.ms/aifoundrysdk/reference) and
 
 ## Azure OpenAI Service
 
-The [Azure OpenAI Service](../../../ai-services/openai/overview.md) provides access to OpenAI's models including the GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, DALLE-3, Whisper, and Embeddings model series with the data residency, scalability, safety, security and enterprise capabilities of Azure.
+The [Azure OpenAI Service](../../../ai-services/openai/overview.md) provides access to OpenAI's models including the GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, DALLE-3, Whisper, and Embeddings model series with the data residency, scalability, safety, security, and enterprise capabilities of Azure.
 
 If you have code that uses the OpenAI SDK, you can easily target your code to use the Azure OpenAI service. First, install the OpenAI SDK:
 
@@ -233,9 +233,9 @@ To learn more about using the Azure AI inferencing client, check out the [Azure
 
 ::: zone pivot="programming-language-python"
 
-## Prompt Templates
+## Prompt templates
 
-The inferencing client supports for creating prompt messages from templates.  The template allows you to dynamically generate prompts using inputs that are available at runtime.
+The inferencing client supports creating prompt messages from templates. The template allows you to dynamically generate prompts using inputs that are available at runtime.
 
 To use prompt templates, install the `azure-ai-inference` package:
 
@@ -356,7 +356,7 @@ To learn more about using Azure AI Search, check out [Azure AI Search documentat
 
 ## Azure AI Agent Service
 
-Azure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents. Using an extensive ecosystem of models, tools and capabilities from OpenAI, Microsoft, and third-party providers, [Azure AI Agent Service](/azure/ai-services/agents) enables building agents for a wide range of generative AI use cases.
+Azure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents. To enable building agents for a wide range of generative AI use cases, [Azure AI Agent Service](/azure/ai-services/agents) uses an extensive ecosystem of models, tools and capabilities from OpenAI, Microsoft, and third-party providers.
 
 ## Evaluation
 
@@ -393,7 +393,7 @@ To learn more, check out [Evaluation using the SDK](evaluate-sdk.md).
 
 ::: zone pivot="programming-language-csharp"
 
-An Azure AI evaluation package is not yet available for C#. For a sample on how to use Prompty and Semantic Kernel for evaluation, see the [contoso-chat-csharp-prompty](https://github.com/Azure-Samples/contoso-chat-csharp-prompty/blob/main/src/ContosoChatAPI/ContosoChat.Evaluation.Tests/Evalutate.cs) sample.
+An Azure AI evaluation package isn't yet available for C#. For a sample on how to use Prompty and Semantic Kernel for evaluation, see the [contoso-chat-csharp-prompty](https://github.com/Azure-Samples/contoso-chat-csharp-prompty/blob/main/src/ContosoChatAPI/ContosoChat.Evaluation.Tests/Evalutate.cs) sample.
 
 ::: zone-end
 
@@ -428,41 +428,43 @@ if application_insights_connection_string:
 
 ::: zone pivot="programming-language-csharp"
 
-Tracing is not yet integrated into the projects package. For instructions on how to instrument and log traces from the Azure AI Inferencing package, see [azure-sdk-for-dotnet](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Inference/samples/Sample8_ChatCompletionsWithOpenTelemetry.md).
+Tracing isn't yet integrated into the projects package. For instructions on how to instrument and log traces from the Azure AI Inferencing package, see [azure-sdk-for-dotnet](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Inference/samples/Sample8_ChatCompletionsWithOpenTelemetry.md).
 
 ::: zone-end
 
-## Related content
+## Other services and frameworks
 
-Below are some helpful links to other services and frameworks that you can use with the Azure AI Foundry SDK.
+The following sections provide helpful links to other services and frameworks that you can use with the Azure AI Foundry SDK.
 
 ### Azure AI Services
 
-Client libraries:
+* Client libraries:
 
-* [Azure AI services SDKs](../../../ai-services/reference/sdk-package-resources.md?context=/azure/ai-studio/context/context)
-* [Azure AI services REST APIs](../../../ai-services/reference/rest-api-resources.md?context=/azure/ai-studio/context/context) 
+    * [Azure AI services SDKs](../../../ai-services/reference/sdk-package-resources.md?context=/azure/ai-studio/context/context)
+    * [Azure AI services REST APIs](../../../ai-services/reference/rest-api-resources.md?context=/azure/ai-studio/context/context) 
 
-Management libraries:
-* [Azure AI Services Python Management Library](/python/api/overview/azure/mgmt-cognitiveservices-readme)
-* [Azure AI Search Python Management Library](/python/api/azure-mgmt-search/azure.mgmt.search)
+* Management libraries:
+
+    * [Azure AI Services Python Management Library](/python/api/overview/azure/mgmt-cognitiveservices-readme)
+    * [Azure AI Search Python Management Library](/python/api/azure-mgmt-search/azure.mgmt.search)
 
 ### Frameworks
 
-Azure Machine Learning
+* Azure Machine Learning
+
+    * [Azure Machine Learning Python SDK (v2)](/python/api/overview/azure/ai-ml-readme)
+    * [Azure Machine Learning CLI (v2)](/azure/machine-learning/how-to-configure-cli)
+    * [Azure Machine Learning REST API](/rest/api/azureml) 
 
-* [Azure Machine Learning Python SDK (v2)](/python/api/overview/azure/ai-ml-readme)
-* [Azure Machine Learning CLI (v2)](/azure/machine-learning/how-to-configure-cli)
-* [Azure Machine Learning REST API](/rest/api/azureml) 
+* Prompt flow
 
-Prompt flow
+    * [Prompt flow SDK](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html)
+    * [pfazure CLI](https://microsoft.github.io/promptflow/reference/pfazure-command-reference.html)
+    * [pfazure Python library](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow-azure/promptflow.azure.html)
 
-* [Prompt flow SDK](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html)
-* [pfazure CLI](https://microsoft.github.io/promptflow/reference/pfazure-command-reference.html)
-* [pfazure Python library](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow-azure/promptflow.azure.html)
+* Semantic Kernel
+    * [Semantic Kernel Overview](/semantic-kernel/overview/)
 
-Semantic Kernel
- * [Semantic Kernel Overview](/semantic-kernel/overview/)
-Agentic frameworks
+* Agentic frameworks
 
-* [LlamaIndex](llama-index.md)
+    * [LlamaIndex](llama-index.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新SDK概述文档"
}
```

### Explanation
此次对 `sdk-overview.md` 文档的修改包含了重要的更新和细节的调整，旨在提供更加直观和易于访问的信息。文档中对Azure AI Foundry SDK的描述进行了修改，添加了直接链接，便于用户访问相关的在线平台。

更新中，文档的日期被修正为2025年2月27日，以确保信息的时效性。此外，多个句子的结构得到合理优化，使其表达更加流畅。例如，“一旦您创建项目客户端...”这一句被修改为“创建项目客户端后，...”，简化了表达，提高了可读性。

在内容组织方面，相关服务和框架的链接被重新整理，形成了更清晰的层次结构。与Azure AI服务相关的管理库和客户端库被分开放置，使得用户在查找相关SDK和API时更加方便。这些细微但有益的修改旨在增强用户的导航体验，帮助开发者更好地利用Azure AI Foundry SDK进行各项开发任务。

## articles/ai-studio/how-to/develop/vscode.md{#item-24bd97}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-Azure AI Foundry supports developing in VS Code - Desktop and Web. In each scenario, your VS Code instance is remotely connected to a prebuilt custom container running on a virtual machine, also known as a compute instance.
+[Azure AI Foundry](https://ai.azure.com) supports developing in VS Code - Desktop and Web. In each scenario, your VS Code instance is remotely connected to a prebuilt custom container running on a virtual machine, also known as a compute instance.
 
 ## Launch VS Code from Azure AI Foundry
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新VS Code开发文档"
}
```

### Explanation
此次对文档 `vscode.md` 的修改主要是对Azure AI Foundry的描述进行了更新，增强了可读性和信息的可访问性。文中添加了对Azure AI Foundry链接的引用，使得用户在阅读过程中可以更方便地访问相关资源。

具体而言，文档指出Azure AI Foundry支持在VS Code（桌面版和网页版）中进行开发，并描述了VS Code实例如何通过远程连接至一个预构建的自定义容器，该容器运行在虚拟机上，即计算实例。这一信息对开发者尤其重要，因为它提供了有关开发环境的清晰指引。

通过这种小幅修改，文档不仅提升了信息的准确性，还有助于开发者更快速地理解如何在VS Code中高效地利用Azure AI Foundry进行开发工作。整体而言，这一更新旨在优化用户体验，使得相关信息更加易于获取。

## articles/ai-studio/how-to/model-catalog-overview.md{#item-278001}

<details>
<summary>Diff</summary>
````diff
@@ -84,7 +84,7 @@ Gretel | Not available | Gretel-Navigator
 Healthcare AI family Models | MedImageParse<BR>  MedImageInsight<BR>  CxrReportGen<BR>  Virchow<BR>  Virchow2<BR>  Prism<BR>  BiomedCLIP-PubMedBERT<BR>  microsoft-llava-med-v1.5<BR>  m42-health-llama3-med4<BR>  biomistral-biomistral-7b<BR>  microsoft-biogpt-large-pub<BR>  microsoft-biomednlp-pub<BR>  stanford-crfm-biomedlm<BR>  medicalai-clinicalbert<BR>  microsoft-biogpt<BR>  microsoft-biogpt-large<BR>  microsoft-biomednlp-pub<BR> | Not Available
 JAIS | Not available | jais-30b-chat
 Meta Llama family models | Llama-3.3-70B-Instruct<BR> Llama-3.2-3B-Instruct<BR>  Llama-3.2-1B-Instruct<BR>  Llama-3.2-1B<BR>  Llama-3.2-90B-Vision-Instruct<BR>  Llama-3.2-11B-Vision-Instruct<BR>  Llama-3.1-8B-Instruct<BR>  Llama-3.1-8B<BR>  Llama-3.1-70B-Instruct<BR>  Llama-3.1-70B<BR>  Llama-3-8B-Instruct<BR>  Llama-3-70B<BR>  Llama-3-8B<BR>  Llama-Guard-3-1B<BR>  Llama-Guard-3-8B<BR>  Llama-Guard-3-11B-Vision<BR>  Llama-2-7b<BR>  Llama-2-70b<BR>  Llama-2-7b-chat<BR>  Llama-2-13b-chat<BR>  CodeLlama-7b-hf<BR>  CodeLlama-7b-Instruct-hf<BR>  CodeLlama-34b-hf<BR>  CodeLlama-34b-Python-hf<BR>  CodeLlama-34b-Instruct-hf<BR>  CodeLlama-13b-Instruct-hf<BR>  CodeLlama-13b-Python-hf<BR>  Prompt-Guard-86M<BR>  CodeLlama-70b-hf<BR> | Llama-3.3-70B-Instruct<BR> Llama-3.2-90B-Vision-Instruct<br>  Llama-3.2-11B-Vision-Instruct<br>  Llama-3.1-8B-Instruct<br>  Llama-3.1-70B-Instruct<br>  Llama-3.1-405B-Instruct<br>  Llama-3-8B-Instruct<br>  Llama-3-70B-Instruct<br>  Llama-2-7b<br>  Llama-2-7b-chat<br>  Llama-2-70b<br>  Llama-2-70b-chat<br>  Llama-2-13b<br>  Llama-2-13b-chat<br>
-Microsoft Phi family models | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> Phi-3-vision-128k-Instruct <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct
+Microsoft Phi family models | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> Phi-3-vision-128k-Instruct <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct <br> Phi-4-multimodal-instruct | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct <br> Phi-4-multimodal-instruct
 Mistral family models | mistralai-Mixtral-8x22B-v0-1 <br> mistralai-Mixtral-8x22B-Instruct-v0-1 <br> mistral-community-Mixtral-8x22B-v0-1 <br> mistralai-Mixtral-8x7B-v01 <br> mistralai-Mistral-7B-Instruct-v0-2 <br> mistralai-Mistral-7B-v01 <br> mistralai-Mixtral-8x7B-Instruct-v01 <br> mistralai-Mistral-7B-Instruct-v01 | Mistral-large (2402) <br> Mistral-large (2407) <br> Mistral-small <br> Ministral-3B <br> Mistral-NeMo
 Nixtla | Not available | TimeGEN-1
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新模型目录概述文档"
}
```

### Explanation
此次对 `model-catalog-overview.md` 文档的修改主要涉及对微软Phi系列模型的更新，增加了“Phi-4-multimodal-instruct”这一新模型的描述。这一补充使得文档中的信息更加全面，帮助用户理解可用于不同任务的模型选项。

修改同样保留了先前Phi系列模型的相关信息，使得用户可以一目了然地获取到该系列模型的完整列表。此外，由于其他模型的描述结构未发生变化，这一改动并未对文档其余部分造成影响，因此整体结构依然保持一致性。

这样的更新不仅增强了文档的实用性，还保证开发者能够获取最新的信息，从而在使用Azure AI的模型时做出更为明智的选择。这一小幅改动有效提升了该文档的参考价值，并确保用户获取最新资源的便利性。

## articles/ai-studio/includes/region-availability-maas.md{#item-35d79c}

<details>
<summary>Diff</summary>
````diff
@@ -61,7 +61,7 @@ Llama 3.1 405B Instruct  | [Microsoft Managed countries/regions](/partner-center
 
 | Model | Offer Availability Region  | Hub/Project Region for Deployment  | Hub/Project Region for Fine tuning  |
 |---------|---------|---------|---------|
-Phi-4 <br>  Phi-4-mini-instruct    | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
+Phi-4 <br>  Phi-4-mini-instruct <br>  Phi-4-multimodal-instruct    | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
 Phi-3.5-vision-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
 Phi-3.5-MoE-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | East US 2       |
 Phi-3.5-Mini-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | East US 2  | East US 2       |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新区域可用性文档"
}
```

### Explanation
此次对 `region-availability-maas.md` 文档的修改主要是在关于Phi-4系列模型的描述中添加了“Phi-4-multimodal-instruct”模型。通过此次更新，用户能够清楚了解Phi-4系列模型的全面性和多种使用场景。

具体来说，文中根据模型的可用性列出了Phi-4及其附属模型的部署区域，保持了其他信息的完整性。除了为Phi-4系列添加新模型，文档依然保留已有的区域可用性信息，如不适用的类别和多个美国东部及欧洲瑞典的地域信息。这种更新提升了内容准确性与完整性，使得用户在选择模型时可以更好地找到适合其需求的选项。

通过这一小幅的修改，文档在反映最新模型可用性的同时，同时确保了用户能够方便地获取关于模型在不同区域的可用性信息，从而作出明智的决策。

## articles/ai-studio/quickstarts/get-started-code.md{#item-8a5082}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
-In this quickstart, we walk you through setting up your local development environment with the Azure AI Foundry SDK. We write a prompt, run it as part of your app code, trace the LLM calls being made, and run a basic evaluation on the outputs of the LLM.
+In this quickstart, we walk you through setting up your local development environment with the [Azure AI Foundry](https://ai.azure.com) SDK. We write a prompt, run it as part of your app code, trace the LLM calls being made, and run a basic evaluation on the outputs of the LLM.
 
 ## Prerequisites
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新快速入门文档中的链接"
}
```

### Explanation
此次对 `get-started-code.md` 文档的修改主要是在快速入门介绍中添加了对“Azure AI Foundry” SDK 的超链接。通过这次更新，用户可以直接访问 Azure AI Foundry 的官方网站，获得更详细的信息和资源。

具体来说，文档的文本描述中修改了一句话，将“Azure AI Foundry SDK” 变更为包含超链接的形式，指向其官方网站。这一小幅更新提高了文档的可用性，为用户提供了更加便利的访问途径，帮助他们更快找到所需资料。

除了添加链接外，文档的其余内容依然保持不变，因此整体结构和信息的连贯性并未受到影响。这种改进使得用户在设置本地开发环境时，可以更轻松地获取相关资源，从而提升用户体验和学习效率。

## articles/ai-studio/tutorials/copilot-sdk-build-rag.md{#item-b77dba}

<details>
<summary>Diff</summary>
````diff
@@ -15,7 +15,7 @@ ms.custom: copilot-learning-hub, ignite-2024
 
 # Tutorial:  Part 2 - Build a custom knowledge retrieval (RAG) app with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI Foundry SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
 
 This part two shows you how to enhance a basic chat application by adding [retrieval augmented generation (RAG)](../concepts/retrieval-augmented-generation.md) to ground the responses in your custom data. Retrieval Augmented Generation (RAG) is a pattern that uses your data with a large language model (LLM) to generate answers specific to your data. In this part two, you learn how to:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新教程文档中的链接"
}
```

### Explanation
此次对 `copilot-sdk-build-rag.md` 文档的修改主要是在教程的简介部分添加了对“Azure AI Foundry” SDK 的超链接，指向其官方网站。此举不仅提升了文档的可读性，同时也便利了用户对相关资源的访问。

具体来说，与之前的文本相比，文档现在包含一个超链接，使得用户可以方便地直接跳转到 Azure AI Foundry 的页面以获取更多信息，这对于想要深入了解或使用该 SDK 的用户而言是非常实用的。

除此之外，文档的其余内容保持不变，仍然详细说明了如何使用 Azure AI Foundry SDK（及其他库）来构建和配置一个针对零售公司 Contoso Trek 的聊天应用，并展示如何增强基本聊天应用以使用检索增强生成（RAG）技术。这种小幅更新使得用户在学习和实现过程中，可以更轻松地获取到所需的工具和信息，进一步提升了用户体验。

## articles/ai-studio/tutorials/copilot-sdk-create-resources.md{#item-552960}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 # Tutorial:  Part 1 - Set up project and development environment to build a custom knowledge retrieval (RAG) app with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI Foundry SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
 
 This tutorial is part one of a three-part tutorial.  This part one gets you ready to write code in part two and evaluate your chat app in part three. In this part, you:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新教程文档中的链接"
}
```

### Explanation
此次对 `copilot-sdk-create-resources.md` 文档的修改主要是在教程简介中的“Azure AI Foundry SDK”部分添加了超链接，指向其官方网站。这项更新提升了文档的可访问性，使得用户可以直接点击链接，获取更多关于 Azure AI Foundry 的信息。

具体来说，文档文本的修改是将提到 SDK 的内容进行修正，使其包含超链接，方便用户在构建和评估适用于零售公司 Contoso Trek 的聊天应用时，能够轻松访问相关资源。此教程为三部分系列教程的第一部分，旨在帮助用户准备好代码编写和后续的应用评估。

除了添加超链接外，文档的主体内容保持不变，依然详细说明了使用 Azure AI Foundry SDK（及其他库）构建、配置和评估聊天应用的过程。这样的更新确保了用户在学习过程中的信息获取更加便捷，进一步提高了教程的实用性。

## articles/ai-studio/tutorials/copilot-sdk-evaluate.md{#item-bb5754}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 # Tutorial: Part 3 - Evaluate a custom chat application with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI SDK (and other libraries) to  evaluate the chat app you built in [Part 2 of the tutorial series](copilot-sdk-build-rag.md). In this part three, you learn how to:
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to  evaluate the chat app you built in [Part 2 of the tutorial series](copilot-sdk-build-rag.md). In this part three, you learn how to:
 
 > [!div class="checklist"]
 > - Create an evaluation dataset
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新教程文档中的链接"
}
```

### Explanation
此次对 `copilot-sdk-evaluate.md` 文档的修改主要是在教程的内容中添加了对“Azure AI Foundry” SDK 的超链接，直接指向其官方网站。这项更新旨在提高文档的可读性和方便性，使得用户能够轻松获取关于 Azure AI Foundry 的更多信息。

具体而言，原文本中提到的 Azure AI SDK 被替换为带超链接的 Azure AI Foundry SDK，这增强了用户在评估其在第二部分构建的聊天应用时的体验。用户在阅读教程时，能够直接点击链接，快速访问相关资源了解更多信息。

除了添加超链接之外，文档的其他内容仍然保持不变，继续指导用户使用 Azure AI Foundry SDK 评估聊天应用，并且提到如何创建评估数据集。这样的细微更新不仅提升了文档的实用性，还方便了用户更好地进行学习与实践。

## articles/ai-studio/tutorials/screen-reader.md{#item-4dc029}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ This article is for people who use screen readers such as [Microsoft's Narrator]
 
 ## Getting oriented in Azure AI Foundry portal 
 
-Most Azure AI Foundry pages are composed of the following landmark structure: 
+Most [Azure AI Foundry](https://ai.azure.com) pages are composed of the following landmark structure: 
 
 - Banner (contains Azure AI Foundry app title, settings, and profile information)
     - Might sometimes contain a breadcrumb navigation element 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新屏幕阅读器教程中的链接"
}
```

### Explanation
此次对 `screen-reader.md` 文档的修改主要是在关于 Azure AI Foundry 页面结构的描述中添加了超链接，使得“Azure AI Foundry”可以直接链接到其官方网站。这一修改旨在提高用户访问相关资源的便利性，特别是对于使用屏幕阅读器的用户。

具体而言，修改前的文本简单提到 Azure AI Foundry 的页面结构，而修改后则将其转化为一个可点击的链接，用户可以通过这些链接快速访问该平台，获得更多信息。此类更新不仅增强了文档的互动性，也使得内容更具教育意义，帮助用户更好地理解和使用 Azure AI Foundry 的界面。

其他内容保持不变，继续旨在帮助使用屏幕阅读器的用户了解 Azure AI Foundry 门户的主要结构，从而提升他们的使用体验。这样的更新在确保信息准确性的同时，也大大提高了易用性和可访问性。


