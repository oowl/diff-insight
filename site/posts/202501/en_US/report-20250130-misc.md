---
date: '2025-01-30'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:b5fac8b...MicrosoftDocs:62147e0
summary: |-
  The recent code updates introduce a new deployment guide for the DeepSeek-R1 reasoning model, enhancing user support and integration across programming languages. Additionally, there are several minor updates, including changes to redirection paths, link corrections, metadata adjustments, and the removal of the `reference-model-inference-completions.md` document, which is a breaking change due to the crucial information it contained about the Azure AI Model Inference Completions API.

  The updates aim to improve the overall documentation quality for Azure AI Studio users. The introduction of the deployment guide marks a significant enhancement, reflecting Azure's commitment to supporting advanced AI models. However, the removal of the reference document signals potential changes in the API, underlining the need for clear communication from Azure AI regarding new guidance.

  Moreover, the updates include corrections to links and improvements to navigation, ensuring that users have access to up-to-date and functional resources. Metadata updates across documentation contribute to maintaining accuracy and relevance, which is vital in a fast-paced tech environment. Changes in the Table of Contents indicate an effort to streamline user experience in locating documentation.

  Overall, these modifications represent important steps towards achieving higher documentation standards, enabling developers to effectively utilize the diverse technologies offered by Azure AI.
title: '[en_US] Diff Insight Report - misc'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:b5fac8b...MicrosoftDocs:62147e0){target="_blank"}

<format>
# Highlights
The code diff introduces a new feature by adding a guide for deploying the DeepSeek-R1 reasoning model while presenting several minor updates including redirection path updates, link corrections, metadata adjustments, and removing the reference document for Model Inference Completions, a breaking change.

## New features
- Addition of a comprehensive deployment guide for the DeepSeek-R1 reasoning model outlining its capabilities and integration methods across several programming languages.

## Breaking changes
- Removal of the `reference-model-inference-completions.md` document, creating a significant gap as it included essential information regarding the Azure AI Model Inference Completions API.

## Other updates
- Redirection paths updated across multiple articles to improve documentation accuracy and navigation.
- Correction and update of links in numerous deployment and development guides to maintain valid references within the AI Studio structure.
- Regular updates to metadata across articles and images, ensuring up-to-date content.
- Minor reorganization in Table of Contents focusing on usability improvements.

# Insights
The recent changes encompass a mix of structural document updates, clarifications, and content enhancements aimed at maintaining a high standard of documentation for Azure AI Studio users. The most significant enhancement is the introduction of the DeepSeek-R1 reasoning model guide which provides in-depth deployment details, reflecting Azure AI's commitment to supporting advanced AI model implementations.

The removal of the `reference-model-inference-completions.md` file indicates potential changes or deprecated features in the Model Inference Completions API, urging users to seek new guidance or updates. This change highlights a necessity for proactive communication from Azure AI to ensure users are equipped with necessary transition information.

Frequent link corrections across various documentation articles underscore the importance of maintaining a seamless user experience in accessing up-to-date resources. A notable effort is seen in refining the navigation paths, which reflects Azure's focus on mitigating user frustration from broken links.

Metadata updates in numerous tools’ documents show a routine practice of content validation ensuring accuracy and relevance, crucial in a rapidly evolving tech landscape. Such updates build trust among users, confirming the reliability of the information they rely on while using Azure tools.

The modifications in the Table of Contents, encompassing the addition of new references and consolidation of API links, reveal an intent to simplify user interaction with the documentation, promoting efficiency in resource location while navigating Azure AI Studio's diverse tools and capabilities. 

Together, these adjustments signify incremental yet essential steps towards elevated documentation quality, facilitating developers in leveraging Azure AI's varied technologies to their full potential.
</format>

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [.openpublishing.redirection.ai-studio.json](#item-c75c21) | minor update | Update redirection paths in AI Studio articles | modified | 32 | 2 | 34 | 
| [toc.yml](#item-cd87b7) | minor update | Update Table of Contents (TOC) in AI Studio breadcrumbs | modified | 4 | 1 | 5 | 
| [deploy-models-cohere-command.md](#item-3e97f4) | minor update | Update link in Deploy Models Cohere Command article | modified | 1 | 1 | 2 | 
| [deploy-models-cohere-embed.md](#item-eab662) | minor update | Update link in Deploy Models Cohere Embed article | modified | 1 | 1 | 2 | 
| [deploy-models-deepseek.md](#item-7c33de) | new feature | Add DeepSeek-R1 Reasoning Model Deployment Guide | added | 1150 | 0 | 1150 | 
| [deploy-models-gretel-navigator.md](#item-2e9806) | minor update | Update link in Deploy Models Gretel Navigator article | modified | 1 | 1 | 2 | 
| [deploy-models-jais.md](#item-0bd11f) | minor update | Update link in Deploy Models Jais article | modified | 1 | 1 | 2 | 
| [deploy-models-jamba.md](#item-eeefca) | minor update | Update links in Deploy Models Jamba article | modified | 3 | 3 | 6 | 
| [deploy-models-llama.md](#item-6274a7) | minor update | Update link in Deploy Models Llama article | modified | 1 | 1 | 2 | 
| [deploy-models-mistral-codestral.md](#item-83ba03) | minor update | Update link in Deploy Models Mistral Codestral article | modified | 1 | 1 | 2 | 
| [deploy-models-mistral-nemo.md](#item-e7b729) | minor update | Update link in Deploy Models Mistral Nemo article | modified | 1 | 1 | 2 | 
| [deploy-models-mistral-open.md](#item-84e005) | minor update | Update link in Deploy Models Mistral Open article | modified | 1 | 1 | 2 | 
| [deploy-models-mistral.md](#item-487a41) | minor update | Update link in Deploy Models Mistral article | modified | 1 | 1 | 2 | 
| [deploy-models-phi-3-5-vision.md](#item-8d6d7d) | minor update | Update link in Deploy Models Phi 3.5 Vision article | modified | 1 | 1 | 2 | 
| [deploy-models-phi-3-vision.md](#item-bd5aae) | minor update | Update link in Deploy Models Phi 3 Vision article | modified | 1 | 1 | 2 | 
| [deploy-models-phi-3.md](#item-47e305) | minor update | Update link in Deploy Models Phi 3 article | modified | 1 | 1 | 2 | 
| [deploy-models-phi-4.md](#item-c40212) | minor update | Update link in Deploy Models Phi 4 article | modified | 1 | 1 | 2 | 
| [deploy-models-serverless.md](#item-f8177f) | minor update | Update links in Deploy Models Serverless article | modified | 2 | 2 | 4 | 
| [deploy-models-tsuzumi.md](#item-d3fd51) | minor update | Update link in Deploy Models Tsuzumi article | modified | 1 | 1 | 2 | 
| [langchain.md](#item-0d59f1) | minor update | Correct links in LangChain development article | modified | 2 | 2 | 4 | 
| [llama-index.md](#item-613372) | minor update | Update links in Llama Index development article | modified | 2 | 2 | 4 | 
| [semantic-kernel.md](#item-565785) | minor update | Correct links in Semantic Kernel development article | modified | 1 | 1 | 2 | 
| [trace-local-sdk.md](#item-f7dfb5) | minor update | Update link in Trace Local SDK article | modified | 1 | 1 | 2 | 
| [azure-open-ai-gpt-4v-tool.md](#item-053d0d) | minor update | Update date in Azure OpenAI GPT-4V Tool article | modified | 1 | 1 | 2 | 
| [content-safety-tool.md](#item-09b048) | minor update | Update date in Content Safety Tool article | modified | 1 | 1 | 2 | 
| [embedding-tool.md](#item-81420c) | minor update | Update date in Embedding Tool article | modified | 1 | 1 | 2 | 
| [index-lookup-tool.md](#item-cad66d) | minor update | Update date in Index Lookup Tool article | modified | 1 | 1 | 2 | 
| [llm-tool.md](#item-6691f4) | minor update | Update date in LLM Tool article | modified | 1 | 1 | 2 | 
| [prompt-flow-tools-overview.md](#item-fd7471) | minor update | Update date in Prompt Flow Tools Overview article | modified | 1 | 1 | 2 | 
| [prompt-tool.md](#item-c6a9a0) | minor update | Update date in Prompt Tool article | modified | 1 | 1 | 2 | 
| [python-tool.md](#item-c9200f) | minor update | Update date in Python Tool article | modified | 1 | 1 | 2 | 
| [rerank-tool.md](#item-dd52bf) | minor update | Update date in Rerank Tool article | modified | 1 | 1 | 2 | 
| [azure-openai-gpt-4-vision-tool.png](#item-94def5) | minor update | Update metadata for Azure OpenAI GPT-4 Vision Tool image | modified | 0 | 0 | 0 | 
| [content-safety-tool.png](#item-c673ee) | minor update | Update metadata for Content Safety Tool image | modified | 0 | 0 | 0 | 
| [prompt-tool.png](#item-245957) | minor update | Update metadata for Prompt Tool image | modified | 0 | 0 | 0 | 
| [python-tool.png](#item-14e9b3) | minor update | Update metadata for Python Tool image | modified | 0 | 0 | 0 | 
| [reference-model-inference-completions.md](#item-bae39d) | breaking change | Remove Reference for Model Inference Completions | removed | 0 | 296 | 296 | 
| [toc.yml](#item-2745cd) | minor update | Update Table of Contents for AI Studio | modified | 3 | 13 | 16 | 


# Modified Contents
## articles/ai-studio/.openpublishing.redirection.ai-studio.json{#item-c75c21}

<details>
<summary>Diff</summary>
````diff
@@ -180,6 +180,11 @@
             "redirect_url": "/azure/ai-studio/concepts/content-filtering",
             "redirect_document_id": false
         },
+        {
+            "source_path_from_root": "/articles/ai-studio/reference/reference-model-inference-completions.md",
+            "redirect_url": "/azure/ai-studio/reference/reference-model-inference-chat-completions",
+            "redirect_document_id": false
+        },
         {
             "source_path_from_root": "/articles/ai-studio/concepts/evaluation-improvement-strategies.md",
             "redirect_url": "/azure/ai-studio/concepts/evaluation-approach-gen-ai",
@@ -207,8 +212,8 @@
           },
           {
             "source_path_from_root": "/articles/ai-studio/ai-services/how-to/content-safety.md",
-            "redirect_url": "/azure/ai-foundry/model-inference/how-to/configure-content-filters",
-            "redirect_document_id": false
+            "redirect_url": "/azure/ai-services/content-safety/how-to/foundry",
+            "redirect_document_id": true
           },
           {
             "source_path_from_root": "/articles/ai-studio/ai-services/concepts/quotas-limits.md",
@@ -234,6 +239,31 @@
               "source_path_from_root": "/articles/ai-studio/how-to/data-image-add.md",
               "redirect_url": "/azure/ai-studio/quickstarts/multimodal-vision",
               "redirect_document_id": false
+          },
+          {
+            "source_path_from_root": "/articles/ai-studio/reference/reference-model-inference-api.md",
+            "redirect_url": "/azure/ai-foundry/model-inference/reference/reference-model-inference-api",
+            "redirect_document_id": false
+          },
+          {
+            "source_path_from_root": "/articles/ai-studio/reference/reference-model-inference-info.md",
+            "redirect_url": "/azure/ai-foundry/model-inference/reference/reference-model-inference-info",
+            "redirect_document_id": false
+          },
+          {
+            "source_path_from_root": "/articles/ai-studio/reference/reference-model-inference-embeddings.md",
+            "redirect_url": "/azure/ai-foundry/model-inference/reference/reference-model-inference-embeddings",
+            "redirect_document_id": false
+          },
+          {
+            "source_path_from_root": "/articles/ai-studio/reference/reference-model-inference-chat-completions.md",
+            "redirect_url": "/azure/ai-foundry/model-inference/reference/reference-model-inference-chat-completions",
+            "redirect_document_id": false
+          },
+          {
+            "source_path_from_root": "/articles/ai-studio/reference/reference-model-inference-images-embeddings.md",
+            "redirect_url": "/azure/ai-foundry/model-inference/reference/reference-model-inference-images-embeddings",
+            "redirect_document_id": false
           }
     ]
 }
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update redirection paths in AI Studio articles"
}
```

### Explanation
This modification involves updates to the redirection paths specified in the `.openpublishing.redirection.ai-studio.json` configuration file for the AI Studio articles. A total of 34 changes were made, including the addition of 32 new lines and the removal of 2 lines. 

The changes primarily include new entries that define redirection paths for various reference articles within the AI Studio documentation. For instance, new redirects were added for articles about model inference completions, model inference APIs, and embeddings, ensuring that users are directed to the correct locations in the documentation when accessing these topics.

Additionally, some existing redirection paths were modified to point to updated URLs, thereby enhancing the navigation experience for users seeking information relevant to Azure AI Studio. These adjustments help in ensuring that the documentation remains accurate and aligned with the latest developments and structure of the content.

## articles/ai-studio/breadcrumb/toc.yml{#item-cd87b7}

<details>
<summary>Diff</summary>
````diff
@@ -2,6 +2,9 @@
   tocHref: /azure/
   topicHref: /azure/index
   items:
-  - name: Azure AI Foundry
+  - name: AI Foundry
     tocHref: /azure/ai-studio/
     topicHref: /azure/ai-studio/index
+  - name: AI Foundry
+    tocHref: /azure/ai-services/
+    topicHref: /azure/ai-studio/index
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update Table of Contents (TOC) in AI Studio breadcrumbs"
}
```

### Explanation
This modification involves a minor update to the Table of Contents (TOC) configuration file, `toc.yml`, for the AI Studio breadcrumb navigation. A total of 5 changes were made, which included 4 new lines being added and 1 line being removed.

The key updates include renaming the entry from "Azure AI Foundry" to "AI Foundry" in the breadcrumb and adding a new entry for "AI Foundry" that links to the Azure AI Services section. This revision aims to simplify the navigation by clarifying the naming and structure of the TOC, ensuring that users can easily find their way through the Azure AI Studio documentation.

Overall, these changes enhance the user experience by refining the breadcrumb links and making the navigation more intuitive for users accessing the AI Studio articles.

## articles/ai-studio/how-to/deploy-models-cohere-command.md{#item-3e97f4}

<details>
<summary>Diff</summary>
````diff
@@ -2161,7 +2161,7 @@ For more information on how to track costs, see [Monitor costs for models offere
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Cohere Command article"
}
```

### Explanation
This modification centers on a minor update to the `deploy-models-cohere-command.md` article within the AI Studio documentation. The changes involve updating a single link with one line added and one line removed, totaling two changes.

Specifically, the link to the "Azure AI Model Inference API" reference has been modified to reflect an updated path. The original link has been changed from a relative path to the AI Studio reference location to a more precise path leading into the AI Foundry structure, thus enhancing the accuracy of the navigation within the documentation.

This adjustment ensures that users accessing this article will be directed to the correct and updated location for further information regarding the Azure AI Model Inference API, ultimately improving the user experience while navigating the AI Studio documentation.

## articles/ai-studio/how-to/deploy-models-cohere-embed.md{#item-eab662}

<details>
<summary>Diff</summary>
````diff
@@ -663,7 +663,7 @@ Quota is managed per deployment. Each deployment has a rate limit of 200,000 tok
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Cohere Embed article"
}
```

### Explanation
This modification is a minor update to the `deploy-models-cohere-embed.md` article in the AI Studio documentation. The changes consist of a single line being added and a single line being removed, resulting in two overall changes.

The update specifically involves adjusting the link for the "Azure AI Model Inference API." The original relative link was modified to provide a more accurate pathway that reflects its location within the AI Foundry context. This oversight correction enhances the clarity and accessibility of related content for users who are following the documentation.

By ensuring that the link directs users to the correct reference for the Azure AI Model Inference API, this update improves the overall user experience when navigating the documentation, enabling easier access to essential resources associated with deploying models.

## articles/ai-studio/how-to/deploy-models-deepseek.md{#item-7c33de}

<details>
<summary>Diff</summary>
````diff
@@ -0,0 +1,1150 @@
+---
+title: How to use DeepSeek-R1 reasoning model with Azure AI Foundry
+titleSuffix: Azure AI Foundry
+description: Learn how to use DeepSeek-R1 reasoning model with Azure AI Foundry.
+manager: scottpolly
+author: msakande
+reviewer: santiagxf
+ms.service: azure-ai-studio
+ms.topic: how-to
+ms.date: 01/29/2025
+ms.author: mopeakande
+ms.reviewer: fasantia
+ms.custom: references_regions, generated
+zone_pivot_groups: azure-ai-model-catalog-samples-chat
+---
+
+# How to use DeepSeek-R1 reasoning model
+
+[!INCLUDE [Feature preview](~/reusable-content/ce-skilling/azure/includes/ai-studio/includes/feature-preview.md)]
+
+In this article, you learn about DeepSeek-R1 and how to use them.
+DeepSeek-R1 excels at reasoning tasks using a step-by-step training process, such as language, scientific reasoning, and coding tasks. It features 671B total parameters with 37B active parameters, and 128k context length.
+
+
+
+::: zone pivot="programming-language-python"
+
+## DeepSeek-R1
+
+DeepSeek-R1 builds on the progress of earlier reasoning-focused models that improved performance by extending Chain-of-Thought (CoT) reasoning. DeepSeek-R1 takes things further by combining reinforcement learning (RL) with fine-tuning on carefully chosen datasets. It evolved from an earlier version, DeepSeek-R1-Zero, which relied solely on RL and showed strong reasoning skills but had issues like hard-to-read outputs and language inconsistencies. To address these limitations, DeepSeek-R1 incorporates a small amount of cold-start data and follows a refined training pipeline that blends reasoning-oriented RL with supervised fine-tuning on curated datasets, resulting in a model that achieves state-of-the-art performance on reasoning benchmarks.
+
+
+You can learn more about the models in their respective model card:
+
+* [DeepSeek-R1](https://aka.ms/azureai/landing/DeepSeek-R1)
+
+
+## Prerequisites
+
+To use DeepSeek-R1 with Azure AI Foundry, you need the following prerequisites:
+
+### A model deployment
+
+**Deployment to serverless APIs**
+
+DeepSeek-R1 can be deployed to serverless API endpoints with pay-as-you-go billing. This kind of deployment provides a way to consume models as an API without hosting them on your subscription, while keeping the enterprise security and compliance that organizations need. 
+
+Deployment to a serverless API endpoint doesn't require quota from your subscription. If your model isn't deployed already, use the Azure AI Studio, Azure Machine Learning SDK for Python, the Azure CLI, or ARM templates to [deploy the model as a serverless API](deploy-models-serverless.md).
+
+> [!div class="nextstepaction"]
+> [Deploy the model to serverless API endpoints](deploy-models-serverless.md)
+
+### The inference package installed
+
+You can consume predictions from this model by using the `azure-ai-inference` package with Python. To install this package, you need the following prerequisites:
+
+* Python 3.8 or later installed, including pip.
+* The endpoint URL. To construct the client library, you need to pass in the endpoint URL. The endpoint URL has the form `https://your-host-name.your-azure-region.inference.ai.azure.com`, where `your-host-name` is your unique model deployment host name and `your-azure-region` is the Azure region where the model is deployed (for example, eastus2).
+* Depending on your model deployment and authentication preference, you need either a key to authenticate against the service, or Microsoft Entra ID credentials. The key is a 32-character string.
+  
+Once you have these prerequisites, install the Azure AI inference package with the following command:
+
+```bash
+pip install azure-ai-inference
+```
+
+Read more about the [Azure AI inference package and reference](https://aka.ms/azsdk/azure-ai-inference/python/reference).
+
+## Work with chat completions
+
+In this section, you use the [Azure AI model inference API](https://aka.ms/azureai/modelinference) with a chat completions model for chat.
+
+> [!TIP]
+> The [Azure AI model inference API](https://aka.ms/azureai/modelinference) allows you to talk with most models deployed in Azure AI Foundry with the same code and structure, including DeepSeek-R1.
+
+### Create a client to consume the model
+
+First, create the client to consume the model. The following code uses an endpoint URL and key that are stored in environment variables.
+
+
+```python
+import os
+from azure.ai.inference import ChatCompletionsClient
+from azure.core.credentials import AzureKeyCredential
+
+client = ChatCompletionsClient(
+    endpoint=os.environ["AZURE_INFERENCE_ENDPOINT"],
+    credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
+)
+```
+
+### Get the model's capabilities
+
+The `/info` route returns information about the model that is deployed to the endpoint. Return the model's information by calling the following method:
+
+
+```python
+model_info = client.get_model_info()
+```
+
+The response is as follows:
+
+
+```python
+print("Model name:", model_info.model_name)
+print("Model type:", model_info.model_type)
+print("Model provider name:", model_info.model_provider_name)
+```
+
+```console
+Model name: DeepSeek-R1
+Model type: chat-completions
+Model provider name: DeepSeek
+```
+
+### Create a chat completion request
+
+The following example shows how you can create a basic chat completions request to the model.
+
+```python
+from azure.ai.inference.models import SystemMessage, UserMessage
+
+response = client.complete(
+    messages=[
+        SystemMessage(content="You are a helpful assistant."),
+        UserMessage(content="How many languages are in the world?"),
+    ],
+)
+```
+
+The response is as follows, where you can see the model's usage statistics:
+
+
+```python
+print("Response:", response.choices[0].message.content)
+print("Model:", response.model)
+print("Usage:")
+print("\tPrompt tokens:", response.usage.prompt_tokens)
+print("\tTotal tokens:", response.usage.total_tokens)
+print("\tCompletion tokens:", response.usage.completion_tokens)
+```
+
+```console
+Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
+Model: DeepSeek-R1
+Usage: 
+  Prompt tokens: 19
+  Total tokens: 91
+  Completion tokens: 72
+```
+
+Inspect the `usage` section in the response to see the number of tokens used for the prompt, the total number of tokens generated, and the number of tokens used for the completion.
+
+#### Understanding reasoning
+
+Some reasoning models, like DeepSeek-R1, generate completions and include the reasoning behind it. The reasoning associated with the completion is included in the response's content within the tags `<think>` and `</think>`. The model may select on which scenarios to generate reasoning content. For example:
+
+
+```python
+response = client.complete(
+    messages=[
+        SystemMessage(content="You are a helpful assistant."),
+        UserMessage(content="How many languages are in the world?"),
+    ],
+)
+```
+
+You can extract the reasoning content from the response to understand the model's thought process as follows:
+
+
+```python
+import re
+
+match = re.match(r"<think>(.*?)</think>(.*)", response.choices[0].message.content, re.DOTALL)
+
+print("Response:", )
+if match:
+    print("\tThinking:", match.group(1))
+    print("\tAnswer:", match.group(2))
+else:
+    print("\tAnswer:", response.choices[0].message.content)
+print("Model:", response.model)
+print("Usage:")
+print("\tPrompt tokens:", response.usage.prompt_tokens)
+print("\tTotal tokens:", response.usage.total_tokens)
+print("\tCompletion tokens:", response.usage.completion_tokens)
+```
+
+```console
+Thinking: Okay, the user is asking how many languages exist in the world. I need to provide a clear and accurate answer. Let's start by recalling the general consensus from linguistic sources. I remember that the number often cited is around 7,000, but maybe I should check some reputable organizations.\n\nEthnologue is a well-known resource for language data, and I think they list about 7,000 languages. But wait, do they update their numbers? It might be around 7,100 or so. Also, the exact count can vary because some sources might categorize dialects differently or have more recent data. \n\nAnother thing to consider is language endangerment. Many languages are endangered, with some having only a few speakers left. Organizations like UNESCO track endangered languages, so mentioning that adds context. Also, the distribution isn't even. Some countries have hundreds of languages, like Papua New Guinea with over 800, while others have just a few. \n\nA user might also wonder why the exact number is hard to pin down. It's because the distinction between a language and a dialect can be political or cultural. For example, Mandarin and Cantonese are considered dialects of Chinese by some, but they're mutually unintelligible, so others classify them as separate languages. Also, some regions are under-researched, making it hard to document all languages. \n\nI should also touch on language families. The 7,000 languages are grouped into families like Indo-European, Sino-Tibetan, Niger-Congo, etc. Maybe mention a few of the largest families. But wait, the question is just about the count, not the families. Still, it's good to provide a bit more context. \n\nI need to make sure the information is up-to-date. Let me think – recent estimates still hover around 7,000. However, languages are dying out rapidly, so the number decreases over time. Including that note about endangerment and language extinction rates could be helpful. For instance, it's often stated that a language dies every few weeks. \n\nAnother point is sign languages. Does the count include them? Ethnologue includes some, but not all sources might. If the user is including sign languages, that adds more to the count, but I think the 7,000 figure typically refers to spoken languages. For thoroughness, maybe mention that there are also over 300 sign languages. \n\nSummarizing, the answer should state around 7,000, mention Ethnologue's figure, explain why the exact number varies, touch on endangerment, and possibly note sign languages as a separate category. Also, a brief mention of Papua New Guinea as the most linguistically diverse country. \n\nWait, let me verify Ethnologue's current number. As of their latest edition (25th, 2022), they list 7,168 living languages. But I should check if that's the case. Some sources might round to 7,000. Also, SIL International publishes Ethnologue, so citing them as reference makes sense. \n\nOther sources, like Glottolog, might have a different count because they use different criteria. Glottolog might list around 7,000 as well, but exact numbers vary. It's important to highlight that the count isn't exact because of differing definitions and ongoing research. \n\nIn conclusion, the approximate number is 7,000, with Ethnologue being a key source, considerations of endangerment, and the challenges in counting due to dialect vs. language distinctions. I should make sure the answer is clear, acknowledges the variability, and provides key points succinctly.
+
+Answer: The exact number of languages in the world is challenging to determine due to differences in definitions (e.g., distinguishing languages from dialects) and ongoing documentation efforts. However, widely cited estimates suggest there are approximately **7,000 languages** globally.
+Model: DeepSeek-R1
+Usage: 
+  Prompt tokens: 11
+  Total tokens: 897
+  Completion tokens: 886
+```
+
+#### Stream content
+
+By default, the completions API returns the entire generated content in a single response. If you're generating long completions, waiting for the response can take many seconds.
+
+You can _stream_ the content to get it as it's being generated. Streaming content allows you to start processing the completion as content becomes available. This mode returns an object that streams back the response as [data-only server-sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events). Extract chunks from the delta field, rather than the message field.
+
+
+```python
+result = client.complete(
+    messages=[
+        SystemMessage(content="You are a helpful assistant."),
+        UserMessage(content="How many languages are in the world?"),
+    ],
+    temperature=0,
+    top_p=1,
+    max_tokens=2048,
+    stream=True,
+)
+```
+
+To stream completions, set `stream=True` when you call the model.
+
+To visualize the output, define a helper function to print the stream.
+
+```python
+def print_stream(result):
+    """
+    Prints the chat completion with streaming.
+    """
+    for update in result:
+        if update.choices:
+            print(update.choices[0].delta.content, end="")
+```
+
+You can visualize how streaming generates content:
+
+
+```python
+print_stream(result)
+```
+
+### Apply content safety
+
+The Azure AI model inference API supports [Azure AI content safety](https://aka.ms/azureaicontentsafety). When you use deployments with Azure AI content safety turned on, inputs and outputs pass through an ensemble of classification models aimed at detecting and preventing the output of harmful content. The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
+
+The following example shows how to handle events when the model detects harmful content in the input prompt and content safety is enabled.
+
+
+```python
+from azure.ai.inference.models import AssistantMessage, UserMessage, SystemMessage
+
+try:
+    response = client.complete(
+        messages=[
+            SystemMessage(content="You are an AI assistant that helps people find information."),
+            UserMessage(content="Chopping tomatoes and cutting them into cubes or wedges are great ways to practice your knife skills."),
+        ]
+    )
+
+    print(response.choices[0].message.content)
+
+except HttpResponseError as ex:
+    if ex.status_code == 400:
+        response = ex.response.json()
+        if isinstance(response, dict) and "error" in response:
+            print(f"Your request triggered an {response['error']['code']} error:\n\t {response['error']['message']}")
+        else:
+            raise
+    raise
+```
+
+> [!TIP]
+> To learn more about how you can configure and control Azure AI content safety settings, check the [Azure AI content safety documentation](https://aka.ms/azureaicontentsafety).
+
+::: zone-end
+
+
+::: zone pivot="programming-language-javascript"
+
+## DeepSeek-R1
+
+DeepSeek-R1 builds on the progress of earlier reasoning-focused models that improved performance by extending Chain-of-Thought (CoT) reasoning. DeepSeek-R1 takes things further by combining reinforcement learning (RL) with fine-tuning on carefully chosen datasets. It evolved from an earlier version, DeepSeek-R1-Zero, which relied solely on RL and showed strong reasoning skills but had issues like hard-to-read outputs and language inconsistencies. To address these limitations, DeepSeek-R1 incorporates a small amount of cold-start data and follows a refined training pipeline that blends reasoning-oriented RL with supervised fine-tuning on curated datasets, resulting in a model that achieves state-of-the-art performance on reasoning benchmarks.
+
+
+You can learn more about the models in their respective model card:
+
+* [DeepSeek-R1](https://aka.ms/azureai/landing/DeepSeek-R1)
+
+
+## Prerequisites
+
+To use DeepSeek-R1 with Azure AI Foundry, you need the following prerequisites:
+
+### A model deployment
+
+**Deployment to serverless APIs**
+
+DeepSeek-R1 can be deployed to serverless API endpoints with pay-as-you-go billing. This kind of deployment provides a way to consume models as an API without hosting them on your subscription, while keeping the enterprise security and compliance that organizations need. 
+
+Deployment to a serverless API endpoint doesn't require quota from your subscription. If your model isn't deployed already, use the Azure AI Studio, Azure Machine Learning SDK for Python, the Azure CLI, or ARM templates to [deploy the model as a serverless API](deploy-models-serverless.md).
+
+> [!div class="nextstepaction"]
+> [Deploy the model to serverless API endpoints](deploy-models-serverless.md)
+
+### The inference package installed
+
+You can consume predictions from this model by using the `@azure-rest/ai-inference` package from `npm`. To install this package, you need the following prerequisites:
+
+* LTS versions of `Node.js` with `npm`.
+* The endpoint URL. To construct the client library, you need to pass in the endpoint URL. The endpoint URL has the form `https://your-host-name.your-azure-region.inference.ai.azure.com`, where `your-host-name` is your unique model deployment host name and `your-azure-region` is the Azure region where the model is deployed (for example, eastus2).
+* Depending on your model deployment and authentication preference, you need either a key to authenticate against the service, or Microsoft Entra ID credentials. The key is a 32-character string.
+
+Once you have these prerequisites, install the Azure Inference library for JavaScript with the following command:
+
+```bash
+npm install @azure-rest/ai-inference
+```
+
+## Work with chat completions
+
+In this section, you use the [Azure AI model inference API](https://aka.ms/azureai/modelinference) with a chat completions model for chat.
+
+> [!TIP]
+> The [Azure AI model inference API](https://aka.ms/azureai/modelinference) allows you to talk with most models deployed in Azure AI Foundry with the same code and structure, including DeepSeek-R1.
+
+### Create a client to consume the model
+
+First, create the client to consume the model. The following code uses an endpoint URL and key that are stored in environment variables.
+
+
+```javascript
+import ModelClient from "@azure-rest/ai-inference";
+import { isUnexpected } from "@azure-rest/ai-inference";
+import { AzureKeyCredential } from "@azure/core-auth";
+
+const client = new ModelClient(
+    process.env.AZURE_INFERENCE_ENDPOINT, 
+    new AzureKeyCredential(process.env.AZURE_INFERENCE_CREDENTIAL)
+);
+```
+
+### Get the model's capabilities
+
+The `/info` route returns information about the model that is deployed to the endpoint. Return the model's information by calling the following method:
+
+
+```javascript
+var model_info = await client.path("/info").get()
+```
+
+The response is as follows:
+
+
+```javascript
+console.log("Model name: ", model_info.body.model_name)
+console.log("Model type: ", model_info.body.model_type)
+console.log("Model provider name: ", model_info.body.model_provider_name)
+```
+
+```console
+Model name: DeepSeek-R1
+Model type: chat-completions
+Model provider name: DeepSeek
+```
+
+### Create a chat completion request
+
+The following example shows how you can create a basic chat completions request to the model.
+
+```javascript
+var messages = [
+    { role: "system", content: "You are a helpful assistant" },
+    { role: "user", content: "How many languages are in the world?" },
+];
+
+var response = await client.path("/chat/completions").post({
+    body: {
+        messages: messages,
+    }
+});
+```
+
+The response is as follows, where you can see the model's usage statistics:
+
+
+```javascript
+if (isUnexpected(response)) {
+    throw response.body.error;
+}
+
+console.log("Response: ", response.body.choices[0].message.content);
+console.log("Model: ", response.body.model);
+console.log("Usage:");
+console.log("\tPrompt tokens:", response.body.usage.prompt_tokens);
+console.log("\tTotal tokens:", response.body.usage.total_tokens);
+console.log("\tCompletion tokens:", response.body.usage.completion_tokens);
+```
+
+```console
+Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
+Model: DeepSeek-R1
+Usage: 
+  Prompt tokens: 19
+  Total tokens: 91
+  Completion tokens: 72
+```
+
+Inspect the `usage` section in the response to see the number of tokens used for the prompt, the total number of tokens generated, and the number of tokens used for the completion.
+
+#### Understanding reasoning
+
+Some reasoning models, like DeepSeek-R1, generate completions and include the reasoning behind it. The reasoning associated with the completion is included in the response's content within the tags `<think>` and `</think>`. The model may select on which scenarios to generate reasoning content. For example:
+
+
+```javascript
+var messages = [
+    { role: "system", content: "You are a helpful assistant" },
+    { role: "user", content: "How many languages are in the world?" },
+];
+
+var response = await client.path("/chat/completions").post({
+    body: {
+        messages: messages,
+    }
+});
+```
+
+You can extract the reasoning content from the response to understand the model's thought process as follows:
+
+
+```javascript
+var content = response.body.choices[0].message.content
+var match = content.match(/<think>(.*?)<\/think>(.*)/s);
+
+console.log("Response:");
+if (match) {
+    console.log("\tThinking:", match[1]);
+    console.log("\Answer:", match[2]);
+}
+else {
+    console.log("Response:", content);
+}
+console.log("Model: ", response.body.model);
+console.log("Usage:");
+console.log("\tPrompt tokens:", response.body.usage.prompt_tokens);
+console.log("\tTotal tokens:", response.body.usage.total_tokens);
+console.log("\tCompletion tokens:", response.body.usage.completion_tokens);
+```
+
+```console
+Thinking: Okay, the user is asking how many languages exist in the world. I need to provide a clear and accurate answer. Let's start by recalling the general consensus from linguistic sources. I remember that the number often cited is around 7,000, but maybe I should check some reputable organizations.\n\nEthnologue is a well-known resource for language data, and I think they list about 7,000 languages. But wait, do they update their numbers? It might be around 7,100 or so. Also, the exact count can vary because some sources might categorize dialects differently or have more recent data. \n\nAnother thing to consider is language endangerment. Many languages are endangered, with some having only a few speakers left. Organizations like UNESCO track endangered languages, so mentioning that adds context. Also, the distribution isn't even. Some countries have hundreds of languages, like Papua New Guinea with over 800, while others have just a few. \n\nA user might also wonder why the exact number is hard to pin down. It's because the distinction between a language and a dialect can be political or cultural. For example, Mandarin and Cantonese are considered dialects of Chinese by some, but they're mutually unintelligible, so others classify them as separate languages. Also, some regions are under-researched, making it hard to document all languages. \n\nI should also touch on language families. The 7,000 languages are grouped into families like Indo-European, Sino-Tibetan, Niger-Congo, etc. Maybe mention a few of the largest families. But wait, the question is just about the count, not the families. Still, it's good to provide a bit more context. \n\nI need to make sure the information is up-to-date. Let me think – recent estimates still hover around 7,000. However, languages are dying out rapidly, so the number decreases over time. Including that note about endangerment and language extinction rates could be helpful. For instance, it's often stated that a language dies every few weeks. \n\nAnother point is sign languages. Does the count include them? Ethnologue includes some, but not all sources might. If the user is including sign languages, that adds more to the count, but I think the 7,000 figure typically refers to spoken languages. For thoroughness, maybe mention that there are also over 300 sign languages. \n\nSummarizing, the answer should state around 7,000, mention Ethnologue's figure, explain why the exact number varies, touch on endangerment, and possibly note sign languages as a separate category. Also, a brief mention of Papua New Guinea as the most linguistically diverse country. \n\nWait, let me verify Ethnologue's current number. As of their latest edition (25th, 2022), they list 7,168 living languages. But I should check if that's the case. Some sources might round to 7,000. Also, SIL International publishes Ethnologue, so citing them as reference makes sense. \n\nOther sources, like Glottolog, might have a different count because they use different criteria. Glottolog might list around 7,000 as well, but exact numbers vary. It's important to highlight that the count isn't exact because of differing definitions and ongoing research. \n\nIn conclusion, the approximate number is 7,000, with Ethnologue being a key source, considerations of endangerment, and the challenges in counting due to dialect vs. language distinctions. I should make sure the answer is clear, acknowledges the variability, and provides key points succinctly.
+
+Answer: The exact number of languages in the world is challenging to determine due to differences in definitions (e.g., distinguishing languages from dialects) and ongoing documentation efforts. However, widely cited estimates suggest there are approximately **7,000 languages** globally.
+Model: DeepSeek-R1
+Usage: 
+  Prompt tokens: 11
+  Total tokens: 897
+  Completion tokens: 886
+```
+
+#### Stream content
+
+By default, the completions API returns the entire generated content in a single response. If you're generating long completions, waiting for the response can take many seconds.
+
+You can _stream_ the content to get it as it's being generated. Streaming content allows you to start processing the completion as content becomes available. This mode returns an object that streams back the response as [data-only server-sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events). Extract chunks from the delta field, rather than the message field.
+
+
+```javascript
+var messages = [
+    { role: "system", content: "You are a helpful assistant" },
+    { role: "user", content: "How many languages are in the world?" },
+];
+
+var response = await client.path("/chat/completions").post({
+    body: {
+        messages: messages,
+    }
+}).asNodeStream();
+```
+
+To stream completions, use `.asNodeStream()` when you call the model.
+
+You can visualize how streaming generates content:
+
+
+```javascript
+var stream = response.body;
+if (!stream) {
+    stream.destroy();
+    throw new Error(`Failed to get chat completions with status: ${response.status}`);
+}
+
+if (response.status !== "200") {
+    throw new Error(`Failed to get chat completions: ${response.body.error}`);
+}
+
+var sses = createSseStream(stream);
+
+for await (const event of sses) {
+    if (event.data === "[DONE]") {
+        return;
+    }
+    for (const choice of (JSON.parse(event.data)).choices) {
+        console.log(choice.delta?.content ?? "");
+    }
+}
+```
+
+### Apply content safety
+
+The Azure AI model inference API supports [Azure AI content safety](https://aka.ms/azureaicontentsafety). When you use deployments with Azure AI content safety turned on, inputs and outputs pass through an ensemble of classification models aimed at detecting and preventing the output of harmful content. The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
+
+The following example shows how to handle events when the model detects harmful content in the input prompt and content safety is enabled.
+
+
+```javascript
+try {
+    var messages = [
+        { role: "system", content: "You are an AI assistant that helps people find information." },
+        { role: "user", content: "Chopping tomatoes and cutting them into cubes or wedges are great ways to practice your knife skills." },
+    ];
+
+    var response = await client.path("/chat/completions").post({
+        body: {
+            messages: messages,
+        }
+    });
+
+    console.log(response.body.choices[0].message.content);
+}
+catch (error) {
+    if (error.status_code == 400) {
+        var response = JSON.parse(error.response._content);
+        if (response.error) {
+            console.log(`Your request triggered an ${response.error.code} error:\n\t ${response.error.message}`);
+        }
+        else
+        {
+            throw error;
+        }
+    }
+}
+```
+
+> [!TIP]
+> To learn more about how you can configure and control Azure AI content safety settings, check the [Azure AI content safety documentation](https://aka.ms/azureaicontentsafety).
+
+::: zone-end
+
+
+::: zone pivot="programming-language-csharp"
+
+## DeepSeek-R1
+
+DeepSeek-R1 builds on the progress of earlier reasoning-focused models that improved performance by extending Chain-of-Thought (CoT) reasoning. DeepSeek-R1 takes things further by combining reinforcement learning (RL) with fine-tuning on carefully chosen datasets. It evolved from an earlier version, DeepSeek-R1-Zero, which relied solely on RL and showed strong reasoning skills but had issues like hard-to-read outputs and language inconsistencies. To address these limitations, DeepSeek-R1 incorporates a small amount of cold-start data and follows a refined training pipeline that blends reasoning-oriented RL with supervised fine-tuning on curated datasets, resulting in a model that achieves state-of-the-art performance on reasoning benchmarks.
+
+
+You can learn more about the models in their respective model card:
+
+* [DeepSeek-R1](https://aka.ms/azureai/landing/DeepSeek-R1)
+
+
+## Prerequisites
+
+To use DeepSeek-R1 with Azure AI Foundry, you need the following prerequisites:
+
+### A model deployment
+
+**Deployment to serverless APIs**
+
+DeepSeek-R1 can be deployed to serverless API endpoints with pay-as-you-go billing. This kind of deployment provides a way to consume models as an API without hosting them on your subscription, while keeping the enterprise security and compliance that organizations need. 
+
+Deployment to a serverless API endpoint doesn't require quota from your subscription. If your model isn't deployed already, use the Azure AI Studio, Azure Machine Learning SDK for Python, the Azure CLI, or ARM templates to [deploy the model as a serverless API](deploy-models-serverless.md).
+
+> [!div class="nextstepaction"]
+> [Deploy the model to serverless API endpoints](deploy-models-serverless.md)
+
+### The inference package installed
+
+You can consume predictions from this model by using the `Azure.AI.Inference` package from [NuGet](https://www.nuget.org/). To install this package, you need the following prerequisites:
+
+* The endpoint URL. To construct the client library, you need to pass in the endpoint URL. The endpoint URL has the form `https://your-host-name.your-azure-region.inference.ai.azure.com`, where `your-host-name` is your unique model deployment host name and `your-azure-region` is the Azure region where the model is deployed (for example, eastus2).
+* Depending on your model deployment and authentication preference, you need either a key to authenticate against the service, or Microsoft Entra ID credentials. The key is a 32-character string.
+
+Once you have these prerequisites, install the Azure AI inference library with the following command:
+
+```dotnetcli
+dotnet add package Azure.AI.Inference --prerelease
+```
+
+You can also authenticate with Microsoft Entra ID (formerly Azure Active Directory). To use credential providers provided with the Azure SDK, install the `Azure.Identity` package:
+
+```dotnetcli
+dotnet add package Azure.Identity
+```
+
+Import the following namespaces:
+
+
+```csharp
+using Azure;
+using Azure.Identity;
+using Azure.AI.Inference;
+```
+
+This example also uses the following namespaces but you may not always need them:
+
+
+```csharp
+using System.Text.Json;
+using System.Text.Json.Serialization;
+using System.Reflection;
+```
+
+## Work with chat completions
+
+In this section, you use the [Azure AI model inference API](https://aka.ms/azureai/modelinference) with a chat completions model for chat.
+
+> [!TIP]
+> The [Azure AI model inference API](https://aka.ms/azureai/modelinference) allows you to talk with most models deployed in Azure AI Foundry with the same code and structure, including DeepSeek-R1.
+
+### Create a client to consume the model
+
+First, create the client to consume the model. The following code uses an endpoint URL and key that are stored in environment variables.
+
+
+```csharp
+ChatCompletionsClient client = new ChatCompletionsClient(
+    new Uri(Environment.GetEnvironmentVariable("AZURE_INFERENCE_ENDPOINT")),
+    new AzureKeyCredential(Environment.GetEnvironmentVariable("AZURE_INFERENCE_CREDENTIAL")),
+    "DeepSeek-R1"
+);
+```
+
+### Get the model's capabilities
+
+The `/info` route returns information about the model that is deployed to the endpoint. Return the model's information by calling the following method:
+
+
+```csharp
+Response<ModelInfo> modelInfo = client.GetModelInfo();
+```
+
+The response is as follows:
+
+
+```csharp
+Console.WriteLine($"Model name: {modelInfo.Value.ModelName}");
+Console.WriteLine($"Model type: {modelInfo.Value.ModelType}");
+Console.WriteLine($"Model provider name: {modelInfo.Value.ModelProviderName}");
+```
+
+```console
+Model name: DeepSeek-R1
+Model type: chat-completions
+Model provider name: DeepSeek
+```
+
+### Create a chat completion request
+
+The following example shows how you can create a basic chat completions request to the model.
+
+```csharp
+ChatCompletionsOptions requestOptions = new ChatCompletionsOptions()
+{
+    Messages = {
+        new ChatRequestSystemMessage("You are a helpful assistant."),
+        new ChatRequestUserMessage("How many languages are in the world?")
+    },
+};
+
+Response<ChatCompletions> response = client.Complete(requestOptions);
+```
+
+The response is as follows, where you can see the model's usage statistics:
+
+
+```csharp
+Console.WriteLine($"Response: {response.Value.Content}");
+Console.WriteLine($"Model: {response.Value.Model}");
+Console.WriteLine("Usage:");
+Console.WriteLine($"\tPrompt tokens: {response.Value.Usage.PromptTokens}");
+Console.WriteLine($"\tTotal tokens: {response.Value.Usage.TotalTokens}");
+Console.WriteLine($"\tCompletion tokens: {response.Value.Usage.CompletionTokens}");
+```
+
+```console
+Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
+Model: DeepSeek-R1
+Usage: 
+  Prompt tokens: 19
+  Total tokens: 91
+  Completion tokens: 72
+```
+
+Inspect the `usage` section in the response to see the number of tokens used for the prompt, the total number of tokens generated, and the number of tokens used for the completion.
+
+#### Understanding reasoning
+
+Some reasoning models, like DeepSeek-R1, generate completions and include the reasoning behind it. The reasoning associated with the completion is included in the response's content within the tags `<think>` and `</think>`. The model may select on which scenarios to generate reasoning content. For example:
+
+
+```csharp
+ChatCompletionsOptions requestOptions = new ChatCompletionsOptions()
+{
+    Messages = {
+        new ChatRequestSystemMessage("You are a helpful assistant."),
+        new ChatRequestUserMessage("How many languages are in the world?")
+    },
+};
+
+Response<ChatCompletions> response = client.Complete(requestOptions);
+```
+
+You can extract the reasoning content from the response to understand the model's thought process as follows:
+
+
+```csharp
+Regex regex = new Regex(pattern, RegexOptions.Singleline);
+Match match = regex.Match(response.Value.Content);
+
+Console.WriteLine("Response:");
+if (match.Success)
+{
+    Console.WriteLine($"\tThinking: {match.Groups[1].Value}");
+    Console.WriteLine($"\tAnswer: {match.Groups[2].Value}");
+else
+{
+    Console.WriteLine($"Response: {response.Value.Content}");
+}
+Console.WriteLine($"Model: {response.Value.Model}");
+Console.WriteLine("Usage:");
+Console.WriteLine($"\tPrompt tokens: {response.Value.Usage.PromptTokens}");
+Console.WriteLine($"\tTotal tokens: {response.Value.Usage.TotalTokens}");
+Console.WriteLine($"\tCompletion tokens: {response.Value.Usage.CompletionTokens}");
+```
+
+```console
+Thinking: Okay, the user is asking how many languages exist in the world. I need to provide a clear and accurate answer. Let's start by recalling the general consensus from linguistic sources. I remember that the number often cited is around 7,000, but maybe I should check some reputable organizations.\n\nEthnologue is a well-known resource for language data, and I think they list about 7,000 languages. But wait, do they update their numbers? It might be around 7,100 or so. Also, the exact count can vary because some sources might categorize dialects differently or have more recent data. \n\nAnother thing to consider is language endangerment. Many languages are endangered, with some having only a few speakers left. Organizations like UNESCO track endangered languages, so mentioning that adds context. Also, the distribution isn't even. Some countries have hundreds of languages, like Papua New Guinea with over 800, while others have just a few. \n\nA user might also wonder why the exact number is hard to pin down. It's because the distinction between a language and a dialect can be political or cultural. For example, Mandarin and Cantonese are considered dialects of Chinese by some, but they're mutually unintelligible, so others classify them as separate languages. Also, some regions are under-researched, making it hard to document all languages. \n\nI should also touch on language families. The 7,000 languages are grouped into families like Indo-European, Sino-Tibetan, Niger-Congo, etc. Maybe mention a few of the largest families. But wait, the question is just about the count, not the families. Still, it's good to provide a bit more context. \n\nI need to make sure the information is up-to-date. Let me think – recent estimates still hover around 7,000. However, languages are dying out rapidly, so the number decreases over time. Including that note about endangerment and language extinction rates could be helpful. For instance, it's often stated that a language dies every few weeks. \n\nAnother point is sign languages. Does the count include them? Ethnologue includes some, but not all sources might. If the user is including sign languages, that adds more to the count, but I think the 7,000 figure typically refers to spoken languages. For thoroughness, maybe mention that there are also over 300 sign languages. \n\nSummarizing, the answer should state around 7,000, mention Ethnologue's figure, explain why the exact number varies, touch on endangerment, and possibly note sign languages as a separate category. Also, a brief mention of Papua New Guinea as the most linguistically diverse country. \n\nWait, let me verify Ethnologue's current number. As of their latest edition (25th, 2022), they list 7,168 living languages. But I should check if that's the case. Some sources might round to 7,000. Also, SIL International publishes Ethnologue, so citing them as reference makes sense. \n\nOther sources, like Glottolog, might have a different count because they use different criteria. Glottolog might list around 7,000 as well, but exact numbers vary. It's important to highlight that the count isn't exact because of differing definitions and ongoing research. \n\nIn conclusion, the approximate number is 7,000, with Ethnologue being a key source, considerations of endangerment, and the challenges in counting due to dialect vs. language distinctions. I should make sure the answer is clear, acknowledges the variability, and provides key points succinctly.
+
+Answer: The exact number of languages in the world is challenging to determine due to differences in definitions (e.g., distinguishing languages from dialects) and ongoing documentation efforts. However, widely cited estimates suggest there are approximately **7,000 languages** globally.
+Model: DeepSeek-R1
+Usage: 
+  Prompt tokens: 11
+  Total tokens: 897
+  Completion tokens: 886
+```
+
+#### Stream content
+
+By default, the completions API returns the entire generated content in a single response. If you're generating long completions, waiting for the response can take many seconds.
+
+You can _stream_ the content to get it as it's being generated. Streaming content allows you to start processing the completion as content becomes available. This mode returns an object that streams back the response as [data-only server-sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events). Extract chunks from the delta field, rather than the message field.
+
+
+```csharp
+static async Task StreamMessageAsync(ChatCompletionsClient client)
+{
+    ChatCompletionsOptions requestOptions = new ChatCompletionsOptions()
+    {
+        Messages = {
+            new ChatRequestSystemMessage("You are a helpful assistant."),
+            new ChatRequestUserMessage("How many languages are in the world? Write an essay about it.")
+        },
+        MaxTokens=4096
+    };
+
+    StreamingResponse<StreamingChatCompletionsUpdate> streamResponse = await client.CompleteStreamingAsync(requestOptions);
+
+    await PrintStream(streamResponse);
+}
+```
+
+To stream completions, use `CompleteStreamingAsync` method when you call the model. Notice that in this example we the call is wrapped in an asynchronous method.
+
+To visualize the output, define an asynchronous method to print the stream in the console.
+
+```csharp
+static async Task PrintStream(StreamingResponse<StreamingChatCompletionsUpdate> response)
+{
+    await foreach (StreamingChatCompletionsUpdate chatUpdate in response)
+    {
+        if (chatUpdate.Role.HasValue)
+        {
+            Console.Write($"{chatUpdate.Role.Value.ToString().ToUpperInvariant()}: ");
+        }
+        if (!string.IsNullOrEmpty(chatUpdate.ContentUpdate))
+        {
+            Console.Write(chatUpdate.ContentUpdate);
+        }
+    }
+}
+```
+
+You can visualize how streaming generates content:
+
+
+```csharp
+StreamMessageAsync(client).GetAwaiter().GetResult();
+```
+
+### Apply content safety
+
+The Azure AI model inference API supports [Azure AI content safety](https://aka.ms/azureaicontentsafety). When you use deployments with Azure AI content safety turned on, inputs and outputs pass through an ensemble of classification models aimed at detecting and preventing the output of harmful content. The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
+
+The following example shows how to handle events when the model detects harmful content in the input prompt and content safety is enabled.
+
+
+```csharp
+try
+{
+    requestOptions = new ChatCompletionsOptions()
+    {
+        Messages = {
+            new ChatRequestSystemMessage("You are an AI assistant that helps people find information."),
+            new ChatRequestUserMessage(
+                "Chopping tomatoes and cutting them into cubes or wedges are great ways to practice your knife skills."
+            ),
+        },
+    };
+
+    response = client.Complete(requestOptions);
+    Console.WriteLine(response.Value.Content);
+}
+catch (RequestFailedException ex)
+{
+    if (ex.ErrorCode == "content_filter")
+    {
+        Console.WriteLine($"Your query has trigger Azure Content Safety: {ex.Message}");
+    }
+    else
+    {
+        throw;
+    }
+}
+```
+
+> [!TIP]
+> To learn more about how you can configure and control Azure AI content safety settings, check the [Azure AI content safety documentation](https://aka.ms/azureaicontentsafety).
+
+::: zone-end
+
+
+::: zone pivot="programming-language-rest"
+
+## DeepSeek-R1
+
+DeepSeek-R1 builds on the progress of earlier reasoning-focused models that improved performance by extending Chain-of-Thought (CoT) reasoning. DeepSeek-R1 takes things further by combining reinforcement learning (RL) with fine-tuning on carefully chosen datasets. It evolved from an earlier version, DeepSeek-R1-Zero, which relied solely on RL and showed strong reasoning skills but had issues like hard-to-read outputs and language inconsistencies. To address these limitations, DeepSeek-R1 incorporates a small amount of cold-start data and follows a refined training pipeline that blends reasoning-oriented RL with supervised fine-tuning on curated datasets, resulting in a model that achieves state-of-the-art performance on reasoning benchmarks.
+
+
+You can learn more about the models in their respective model card:
+
+* [DeepSeek-R1](https://aka.ms/azureai/landing/DeepSeek-R1)
+
+
+## Prerequisites
+
+To use DeepSeek-R1 with Azure AI Foundry, you need the following prerequisites:
+
+### A model deployment
+
+**Deployment to serverless APIs**
+
+DeepSeek-R1 can be deployed to serverless API endpoints with pay-as-you-go billing. This kind of deployment provides a way to consume models as an API without hosting them on your subscription, while keeping the enterprise security and compliance that organizations need. 
+
+Deployment to a serverless API endpoint doesn't require quota from your subscription. If your model isn't deployed already, use the Azure AI Studio, Azure Machine Learning SDK for Python, the Azure CLI, or ARM templates to [deploy the model as a serverless API](deploy-models-serverless.md).
+
+> [!div class="nextstepaction"]
+> [Deploy the model to serverless API endpoints](deploy-models-serverless.md)
+
+### A REST client
+
+Models deployed with the [Azure AI model inference API](https://aka.ms/azureai/modelinference) can be consumed using any REST client. To use the REST client, you need the following prerequisites:
+
+* To construct the requests, you need to pass in the endpoint URL. The endpoint URL has the form `https://your-host-name.your-azure-region.inference.ai.azure.com`, where `your-host-name`` is your unique model deployment host name and `your-azure-region`` is the Azure region where the model is deployed (for example, eastus2).
+* Depending on your model deployment and authentication preference, you need either a key to authenticate against the service, or Microsoft Entra ID credentials. The key is a 32-character string.
+
+## Work with chat completions
+
+In this section, you use the [Azure AI model inference API](https://aka.ms/azureai/modelinference) with a chat completions model for chat.
+
+> [!TIP]
+> The [Azure AI model inference API](https://aka.ms/azureai/modelinference) allows you to talk with most models deployed in Azure AI Foundry with the same code and structure, including DeepSeek-R1.
+
+### Create a client to consume the model
+
+First, create the client to consume the model. The following code uses an endpoint URL and key that are stored in environment variables.
+
+### Get the model's capabilities
+
+The `/info` route returns information about the model that is deployed to the endpoint. Return the model's information by calling the following method:
+
+```http
+GET /info HTTP/1.1
+Host: <ENDPOINT_URI>
+Authorization: Bearer <TOKEN>
+Content-Type: application/json
+```
+
+The response is as follows:
+
+
+```json
+{
+    "model_name": "DeepSeek-R1",
+    "model_type": "chat-completions",
+    "model_provider_name": "DeepSeek"
+}
+```
+
+### Create a chat completion request
+
+The following example shows how you can create a basic chat completions request to the model.
+
+```json
+{
+    "model": "DeepSeek-R1",
+    "messages": [
+        {
+            "role": "system",
+            "content": "You are a helpful assistant."
+        },
+        {
+            "role": "user",
+            "content": "How many languages are in the world?"
+        }
+    ]
+}
+```
+
+The response is as follows, where you can see the model's usage statistics:
+
+
+```json
+{
+    "id": "0a1234b5de6789f01gh2i345j6789klm",
+    "object": "chat.completion",
+    "created": 1718726686,
+    "model": "DeepSeek-R1",
+    "choices": [
+        {
+            "index": 0,
+            "message": {
+                "role": "assistant",
+                "content": "As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.",
+                "tool_calls": null
+            },
+            "finish_reason": "stop",
+            "logprobs": null
+        }
+    ],
+    "usage": {
+        "prompt_tokens": 19,
+        "total_tokens": 91,
+        "completion_tokens": 72
+    }
+}
+```
+
+Inspect the `usage` section in the response to see the number of tokens used for the prompt, the total number of tokens generated, and the number of tokens used for the completion.
+
+#### Understanding reasoning
+
+Some reasoning models, like DeepSeek-R1, generate completions and include the reasoning behind it. The reasoning associated with the completion is included in the response's content within the tags `<think>` and `</think>`. The model may select on which scenarios to generate reasoning content. For example:
+
+
+```json
+{
+    "model": "DeepSeek-R1",
+    "messages": [
+        {
+            "role": "system",
+            "content": "You are a helpful assistant."
+        },
+        {
+            "role": "user",
+            "content": "How many languages are in the world?"
+        }
+    ]
+}
+```
+
+You can extract the reasoning content from the response to understand the model's thought process as follows:
+
+
+```json
+{
+    "id": "0a1234b5de6789f01gh2i345j6789klm",
+    "object": "chat.completion",
+    "created": 1718726686,
+    "model": "DeepSeek-R1",
+    "choices": [
+        {
+            "index": 0,
+            "message": {
+                "role": "assistant",
+                "content": "<think>\nOkay, the user is asking how many languages exist in the world. I need to provide a clear and accurate answer. Let's start by recalling the general consensus from linguistic sources. I remember that the number often cited is around 7,000, but maybe I should check some reputable organizations.\n\nEthnologue is a well-known resource for language data, and I think they list about 7,000 languages. But wait, do they update their numbers? It might be around 7,100 or so. Also, the exact count can vary because some sources might categorize dialects differently or have more recent data. \n\nAnother thing to consider is language endangerment. Many languages are endangered, with some having only a few speakers left. Organizations like UNESCO track endangered languages, so mentioning that adds context. Also, the distribution isn't even. Some countries have hundreds of languages, like Papua New Guinea with over 800, while others have just a few. \n\nA user might also wonder why the exact number is hard to pin down. It's because the distinction between a language and a dialect can be political or cultural. For example, Mandarin and Cantonese are considered dialects of Chinese by some, but they're mutually unintelligible, so others classify them as separate languages. Also, some regions are under-researched, making it hard to document all languages. \n\nI should also touch on language families. The 7,000 languages are grouped into families like Indo-European, Sino-Tibetan, Niger-Congo, etc. Maybe mention a few of the largest families. But wait, the question is just about the count, not the families. Still, it's good to provide a bit more context. \n\nI need to make sure the information is up-to-date. Let me think – recent estimates still hover around 7,000. However, languages are dying out rapidly, so the number decreases over time. Including that note about endangerment and language extinction rates could be helpful. For instance, it's often stated that a language dies every few weeks. \n\nAnother point is sign languages. Does the count include them? Ethnologue includes some, but not all sources might. If the user is including sign languages, that adds more to the count, but I think the 7,000 figure typically refers to spoken languages. For thoroughness, maybe mention that there are also over 300 sign languages. \n\nSummarizing, the answer should state around 7,000, mention Ethnologue's figure, explain why the exact number varies, touch on endangerment, and possibly note sign languages as a separate category. Also, a brief mention of Papua New Guinea as the most linguistically diverse country. \n\nWait, let me verify Ethnologue's current number. As of their latest edition (25th, 2022), they list 7,168 living languages. But I should check if that's the case. Some sources might round to 7,000. Also, SIL International publishes Ethnologue, so citing them as reference makes sense. \n\nOther sources, like Glottolog, might have a different count because they use different criteria. Glottolog might list around 7,000 as well, but exact numbers vary. It's important to highlight that the count isn't exact because of differing definitions and ongoing research. \n\nIn conclusion, the approximate number is 7,000, with Ethnologue being a key source, considerations of endangerment, and the challenges in counting due to dialect vs. language distinctions. I should make sure the answer is clear, acknowledges the variability, and provides key points succinctly.\n</think>\n\nThe exact number of languages in the world is challenging to determine due to differences in definitions (e.g., distinguishing languages from dialects) and ongoing documentation efforts. However, widely cited estimates suggest there are approximately **7,000 languages** globally.",
+                "tool_calls": null
+            },
+            "finish_reason": "stop"
+        }
+    ],
+    "usage": {
+        "prompt_tokens": 11,
+        "total_tokens": 897,
+        "completion_tokens": 886
+    }
+}
+```
+
+#### Stream content
+
+By default, the completions API returns the entire generated content in a single response. If you're generating long completions, waiting for the response can take many seconds.
+
+You can _stream_ the content to get it as it's being generated. Streaming content allows you to start processing the completion as content becomes available. This mode returns an object that streams back the response as [data-only server-sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events). Extract chunks from the delta field, rather than the message field.
+
+
+```json
+{
+    "model": "DeepSeek-R1",
+    "messages": [
+        {
+            "role": "system",
+            "content": "You are a helpful assistant."
+        },
+        {
+            "role": "user",
+            "content": "How many languages are in the world?"
+        }
+    ],
+    "stream": true,
+    "temperature": 0,
+    "top_p": 1,
+    "max_tokens": 2048
+}
+```
+
+You can visualize how streaming generates content:
+
+
+```json
+{
+    "id": "23b54589eba14564ad8a2e6978775a39",
+    "object": "chat.completion.chunk",
+    "created": 1718726371,
+    "model": "DeepSeek-R1",
+    "choices": [
+        {
+            "index": 0,
+            "delta": {
+                "role": "assistant",
+                "content": ""
+            },
+            "finish_reason": null,
+            "logprobs": null
+        }
+    ]
+}
+```
+
+The last message in the stream has `finish_reason` set, indicating the reason for the generation process to stop.
+
+
+```json
+{
+    "id": "23b54589eba14564ad8a2e6978775a39",
+    "object": "chat.completion.chunk",
+    "created": 1718726371,
+    "model": "DeepSeek-R1",
+    "choices": [
+        {
+            "index": 0,
+            "delta": {
+                "content": ""
+            },
+            "finish_reason": "stop",
+            "logprobs": null
+        }
+    ],
+    "usage": {
+        "prompt_tokens": 19,
+        "total_tokens": 91,
+        "completion_tokens": 72
+    }
+}
+```
+
+### Apply content safety
+
+The Azure AI model inference API supports [Azure AI content safety](https://aka.ms/azureaicontentsafety). When you use deployments with Azure AI content safety turned on, inputs and outputs pass through an ensemble of classification models aimed at detecting and preventing the output of harmful content. The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.
+
+The following example shows how to handle events when the model detects harmful content in the input prompt and content safety is enabled.
+
+
+```json
+{
+    "model": "DeepSeek-R1",
+    "messages": [
+        {
+            "role": "system",
+            "content": "You are an AI assistant that helps people find information."
+        },
+                {
+            "role": "user",
+            "content": "Chopping tomatoes and cutting them into cubes or wedges are great ways to practice your knife skills."
+        }
+    ]
+}
+```
+
+
+```json
+{
+    "error": {
+        "message": "The response was filtered due to the prompt triggering Microsoft's content management policy. Please modify your prompt and retry.",
+        "type": null,
+        "param": "prompt",
+        "code": "content_filter",
+        "status": 400
+    }
+}
+```
+
+> [!TIP]
+> To learn more about how you can configure and control Azure AI content safety settings, check the [Azure AI content safety documentation](https://aka.ms/azureaicontentsafety).
+
+::: zone-end
+
+## More inference examples
+
+For more examples of how to use DeepSeek models, see the following examples and tutorials:
+
+| Description                               | Language          | Sample                                                          |
+|-------------------------------------------|-------------------|-----------------------------------------------------------------|
+| Azure AI Inference package for Python     | Python            | [Link](https://aka.ms/azsdk/azure-ai-inference/python/samples)  |
+| Azure AI Inference package for JavaScript | JavaScript        | [Link](https://aka.ms/azsdk/azure-ai-inference/javascript/samples)  |
+| Azure AI Inference package for C#         | C#                | [Link](https://aka.ms/azsdk/azure-ai-inference/csharp/samples)  |
+| Azure AI Inference package for Java       | Java              | [Link](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/ai/azure-ai-inference/src/samples)  |
+
+## Cost and quota considerations for DeepSeek models deployed as serverless API endpoints
+
+Quota is managed per deployment. Each deployment has a rate limit of 200,000 tokens per minute and 1,000 API requests per minute. However, we currently limit one deployment per model per project. Contact Microsoft Azure Support if the current rate limits aren't sufficient for your scenarios.
+
+## Related content
+
+
+* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Deploy models as serverless APIs](deploy-models-serverless.md)
+* [Consume serverless API endpoints from a different Azure AI Studio project or hub](deploy-models-serverless-connect.md)
+* [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
+* [Plan and manage costs (marketplace)](costs-plan-manage.md#monitor-costs-for-models-offered-through-the-azure-marketplace)
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Add DeepSeek-R1 Reasoning Model Deployment Guide"
}
```

### Explanation
This code modification introduces a significant new feature by adding a comprehensive guide for deploying the DeepSeek-R1 reasoning model in the Azure AI Foundry. The addition consists of approximately 1,150 lines of content, outlining the capabilities, deployment methods, prerequisites, and instructions for utilizing the model through various programming languages, including Python, JavaScript, C#, and REST API.

The document starts with an overview of the DeepSeek-R1 model, describing its architecture and training methodologies that combine reinforcement learning with fine-tuning to achieve advanced reasoning performance. Guidance for deploying the model as a serverless API is provided, which emphasizes the benefits of using pay-as-you-go billing while maintaining enterprise-level security.

Furthermore, the guide offers detailed instructions for setting up necessary client libraries, acquiring model deployment information, and executing chat completion requests. Each programming language section includes examples that demonstrate how to integrate and utilize the model effectively. The new document ensures users can seamlessly access the features of DeepSeek-R1 and is essential for both beginners and seasoned developers looking to enhance their AI solutions with robust reasoning capabilities.

By adding this comprehensive resource, the documentation significantly enriches the user experience and empowers developers to effectively leverage the DeepSeek-R1 model in their applications.

## articles/ai-studio/how-to/deploy-models-gretel-navigator.md{#item-2e9806}

<details>
<summary>Diff</summary>
````diff
@@ -537,7 +537,7 @@ For more information on how to track costs, see [Monitor costs for models offere
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Gretel Navigator article"
}
```

### Explanation
This code modification is a minor update to the `deploy-models-gretel-navigator.md` article within the AI Studio documentation. The change involves the update of a single related content link.

The original relative link to the "Azure AI Model Inference API" resource has been modified to provide a more accurate navigation path that reflects its new location within the documentation structure. This ensures that users can find the relevant information more efficiently.

By improving the accuracy of this link, the documentation enhances the user experience, enabling easier access to essential resources related to the deployment of models within Azure AI Foundry. The change contributes to a more seamless navigation experience for users seeking guidance on leveraging the services offered.

## articles/ai-studio/how-to/deploy-models-jais.md{#item-0bd11f}

<details>
<summary>Diff</summary>
````diff
@@ -1186,7 +1186,7 @@ For more information on how to track costs, see [Monitor costs for models offere
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Jais article"
}
```

### Explanation
This code modification represents a minor update to the `deploy-models-jais.md` article within the AI Studio documentation. The primary change involves the adjustment of a related content link.

The original link to the "Azure AI Model Inference API" has been modified to a new relative path that correctly directs users to the updated location of the resource within the documentation hierarchy. This update enhances user navigation by ensuring that the link points to the most relevant and current information available.

By refining the accuracy of this link, the documentation improves the overall user experience, allowing users to access essential resources regarding the deployment of models in Azure AI Foundry more efficiently. This ensures that users have the most up-to-date information while working with the AI services.

## articles/ai-studio/how-to/deploy-models-jamba.md{#item-eeefca}

<details>
<summary>Diff</summary>
````diff
@@ -136,12 +136,12 @@ For more information on using the APIs, see the [reference](#reference-for-jamba
 
 Jamba family models accept both of these APIs:
 
-- The [Azure AI Model Inference API](../reference/reference-model-inference-api.md) on the route `/chat/completions` for multi-turn chat or single-turn question-answering. This API is supported because Jamba family models are fine-tuned for chat completion.
-- [AI21's Azure Client](https://docs.ai21.com/reference/jamba-instruct-api). For more information about the REST endpoint being called, visit [AI21's REST documentation](https://docs.ai21.com/reference/jamba-instruct-api).
+- The [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md) on the route `/chat/completions` for multi-turn chat or single-turn question-answering. This API is supported because Jamba family models are fine-tuned for chat completion.
+- [AI21's Azure Client](https://docs.ai21.com/reference/jamba-15-api-ref). For more information about the REST endpoint being called, visit [AI21's REST documentation](https://docs.ai21.com/reference/jamba-15-api-ref).
 
 ### Azure AI model inference API
 
-The [Azure AI model inference API](../reference/reference-model-inference-api.md) schema can be found in the [reference for Chat Completions](../reference/reference-model-inference-chat-completions.md) article and an [OpenAPI specification can be obtained from the endpoint itself](../reference/reference-model-inference-api.md?tabs=rest#getting-started).
+The [Azure AI model inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md) schema can be found in the [reference for Chat Completions](../../ai-foundry/model-inference/reference/reference-model-inference-chat-completions.md) article and an [OpenAPI specification can be obtained from the endpoint itself](../../ai-foundry/model-inference/reference/reference-model-inference-api.md?tabs=rest#getting-started).
 
 Single-turn and multi-turn chat have the same request and response format, except that question answering (single-turn) involves only a single user message in the request, while multi-turn chat requires that you send the entire chat message history in each request. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update links in Deploy Models Jamba article"
}
```

### Explanation
This code modification is a minor update to the `deploy-models-jamba.md` article within the AI Studio documentation. It primarily involves updating related content links to reflect their new locations within the documentation structure.

The modifications include changes to the paths of the links directing users to the "Azure AI Model Inference API" and "AI21's Azure Client." The links have been adjusted to ensure that users can navigate to the correct resources that provide necessary information regarding the APIs supported by the Jamba family models.

Additionally, the documentation includes updates to references for the API's schema and OpenAPI specifications, further enhancing the clarity and usability of the content. By ensuring that all links point to accurate sources, this update improves the overall user experience, allowing easier access to relevant information for deploying models using the Jamba framework.

## articles/ai-studio/how-to/deploy-models-llama.md{#item-6274a7}

<details>
<summary>Diff</summary>
````diff
@@ -1491,7 +1491,7 @@ It is a good practice to start with a low number of instances and scale up as ne
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Llama article"
}
```

### Explanation
This code modification represents a minor update to the `deploy-models-llama.md` article within the AI Studio documentation. The key change involves altering the link for the "Azure AI Model Inference API."

The original link path has been replaced with a new relative path that directs users to the correct location within the documentation structure. This adjustment ensures that readers can access the most current information regarding the Azure AI Model Inference API, which is essential for deploying models effectively.

By refining this link, the document enhances user navigation and accuracy, promoting a better user experience. Users will find it easier to locate and utilize resources related to the deployment of models using Llama within Azure AI, thereby improving their overall productivity and understanding of the subject matter.

## articles/ai-studio/how-to/deploy-models-mistral-codestral.md{#item-83ba03}

<details>
<summary>Diff</summary>
````diff
@@ -2060,7 +2060,7 @@ For more information on how to track costs, see [Monitor costs for models offere
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Mistral Codestral article"
}
```

### Explanation
This code modification is a minor update to the `deploy-models-mistral-codestral.md` article in the AI Studio documentation. The primary change involves updating the link for the "Azure AI Model Inference API."

The link has been adjusted to reflect a new relative path. This ensures that users are directed to the correct documentation resource for the Azure AI Model Inference API, which is crucial for effectively deploying models using Mistral Codestral.

The update enhances the accuracy of the related content section, improving user navigation within the documentation. By ensuring the link points to the current source, the article facilitates users in accessing essential information related to model deployment in Azure AI, thereby supporting better decision-making and understanding of the platform's capabilities.

## articles/ai-studio/how-to/deploy-models-mistral-nemo.md{#item-e7b729}

<details>
<summary>Diff</summary>
````diff
@@ -2043,7 +2043,7 @@ For more information on how to track costs, see [Monitor costs for models offere
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Mistral Nemo article"
}
```

### Explanation
This code modification is a minor update to the `deploy-models-mistral-nemo.md` article within the AI Studio documentation. The main change consists of modifying the link for the "Azure AI Model Inference API."

The previous link has been replaced with a new relative path that directs users to the appropriate documentation section, ensuring that they have access to the latest information regarding the Azure AI Model Inference API. This API is critical for users looking to deploy models using Mistral Nemo effectively.

By updating this link, the article improves the accuracy and usability of its related content section, allowing users to navigate more easily to essential resources. This enhancement contributes to a better overall user experience, ensuring that they can efficiently find the necessary documentation to support their model deployment tasks in Azure AI.

## articles/ai-studio/how-to/deploy-models-mistral-open.md{#item-84e005}

<details>
<summary>Diff</summary>
````diff
@@ -1308,7 +1308,7 @@ It's a good practice to start with a low number of instances and scale up as nee
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Mistral Open article"
}
```

### Explanation
This code modification is a minor update to the `deploy-models-mistral-open.md` article in the AI Studio documentation. The change involves updating the link for the "Azure AI Model Inference API."

The modification replaces the original link with a new relative path that guides users to the correct documentation for the Azure AI Model Inference API. This API plays a crucial role for users looking to deploy models using Mistral Open.

By refining this link, the article enhances the accuracy of its related content section, streamlining user navigation to important resources. This update ultimately supports a better user experience by providing direct access to the latest information necessary for effectively utilizing Azure AI in model deployment.

## articles/ai-studio/how-to/deploy-models-mistral.md{#item-487a41}

<details>
<summary>Diff</summary>
````diff
@@ -2242,7 +2242,7 @@ For more information on how to track costs, see [Monitor costs for models offere
 ## Related content
 
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Model deprecation and retirement in Azure AI model catalog](../concepts/model-lifecycle-and-retirement.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Mistral article"
}
```

### Explanation
This code modification is a minor update to the `deploy-models-mistral.md` article within the AI Studio documentation. The update primarily consists of changing the link for the "Azure AI Model Inference API."

The previous link has been updated to a new relative path that accurately directs users to the current documentation for the Azure AI Model Inference API. This adjustment is important for users seeking to deploy models using Mistral, as the API provides essential information and resources.

By refining this link, the documentation enhances the clarity and accessibility of its related content, facilitating smoother navigation for users. This update ultimately contributes to an improved user experience by ensuring direct access to up-to-date information necessary for effective model deployment on the Azure platform.

## articles/ai-studio/how-to/deploy-models-phi-3-5-vision.md{#item-8d6d7d}

<details>
<summary>Diff</summary>
````diff
@@ -1633,7 +1633,7 @@ It's a good practice to start with a low number of instances and scale up as nee
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Phi 3.5 Vision article"
}
```

### Explanation
This code modification represents a minor update to the `deploy-models-phi-3-5-vision.md` article in the AI Studio documentation. The specific change involves updating the link for the "Azure AI Model Inference API."

The update modifies the relative path for the link, ensuring that it directs users to the appropriate and current documentation for the Azure AI Model Inference API. This API is crucial for users looking to deploy models utilizing Phi 3.5 Vision, as it provides essential guidance and information.

By making this link more accurate, the documentation improves its related content section, facilitating better navigation for users. Overall, this minor update enhances the user experience by providing direct access to relevant resources necessary for effectively deploying models in the Azure environment.

## articles/ai-studio/how-to/deploy-models-phi-3-vision.md{#item-bd5aae}

<details>
<summary>Diff</summary>
````diff
@@ -1420,7 +1420,7 @@ It's a good practice to start with a low number of instances and scale up as nee
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Phi 3 Vision article"
}
```

### Explanation
This code modification reflects a minor update to the `deploy-models-phi-3-vision.md` documentation in the AI Studio section. The primary change involved is the update of the link for the "Azure AI Model Inference API."

The link's relative path has been modified to point to a more accurate location within the documentation, ensuring users can access the correct resources related to the Azure AI Model Inference API. This API is essential for users working with Phi 3 Vision models, as it offers important information and guidelines for deployment.

By enhancing this link, the article improves the clarity and functionality of its related content section, making it easier for users to navigate to relevant documentation. Overall, this minor update bolsters the user experience by providing direct access to critical information needed for deploying models in the Azure AI environment.

## articles/ai-studio/how-to/deploy-models-phi-3.md{#item-47e305}

<details>
<summary>Diff</summary>
````diff
@@ -1481,7 +1481,7 @@ You can use this [sample notebook](https://github.com/Azure/azureml-examples/blo
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Phi 3 article"
}
```

### Explanation
This code modification indicates a minor update made to the `deploy-models-phi-3.md` document in the AI Studio section. The principal change involves the update of the link for the "Azure AI Model Inference API."

The change specifically modifies the relative path of the link to ensure it directs users to the correct and updated documentation for the Azure AI Model Inference API. This is important for users who are implementing models using Phi 3, as accessing the right resources is critical for successful deployment.

By refining this link, the documentation enhances the related content section, improving navigation for users seeking relevant information. Ultimately, this minor update strengthens the user experience by ensuring that documentation remains current and accessible for deploying models in the Azure ecosystem.

## articles/ai-studio/how-to/deploy-models-phi-4.md{#item-c40212}

<details>
<summary>Diff</summary>
````diff
@@ -1357,7 +1357,7 @@ It's a good practice to start with a low number of instances and scale up as nee
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Phi 4 article"
}
```

### Explanation
This code modification pertains to a minor update made to the `deploy-models-phi-4.md` document within the AI Studio section. The main focus of this change is the adjustment of the link for the "Azure AI Model Inference API."

The modification involves altering the relative path of the link to ensure that it now points to the correct and updated location in the documentation, which is vital for users involved in deploying models with Phi 4. This update ensures that users can access the relevant and necessary information for successful model deployment.

By improving this link, the article enhances its related content section, allowing for better navigation and accessibility to critical resources. Overall, this minor update enhances the user experience by maintaining accurate and up-to-date documentation for users working with Azure AI models.

## articles/ai-studio/how-to/deploy-models-serverless.md{#item-f8177f}

<details>
<summary>Diff</summary>
````diff
@@ -553,9 +553,9 @@ In this section, you create an endpoint with the name **meta-llama3-8b-qwerty**.
 
 ## Use the serverless API endpoint
 
-Models deployed in Azure Machine Learning and Azure AI Foundry in Serverless API endpoints support the [Azure AI Model Inference API](../reference/reference-model-inference-api.md) that exposes a common set of capabilities for foundational models and that can be used by developers to consume predictions from a diverse set of models in a uniform and consistent way. 
+Models deployed in Azure Machine Learning and Azure AI Foundry in Serverless API endpoints support the [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md) that exposes a common set of capabilities for foundational models and that can be used by developers to consume predictions from a diverse set of models in a uniform and consistent way. 
 
-Read more about the [capabilities of this API](../reference/reference-model-inference-api.md#capabilities) and how [you can use it when building applications](../reference/reference-model-inference-api.md#getting-started). 
+Read more about the [capabilities of this API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md#capabilities) and how [you can use it when building applications](../../ai-foundry/model-inference/reference/reference-model-inference-api.md#getting-started). 
 
 ## Network isolation
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update links in Deploy Models Serverless article"
}
```

### Explanation
This code modification reflects a minor update to the `deploy-models-serverless.md` document in the AI Studio section. The key changes involve updating the links related to the "Azure AI Model Inference API."

The alterations ensure that the links now point to the correct and updated paths, facilitating better navigation for users seeking to understand the capabilities of the Azure AI Model Inference API. Specifically, both instances of the link to the API documentation have been modified to include the proper directory structure, which enhances the clarity and accuracy of the information provided.

Moreover, these changes also affect the additional details about the API's capabilities and usage in application development, thereby ensuring that users have access to the most relevant and effective resources. Overall, this minor update aims to improve the user experience by streamlining access to updated documentation pertinent to deploying models on serverless API endpoints in Azure.

## articles/ai-studio/how-to/deploy-models-tsuzumi.md{#item-d3fd51}

<details>
<summary>Diff</summary>
````diff
@@ -1335,7 +1335,7 @@ For more information on how to track costs, see [Monitor costs for models offere
 ## Related content
 
 
-* [Azure AI Model Inference API](../reference/reference-model-inference-api.md)
+* [Azure AI Model Inference API](../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
 * [Deploy models as serverless APIs](deploy-models-serverless.md)
 * [Consume serverless API endpoints from a different Azure AI Foundry project or hub](deploy-models-serverless-connect.md)
 * [Region availability for models in serverless API endpoints](deploy-models-serverless-availability.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Deploy Models Tsuzumi article"
}
```

### Explanation
This code modification signifies a minor update to the `deploy-models-tsuzumi.md` document found in the AI Studio section. The chief change involves modifying the link to the "Azure AI Model Inference API" within the related content section of the article.

The link's path has been adjusted to ensure it correctly points to a more appropriate directory structure, thereby improving the accuracy of navigational references in the documentation. This update is crucial as it helps users easily access pertinent information regarding the Azure AI Model Inference API, leading to a better understanding of how to utilize this API effectively.

Overall, this minor update enhances the user experience by ensuring that the related content is not only accurate but also up-to-date, allowing users working with Tsuzumi models to find the information they need in a more accessible manner.

## articles/ai-studio/how-to/develop/langchain.md{#item-0d59f1}

<details>
<summary>Diff</summary>
````diff
@@ -19,7 +19,7 @@ LangChain is a development ecosystem that makes as easy possible for developers
 
 Models deployed to Azure AI Foundry can be used with LangChain in two ways:
 
-- **Using the Azure AI model inference API:** All models deployed to Azure AI Foundry support the [Azure AI model inference API](../../reference/reference-model-inference-api.md), which offers a common set of functionalities that can be used for most of the models in the catalog. The benefit of this API is that, since it's the same for all the models, changing from one to another is as simple as changing the model deployment being use. No further changes are required in the code. When working with LangChain, install the extensions `langchain-azure-ai`.
+- **Using the Azure AI model inference API:** All models deployed to Azure AI Foundry support the [Azure AI model inference API](../../../ai-foundry/model-inference/reference/reference-model-inference-api.md), which offers a common set of functionalities that can be used for most of the models in the catalog. The benefit of this API is that, since it's the same for all the models, changing from one to another is as simple as changing the model deployment being use. No further changes are required in the code. When working with LangChain, install the extensions `langchain-azure-ai`.
 
 - **Using the model's provider specific API:** Some models, like OpenAI, Cohere, or Mistral, offer their own set of APIs and extensions for LlamaIndex. Those extensions may include specific functionalities that the model support and hence are suitable if you want to exploit them. When working with LangChain, install the extension specific for the model you want to use, like `langchain-openai` or `langchain-cohere`.
 
@@ -467,4 +467,4 @@ Learn more about [how to visualize and manage traces](visualize-traces.md).
 * [Develop applications with LlamaIndex](llama-index.md)
 * [Visualize and manage traces in Azure AI Foundry](visualize-traces.md)
 * [Use the Azure AI model inference service](../../ai-services/model-inference.md)
-* [Reference: Azure AI model inference API](../../reference/reference-model-inference-api.md)
+* [Reference: Azure AI model inference API](../../../ai-foundry/model-inference/reference/reference-model-inference-api.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Correct links in LangChain development article"
}
```

### Explanation
This modification involves a minor update to the `langchain.md` document, which is part of the AI Studio development guides. The main focus of this update is the correction of links that reference the Azure AI model inference API.

Specifically, the paths for two instances of the link to the Azure AI model inference API have been updated to a new directory structure that improves access to the relevant documentation. This change ensures that users working with LangChain will be able to navigate to the correct resources without encountering broken links or misdirected pages.

By refining these links, the update enhances the overall usability and accuracy of the document, allowing developers to more easily find and utilize the API functionalities that support their developments in the Azure AI Foundry environment. This is particularly important for users looking to implement model features seamlessly within LangChain applications.

## articles/ai-studio/how-to/develop/llama-index.md{#item-613372}

<details>
<summary>Diff</summary>
````diff
@@ -19,7 +19,7 @@ In this article, you learn how to use [LlamaIndex](https://github.com/run-llama/
 
 Models deployed to Azure AI Foundry can be used with LlamaIndex in two ways:
 
-- **Using the Azure AI model inference API:** All models deployed to Azure AI Foundry support the [Azure AI model inference API](../../reference/reference-model-inference-api.md), which offers a common set of functionalities that can be used for most of the models in the catalog. The benefit of this API is that, since it's the same for all the models, changing from one to another is as simple as changing the model deployment being use. No further changes are required in the code. When working with LlamaIndex, install the extensions `llama-index-llms-azure-inference` and `llama-index-embeddings-azure-inference`.
+- **Using the Azure AI model inference API:** All models deployed to Azure AI Foundry support the [Azure AI model inference API](../../../ai-foundry/model-inference/reference/reference-model-inference-api.md), which offers a common set of functionalities that can be used for most of the models in the catalog. The benefit of this API is that, since it's the same for all the models, changing from one to another is as simple as changing the model deployment being use. No further changes are required in the code. When working with LlamaIndex, install the extensions `llama-index-llms-azure-inference` and `llama-index-embeddings-azure-inference`.
 
 - **Using the model's provider specific API:** Some models, like OpenAI, Cohere, or Mistral, offer their own set of APIs and extensions for LlamaIndex. Those extensions may include specific functionalities that the model support and hence are suitable if you want to exploit them. When working with `llama-index`, install the extension specific for the model you want to use, like `llama-index-llms-openai` or `llama-index-llms-cohere`.
 
@@ -175,7 +175,7 @@ llm = AzureAICompletionsModel(
 )
 ```
 
-Parameters not supported in the Azure AI model inference API ([reference](../../reference/reference-model-inference-chat-completions.md)) but available in the underlying model, you can use the `model_extras` argument. In the following example, the parameter `safe_prompt`, only available for Mistral models, is being passed.
+Parameters not supported in the Azure AI model inference API ([reference](../../../ai-foundry/model-inference/reference/reference-model-inference-chat-completions.md)) but available in the underlying model, you can use the `model_extras` argument. In the following example, the parameter `safe_prompt`, only available for Mistral models, is being passed.
 
 ```python
 llm = AzureAICompletionsModel(
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update links in Llama Index development article"
}
```

### Explanation
This code modification pertains to a minor update made to the `llama-index.md` document within the AI Studio development guidance. The primary change involves the correction of links that lead to the Azure AI model inference API and related references.

The updated links adjust the directory structure to provide a more accurate path, ensuring that users can readily access the official documentation associated with the Azure AI model inference capabilities. Specifically, this includes modifying the links to point correctly to the resources that detail the API's functionalities and parameters, which are crucial for developers leveraging LlamaIndex within the Azure environment.

By refining these references, the update enhances the document's clarity and usability, assisting developers in implementing and integrating the Azure AI model inference with LlamaIndex effectively. This ensures that they remain informed about the tools and options available when developing applications with this technology. Overall, these improvements contribute to a streamlined development experience within the Azure AI ecosystem.

## articles/ai-studio/how-to/develop/semantic-kernel.md{#item-565785}

<details>
<summary>Diff</summary>
````diff
@@ -19,7 +19,7 @@ In this article, you learn how to use [Semantic Kernel](/semantic-kernel/overvie
 
 - An [Azure subscription](https://azure.microsoft.com).
 - An Azure AI project as explained at [Create a project in Azure AI Foundry portal](../create-projects.md).
-- A model supporting the [Azure AI model inference API](../../reference/reference-model-inference-api.md?tabs=python) deployed. In this example, we use a `Mistral-Large` deployment, but use any model of your preference. For using embeddings capabilities in LlamaIndex, you need an embedding model like `cohere-embed-v3-multilingual`.
+- A model supporting the [Azure AI model inference API](../../../ai-foundry/model-inference/reference/reference-model-inference-api.md?tabs=python) deployed. In this example, we use a `Mistral-Large` deployment, but use any model of your preference. For using embeddings capabilities in LlamaIndex, you need an embedding model like `cohere-embed-v3-multilingual`.
 
   - You can follow the instructions at [Deploy models as serverless APIs](../deploy-models-serverless.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Correct links in Semantic Kernel development article"
}
```

### Explanation
This recent modification to the `semantic-kernel.md` document includes a minor update focusing on correcting the link structure that references the Azure AI model inference API. 

The update specifically changes the link to ensure that it accurately points to the documentation for the API, reflecting the current directory hierarchy. This adjustment allows users to easily access the necessary resources regarding the API functionalities which are critical for integrating models in and working with the Semantic Kernel.

The corrected link enhances the clarity and user experience of the article, contributing to better accessibility of supportive documentation for developers working with the Azure AI environment. By ensuring that developers have accurate references, this update aids in their implementation efforts and helps in streamlining the development process with the Semantic Kernel.

## articles/ai-studio/how-to/develop/trace-local-sdk.md{#item-f7dfb5}

<details>
<summary>Diff</summary>
````diff
@@ -72,7 +72,7 @@ To learn more Azure AI Inference SDK for C# and observability, see the [Tracing
 
 ---
 
-To learn more , see the [Inference SDK reference](../../reference/reference-model-inference-api.md).
+To learn more , see the [Inference SDK reference](../../../ai-foundry/model-inference/reference/reference-model-inference-api.md).
 
 ### Configuration
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update link in Trace Local SDK article"
}
```

### Explanation
The recent change made to the `trace-local-sdk.md` document involves a minor update that corrects the hyperlink referencing the Inference SDK documentation. 

This update modifies the link's path to ensure it accurately directs users to the current location of the Inference SDK reference under the Azure AI Foundry documentation. This improvement enhances the accessibility of essential resources for developers seeking to utilize the Azure AI Inference SDK for C#.

By clarifying this link, the modification contributes to a better user experience, ensuring that developers can effectively find and utilize the documentation needed to leverage the functionalities of the SDK, particularly in relation to observability and tracing capabilities. Overall, this small but significant update aids users in their development efforts within the Azure AI ecosystem.

## articles/ai-studio/how-to/prompt-flow-tools/azure-open-ai-gpt-4v-tool.md{#item-053d0d}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ ms.service: azure-ai-studio
 ms.custom:
   - build-2024
 ms.topic: how-to
-ms.date: 5/21/2024
+ms.date: 01/29/2025
 ms.reviewer: keli19
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Azure OpenAI GPT-4V Tool article"
}
```

### Explanation
The recent modification to the `azure-open-ai-gpt-4v-tool.md` document features a minor update that changes the date listed in the metadata section of the article. 

Specifically, the date has been updated from May 21, 2024, to January 29, 2025. This modification is typically part of standard maintenance to ensure that the documentation reflects the correct timeline for the content's relevance, review, and publication. 

Updating dates in documentation is crucial as it helps users identify the most current and applicable guidance available, thereby supporting developers in utilizing the Azure OpenAI tools effectively as they stay informed about the latest information. This minor adjustment contributes to maintaining the integrity and timeliness of the overall content within the Azure AI documentation landscape.

## articles/ai-studio/how-to/prompt-flow-tools/content-safety-tool.md{#item-09b048}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom:
   - ignite-2023
   - build-2024
 ms.topic: how-to
-ms.date: 5/21/2024
+ms.date: 01/29/2025
 ms.reviewer: keli19
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Content Safety Tool article"
}
```

### Explanation
The recent edit to the `content-safety-tool.md` document involves a minor update that alters the date in the metadata section of the article.

The date has been changed from May 21, 2024, to January 29, 2025. This type of update is important for ensuring that the content remains accurate and reflects the appropriate timeline for its publication and relevance.

Updating the date helps users quickly identify the most current information available, which is particularly vital in the fast-evolving field of AI and its associated tools. By keeping the documentation up to date, the authors enhance the user experience, aiding developers and stakeholders who rely on the Content Safety Tool within the Azure AI Studio to create safe and compliant content. This minor but meaningful modification contributes to the overall quality and reliability of the documentation.

## articles/ai-studio/how-to/prompt-flow-tools/embedding-tool.md{#item-81420c}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom:
   - ignite-2023
   - build-2024
 ms.topic: how-to
-ms.date: 5/21/2024
+ms.date: 01/29/2025
 ms.reviewer: keli19
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Embedding Tool article"
}
```

### Explanation
The recent change to the `embedding-tool.md` file consists of a minor update that modifies the date in the article's metadata section.

Specifically, the date has been revised from May 21, 2024, to January 29, 2025. This adjustment is part of routine documentation maintenance to ensure that users have access to the most accurate and timely information regarding the content.

Ensuring that the publication dates are current is crucial for users who rely on the Embedding Tool within the Azure AI Studio, as it allows them to identify when the content was last reviewed or modified. This minor modification enhances the credibility of the documentation and aids users in making informed decisions based on the most recent guidance available. Overall, keeping the dates updated reflects the commitment to high-quality documentation in the rapidly evolving field of AI tools.

## articles/ai-studio/how-to/prompt-flow-tools/index-lookup-tool.md{#item-cad66d}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ ms.service: azure-ai-studio
 ms.custom:
   - build-2024
 ms.topic: how-to
-ms.date: 5/21/2024
+ms.date: 01/29/2025
 ms.reviewer: estraight
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Index Lookup Tool article"
}
```

### Explanation
The recent update to the `index-lookup-tool.md` file involves a minor modification that changes the date in the article's metadata.

The date has been updated from May 21, 2024, to January 29, 2025. This change is part of standard documentation practices aimed at ensuring the accuracy and relevance of the content.

Maintaining up-to-date publication dates is essential for users who depend on the Index Lookup Tool within the Azure AI Studio, as it aids them in identifying the most current and trustworthy information available. This minor change reinforces the documentation's reliability and supports users in making informed decisions based on the latest insights and guidelines related to the tool. Overall, it exemplifies the commitment to providing timely and accurate documentation in the rapidly changing landscape of AI technology.

## articles/ai-studio/how-to/prompt-flow-tools/llm-tool.md{#item-6691f4}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom:
   - ignite-2023
   - build-2024
 ms.topic: how-to
-ms.date: 5/21/2024
+ms.date: 1/29/2025
 ms.reviewer: keli19
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in LLM Tool article"
}
```

### Explanation
The recent modification to the `llm-tool.md` file includes a minor update in which the date in the article's metadata has been changed.

Initially, the date was set to May 21, 2024, and it has now been updated to January 29, 2025. Such updates are a part of regular documentation maintenance, which aims to keep the content relevant and accurate for users.

Having the correct publication dates is critical for users using the LLM Tool in Azure AI Studio, as this allows them to discern when the content was last updated or reviewed. This minor change enhances the overall credibility of the documentation, helping users to rely on the most current information for their work with the tool. It reflects a commitment to providing precise and timely documentation in the fast-evolving field of AI technologies.

## articles/ai-studio/how-to/prompt-flow-tools/prompt-flow-tools-overview.md{#item-fd7471}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ ms.service: azure-ai-studio
 ms.custom:
   - build-2024
 ms.topic: overview
-ms.date: 5/21/2024
+ms.date: 01/29/2025
 ms.reviewer: keli19
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Prompt Flow Tools Overview article"
}
```

### Explanation
The recent modification to the `prompt-flow-tools-overview.md` file consists of a minor update where the date in the article metadata has been modified.

The publication date has been changed from May 21, 2024, to January 29, 2025. This adjustment is part of routine content management practices that ensure the documentation remains accurate and current.

Providing an up-to-date publication date is essential for users engaging with the Prompt Flow Tools in Azure AI Studio, as it helps them identify the last revision of the guidance offered. This minor change enhances the reliability of the documentation, allowing users to be confident that they are accessing the latest information. It reflects a proactive approach to maintaining high-quality, relevant documentation in a rapidly evolving technology landscape.

## articles/ai-studio/how-to/prompt-flow-tools/prompt-tool.md{#item-c6a9a0}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom:
   - ignite-2023
   - build-2024
 ms.topic: how-to
-ms.date: 5/21/2024
+ms.date: 01/29/2025
 ms.reviewer: keli19
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Prompt Tool article"
}
```

### Explanation
The recent modification to the `prompt-tool.md` file involves a minor update where the article's metadata date has been revised.

Originally, the date was set to May 21, 2024, and it has now been updated to January 29, 2025. Such updates are essential for maintaining accurate and reliable documentation, particularly in the fast-paced field of AI tools.

Having the correct date in documentation allows users of the Azure AI Studio Prompt Tool to understand the recency of the information they are accessing. This minor change enhances the reliability of the documentation and assures users that they are working with the most up-to-date content available, which is crucial for effective utilization of the tool. It reflects a commitment to delivering precise and relevant guidance to users navigating prompt flow tools.

## articles/ai-studio/how-to/prompt-flow-tools/python-tool.md{#item-c9200f}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ manager: scottpolly
 ms.service: azure-ai-studio
 ms.custom: ignite-2023, devx-track-python, build-2024, ignite-2024
 ms.topic: how-to
-ms.date: 5/21/2024
+ms.date: 01/29/2025
 ms.reviewer: keli19
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Python Tool article"
}
```

### Explanation
The recent modification to the `python-tool.md` file includes a minor update where the publication date in the article's metadata has been revised.

Previously set to May 21, 2024, the date has now been updated to January 29, 2025. This change reflects ongoing content management efforts to ensure that the documentation is accurate and current, especially in the context of evolving technologies and practices.

Updating the publication date is important as it helps users identify the most recent information regarding the Python Tool within the Azure AI Studio framework. This minor update enhances the credibility of the documentation, reassuring users that they are accessing the latest guidelines and instructions. It underscores the commitment to keeping the provided resources relevant and useful for effective engagement with the tool.

## articles/ai-studio/how-to/prompt-flow-tools/rerank-tool.md{#item-dd52bf}

<details>
<summary>Diff</summary>
````diff
@@ -5,7 +5,7 @@ description: This article introduces you to the Rerank tool for flows in Azure A
 manager: scottpolly
 ms.service: azure-ai-studio
 ms.topic: how-to
-ms.date: 08/29/2024
+ms.date: 01/29/2025
 ms.reviewer: jingyizhu
 ms.author: lagayhar
 author: lgayhardt
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in Rerank Tool article"
}
```

### Explanation
The recent modification to the `rerank-tool.md` file involves a minor update where the article's metadata date has been adjusted.

The publication date was previously noted as August 29, 2024, and has now been changed to January 29, 2025. This modification is part of routine content management to ensure that the information remains accurate, particularly in a field that advances rapidly like AI.

Updating the publication date is significant as it signals to readers that they are accessing the most up-to-date information regarding the Rerank Tool in Azure AI Studio. Such updates are crucial for maintaining the relevance and reliability of documentation, ensuring users have access to the latest guidance when utilizing the tool. This minor change reinforces the commitment to providing accurate and timely resources for users navigating prompt flow tools.

## articles/ai-studio/media/prompt-flow/azure-openai-gpt-4-vision-tool.png{#item-94def5}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update metadata for Azure OpenAI GPT-4 Vision Tool image"
}
```

### Explanation
The recent update to the `azure-openai-gpt-4-vision-tool.png` file indicates a minor change where the file's metadata was modified, although no actual content changes, additions, or deletions were made to the image itself.

This modification likely pertains to organizational or administrative changes in the repository aimed at keeping project resources well-managed and up-to-date. While the image remains the same, updating the associated metadata can help ensure that the image aligns with current documentation and represents the intended visual content accurately.

Such updates to metadata are common in maintaining digital assets, facilitating better resource management, and providing clarity to users referencing these images in the documentation. This minor modification reinforces the effort to ensure that all elements within the documentation ecosystem are accurately represented and maintained, contributing to an overall polished presentation of the resources related to the Azure OpenAI platform.

## articles/ai-studio/media/prompt-flow/content-safety-tool.png{#item-c673ee}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update metadata for Content Safety Tool image"
}
```

### Explanation
The latest modification to the `content-safety-tool.png` file indicates a minor update where the image's metadata has been altered, despite no changes being made to the actual image content.

This update typically involves adjustments to how the image is cataloged within the repository, enhancing the overall organization and management of assets. While the visual representation of the Content Safety Tool remains unchanged, updating the metadata can ensure that the image is accurately referenced and aligned with the current documentation.

These types of modifications are essential for maintaining consistency and clarity within a documentation set, allowing users to rely on the correctness of visual resources used in conjunction with written guidelines. Ensuring that metadata is up-to-date not only aids in resource management but also supports users in finding and understanding the materials related to AI Studio's prompt flow features effectively.

## articles/ai-studio/media/prompt-flow/prompt-tool.png{#item-245957}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update metadata for Prompt Tool image"
}
```

### Explanation
The recent modification to the `prompt-tool.png` file reflects a minor update where the image's metadata has been adjusted without any changes to its actual content.

This type of update typically relates to the organization and management of files within the repository, ensuring that all assets are accurately cataloged and referenced. Even though the image itself remains unchanged, updating its metadata is crucial for maintaining clarity and consistency in documentation.

Such modifications contribute to a better user experience by making sure that images are correctly represented in association with the accompanying documentation. This is particularly important in a dynamic environment like AI Studio, where prompt flow tools are pivotal. Keeping the metadata in sync helps users locate and understand the relevant visual resources pertaining to the features and functionalities provided by the documentation more effectively.

## articles/ai-studio/media/prompt-flow/python-tool.png{#item-14e9b3}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update metadata for Python Tool image"
}
```

### Explanation
The recent update to the `python-tool.png` file signifies a minor modification focused on updating the metadata associated with the image, while the content of the image itself remains unchanged.

This type of update is vital for keeping the documentation organized and ensuring that all graphical assets are appropriately referenced throughout the project. Although no alterations were made to the visual component of the image, the adjustments to its metadata enhance the clarity and management of resources in the documentation.

By ensuring that the metadata is accurate and current, users can more effectively navigate and understand the visual elements related to the Python Tool within the AI Studio's prompt flow. This helps maintain an organized resource library, which is essential for developers and stakeholders when utilizing or referencing the documentation in their projects.

## articles/ai-studio/reference/reference-model-inference-completions.md{#item-bae39d}

<details>
<summary>Diff</summary>
````diff
@@ -1,296 +0,0 @@
----
-title: Azure AI Model Inference Completions
-titleSuffix: Azure AI Foundry
-description: Reference for Azure AI Model Inference Completions API
-manager: scottpolly
-ms.service: azure-ai-studio
-ms.topic: conceptual
-ms.date: 5/21/2024
-ms.reviewer: fasantia 
-reviewer: santiagxf
-ms.author: mopeakande
-author: msakande
-ms.custom: 
- - build-2024
----
-
-# Reference: Completions | Azure AI Foundry
-
-[!INCLUDE [feature-preview](../includes/feature-preview.md)]
-
-Creates a completion for the provided prompt and parameters.
-
-```http
-POST /completions?api-version=2024-04-01-preview
-```
-
-| Name | In  | Required | Type | Description |
-| --- | --- | --- | --- | --- |
-| api-version | query | True | string | The version of the API in the format "YYYY-MM-DD" or "YYYY-MM-DD-preview". |
-
-## Request Header
-
-
-| Name | Required | Type | Description |
-| --- | --- | --- | --- |
-| extra-parameters | | string | The behavior of the API when extra parameters are indicated in the payload. Using `pass-through` makes the API to pass the parameter to the underlying model. Use this value when you want to pass parameters that you know the underlying model can support. Using `drop` makes the API to drop any unsupported parameter. Use this value when you need to use the same payload across different models, but one of the extra parameters may make a model to error out if not supported. Using `error` makes the API to reject any extra parameter in the payload. Only parameters specified in this API can be indicated, or a 400 error is returned. |
-| azureml-model-deployment |     | string | Name of the deployment you want to route the request to. Supported for endpoints that support multiple deployments. |
-
-
-## Request Body
-
-
-| Name | Required | Type | Description |
-| --- | --- | --- | --- |
-| prompt | True |     | The prompts to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays. Note that `<\|endoftext\|>` is the document separator that the model sees during training, so if a prompt is not specified the model generates as if from the beginning of a new document. |
-| frequency\_penalty |     | number | Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. |
-| max\_tokens |     | integer | The maximum number of tokens that can be generated in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length. |
-| presence\_penalty |     | number | Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. |
-| seed |     | integer | If specified, the model makes a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.<br><br>Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. |
-| stop |     |     | Sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. |
-| stream |     | boolean | Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. |
-| temperature |     | number | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.<br><br>We generally recommend altering `temperature` or `top_p` but not both. |
-| top\_p |     | number | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.<br><br>We generally recommend altering `top_p` or `temperature` but not both. |
-
-## Responses
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| 200 OK | [CreateCompletionResponse](#createcompletionresponse) | OK  |
-| 401 Unauthorized         | [UnauthorizedError](#unauthorizederror)                 | Access token is missing or invalid<br><br>Headers<br><br>x-ms-error-code: string                                                                           |
-| 404 Not Found            | [NotFoundError](#notfounderror)                         | Modality not supported by the model. Check the documentation of the model to see which routes are available.<br><br>Headers<br><br>x-ms-error-code: string |
-| 422 Unprocessable Entity | [UnprocessableContentError](#unprocessablecontenterror) | The request contains unprocessable content<br><br>Headers<br><br>x-ms-error-code: string                                                                   |
-| 429 Too Many Requests    | [TooManyRequestsError](#toomanyrequestserror)           | You have hit your assigned rate limit and your request need to be paced.<br><br>Headers<br><br>x-ms-error-code: string                                     |
-| Other Status Codes       | [ContentFilterError](#contentfiltererror)               | Bad request<br><br>Headers<br><br>x-ms-error-code: string                                                                                                  |
-
-
-## Security
-
-
-### Authorization
-
-The token with the `Bearer:` prefix, e.g. `Bearer abcde12345`
-
-**Type**: apiKey  
-**In**: header  
-
-
-### AADToken
-
-Azure Active Directory OAuth2 authentication
-
-**Type**: oauth2  
-**Flow**: application  
-**Token URL**: https://login.microsoftonline.com/common/oauth2/v2.0/token  
-
-
-## Examples
-
-
-### Creates a completion for the provided prompt and parameters
-
-#### Sample Request
-
-```http
-POST /completions?api-version=2024-04-01-preview
-
-{
-  "prompt": "This is a very good text",
-  "frequency_penalty": 0,
-  "presence_penalty": 0,
-  "max_tokens": 256,
-  "seed": 42,
-  "stop": "<|endoftext|>",
-  "stream": false,
-  "temperature": 0,
-  "top_p": 1
-}
-
-```
-
-#### Sample Response
-
-Status code: 200
-
-```json
-{
-  "id": "1234567890",
-  "model": "llama2-7b",
-  "choices": [
-    {
-      "index": 0,
-      "finish_reason": "stop",
-      "text": ", indeed it is a good one."
-    }
-  ],
-  "created": 1234567890,
-  "object": "text_completion",
-  "usage": {
-    "prompt_tokens": 15,
-    "completion_tokens": 8,
-    "total_tokens": 23
-  }
-}
-```
-
-## Definitions
-
-
-| Name | Description |
-| --- | --- |
-| [Choices](#choices) | A list of chat completion choices. |
-| [CompletionFinishReason](#completionfinishreason) | The reason the model stopped generating tokens. This is `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters. |
-| [CompletionUsage](#completionusage) | Usage statistics for the completion request. |
-| [ContentFilterError](#contentfiltererror) | The API call fails when the prompt triggers a content filter as configured. Modify the prompt and try again. |
-| [CreateCompletionRequest](#createcompletionrequest) |     |
-| [CreateCompletionResponse](#createcompletionresponse) | Represents a completion response from the API. |
-| [Detail](#detail) |     |
-| [TextCompletionObject](#textcompletionobject) | The object type, which is always "text\_completion" |
-| [UnprocessableContentError](#unprocessablecontenterror) |     |
-
-
-### Choices
-
-A list of chat completion choices.
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| finish\_reason | [CompletionFinishReason](#completionfinishreason) | The reason the model stopped generating tokens. This is `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool. |
-| index | integer | The index of the choice in the list of choices. |
-| text | string | The generated text. |
-
-
-### CompletionFinishReason
-
-The reason the model stopped generating tokens. This is `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters.
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| content\_filter | string |     |
-| length | string |     |
-| stop | string |     |
-
-### CompletionUsage
-
-Usage statistics for the completion request.
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| completion\_tokens | integer | Number of tokens in the generated completion. |
-| prompt\_tokens | integer | Number of tokens in the prompt. |
-| total\_tokens | integer | Total number of tokens used in the request (prompt + completion). |
-
-
-### ContentFilterError
-
-The API call fails when the prompt triggers a content filter as configured. Modify the prompt and try again.
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| code | string | The error code. |
-| error | string | The error description. |
-| message | string | The error message. |
-| param | string | The parameter that triggered the content filter. |
-| status | integer | The HTTP status code. |
-
-
-### CreateCompletionRequest
-
-
-| Name | Type | Default Value | Description |
-| --- | --- | --- | --- |
-| frequency\_penalty | number | 0   | Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. |
-| max\_tokens | integer | 256 | The maximum number of tokens that can be generated in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length. |
-| presence\_penalty | number | 0   | Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. |
-| prompt |     | `<\|endoftext\|>` | The prompts to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays. Note that `<\|endoftext\|>` is the document separator that the model sees during training, so if a prompt is not specified the model generates as if from the beginning of a new document. |
-| seed | integer |     | If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.<br><br>Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. |
-| stop |     |     | Sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. |
-| stream | boolean | False | Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. |
-| temperature | number | 1   | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.<br><br>We generally recommend altering this or `top_p` but not both. |
-| top\_p | number | 1   | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.<br><br>We generally recommend altering this or `temperature` but not both. |
-
-
-### CreateCompletionResponse
-
-Represents a completion response from the API. Note: both the streamed and nonstreamed response objects share the same shape (unlike the chat endpoint).
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| choices | [Choices](#choices)\[\] | The list of completion choices the model generated for the input prompt. |
-| created | integer | The Unix timestamp (in seconds) of when the completion was created. |
-| ID  | string | A unique identifier for the completion. |
-| model | string | The model used for completion. |
-| object | [TextCompletionObject](#textcompletionobject) | The object type, which is always "text\_completion" |
-| system\_fingerprint | string | This fingerprint represents the backend configuration that the model runs with.<br><br>Can be used with the `seed` request parameter to understand when backend changes have been made that might impact determinism. |
-| usage | [CompletionUsage](#completionusage) | Usage statistics for the completion request. |
-
-### Detail
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| loc | string\[\] | The parameter causing the issue |
-| value | string | The value passed to the parameter causing issues. |
-
-
-### TextCompletionObject
-
-The object type, which is always "text\_completion"
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| text\_completion | string |     |
-
-### ListObject
-
-The object type, which is always "list".
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| list | string |     |
-
-### NotFoundError
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| error | string | The error description. |
-| message | string | The error message. |
-| status | integer | The HTTP status code. |
-
-### TooManyRequestsError
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| error | string | The error description. |
-| message | string | The error message. |
-| status | integer | The HTTP status code. |
-
-### UnauthorizedError
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| error | string | The error description. |
-| message | string | The error message. |
-| status | integer | The HTTP status code. |
-
-### UnprocessableContentError
-
-
-| Name | Type | Description |
-| --- | --- | --- |
-| code | string | The error code. |
-| detail | [Detail](#detail) |     |
-| error | string | The error description. |
-| message | string | The error message. |
-| status | integer | The HTTP status code. |
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "Remove Reference for Model Inference Completions"
}
```

### Explanation
The modification involves the complete removal of the `reference-model-inference-completions.md` file from the documentation repository, which contained critical information regarding the Azure AI Model Inference Completions API.

This file included details such as the API endpoint, parameters, request and response formats, and various error handling scenarios. Deleting this document represents a breaking change since users who relied on this reference for implementation will no longer have access to essential guidelines for using the Model Inference Completions API.

Such a removal could indicate a significant update or restructuring of the API itself, necessitating users to find alternative resources or updated documentation to understand the latest functionalities and best practices surrounding model inference completions. The lack of reference can impact developers and users who depend heavily on the documentation for operational and implementation guidance. It emphasizes the need for timely communication of such changes to those affected within the Azure AI community.

## articles/ai-studio/toc.yml{#item-2745cd}

<details>
<summary>Diff</summary>
````diff
@@ -140,6 +140,8 @@ items:
         href: how-to/deploy-models-cohere-embed.md
       - name: Cohere Rerank models
         href: how-to/deploy-models-cohere-rerank.md
+    - name: DeepSeek-R1 reasoning models
+      href: how-to/deploy-models-deepseek.md
     - name: Meta Llama models
       items:
       - name: Meta Llama family models
@@ -443,19 +445,7 @@ items:
   - name: Prompt flow Python SDK
     href: https://microsoft.github.io/promptflow/reference/index.html#
   - name: Azure AI Model Inference API
-    items:
-      - name: What's the Azure AI Model Inference API?
-        href: reference/reference-model-inference-api.md
-      - name: Reference
-        items:
-        - name: Get Info
-          href: reference/reference-model-inference-info.md
-        - name: Embeddings
-          href: reference/reference-model-inference-embeddings.md
-        - name: Chat Completions
-          href: reference/reference-model-inference-chat-completions.md
-        - name: Images Embeddings
-          href: reference/reference-model-inference-images-embeddings.md
+    href: ../ai-foundry/model-inference/reference/reference-model-inference-api.md
   - name: Azure Policy built-ins
     displayName: samples, policies, definitions
     href: ../ai-services/policy-reference.md?context=/azure/ai-studio/context/context
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update Table of Contents for AI Studio"
}
```

### Explanation
The recent changes made to the `toc.yml` file for the AI Studio documentation reflect a minor update that reorganizes the structure of the Table of Contents. This modification includes the addition of new content and the restructuring of existing sections to enhance usability.

Three new entries have been introduced, including a reference to the "DeepSeek-R1 reasoning models." This addition provides users with updated resources related to deploying and using these models within the AI Studio framework. 

Conversely, several existing links and sub-items related to the "Azure AI Model Inference API" have been removed and replaced with a single direct link to a new reference document. This streamlining of content reflects an effort to consolidate information, potentially making it easier for users to navigate the documentation.

Overall, these updates contribute to better content accessibility and clarity within the AI Studio documentation, making it easier for users to find relevant guides and references as they work with Azure's capabilities.


