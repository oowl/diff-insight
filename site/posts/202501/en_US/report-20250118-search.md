---
date: '2025-01-18'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:0fcd6d7...MicrosoftDocs:1d7cc3c
summary: |-
  The recent updates to the Azure AI Search documentation include minor modifications aimed at enhancing clarity, correcting references, and updating publication dates. Some key changes are the introduction of the `threshold` parameter in vector search queries, which helps to filter out low-scoring results, and revisions to ensure the correct use of `maxTextRecallSize`. There are no breaking changes in this update.

  Overall, these updates focus on improving user experience by providing clearer and more accurate information, emphasizing role-based access control, and refining SDK documentation. By updating outdated references and standardizing terminology, the documentation aims to help developers understand and utilize Azure AI's search capabilities more effectively. These changes not only enhance usability but also align with best practices in security, ensuring that developers have the necessary guidance as they adapt to new protocols.
title: '[en_US] Diff Insight Report - search'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:0fcd6d7...MicrosoftDocs:1d7cc3c){target="_blank"}

# Highlights
The documentation across various Azure AI Search articles has undergone several minor updates focusing on enhancing clarity, correcting references, and updating publication dates. Notable changes include improved descriptions of query parameters, emphasizing role-based access control, and correcting hyperlinks.

## New Features
- Introduction of the `threshold` parameter in vector search queries, excluding low-scoring results.

## Breaking Changes
- No breaking changes were introduced in this update.

## Other Updates
- Revisions to documents regarding the use of `maxTextRecallSize`, ensuring accurate references and improved navigability.
- Updates include standardizing terminology, enhancing SDK documentation, and refining language for clarity.
- Dates in various documents were updated to reflect recent revisions and maintain the currency of the content.

# Insights
Documentation plays a crucial role in guiding users on effectively utilizing Azure AI's search capabilities. The recent updates, although minor, significantly enhance user experience by providing clearer, more accurate information. By updating and correcting the references for elements like `maxTextRecallSize` and introducing new query parameters like `threshold`, the documentation ensures developers are equipped with the latest and most relevant knowledge.

A key focus of these updates is improving the clarity and accessibility of technical documentation. Removing outdated references, emphasizing best security practices, and updating date metadata help maintain the documents' relevance and reliability. The introduction of features like `threshold` increases the customization options available to developers, allowing for more refined query results, which is pivotal in optimizing search functionalities in complex environments.

Moreover, the emphasis on role-based access control (RBAC) highlights a shift towards more secure authentication methods, aligning with industry best practices to safeguard sensitive data. The documentation changes facilitate a smoother transition for developers adapting to these new security norms by providing detailed instructions and removing redundant content that could cause confusion.

In summary, these documentation updates reflect an ongoing commitment to improving user experience, both in terms of usability and security, ensuring Azure AI Search remains a robust tool for developers worldwide.

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [hybrid-search-how-to-query.md](#item-345ce6) | minor update | Update hybrid search query instructions in documentation. Locale: en_US | modified | 6 | 4 | 10 | 
| [hybrid-search-ranking.md](#item-dad887) | minor update | Update reference for maxTextRecallSize in hybrid search ranking documentation. Locale: en_US | modified | 1 | 1 | 2 | 
| [index-add-custom-analyzers.md](#item-11325e) | minor update | Revise content and links for adding custom analyzers in Azure AI Search documentation. Locale: en_US | modified | 18 | 18 | 36 | 
| [index-add-language-analyzers.md](#item-004ac0) | minor update | Update the modification date in language analyzers documentation. Locale: en_US | modified | 1 | 1 | 2 | 
| [index-similarity-and-scoring.md](#item-75603d) | minor update | Adjust content and date in similarity and scoring documentation. Locale: en_US | modified | 4 | 2 | 6 | 
| [search-analyzers.md](#item-9dccd9) | minor update | Revise content and update date in search analyzers documentation. Locale: en_US | modified | 3 | 3 | 6 | 
| [search-api-preview.md](#item-511f5d) | minor update | Update hyperlink format in search API preview documentation. Locale: en_US | modified | 1 | 1 | 2 | 
| [search-api-versions.md](#item-69ca3e) | minor update | Revise SDK version details and update document date in search API versions documentation. Locale: en_US | modified | 17 | 19 | 36 | 
| [search-faceted-navigation.md](#item-f29d1e) | minor update | Update document date in faceted navigation documentation. Locale: en_US | modified | 1 | 1 | 2 | 
| [search-faq-frequently-asked-questions.yml](#item-eab2ba) | minor update | Update date and refine answers in the FAQs for Azure AI Search. Locale: en_US | modified | 6 | 6 | 12 | 
| [search-howto-concurrency.md](#item-863358) | minor update | Update document date in concurrency guidelines. Locale: en_US | modified | 1 | 1 | 2 | 
| [search-howto-index-json-blobs.md](#item-b8230c) | minor update | Update document date and refine language in JSON blobs indexing guide. Locale: en_US | modified | 3 | 5 | 8 | 
| [search-howto-managed-identities-storage.md](#item-8209c4) | minor update | Update document date and enhance clarity in managed identities storage guide. Locale: en_US | modified | 9 | 9 | 18 | 
| [search-indexer-troubleshooting.md](#item-087365) | minor update | Enhance troubleshooting documentation for indexers and add specific error handling guidelines. Locale: en_US | modified | 14 | 4 | 18 | 
| [search-pagination-page-layout.md](#item-115902) | minor update | Correct link to documentation for maxTextRecallSize in pagination layout guide. Locale: en_US | modified | 1 | 1 | 2 | 
| [search-performance-analysis.md](#item-5032b3) | minor update | Update date references and improve language clarity in performance analysis document. Locale: en_US | modified | 11 | 11 | 22 | 
| [search-security-api-keys.md](#item-d8c908) | minor update | Update to API key security practices and connection methods in Azure AI Search documentation. Locale: en_US | modified | 13 | 12 | 25 | 
| [search-security-enable-roles.md](#item-4985c4) | minor update | Enhancements to role-based access control documentation for Azure AI Search. Locale: en_US | modified | 11 | 15 | 26 | 
| [search-security-rbac.md](#item-a5d129) | minor update | Removal of limitations section in RBAC documentation for Azure AI Search. Locale: en_US | modified | 0 | 6 | 6 | 
| [tutorial-csharp-create-load-index.md](#item-0a6ffd) | minor update | Update of publication date in C# tutorial for Azure AI Search. Locale: en_US | modified | 1 | 1 | 2 | 
| [tutorial-csharp-deploy-static-web-app.md](#item-a2300f) | minor update | Update of publication date in C# tutorial for deploying static web apps. Locale: en_US | modified | 1 | 1 | 2 | 
| [tutorial-csharp-overview.md](#item-57fa0d) | minor update | Update of publication date and .NET version in C# overview tutorial. Locale: en_US | modified | 2 | 2 | 4 | 
| [tutorial-csharp-search-query-integration.md](#item-8ad6b5) | minor update | Update of publication date in C# search query integration tutorial. Locale: en_US | modified | 1 | 1 | 2 | 
| [vector-search-how-to-query.md](#item-9bb93c) | minor update | Adjustment of references and details in vector search how-to query tutorial. Locale: en_US | modified | 4 | 4 | 8 | 
| [whats-new.md](#item-fa71b4) | minor update | Update of vector relevance features in what's new documentation. Locale: en_US | modified | 1 | 1 | 2 | 


# Modified Contents
## articles/search/hybrid-search-how-to-query.md{#item-345ce6}

<details>
<summary>Diff</summary>
````diff
@@ -30,7 +30,7 @@ In this article, learn how to:
 
 + [vector.queries.weight](vector-search-how-to-query.md#vector-weighting) lets you set the relative weight of the vector query. This feature is particularly useful in complex queries where two or more distinct result sets need to be combined, as is the case for hybrid search. This feature is generally available.
 
-+ [hybridsearch.maxTextRecallSize and countAndFacetMode (preview)](#set-maxtextrecallsize-and-countandfacetmode-preview) give you more control over text inputs into a hybrid query. This feature requires a preview API version.
++ [hybridsearch.maxTextRecallSize and countAndFacetMode (preview)](#set-maxtextrecallsize-and-countandfacetmode) give you more control over text inputs into a hybrid query. This feature requires a preview API version.
  -->
 ## Prerequisites
 
@@ -44,7 +44,7 @@ In this article, learn how to:
 
 + Search Explorer in the Azure portal (supports both stable and preview API search syntax) has a JSON view that lets you paste in a hybrid request.
 
-+ [**2024-07-01**](/rest/api/searchservice/documents/search-post) stable version or a recent preview API version if you're using preview features like [maxTextRecallSize and countAndFacetMode(preview)](#set-maxtextrecallsize-and-countandfacetmode-preview).
++ [**2024-07-01**](/rest/api/searchservice/documents/search-post) stable version or a recent preview API version if you're using preview features like [maxTextRecallSize and countAndFacetMode(preview)](#set-maxtextrecallsize-and-countandfacetmode).
 
   For readability, we use REST examples to explain how the APIs work. You can use a REST client like Visual Studio Code with the REST extension to build hybrid queries. For more information, see [Quickstart: Vector search using REST APIs](search-get-started-vector.md).
 
@@ -310,7 +310,9 @@ api-key: {{admin-api-key}}
 
 + Postfilter is applied after query execution. If k=50 returns 50 matches on the vector query side, followed by a post-filter applied to the 50 matches, your results are reduced by the number of documents that meet filter criteria. This leaves you with fewer than 50 documents to pass to semantic ranker. Keep this in mind if you're using semantic ranking. The semantic ranker works best if it has 50 documents as input.
 
-## Set maxTextRecallSize and countAndFacetMode (preview)
+## Set maxTextRecallSize and countAndFacetMode
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
 
 This section explains how to adjust the inputs to a hybrid query by controlling the amount BM25-ranked results that flow to the hybrid ranking model. Controlling over the BM25-ranked input gives you more options for relevance tuning in hybrid scenarios.
 
@@ -407,7 +409,7 @@ A query might match to any number of documents, as many as all of them if the se
 Both "k" and "top" are optional. Unspecified, the default number of results in a response is 50. You can set "top" and "skip" to [page through more results](search-pagination-page-layout.md#paging-results) or change the default.
 
 > [!NOTE]
-> If you're using hybrid search in 2024-05-01-preview API, you can control the number of results from the keyword query using [maxTextRecallSize](#set-maxtextrecallsize-and-countandfacetmode-preview). Combine this with a setting for "k" to control the representation from each search subsystem (keyword and vector).
+> If you're using hybrid search in 2024-05-01-preview API, you can control the number of results from the keyword query using [maxTextRecallSize](#set-maxtextrecallsize-and-countandfacetmode). Combine this with a setting for "k" to control the representation from each search subsystem (keyword and vector).
 
 #### Semantic ranker results
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update hybrid search query instructions in documentation. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the documentation for hybrid search queries in the Azure AI context. The modifications primarily consist of clarifying and correcting references within the text. 

Significantly, the label for features related to `maxTextRecallSize` and `countAndFacetMode` has been revised for consistency, removing the "preview" from the hyperlink reference while retaining it in the surrounding text for better understanding. Additionally, an explanation regarding the application of postfilters on query results has been added, providing users more insight into how this impacts the results returned by the query. 

Overall, the changes enhance the clarity and accuracy of the documentation, ensuring users have the correct information when working with hybrid search functionalities.

## articles/search/hybrid-search-ranking.md{#item-dad887}

<details>
<summary>Diff</summary>
````diff
@@ -135,7 +135,7 @@ In this example, the @search.score (weighted) values are passed to the RRF ranki
 
 By default, if you aren't using pagination, the search engine returns the top 50 highest ranking matches for full text search, and the most similar `k` matches for vector search. In a hybrid query, `top` determines the number of results in the response. Based on defaults, the top 50 highest ranked matches of the unified result set are returned. 
 
-Often, the search engine finds more results than `top` and `k`. To return more results, use the paging parameters `top`, `skip`, and `next`. Paging is how you determine the number of results on each logical page and navigate through the full payload. You can set `maxTextRecallSize` to larger values (the default is 1,000) to return more results from the text side of hybrid query.
+Often, the search engine finds more results than `top` and `k`. To return more results, use the paging parameters `top`, `skip`, and `next`. Paging is how you determine the number of results on each logical page and navigate through the full payload. You can [set `maxTextRecallSize`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) to larger values (the default is 1,000) to return more results from the text side of hybrid query.
 
 By default, full text search is subject to a maximum limit of 1,000 matches (see [API response limits](search-limits-quotas-capacity.md#api-response-limits)). Once 1,000 matches are found, the search engine no longer looks for more.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update reference for maxTextRecallSize in hybrid search ranking documentation. Locale: en_US"
}
```

### Explanation
The code diff reflects a minor update made to the hybrid search ranking documentation. The change involves adjusting the way the `maxTextRecallSize` parameter is referenced in the text. 

Previously, the documentation stated the maximum text recall size directly, but now it includes a link to the specific section where users can find more information about setting `maxTextRecallSize`. This change improves navigability and helps users quickly access details regarding configuration options for increasing the number of results returned from the text side of hybrid searches.

Other content remains unchanged, ensuring that the context for how paging parameters like `top`, `skip`, and `next` function alongside the `maxTextRecallSize` is still intact. Overall, this update enhances usability without altering the substance of the guidance provided.

## articles/search/index-add-custom-analyzers.md{#item-11325e}

<details>
<summary>Diff</summary>
````diff
@@ -9,20 +9,20 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 05/23/2024
+ms.date: 01/16/2025
 ---
 
 # Add custom analyzers to string fields in an Azure AI Search index
 
-A *custom analyzer* is a user-defined combination of one tokenizer, one or more token filters, and one or more character filters. A custom analyzer is specified within a search index, and then referenced by name on field definitions that require custom analysis. A custom analyzer is invoked on a per-field basis. Attributes on the field will determine whether it's used for indexing, queries, or both.
+A *custom analyzer* is a component of lexical analysis over plain text content. It's a user-defined combination of one tokenizer, one or more token filters, and one or more character filters. A custom analyzer is specified within a search index, and then referenced by name on field definitions that require custom analysis. A custom analyzer is invoked on a per-field basis. Attributes on the field determine whether it's used for indexing, queries, or both.
 
-In a custom analyzer, character filters prepare the input text before it's processed by the tokenizer (for example, removing markup). Next, the tokenizer breaks text into tokens. Finally, token filters modify the tokens emitted by the tokenizer. For concepts and examples, see [Analyzers in Azure AI Search](search-analyzers.md).
+In a custom analyzer, character filters prepare the input text before it's processed by the tokenizer (for example, removing markup). Next, the tokenizer breaks text into tokens. Finally, token filters modify the tokens emitted by the tokenizer. For concepts and examples, see [Analyzers in Azure AI Search](search-analyzers.md) and [Tutorial: Create a custom analyzer for phone numbers](tutorial-create-custom-analyzer.md).
 
 ## Why use a custom analyzer?
 
-A custom analyzer gives you control over the process of converting text into indexable and searchable tokens by allowing you to choose which types of analysis or filtering to invoke, and the order in which they occur. 
+A custom analyzer gives you control over the process of converting plain text into indexable and searchable tokens by allowing you to choose which types of analysis or filtering to invoke, and the order in which they occur. 
 
-Create and assign a custom analyzer if none of default (Standard Lucence), built-in, or language analyzers are sufficient for your needs. You might also create a custom analyzer if you want to use a built-in analyzer with custom options. For example, if you wanted to change the maxTokenLength on Standard, you would create a custom analyzer, with a user-defined name, to set that option.
+Create and assign a custom analyzer if none of default (Standard Lucene), built-in, or language analyzers are sufficient for your needs. You might also create a custom analyzer if you want to use a built-in analyzer with custom options. For example, if you wanted to change the `maxTokenLength` on Standard Lucene, you would create a custom analyzer, with a user-defined name, to set that option.
 
 Scenarios where custom analyzers can be helpful include:
 
@@ -39,15 +39,15 @@ Scenarios where custom analyzers can be helpful include:
 - ASCII folding. Add the Standard ASCII folding filter to normalize diacritics like ö or ê in search terms.  
 
 > [!NOTE]  
-> Custom analyzers aren't exposed in the Azure portal. The only way to add a custom analyzer is through code that defines an index. 
+> Custom analyzers aren't exposed in the Azure portal. The only way to add a custom analyzer is through code that [creates an index schema](/rest/api/searchservice/indexes/create-or-update).
 
 ## Create a custom analyzer
 
 To create a custom analyzer, specify it in the `analyzers` section of an index at design time, and then reference it on searchable, `Edm.String` fields using either the `analyzer` property, or the `indexAnalyzer` and `searchAnalyzer` pair.
 
 An analyzer definition includes a name, type, one or more character filters, a maximum of one tokenizer, and one or more token filters for post-tokenization processing. Character filters are applied before tokenization. Token filters and character filters are applied from left to right.
 
-- Names in a custom analyzer must be unique and can't be the same as any of the built-in analyzers, tokenizers, token filters, or characters filters. Names consist of letters, digits, spaces, dashes or underscores. Names must start and end with plain text characters. Names must be under 128 characters in length.
+- Names in a custom analyzer must be unique and can't be the same as any of the built-in analyzers, tokenizers, token filters, or characters filters. Names consist of letters, digits, spaces, dashes, or underscores. Names must start and end with plain text characters. Names must be under 128 characters in length.
 
 - Type must be #Microsoft.Azure.Search.CustomAnalyzer.
 
@@ -224,7 +224,7 @@ Azure AI Search supports character filters in the following list. More informati
 |[mapping](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/charfilter/MappingCharFilter.html)|MappingCharFilter|A char filter that applies mappings defined with the mappings option. Matching is greedy (longest pattern matching at a given point wins). Replacement is allowed to be the empty string.  <br><br>**Options**  <br><br> mappings (type: string array) - A list of mappings of the following format: `a=>b` (all occurrences of the character `a` are replaced with character `b`). Required.|  
 |[pattern_replace](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.html)|PatternReplaceCharFilter|A char filter that replaces characters in the input string. It uses a regular expression to identify character sequences to preserve and a replacement pattern to identify characters to replace. For example, input text = `aa  bb aa bb`, pattern=`(aa)\\\s+(bb)` replacement=`$1#$2`, result = `aa#bb aa#bb`.  <br><br>**Options**  <br><br>pattern (type: string) - Required.  <br><br>replacement (type: string) - Required.|  
 
- <sup>1</sup> Char Filter Types are always prefixed in code with `#Microsoft.Azure.Search` such that `MappingCharFilter` would actually be specified as `#Microsoft.Azure.Search.MappingCharFilter`. We removed the prefix to reduce the width of the table, but please remember to include it in your code. Notice that char_filter_type is only provided for filters that can be customized. If there are no options, as is the case with html_strip, there's no associated #Microsoft.Azure.Search type.
+ <sup>1</sup> Char Filter Types are always prefixed in code with `#Microsoft.Azure.Search` such that `MappingCharFilter` would actually be specified as `#Microsoft.Azure.Search.MappingCharFilter`. We removed the prefix to reduce the width of the table, but remember to include it in your code. Notice that char_filter_type is only provided for filters that can be customized. If there are no options, as is the case with html_strip, there's no associated #Microsoft.Azure.Search type.
 
 <a name="tokenizers"></a>
 
@@ -237,40 +237,40 @@ Azure AI Search supports tokenizers in the following list. More information abou
 |**tokenizer_name**|**tokenizer_type** <sup>1</sup>|**Description and Options**|  
 |------------------|-------------------------------|---------------------------|  
 |[classic](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html)|ClassicTokenizer|Grammar based tokenizer that is suitable for processing most European-language documents.  <br><br>**Options**  <br><br>maxTokenLength (type: int) - The maximum token length. Default: 255, maximum: 300. Tokens longer than the maximum length are split.|  
-|[edgeNGram](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html)|EdgeNGramTokenizer|Tokenizes the input from an edge into n-grams of given size(s).  <br><br> **Options**  <br><br>minGram (type: int) - Default: 1, maximum: 300.  <br><br>maxGram (type: int) - Default: 2, maximum: 300. Must be greater than minGram.  <br><br>tokenChars (type: string array) - Character classes to keep in the tokens. Allowed values: <br>`letter`, `digit`, `whitespace`, `punctuation`, `symbol`. Defaults to an empty array - keeps all characters. |  
+|[edgeNGram](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html)|EdgeNGramTokenizer|Tokenizes the input from an edge into n-grams of given sizes.  <br><br> **Options**  <br><br>minGram (type: int) - Default: 1, maximum: 300.  <br><br>maxGram (type: int) - Default: 2, maximum: 300. Must be greater than minGram.  <br><br>tokenChars (type: string array) - Character classes to keep in the tokens. Allowed values: <br>`letter`, `digit`, `whitespace`, `punctuation`, `symbol`. Defaults to an empty array - keeps all characters. |  
 |[keyword_v2](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html)|KeywordTokenizerV2|Emits the entire input as a single token.  <br><br>**Options**  <br><br>maxTokenLength (type: int) - The maximum token length. Default: 256, maximum: 300. Tokens longer than the maximum length are split.|  
 |[letter](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html)|(type applies only when options are available)  |Divides text at non-letters. Tokens that are longer than 255 characters are split.|  
 |[lowercase](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html)|(type applies only when options are available)  |Divides text at non-letters and converts them to lower case. Tokens that are longer than 255 characters are split.|  
 | microsoft_language_tokenizer| MicrosoftLanguageTokenizer| Divides text using language-specific rules.  <br><br>**Options**  <br><br>maxTokenLength (type: int) - The maximum token length, default: 255, maximum: 300. Tokens longer than the maximum length are split. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the maxTokenLength set.  <br><br>isSearchTokenizer (type: bool) - Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. <br><br>language (type: string) - Language to use, default `english`. Allowed values include: <br>`bangla`, `bulgarian`, `catalan`, `chineseSimplified`,  `chineseTraditional`, `croatian`, `czech`, `danish`, `dutch`, `english`,  `french`, `german`, `greek`, `gujarati`, `hindi`, `icelandic`, `indonesian`, `italian`, `japanese`, `kannada`, `korean`, `malay`, `malayalam`, `marathi`, `norwegianBokmaal`, `polish`, `portuguese`, `portugueseBrazilian`, `punjabi`, `romanian`, `russian`, `serbianCyrillic`, `serbianLatin`, `slovenian`, `spanish`, `swedish`, `tamil`, `telugu`, `thai`, `ukrainian`, `urdu`, `vietnamese` |
 | microsoft_language_stemming_tokenizer | MicrosoftLanguageStemmingTokenizer| Divides text using language-specific rules and reduces words to their base forms. This tokenizer performs lemmatization. <br><br>**Options** <br><br>maxTokenLength (type: int) - The maximum token length, default: 255, maximum: 300. Tokens longer than the maximum length are split. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the maxTokenLength set. <br><br> isSearchTokenizer (type: bool) - Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. <br><br>language (type: string) - Language to use, default `english`. Allowed values include: <br>`arabic`, `bangla`, `bulgarian`, `catalan`, `croatian`, `czech`, `danish`, `dutch`, `english`, `estonian`, `finnish`, `french`, `german`, `greek`, `gujarati`, `hebrew`, `hindi`, `hungarian`, `icelandic`, `indonesian`, `italian`, `kannada`, `latvian`, `lithuanian`, `malay`, `malayalam`, `marathi`, `norwegianBokmaal`, `polish`, `portuguese`, `portugueseBrazilian`, `punjabi`, `romanian`, `russian`, `serbianCyrillic`, `serbianLatin`, `slovak`, `slovenian`, `spanish`, `swedish`, `tamil`, `telugu`, `turkish`, `ukrainian`, `urdu` |
-|[nGram](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html)|NGramTokenizer|Tokenizes the input into n-grams of the given size(s). <br><br>**Options** <br><br>minGram (type: int) - Default: 1, maximum: 300. <br><br>maxGram (type: int) - Default: 2, maximum: 300. Must be greater than minGram. <br><br>tokenChars (type: string array) - Character classes to keep in the tokens. Allowed values: `letter`, `digit`, `whitespace`, `punctuation`, `symbol`. Defaults to an empty array - keeps all characters. |  
+|[nGram](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html)|NGramTokenizer|Tokenizes the input into n-grams of the given sizes. <br><br>**Options** <br><br>minGram (type: int) - Default: 1, maximum: 300. <br><br>maxGram (type: int) - Default: 2, maximum: 300. Must be greater than minGram. <br><br>tokenChars (type: string array) - Character classes to keep in the tokens. Allowed values: `letter`, `digit`, `whitespace`, `punctuation`, `symbol`. Defaults to an empty array - keeps all characters. |  
 |[path_hierarchy_v2](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html)|PathHierarchyTokenizerV2|Tokenizer for path-like hierarchies. **Options** <br><br>delimiter (type: string) - Default: '/. <br><br>replacement (type: string) - If set, replaces the delimiter character. Default same as the value of delimiter. <br><br>maxTokenLength (type: int) -  The maximum token length. Default: 300, maximum: 300. Paths longer than maxTokenLength are ignored. <br><br>reverse (type: bool) - If true, generates token in reverse order. Default: false. <br><br>skip (type: bool) - Initial tokens to skip. The default is 0.|  
 |[pattern](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html)|PatternTokenizer|This tokenizer uses regex pattern matching to construct distinct tokens. <br><br>**Options** <br><br> [pattern](https://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html) (type: string) - Regular expression pattern to match token separators. The default is `\W+`, which matches non-word characters. <br><br>[flags](https://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html#field_summary) (type: string) - Regular expression flags. The default is an empty string. Allowed values: CANON_EQ, CASE_INSENSITIVE, COMMENTS, DOTALL, LITERAL, MULTILINE, UNICODE_CASE, UNIX_LINES <br><br>group (type: int) - Which group to extract into tokens. The default is -1 (split).|
 |[standard_v2](https://lucene.apache.org/core/6_6_1/core/org/apache/lucene/analysis/standard/StandardTokenizer.html)|StandardTokenizerV2|Breaks text following the [Unicode Text Segmentation rules](https://unicode.org/reports/tr29/). <br><br>**Options** <br><br>maxTokenLength (type: int) - The maximum token length. Default: 255, maximum: 300. Tokens longer than the maximum length are split.|  
 |[uax_url_email](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html)|UaxUrlEmailTokenizer|Tokenizes urls and emails as one token. <br><br>**Options** <br><br> maxTokenLength (type: int) - The maximum token length. Default: 255, maximum: 300. Tokens longer than the maximum length are split.|  
 |[whitespace](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html)|(type applies only when options are available) |Divides text at whitespace. Tokens that are longer than 255 characters are split.|  
 
- <sup>1</sup> Tokenizer Types are always prefixed in code with `#Microsoft.Azure.Search` such that  `ClassicTokenizer` would actually be specified as `#Microsoft.Azure.Search.ClassicTokenizer`. We removed the prefix to reduce the width of the table, but please remember to include it in your code. Notice that tokenizer_type is only provided for tokenizers that can be customized. If there are no options, as is the case with the letter tokenizer, there's no associated #Microsoft.Azure.Search type.
+ <sup>1</sup> Tokenizer Types are always prefixed in code with `#Microsoft.Azure.Search` such that  `ClassicTokenizer` would actually be specified as `#Microsoft.Azure.Search.ClassicTokenizer`. We removed the prefix to reduce the width of the table, but remember to include it in your code. Notice that tokenizer_type is only provided for tokenizers that can be customized. If there are no options, as is the case with the letter tokenizer, there's no associated #Microsoft.Azure.Search type.
 
 <a name="TokenFilters"></a>
 
 ## Token filters
 
 A token filter is used to filter out or modify the tokens generated by a tokenizer. For example, you can specify a lowercase filter that converts all characters to lowercase. You can have multiple token filters in a custom analyzer. Token filters run in the order in which they're listed.
 
-In the table below, the token filters that are implemented using Apache Lucene are linked to the Lucene API documentation.
+In the following table, the token filters that are implemented using Apache Lucene are linked to the Lucene API documentation.
 
 |**token_filter_name**|**token_filter_type** <sup>1</sup>|**Description and Options**|  
 |-|-|-|  
 |[arabic_normalization](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ar/ArabicNormalizationFilter.html)|(type applies only when options are available)  |A token filter that applies the Arabic normalizer to normalize the orthography.|  
 |[apostrophe](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/tr/ApostropheFilter.html)|(type applies only when options are available)  |Strips all characters after an apostrophe (including the apostrophe itself). |  
 |[asciifolding](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html)|AsciiFoldingTokenFilter|Converts alphabetic, numeric, and symbolic Unicode characters which aren't in the first 127 ASCII characters (the `Basic Latin` Unicode block) into their ASCII equivalents, if one exists.<br><br> **Options**<br><br> preserveOriginal (type: bool) - If true, the original token is kept. The default is false.|  
 |[cjk_bigram](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html)|CjkBigramTokenFilter|Forms bigrams of CJK terms that are generated from StandardTokenizer.<br><br> **Options**<br><br> ignoreScripts (type: string array) - Scripts to ignore. Allowed values include: `han`, `hiragana`, `katakana`, `hangul`. The default is an empty list.<br><br> outputUnigrams (type: bool) - Set to true if you always want to output both unigrams and bigrams. The default is false.|  
-|[cjk_width](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html)|(type applies only when options are available)  |Normalizes CJK width differences. Folds full width ASCII variants into the equivalent basic latin and half-width Katakana variants into the equivalent kana. |  
+|[cjk_width](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html)|(type applies only when options are available)  |Normalizes CJK width differences. Folds full width ASCII variants into the equivalent basic Latin and half-width Katakana variants into the equivalent kana. |  
 |[classic](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/standard/ClassicFilter.html)|(type applies only when options are available)  |Removes the English possessives, and dots from acronyms. |  
 |[common_grams](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html)|CommonGramTokenFilter|Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed too, with bigrams overlaid.<br><br> **Options**<br><br> commonWords (type: string array) - The set of common words. The default is an empty list. Required.<br><br> ignoreCase (type: bool) - If true, matching is case insensitive. The default is false.<br><br> queryMode (type: bool) - Generates bigrams then removes common words and single terms followed by a common word. The default is false.|  
 |[dictionary_decompounder](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.html)|DictionaryDecompounderTokenFilter|Decomposes compound words found in many Germanic languages.<br><br> **Options**<br><br> wordList (type: string array) - The list of words to match against. The default is an empty list. Required.<br><br> minWordSize (type: int) - Only words longer than this will be processed. The default is 5.<br><br> minSubwordSize (type: int) - Only subwords longer than this will be outputted. The default is 2.<br><br> maxSubwordSize (type: int) - Only subwords shorter than this will be outputted. The default is 15.<br><br> onlyLongestMatch (type: bool) - Add only the longest matching subword to output. The default is false.|  
-|[edgeNGram_v2](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html)|EdgeNGramTokenFilterV2|Generates n-grams of the given size(s) from starting from the front or the back of an input token.<br><br> **Options**<br><br> minGram (type: int) - Default: 1, maximum: 300.<br><br> maxGram (type: int) - Default: 2, maximum 300. Must be greater than minGram.<br><br> side (type: string) - Specifies which side of the input the n-gram should be generated from. Allowed values: `front`, `back` |  
+|[edgeNGram_v2](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html)|EdgeNGramTokenFilterV2|Generates n-grams of the given sizes from starting from the front or the back of an input token.<br><br> **Options**<br><br> minGram (type: int) - Default: 1, maximum: 300.<br><br> maxGram (type: int) - Default: 2, maximum 300. Must be greater than minGram.<br><br> side (type: string) - Specifies which side of the input the n-gram should be generated from. Allowed values: `front`, `back` |  
 |[elision](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html)|ElisionTokenFilter|Removes elisions. For example, `l'avion` (the plane) is converted to `avion` (plane).<br><br> **Options**<br><br> articles (type: string array) - A set of articles to remove. The default is an empty list. If there's no list of articles set, by default all French articles are removed.|  
 |[german_normalization](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html)|(type applies only when options are available)  |Normalizes German characters according to the heuristics of the [German2 snowball algorithm](https://snowballstem.org/algorithms/german2/stemmer.html) .|  
 |[hindi_normalization](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/hi/HindiNormalizationFilter.html)|(type applies only when options are available)  |Normalizes text in Hindi to remove some differences in spelling variations. |  
@@ -282,7 +282,7 @@ In the table below, the token filters that are implemented using Apache Lucene a
 |[length](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html)|LengthTokenFilter|Removes words that are too long or too short.<br><br> **Options**<br><br> min (type: int) - The minimum number. Default: 0, maximum: 300.<br><br> max (type: int) - The maximum number. Default: 300, maximum: 300.|  
 |[limit](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilter.html)|Microsoft.Azure.Search.LimitTokenFilter|Limits the number of tokens while indexing.<br><br> **Options**<br><br> maxTokenCount (type: int) - Max number of tokens to produce. The default is 1.<br><br> consumeAllTokens (type: bool) - Whether all tokens from the input must be consumed even if maxTokenCount is reached. The default is false.|  
 |[lowercase](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.html)|(type applies only when options are available)  |Normalizes token text to lower case. |  
-|[nGram_v2](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html)|NGramTokenFilterV2|Generates n-grams of the given size(s).<br><br> **Options**<br><br> minGram (type: int) - Default: 1, maximum: 300.<br><br> maxGram (type: int) - Default: 2, maximum 300. Must be greater than minGram.|  
+|[nGram_v2](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html)|NGramTokenFilterV2|Generates n-grams of the given sizes.<br><br> **Options**<br><br> minGram (type: int) - Default: 1, maximum: 300.<br><br> maxGram (type: int) - Default: 2, maximum 300. Must be greater than minGram.|  
 |[pattern_capture](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/pattern/PatternCaptureGroupTokenFilter.html)|PatternCaptureTokenFilter|Uses Java regexes to emit multiple tokens, one for each capture group in one or more patterns.<br><br> **Options**<br><br> patterns (type: string array) - A list of patterns to match against each token. Required.<br><br> preserveOriginal (type: bool) - Set to true to return the original token even if one of the patterns matches, default: true |  
 |[pattern_replace](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceFilter.html)|PatternReplaceTokenFilter|A token filter which applies a pattern to each token in the stream, replacing match occurrences with the specified replacement string.<br><br> **Options**<br><br> pattern (type: string) - Required.<br><br> replacement (type: string) - Required.|  
 |[persian_normalization](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/fa/PersianNormalizationFilter.html)|(type applies only when options are available) |Applies normalization for Persian. |  
@@ -291,20 +291,20 @@ In the table below, the token filters that are implemented using Apache Lucene a
 |[reverse](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/reverse/ReverseStringFilter.html)|(type applies only when options are available)  |Reverses the token string. |  
 |[scandinavian_normalization](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.html)|(type applies only when options are available)  |Normalizes use of the interchangeable Scandinavian characters. |  
 |[scandinavian_folding](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.html)|(type applies only when options are available)  |Folds Scandinavian characters `åÅäæÄÆ`into `a` and `öÖøØ`into `o`. It also discriminates against use of double vowels `aa`, `ae`, `ao`, `oe` and `oo`, leaving just the first one. |  
-|[shingle](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html)|ShingleTokenFilter|Creates combinations of tokens as a single token.<br><br> **Options**<br><br> maxShingleSize (type: int) - Defaults to 2.<br><br> minShingleSize (type: int) - Defaults to 2.<br><br> outputUnigrams (type: bool) - if true, the output stream contains the input tokens (unigrams) as well as shingles. The default is true.<br><br> outputUnigramsIfNoShingles (type: bool) - If true, override the behavior of outputUnigrams==false for those times when no shingles are available. The default is false.<br><br> tokenSeparator (type: string) - The string to use when joining adjacent tokens to form a shingle. The default is a single empty space ` `. <br><br> filterToken (type: string) - The string to insert for each position for which there is no token. The default is `_`.|  
+|[shingle](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html)|ShingleTokenFilter|Creates combinations of tokens as a single token.<br><br> **Options**<br><br> maxShingleSize (type: int) - Defaults to 2.<br><br> minShingleSize (type: int) - Defaults to 2.<br><br> outputUnigrams (type: bool) - if true, the output stream contains the input tokens (unigrams) as well as shingles. The default is true.<br><br> outputUnigramsIfNoShingles (type: bool) - If true, override the behavior of outputUnigrams==false for those times when no shingles are available. The default is false.<br><br> tokenSeparator (type: string) - The string to use when joining adjacent tokens to form a shingle. The default is a single empty space ` `. <br><br> filterToken (type: string) - The string to insert for each position for which there's no token. The default is `_`.|  
 |[snowball](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/snowball/SnowballFilter.html)|SnowballTokenFilter|Snowball Token Filter.<br><br> **Options**<br><br> language (type: string) - Allowed values include: `armenian`, `basque`, `catalan`, `danish`, `dutch`, `english`, `finnish`, `french`, `german`, `german2`, `hungarian`, `italian`, `kp`, `lovins`, `norwegian`, `porter`, `portuguese`, `romanian`, `russian`, `spanish`, `swedish`, `turkish`|  
 |[sorani_normalization](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ckb/SoraniNormalizationFilter.html)|SoraniNormalizationTokenFilter|Normalizes the Unicode representation of `Sorani` text.<br><br> **Options**<br><br> None.|  
 |stemmer|StemmerTokenFilter|Language-specific stemming filter.<br><br> **Options**<br><br> language (type: string) - Allowed values include: <br> -   [`arabic`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ar/ArabicStemmer.html)<br>-   [`armenian`](https://snowballstem.org/algorithms/armenian/stemmer.html)<br>-   [`basque`](https://snowballstem.org/algorithms/basque/stemmer.html)<br>-   [`brazilian`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/br/BrazilianStemmer.html)<br>-   `bulgarian`<br>-   [`catalan`](https://snowballstem.org/algorithms/catalan/stemmer.html)<br>-   [`czech`](https://portal.acm.org/citation.cfm?id=1598600)<br>-   [`danish`](https://snowballstem.org/algorithms/danish/stemmer.html)<br>-   [`dutch`](https://snowballstem.org/algorithms/dutch/stemmer.html)<br>-   [`dutchKp`](https://snowballstem.org/algorithms/kraaij_pohlmann/stemmer.html)<br>-   [`english`](https://snowballstem.org/algorithms/porter/stemmer.html)<br>-   [`lightEnglish`](https://ciir.cs.umass.edu/pubfiles/ir-35.pdf)<br>-   [`minimalEnglish`](https://www.researchgate.net/publication/220433848_How_effective_is_suffixing)<br>-   [`possessiveEnglish`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/en/EnglishPossessiveFilter.html)<br>-   [`porter2`](https://snowballstem.org/algorithms/english/stemmer.html)<br>-   [`lovins`](https://snowballstem.org/algorithms/lovins/stemmer.html)<br>-   [`finnish`](https://snowballstem.org/algorithms/finnish/stemmer.html)<br>-   `lightFinnish`<br>-   [`french`](https://snowballstem.org/algorithms/french/stemmer.html)<br>-   [`lightFrench`](https://dl.acm.org/citation.cfm?id=1141523)<br>-   [`minimalFrench`](https://dl.acm.org/citation.cfm?id=318984)<br>-   `galician`<br>-   `minimalGalician`<br>-   [`german`](https://snowballstem.org/algorithms/german/stemmer.html)<br>-   [`german2`](https://snowballstem.org/algorithms/german2/stemmer.html)<br>-   [`lightGerman`](https://dl.acm.org/citation.cfm?id=1141523)<br>-   `minimalGerman`<br>-   [`greek`](https://sais.se/mthprize/2007/ntais2007.pdf)<br>-   `hindi`<br>-   [`hungarian`](https://snowballstem.org/algorithms/hungarian/stemmer.html)<br>-   [`lightHungarian`](https://dl.acm.org/citation.cfm?id=1141523&dl=ACM&coll=DL&CFID=179095584&CFTOKEN=80067181)<br>-   [`indonesian`](https://eprints.illc.uva.nl/741/2/MoL-2003-03.text.pdf)<br>-   [`irish`](https://snowballstem.org/algorithms/irish/stemmer.html)<br>-   [`italian`](https://snowballstem.org/algorithms/italian/stemmer.html)<br>-   [`lightItalian`](https://www.ercim.eu/publication/ws-proceedings/CLEF2/savoy.pdf)<br>-   [`sorani`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ckb/SoraniStemmer.html)<br>-   [`latvian`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/lv/LatvianStemmer.html)<br>-   [`norwegian`](https://snowballstem.org/algorithms/norwegian/stemmer.html)<br>-   [`lightNorwegian`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/no/NorwegianLightStemmer.html)<br>-   [`minimalNorwegian`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/no/NorwegianMinimalStemmer.html)<br>-   [`lightNynorsk`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/no/NorwegianLightStemmer.html)<br>-   [`minimalNynorsk`](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/no/NorwegianMinimalStemmer.html)<br>-   [`portuguese`](https://snowballstem.org/algorithms/portuguese/stemmer.html)<br>-   [`lightPortuguese`](https://dl.acm.org/citation.cfm?id=1141523&dl=ACM&coll=DL&CFID=179095584&CFTOKEN=80067181)<br>-   [`minimalPortuguese`](https://web.archive.org/web/20230425141918/https://www.inf.ufrgs.br/~buriol/papers/Orengo_CLEF07.pdf)<br>-   [`portugueseRslp`](https://web.archive.org/web/20230422082818/https://www.inf.ufrgs.br/~viviane/rslp/index.htm)<br>-   [`romanian`](https://snowballstem.org/otherapps/romanian/)<br>-   [`russian`](https://snowballstem.org/algorithms/russian/stemmer.html)<br>-   [`lightRussian`](https://doc.rero.ch/lm.php?url=1000%2C43%2C4%2C20091209094227-CA%2FDolamic_Ljiljana_-_Indexing_and_Searching_Strategies_for_the_Russian_20091209.pdf)<br>-   [`spanish`](https://snowballstem.org/algorithms/spanish/stemmer.html)<br>-   [`lightSpanish`](https://www.ercim.eu/publication/ws-proceedings/CLEF2/savoy.pdf)<br>-   [`swedish`](https://snowballstem.org/algorithms/swedish/stemmer.html)<br>-   `lightSwedish`<br>-   [`turkish`](https://snowballstem.org/algorithms/turkish/stemmer.html)|  
 |[stemmer_override](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.html)|StemmerOverrideTokenFilter|Any dictionary-Stemmed terms are marked as keywords, which prevents stemming down the chain. Must be placed before any stemming filters.<br><br> **Options**<br><br> rules (type: string array) - Stemming rules in the following format `word => stem` for example `ran => run`. The default is an empty list.  Required.|  
 |[stopwords](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/StopFilter.html)|StopwordsTokenFilter|Removes stop words from a token stream. By default, the filter uses a predefined stop word list for English.<br><br> **Options**<br><br> stopwords (type: string array) - A list of stopwords. Can't be specified if a stopwordsList is specified.<br><br> stopwordsList (type: string) - A predefined list of stopwords. Can't be specified if `stopwords` is specified. Allowed values include:`arabic`, `armenian`, `basque`, `brazilian`, `bulgarian`, `catalan`, `czech`, `danish`, `dutch`, `english`, `finnish`, `french`, `galician`, `german`, `greek`, `hindi`, `hungarian`, `indonesian`, `irish`, `italian`, `latvian`, `norwegian`, `persian`, `portuguese`, `romanian`, `russian`, `sorani`, `spanish`, `swedish`, `thai`, `turkish`, default: `english`. Can't be specified if `stopwords` is specified. <br><br> ignoreCase (type: bool) - If true, all words are lower cased first. The default is false.<br><br> removeTrailing (type: bool) - If true, ignore the last search term if it's a stop word. The default is true.
-|[synonym](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/synonym/SynonymFilter.html)|SynonymTokenFilter|Matches single or multi word synonyms in a token stream.<br><br> **Options**<br><br> synonyms (type: string array) - Required. List of synonyms in one of the following two formats:<br><br> -incredible, unbelievable, fabulous => amazing - all terms on the left side of => symbol are replaced with all terms on its right side.<br><br> -incredible, unbelievable, fabulous, amazing - A comma-separated list of equivalent words. Set the expand option to change how this list is interpreted.<br><br> ignoreCase (type: bool) - Case-folds input for matching. The default is false.<br><br> expand (type: bool) - If true, all words in the list of synonyms (if => notation is not used) map to one another. <br>The following list: incredible, unbelievable, fabulous, amazing is equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable, fabulous, amazing<br><br>- If false, the following list: incredible, unbelievable, fabulous, amazing are equivalent to: incredible, unbelievable, fabulous, amazing => incredible.|  
+|[synonym](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/synonym/SynonymFilter.html)|SynonymTokenFilter|Matches single or multi word synonyms in a token stream.<br><br> **Options**<br><br> synonyms (type: string array) - Required. List of synonyms in one of the following two formats:<br><br> -incredible, unbelievable, fabulous => amazing - all terms on the left side of => symbol are replaced with all terms on its right side.<br><br> -incredible, unbelievable, fabulous, amazing - A comma-separated list of equivalent words. Set the expand option to change how this list is interpreted.<br><br> ignoreCase (type: bool) - Case-folds input for matching. The default is false.<br><br> expand (type: bool) - If true, all words in the list of synonyms (if => notation isn't used) map to one another. <br>The following list: incredible, unbelievable, fabulous, amazing is equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable, fabulous, amazing<br><br>- If false, the following list: incredible, unbelievable, fabulous, amazing are equivalent to: incredible, unbelievable, fabulous, amazing => incredible.|  
 |[trim](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/TrimFilter.html)|(type applies only when options are available)  |Trims leading and trailing whitespace from tokens. |  
 |[truncate](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html)|TruncateTokenFilter|Truncates the terms into a specific length.<br><br> **Options**<br><br> length (type: int) - Default: 300, maximum: 300. Required.|  
 |[unique](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html)|UniqueTokenFilter|Filters out tokens with same text as the previous token.<br><br> **Options**<br><br> onlyOnSamePosition (type: bool) - If set, remove duplicates only at the same position. The default is true.|  
 |[uppercase](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/core/UpperCaseFilter.html)|(type applies only when options are available)  |Normalizes token text to upper case. |  
 |[word_delimiter](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.html)|WordDelimiterTokenFilter|Splits words into subwords and performs optional transformations on subword groups.<br><br> **Options**<br><br> generateWordParts (type: bool) - Causes parts of words to be generated, for example `AzureSearch` becomes `Azure` `Search`. The default is true.<br><br> generateNumberParts (type: bool) - Causes number subwords to be generated. The default is true.<br><br> catenateWords (type: bool) - Causes maximum runs of word parts to be catenated, for example `Azure-Search` becomes `AzureSearch`. The default is false.<br><br> catenateNumbers (type: bool) - Causes maximum runs of number parts to be catenated, for example `1-2` becomes `12`. The default is false.<br><br> catenateAll (type: bool) - Causes all subword parts to be catenated, e.g `Azure-Search-1` becomes `AzureSearch1`. The default is false.<br><br> splitOnCaseChange (type: bool) - If true, splits words on caseChange, for example `AzureSearch` becomes `Azure` `Search`. The default is true.<br><br> preserveOriginal - Causes original words to be preserved and added to the subword list. The default is false.<br><br> splitOnNumerics (type: bool) - If true, splits on numbers, for example `Azure1Search` becomes `Azure` `1` `Search`. The default is true.<br><br> stemEnglishPossessive (type: bool) - Causes trailing `'s` to be removed for each subword. The default is true.<br><br> protectedWords (type: string array) - Tokens to protect from being delimited. The default is an empty list.|  
 
- <sup>1</sup> Token Filter Types are always prefixed in code with `#Microsoft.Azure.Search` such that  `ArabicNormalizationTokenFilter` would actually be specified as `#Microsoft.Azure.Search.ArabicNormalizationTokenFilter`.  We removed the prefix to reduce the width of the table, but please remember to include it in your code.  
+ <sup>1</sup> Token Filter Types are always prefixed in code with `#Microsoft.Azure.Search` such that  `ArabicNormalizationTokenFilter` would actually be specified as `#Microsoft.Azure.Search.ArabicNormalizationTokenFilter`.  We removed the prefix to reduce the width of the table, but remember to include it in your code.  
 
 ## See also
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise content and links for adding custom analyzers in Azure AI Search documentation. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the documentation concerning the process of adding custom analyzers in Azure AI Search. This modification primarily focuses on enhancing clarity and providing more comprehensive references within the text.

Key changes include:
1. **Definition Clarity**: The definition of a *custom analyzer* has been improved by specifying that it is a component of lexical analysis over plain text content, making the explanation clearer for users.
2. **Additional References**: A new link has been added to a tutorial on creating a custom analyzer for phone numbers, providing users with more practical examples.
3. **Consistency in Terminology**: Certain terms, such as "maxTokenLength" and "Standard Lucene," have been italicized for consistency and better readability.
4. **Navigation Improvements**: Adjustments have been made to ensure links within the documentation direct users to relevant sections, such as the one for creating an index schema.

Overall, these enhancements contribute to a more user-friendly experience by offering clearer guidance on utilizing custom analyzers while ensuring that the information remains accurate and accessible.

## articles/search/index-add-language-analyzers.md{#item-004ac0}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 05/23/2024
+ms.date: 01/16/2025
 ---
 
 # Add language analyzers to string fields in an Azure AI Search index
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update the modification date in language analyzers documentation. Locale: en_US"
}
```

### Explanation
The code diff represents a minor update to the documentation regarding adding language analyzers in an Azure AI Search index. The primary change involves the modification of the document's date.

The original date, identified as May 23, 2024, has been changed to January 16, 2025. This update likely reflects a rescheduling or re-evaluation of the content's relevance or publication timeline. No other changes to the content or structure of the documentation were made, indicating that the document remains intact and the focus was solely on the date adjustment to ensure accurate representation of the document's currency and relevance.

## articles/search/index-similarity-and-scoring.md{#item-75603d}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: conceptual
-ms.date: 12/10/2024
+ms.date: 01/17/2025
 ---
 
 # Relevance in keyword search (BM25 scoring)
@@ -117,7 +117,7 @@ POST https://[service name].search.windows.net/indexes/hotels/docs/search?api-ve
 As long as the same `sessionId` is used, a best-effort attempt is made to target the same replica, increasing the consistency of results your users will see. 
 
 > [!NOTE]
-> Reusing the same `sessionId` values repeatedly can interfere with the load balancing of the requests across replicas and adversely affect the performance of the search service. The value used as sessionId cannot start with a '_' character.
+> Reusing the same `sessionId` values repeatedly can interfere with the load balancing of the requests across replicas and adversely affect the performance of the search service. The value used as sessionId can't start with a '_' character.
 
 ## Relevance tuning
 
@@ -172,6 +172,8 @@ The featuresMode parameter isn't documented in the REST APIs, but you can use it
 
 By default, if you aren't using pagination, the search engine returns the top 50 highest ranking matches for full text search. You can use the `top` parameter to return a smaller or larger number of items (up to 1,000 in a single response). You can use `skip` and `next` to page results. Paging determines the number of results on each logical page and supports content navigation. For more information, see [Shape search results](search-pagination-page-layout.md).
 
+If your full text query is part of a [hybrid query](hybrid-search-how-to-query.md), you can [set `maxTextRecallSize`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) to increase or decrease the number of results from the text side of the query.
+
 Full text search is subject to a maximum limit of 1,000 matches (see [API response limits](search-limits-quotas-capacity.md#api-response-limits)). Once 1,000 matches are found, the search engine no longer looks for more.
 
 ## See also
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Adjust content and date in similarity and scoring documentation. Locale: en_US"
}
```

### Explanation
The code diff reflects a minor update to the documentation related to similarity and scoring in Azure AI Search. The changes primarily involve text adjustments and a modification of the document date.

Key updates include:

1. **Date Change**: The document date has been updated from December 10, 2024, to January 17, 2025. This likely indicates a revision in the timeline for the content's relevance or expected publication.

2. **Text Clarification**: The note about reusing the `sessionId` has had the phrase "can’t start" instead of "cannot start," thereby improving clarity and consistency in language usage.

3. **Additional Information**: A new sentence has been added to highlight the ability to set the `maxTextRecallSize` parameter in the context of hybrid queries. This addition provides users with useful information on how to control the number of results from full-text queries within hybrid search scenarios.

These modifications collectively enhance the documentation by ensuring the information is current, clarifying the consequences of session ID usage, and providing further guidance on query parameters.

## articles/search/search-analyzers.md{#item-9dccd9}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ manager: nitinme
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 05/23/2024
+ms.date: 01/16/2025
 ms.custom:
   - devx-track-csharp
   - ignite-2023
@@ -22,9 +22,9 @@ An *analyzer* is a component of the [full text search engine](search-lucene-quer
 + Lower-case any upper-case words
 + Reduce words into primitive root forms for storage efficiency and so that matches can be found regardless of tense
 
-Analysis applies to `Edm.String` fields that are marked as "searchable", which indicates full text search. 
+The output of a lexical analyzer is a sequence of [tokens](https://suif.stanford.edu/dragonbook/lecture-notes/Stanford-CS143/03-Lexical-Analysis.pdf).
 
-For fields of this configuration, analysis occurs during indexing when tokens are created, and then again during query execution when queries are parsed and the engine scans for matching tokens. A match is more likely to occur when the same analyzer is used for both indexing and queries, but you can set the analyzer for each workload independently, depending on your requirements.
+Lexical analysis applies to `Edm.String` fields that are marked as "searchable", which indicates full text search. For fields of this configuration, analysis occurs during indexing when tokens are created, and then again during query execution when queries are parsed and the engine scans for matching tokens. A match is more likely to occur when the same analyzer is used for both indexing and queries, but you can set the analyzer for each workload independently, depending on your requirements.
 
 Query types that are *not* full text search, such as filters or fuzzy search, don't go through the analysis phase on the query side. Instead, the parser sends those strings directly to the search engine, using the pattern that you provide as the basis for the match. Typically, these query forms require whole-string tokens to make pattern matching work. To ensure whole term tokens are preserved during indexing, you might need [custom analyzers](index-add-custom-analyzers.md). For more information about when and why query terms are analyzed, see [Full text search in Azure AI Search](search-lucene-query-architecture.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise content and update date in search analyzers documentation. Locale: en_US"
}
```

### Explanation
The code diff signifies a minor update to the documentation for search analyzers in Azure AI Search. The focus of the changes is on enhancing clarity and ensuring that the date is current.

Key updates include:

1. **Date Change**: The document date has been changed from May 23, 2024, to January 16, 2025. This adjustment reflects a new timeline for the content's relevance or expected publication.

2. **Text Modifications**: Several phrases have been modified for clarity:
   - The definition of the output of a lexical analyzer has been added, stating that it produces a sequence of tokens. This addition helps clarify the process and output of lexical analysis.
   - The overall text structure has been improved, ensuring that it clearly delineates the operations of analyzing `Edm.String` fields marked as "searchable."
   - The explanation about the matching likelihood when using the same analyzer for both indexing and queries has been retained but is now nested appropriately within the revised context, enhancing readability.

Overall, the changes improve the documentation's clarity and accuracy, providing users with a better understanding of how search analyzers function within the Azure AI Search framework.

## articles/search/search-api-preview.md{#item-511f5d}

<details>
<summary>Diff</summary>
````diff
@@ -50,7 +50,7 @@ Preview features are removed from this list if they're retired or transition to
 | [**Normalizers**](search-normalizers.md) | Query |  Normalizers provide simple text preprocessing: consistent casing, accent removal, and ASCII folding, without invoking the full text analysis chain.| [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
 | [**featuresMode parameter**](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true) | Relevance (scoring) | BM25 relevance score expansion to include details: per field similarity score, per field term frequency, and per field number of unique tokens matched. You can consume these data points in [custom scoring solutions](https://github.com/Azure-Samples/search-ranking-tutorial). | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true).|
 | [**vectorQueries.threshold parameter**](vector-search-how-to-query.md#vector-weighting) | Relevance (scoring)  | Exclude low-scoring search result based on a minimum score. | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
-| [**hybridSearch.maxTextRecallSize and countAndFacetMode parameters**](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode-preview) | Relevance (scoring)  |  adjust the inputs to a hybrid query by controlling the amount BM25-ranked results that flow to the hybrid ranking model.  | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
+| [**hybridSearch.maxTextRecallSize and countAndFacetMode parameters**](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) | Relevance (scoring)  |  adjust the inputs to a hybrid query by controlling the amount BM25-ranked results that flow to the hybrid ranking model.  | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
 | [**moreLikeThis**](search-more-like-this.md) | Query | Finds documents that are relevant to a specific document. This feature has been in earlier previews. | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
 
 ## Control plane preview features
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update hyperlink format in search API preview documentation. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the search API preview documentation, which primarily involves a change in how a hyperlink is referenced within the content.

Key updates include:

1. **Hyperlink Format Change**: The entry related to `hybridSearch.maxTextRecallSize and countAndFacetMode parameters` has been modified to use a direct linking format without the "preview" suffix in the hyperlink's reference. This simplifies the link structure and ensures consistency across the documentation.

2. **Contextual Clarity**: The description of the `hybridSearch.maxTextRecallSize and countAndFacetMode parameters` remains unchanged, continuing to explain the purpose of these parameters in adjusting inputs to hybrid queries effectively.

Overall, the change improves the structure of the documentation, specifically how links are presented, which aids in better user navigation and understanding of related concepts in the Azure AI Search functionality.

## articles/search/search-api-versions.md{#item-69ca3e}

<details>
<summary>Diff</summary>
````diff
@@ -14,7 +14,7 @@ ms.custom:
   - devx-track-python
   - ignite-2023
 ms.topic: conceptual
-ms.date: 06/24/2024
+ms.date: 01/16/2025
 ---
 
 # API versions in Azure AI Search
@@ -57,33 +57,31 @@ Support for the above-listed versions ended on October 15, 2020. If you have cod
 
 The following  table provides links to more recent SDK versions. 
 
-| SDK version | Status | Description |
-|-------------|--------|------------------------------|
-| [Azure.Search.Documents 11](/dotnet/api/overview/azure/search.documents-readme) | Active | New client library from the Azure .NET SDK team, initially released July 2020. See the [Change Log](https://github.com/Azure/azure-sdk-for-net/blob/Azure.Search.Documents_11.3.0/sdk/search/Azure.Search.Documents/CHANGELOG.md) for information about minor releases. |
-| [Microsoft.Azure.Search 10](https://www.nuget.org/packages/Microsoft.Azure.Search/) | Retired | Released May 2019. This is the last version of the Microsoft.Azure.Search package and it's now deprecated. It's succeeded by Azure.Search.Documents. |
-| [Microsoft.Azure.Management.Search 4.0.0](https://www.nuget.org/packages/Microsoft.Azure.Management.Search/4.0.0) | Active | Targets the Management REST api-version=2020-08-01. |
-| [Microsoft.Azure.Management.Search 3.0.0](https://www.nuget.org/packages/Microsoft.Azure.Management.Search/3.0.0) | Retired | Targets the Management REST api-version=2015-08-19.  |
+| SDK version | Status | Change log | Description |
+|-------------|--------|------------ |-----------------|
+| [Azure.Search.Documents 11](/dotnet/api/overview/azure/search.documents-readme) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/search/Azure.Search.Documents/CHANGELOG.md) | APIs for data plane operations on a service, such as read-write operations on content and objects. |
+| [Azure.ResourceManager.Search](https://www.nuget.org/packages/Microsoft.Azure.Management.Search/4.0.0) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/search/Azure.ResourceManager.Search/CHANGELOG.md) | APIs for control plane operations on the search service. |
 
 ## Azure SDK for Java
 
-| SDK version | Status | Description  |
-|-------------|--------|------------------------------|
-| [Java azure-search-documents 11](/java/api/overview/azure/search-documents-readme) | Active | Use the `azure-search-documents` client library for data plane operations. |
-| [Java Management Client 1.35.0](/java/api/overview/azure/search/management) | Active | Use the `azure-mgmt-search` client library for control plane operations. |
+| SDK version | Status | Change log | Description |
+|-------------|--------|------------|-----------------|
+| [azure-search-documents 11](/java/api/overview/azure/search-documents-readme) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/search/azure-search-documents/CHANGELOG.md) Use the `azure-search-documents` client library for data plane operations. |
+| [azure-resourcemanager-search 2](/java/api/overview/azure/resourcemanager-search-readme) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/resourcemanager/azure-resourcemanager-search/CHANGELOG.md) | Use the `azure-resourcemanager-search` client library for control plane operations. |
 
 ## Azure SDK for JavaScript
 
-| SDK version | Status | Description  |
-|-------------|--------|------------------------------|
-| [JavaScript @azure/search-documents 11.0](/javascript/api/overview/azure/search-documents-readme) | Active | Use the `@azure/search-documents` client library for data plane operations. |
-| [JavaScript @azure/arm-search](https://www.npmjs.com/package/@azure/arm-search) | Active | Use the `@azure/arm-search` client library for control plane operations. |
+| SDK version | Status | Change log | Description |
+|-------------|--------|------------|------------------|
+| [@azure/search-documents 12](/javascript/api/overview/azure/search-documents-readme) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/search/search-documents/CHANGELOG.md) | Use the `@azure/search-documents` client library for data plane operations. |
+| [@azure/arm-search 4](/javascript/api/overview/azure/arm-search-readme) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/search/arm-search/CHANGELOG.md) | Use the `@azure/arm-search` package for control plane operations. |
 
 ## Azure SDK for Python
 
-| SDK version | Status | Description  |
-|-------------|--------|------------------------------|
-| [Python azure-search-documents 11.0](/python/api/azure-search-documents) | Active | Use the `azure-search-documents` client library for data plane operations. |
-| [Python azure-mgmt-search 8.0](https://pypi.org/project/azure-mgmt-search/) | Active | Use the `azure-mgmt-search` client library for control plane operations. |
+| SDK version | Status | Change log | Description |
+|-------------|--------|------------|------------------|
+| [azure-search-documents 11](/python/api/overview/azure/search-documents-readme) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/CHANGELOG.md) | Use the `azure-search-documents` client library for data plane operations. |
+| [azure-mgmt-search 9](https://pypi.org/project/azure-mgmt-search/) | Active | [Change Log](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-mgmt-search/CHANGELOG.md) | Use the `azure-mgmt-search` client library for control plane operations. |
 
 ## All Azure SDKs
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise SDK version details and update document date in search API versions documentation. Locale: en_US"
}
```

### Explanation
The code diff represents a minor update to the documentation of search API versions in Azure AI Search. The modifications mainly focus on updating the content about SDK versions, their changelogs, and refreshing the document date.

Key updates include:

1. **Date Change**: The date of the document has been updated from June 24, 2024, to January 16, 2025, indicating a more current timeline for the content.

2. **SDK Information Revisions**: 
   - The format of the SDK version table has been enhanced to include a "Change log" column, improving the clarity and organization of the information presented. This change allows users to quickly access information about version changes and updates.
   - Specific SDK entries have been updated, including:
     - The addition of `Azure.ResourceManager.Search` for control plane operations with a corresponding changelog link.
     - The renaming of some libraries, such as updating `@azure/search-documents` from version 11.0 to 12.0, reflecting the latest available version in the documentation.
     - Modification of descriptive texts to provide a clearer understanding of the functionalities each SDK version serves. 

3. **General Clarity**: The overall structure of the document has been refined, making it more user-friendly and easier to navigate when locating SDK version details.

These changes collectively enhance the accuracy and usability of the documentation, making it more informative and up-to-date for developers utilizing Azure's search services.

## articles/search/search-faceted-navigation.md{#item-f29d1e}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 10/31/2024
+ms.date: 01/16/2025
 ---
 
 # Add faceted navigation to a search app
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update document date in faceted navigation documentation. Locale: en_US"
}
```

### Explanation
The code diff reflects a minor update to the documentation for faceted navigation in Azure AI Search. The primary modification involves the update of the document's date.

Key updates include:

1. **Date Revision**: The date in the metadata section of the document has been changed from October 31, 2024, to January 16, 2025. This update may indicate a refresh of the content or a verification of the document's relevance as of the new date.

Overall, this change ensures that readers are aware of the most recent update to the document, maintaining the accuracy and timeliness of the information related to implementing faceted navigation in search applications. There are no substantial content changes within the body of the text, but the date update is important for users referencing the documentation for the latest guidance.

## articles/search/search-faq-frequently-asked-questions.yml{#item-eab2ba}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ metadata:
   ms.author: heidist
   ms.service: azure-ai-search
   ms.topic: faq
-  ms.date: 05/28/2024
+  ms.date: 01/16/2025
 title: Azure AI Search Frequently Asked Questions
 summary:  Find answers to commonly asked questions about Azure AI Search.
 
@@ -36,7 +36,7 @@ sections:
         answer: |
           For vectors, the embedding models you use determines the linguistic experience. 
           
-          For nonvector strings and numbers, the default analyzer used for tokenization is standard Lucene and it is language agnostic. Otherwise, language support is expressed through [language analyzers](index-add-language-analyzers.md#supported-language-analyzers) that apply linguistic rules to inbound (indexing) and outbound (queries) content. Some features, such as [speller](speller-how-to-add.md#supported-languages), are limited to a subset of languages.
+          For nonvector strings and numbers, the default analyzer used for tokenization is standard Lucene and it's language agnostic. Otherwise, language support is expressed through [language analyzers](index-add-language-analyzers.md#supported-language-analyzers) that apply linguistic rules to inbound (indexing) and outbound (queries) content. Some features, such as [speller](speller-how-to-add.md#supported-languages) and [query rewrite](semantic-how-to-query-rewrite.md), are limited to a subset of languages.
 
       - question: |
           How do I integrate search into my solution?
@@ -101,12 +101,12 @@ sections:
       - question: |
           Does Azure AI Search support vector search?
         answer: |
-          Azure AI Search supports vector indexing and retrieval. It can vectorize query strings and content if you use the preview and beta libraries. 
+          Azure AI Search supports vector indexing and retrieval. It can chunk and vectorize query strings and content if you use [integrated vectorization](vector-search-integrated-vectorization.md) and take a dependency on indexers and skillsets. 
 
       - question: |
           How does vector search work in Azure AI Search?
         answer: |
-          With standalone vector search, you first use an embedding model to transform content into a vector representation within an embedding space. You can then provide these vectors in a document payload to the search index for indexing. To serve search requests, you use the same deep neural network (DNN) from indexing to transform the search query into a vector representation, and vector search finds the most similar vectors and return the corresponding documents.
+          With standalone vector search, you first use an embedding model to transform content into a vector representation within an embedding space. You can then provide these vectors in a document payload to the search index for indexing. To serve search requests, you use the same embedding model to transform the search query into a vector representation, and vector search finds the most similar vectors and return the corresponding documents.
           
           In Azure AI Search, you can index vector data as fields in documents alongside textual and other types of content. There are [multiple data types](/rest/api/searchservice/supported-data-types#edm-data-types-for-vector-fields) for vector fields. 
           
@@ -130,7 +130,7 @@ sections:
       - question: |
           Why do I see different vector index size limits between my new search services and existing search services?
         answer: |
-          We're rolling out improved vector index size limits worldwide for new search services, but we're still building out infrastructure capacity in certain regions. New search services created in supported regions will see increased vector index size limits. Unfortunately, we can't migrate existing services to the new limits.
+          Azure AI Search rolled out improved vector index size limits worldwide for new search services, but [some regions experience capacity constraints](search-region-support.md), and some regions don't have the required infrastructure. New search services created in supported regions should see increased vector index size limits. Unfortunately, we can't migrate existing services to the new limits. Also, only vector indexes that use the Hierarchical Navigable Small World (HNSW) algorithm report on vector index size in the Azure portal. If your index uses exhaustive KNN, vector index size is reported as zero, even though the index contains vectors. 
 
       - question: |
           How do I enable vector search on a search index?
@@ -141,7 +141,7 @@ sections:
           
           * Add a "vectorSearch" section to the index schema specifying the configuration used by vector search fields, including the parameters of the Approximate Nearest Neighbor algorithm used, like HNSW.
           
-          * Use the latest stable version[**2024-07-01**](/rest/api/searchservice), or an Azure SDK to create or update the index, load documents, and issue queries.
+          * Use the latest stable version[**2024-07-01**](/rest/api/searchservice), or an Azure SDK to create or update the index, load documents, and issue queries. For more information, see [Create a vector index](vector-search-how-to-create-index.md).
 
   - name: Queries
     questions:
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and refine answers in the FAQs for Azure AI Search. Locale: en_US"
}
```

### Explanation
The code diff shows a minor update to the frequently asked questions (FAQ) document related to Azure AI Search. The modifications include a date change and refinements to the answers provided within the FAQ section to enhance clarity and detail.

Key updates include:

1. **Date Change**: The document date has been updated from May 28, 2024, to January 16, 2025, indicating a refresh in the content's relevance and timing.

2. **Clarifications in Answers**:
   - Various FAQs received clarification and enhancements in their answers. For example:
     - The explanation regarding nonvector strings and numbers was nuanced by specifying that the default analyzer is "language agnostic."
     - Additional detail was added on vector indexing and retrieval, specifically mentioning "integrated vectorization" and dependency on indexers and skillsets, which helps users understand the requirement more clearly.
     - The description of how standalone vector search works now consistently references the "embedding model" instead of a deep neural network (DNN), bringing clarity to the terminology.
     - A new note was added concerning the infrastructure capacity constraints in certain regions concerning vector index size limits, improving user awareness of potential limitations.

These enhancements contribute to a more informative and cohesive document for users seeking answers about Azure AI Search functionalities, helping them make better-informed decisions regarding implementation.

## articles/search/search-howto-concurrency.md{#item-863358}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 04/23/2024
+ms.date: 01/16/2025
 ms.custom:
   - devx-track-csharp
   - ignite-2023
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update document date in concurrency guidelines. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the document related to handling concurrency in Azure AI Search. The primary change is the update of the document's date.

Key updates include:

1. **Date Modification**: The metadata of the document has been changed from April 23, 2024, to January 16, 2025. This adjustment likely reflects a refresh in the content's relevance and timing, ensuring that users are aware of the most current version of the guidelines.

This simple yet significant change helps maintain the accuracy and credibility of the documentation, signaling to users that the content should be considered for their current implementation practices regarding concurrency in Azure AI Search. There are no changes to the content of the document itself in this commit.

## articles/search/search-howto-index-json-blobs.md{#item-b8230c}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 06/25/2024
+ms.date: 01/16/2025
 ---
 
 # Index JSON blobs and files in Azure AI Search
@@ -39,7 +39,6 @@ Within the indexer definition, you can optionally set [field mappings](search-in
 > [!NOTE]
 > When a JSON parsing mode is used, Azure AI Search assumes that all blobs use the same parser (either for **`json`**, **`jsonArray`** or **`jsonLines`**). If you have a mix of different file types in the same data source, consider using [file extension filters](search-blob-storage-integration.md#controlling-which-blobs-are-indexed) to control which files are imported.
 
-
 The following sections describe each mode in more detail. If you're unfamiliar with indexer clients and concepts, see [Create a search indexer](search-howto-create-indexers.md). You should also be familiar with the details of [basic blob indexer configuration](search-howto-indexing-azure-blob-storage.md), which isn't repeated here.
 
 <a name="parsing-single-blobs"></a>
@@ -76,8 +75,7 @@ api-key: [admin key]
 ```
 
 > [!NOTE]
-> As with all indexers, if fields do not clearly match, you should expect to explicitly specify individual [field mappings](search-indexer-field-mappings.md) unless you are using the implicit fields mappings available for blob content and metadata, as described in [basic blob indexer configuration](search-howto-indexing-azure-blob-storage.md).
-
+> As with all indexers, if fields don't clearly match, you should expect to explicitly specify individual [field mappings](search-indexer-field-mappings.md) unless you're using the implicit fields mappings available for blob content and metadata, as described in [basic blob indexer configuration](search-howto-indexing-azure-blob-storage.md).
 
 ### json example (single hotel JSON files)
 
@@ -208,7 +206,7 @@ You can also refer to individual array elements by using a zero-based index. For
 ```
 
 > [!NOTE]
-> If "sourceFieldName" refers to a property that doesn't exist in the JSON blob, that mapping is skipped without an error. This behavior allows indexing to continue for JSON blobs that have a different schema (which is a common use case). Because there is no validation check, check the mappings carefully for typos so that you aren't losing documents for the wrong reason.
+> If "sourceFieldName" refers to a property that doesn't exist in the JSON blob, that mapping is skipped without an error. This behavior allows indexing to continue for JSON blobs that have a different schema (which is a common use case). Because there's no validation check, check the mappings carefully for typos so that you aren't losing documents for the wrong reason.
 >
 
 ## Next steps
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update document date and refine language in JSON blobs indexing guide. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the document providing guidance on indexing JSON blobs in Azure AI Search. The changes primarily consist of an update to the document's date and refinements in the language used throughout the document.

Key updates include:

1. **Date Change**: The document's date has been changed from June 25, 2024, to January 16, 2025. This update ensures that users are accessing the most current version of the content.

2. **Text Refinements**: The modifications include minor language tweaks that enhance clarity, such as:
   - Substituting "don’t" with "don't" and "you're" with "you’re" for consistency in style.
   - Removal of extraneous line breaks within notes, streamlining the presentation and readability of the document. 
   - The adjustment in phrasing helps to maintain a professional tone and improves the overall flow of the guidelines.

These updates collectively contribute to a more polished and relevant document, improving user comprehension of how to effectively index JSON blobs in Azure AI Search.

## articles/search/search-howto-managed-identities-storage.md{#item-8209c4}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ manager: nitinme
 
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 06/03/2024
+ms.date: 01/16/2025
 ms.custom:
   - subject-rbac-steps
   - ignite-2023
@@ -43,9 +43,9 @@ You can use a system-assigned managed identity or a user-assigned managed identi
    | ADLS Gen2 indexing using an indexer | Add **Storage Blob Data Reader** |
    | Table indexing using an indexer | Add **Reader and Data Access** |
    | File indexing using an indexer | Add **Reader and Data Access** |
-   | Write to a knowledge store | Add **Storage Blob DataContributor** for object and file projections, and **Reader and Data Access** for table projections. |
-   | Write to an enrichment cache | Add **Storage Blob Data Contributor**  |
-   | Save debug session state | Add **Storage Blob Data Contributor**  |
+   | Write to a [knowledge store](knowledge-store-concept-intro.md) | Add **Storage Blob Data Contributor** for object and file projections, and **Reader and Data Access** for table projections. |
+   | Write to an [enrichment cache](cognitive-search-incremental-indexing-conceptual.md) | Add **Storage Blob Data Contributor**  |
+   | Save [debug session state](cognitive-search-debug-session.md) | Add **Storage Blob Data Contributor**  |
 
 1. Select **Next**.
 
@@ -59,7 +59,7 @@ You can use a system-assigned managed identity or a user-assigned managed identi
 
 Once you have a role assignment, you can set up a connection to Azure Storage that operates under that role.
 
-Indexers use a data source object for connections to an external data source. This section explains how to specify a system-assigned managed identity or a user-assigned managed identity on a data source connection string. You can find more [connection string examples](search-howto-managed-identities-data-sources.md#connection-string-examples) in the managed identity article.
+[Indexers](search-indexer-overview.md) use a data source object for connections to an external data source. This section explains how to specify a system-assigned managed identity or a user-assigned managed identity on a data source connection string. You can find more [connection string examples](search-howto-managed-identities-data-sources.md#connection-string-examples) in the managed identity article.
 
 > [!TIP]
 > You can create a data source connection to Azure Storage in the Azure portal, specifying either a system or user-assigned managed identity, and then view the JSON definition to see how the connection string is formulated.
@@ -70,7 +70,7 @@ You must have a [system-assigned managed identity already configured](search-how
 
 For connections made using a system-assigned managed identity, the only change to the [data source definition](/rest/api/searchservice/data-sources/create) is the format of the `credentials` property. 
 
-Provide a `ResourceId` that has no account key or password. The `ResourceId` must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name.
+Provide a connection string that contains a `ResourceId`, with no account key or password. The `ResourceId` must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name.
 
 ```http
 POST https://[service name].search.windows.net/datasources?api-version=2024-07-01
@@ -91,11 +91,11 @@ POST https://[service name].search.windows.net/datasources?api-version=2024-07-0
 
 You must have a [user-assigned managed identity already configured](search-howto-managed-identities-data-sources.md) and associated with your search service, and the identity must have a role-assignment on Azure Storage. 
 
-Connections made through user-assigned managed identities use the same credentials as a system-assigned managed identity, plus an extra identity property that contains the collection of user-assigned managed identities. Only one user-assigned managed identity should be provided when creating the data source. Set `userAssignedIdentity` to the user-assigned managed identity.
+Connections made through user-assigned managed identities use the same credentials as a system-assigned managed identity, plus an extra identity property that contains the collection of user-assigned managed identities. Only one user-assigned managed identity should be provided when creating the data source. 
 
-Provide a `ResourceId` that has no account key or password. The `ResourceId` must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name.
+Provide a connection string that contains a `ResourceId`, with no account key or password. The `ResourceId` must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name.
 
-Provide an `identity` using the syntax shown in the following example.
+Provide an `identity` using the syntax shown in the following example. Set `userAssignedIdentity` to the user-assigned managed identity.
 
 ```http
 POST https://[service name].search.windows.net/datasources?api-version=2024-07-01
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update document date and enhance clarity in managed identities storage guide. Locale: en_US"
}
```

### Explanation
The code diff reveals a minor update to the document covering how to use managed identities with Azure Storage within the Azure AI Search context. The changes consist of a date update and enhancements in the language used to improve clarity.

Key updates include:

1. **Date Change**: The document date has been updated from June 3, 2024, to January 16, 2025, ensuring users access the latest information.

2. **Language Enhancements**: Several lines of text have been refined for clarity, including:
   - Adding links to specific concepts (e.g., [knowledge store], [enrichment cache]) to provide direct access to related content.
   - Modifying phrases for smoother reading and reducing the repetition of words for better comprehension (e.g., clarifying instructions on specifying `ResourceId`).

3. **Structural Adjustments**: The overall structure has been slightly altered by removing some line breaks and improving the flow of instructions concerning how to set up connections to Azure Storage.

These updates contribute to a more user-friendly document that facilitates better understanding of the procedures for leveraging managed identities in Azure Storage, thereby enhancing the user experience for developers and administrators.

## articles/search/search-indexer-troubleshooting.md{#item-087365}

<details>
<summary>Diff</summary>
````diff
@@ -25,6 +25,16 @@ Occasionally, indexers run into problems that don't produce errors or that occur
 
 For data sources under Azure network security, indexers are limited in how they make the connection. Currently, indexers can access restricted data sources [behind an IP firewall](search-indexer-howto-access-ip-restricted.md) or on a virtual network through a [private endpoint](search-indexer-howto-access-private.md) using a shared private link.
 
+### Error connecting to Azure AI services on a private connection
+
+If you get an error code 403 with the following message, you might have a problem with how the resource endpoint is specified in a skillset:
+
+* `"A Virtual Network is configured for this resource. Please use the correct endpoint for making requests. Check https://aka.ms/cogsvc-vnet for more details."`
+
+This error occurs if you have [configured a shared private link](search-indexer-howto-access-private.md) for connections to Azure AI multi-service, and the endpoint is missing a custom subdomain. A custom subdomain is the first part of the endpoint (for example, `http://my-custom-subdomain.cognitiveservices.azure.com`). A custom domain might be missing if you created the resource in Azure AI Foundry.
+
+If the Azure AI multi-service account isn't in the same region as Azure AI Search, [use a keyless connection](cognitive-search-attach-cognitive-services.md) when attaching a billable Azure AI resource.
+
 ### Firewall rules
 
 Azure Storage, Azure Cosmos DB and Azure SQL provide a configurable firewall. There's no specific error message when the firewall blocks the request. Typically, firewall errors are generic. Some common errors include:
@@ -167,7 +177,7 @@ To update the policy and allow indexer access to the document library:
 
 If you're indexing content from Azure Blob Storage, and the container includes blobs of an [unsupported content type](search-howto-indexing-azure-blob-storage.md#SupportedFormats), the indexer skips that document. In other cases, there might be problems with individual documents. 
 
-In this situation, you can [set configuration options](search-howto-indexing-azure-blob-storage.md#DealingWithErrors) to allow indexer processing to continue in the event of problems with individual documents.
+In this situation, you can [set configuration options](search-howto-indexing-azure-blob-storage.md#DealingWithErrors) to allow indexer processing to continue if there are problems with individual documents.
 
 ```http
 PUT https://[service name].search.windows.net/indexers/[indexer name]?api-version=2024-07-01
@@ -190,7 +200,7 @@ Indexers extract documents or rows from an external [data source](/rest/api/sear
 * Change tracking values are erroneous or prerequisites are missing. If your high watermark value is a date set to a future time, then any documents that have an earlier date are skipped by the indexer. You can determine your indexer's change tracking state using the 'initialTrackingState' and 'finalTrackingState' fields in the [indexer status](/rest/api/searchservice/indexers/get-status). Indexers for Azure SQL and MySQL must have an index on the high water mark column of the source table, or queries used by the indexer might time out. 
 
 > [!TIP]
-> If documents are missing, check the [query](/rest/api/searchservice/documents/search-post) you are using to make sure it isn't excluding the document in question. To query for a specific document, use the [Lookup Document REST API](/rest/api/searchservice/documents/get?).
+> If documents are missing, check the [query](/rest/api/searchservice/documents/search-post) you're using to make sure it isn't excluding the document in question. To query for a specific document, use the [Lookup Document REST API](/rest/api/searchservice/documents/get?).
 
 ## Missing content from Blob Storage
 
@@ -268,9 +278,9 @@ In practice, this scenario only happens when on-demand indexers are manually inv
 
 ## Parallel indexing
 
-When multiple indexers are operating simultaneously, it's typical for some to enter a queue, waiting for available resources to begin execution. The number of indexers that can run concurrently depends on several factors. If the indexers are not linked with [skillsets](cognitive-search-working-with-skillsets.md), the capacity to run in parallel relies on the number of [replicas and partitions](search-capacity-planning.md#concepts-search-units-replicas-partitions) set up in the AI Search service.
+When multiple indexers are operating simultaneously, it's typical for some to enter a queue, waiting for available resources to begin execution. The number of indexers that can run concurrently depends on several factors. If the indexers aren't linked with [skillsets](cognitive-search-working-with-skillsets.md), the capacity to run in parallel relies on the number of [replicas and partitions](search-capacity-planning.md#concepts-search-units-replicas-partitions) set up in the AI Search service.
 
-On the other hand, if an indexer is associated with a skillset, it operates within the AI Search's internal clusters. The ability to run concurrently in this case is determined by the complexity of the skillset and whether other skillsets are running simultaneously. Built-in indexers are designed to reliably extract data from the source, so no data is missed if running on a schedule. However, it is expected that the indexer processes of parallelization and scaling out may require some time. 
+On the other hand, if an indexer is associated with a skillset, it operates within the AI Search's internal clusters. The ability to run concurrently in this case is determined by the complexity of the skillset and whether other skillsets are running simultaneously. Built-in indexers are designed to reliably extract data from the source, so no data is missed if running on a schedule. However, it's expected that the indexer processes of parallelization and scaling out require some time to complete. 
 
 ## Indexing documents with sensitivity labels
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhance troubleshooting documentation for indexers and add specific error handling guidelines. Locale: en_US"
}
```

### Explanation
The code diff reflects a minor update to the troubleshooting document for indexers within Azure AI Search. This update introduces new information related to error handling, particularly when connecting to Azure AI services, and makes general clarifications to existing content.

Key updates include:

1. **New Section on Connection Errors**: 
   - A new section has been added addressing specific errors that occur when connecting to Azure AI services through private connections. This includes an explanation of the error code 403 and guidance on the proper configuration for resource endpoints, particularly highlighting the need for a custom subdomain when a shared private link is used.

2. **Clarifications and Edits**: 
   - Existing content has been refined for clarity, including consistent use of language such as changing "if there are" to "if there are problems" to streamline phrasing and improve readability.
   - Minor rephrasing throughout the document has been done to ensure clarity, like changing "don’t" to "aren’t" for consistency in tone.

3. **General Improvements**: 
   - The use of clearer instructions aims to aid users when troubleshooting issues with indexers, particularly in high-complexity scenarios or when managing multiple indexers.

These modifications contribute to a more comprehensive and user-friendly troubleshooting guide, ultimately enhancing user experience and effectiveness when dealing with indexing issues in Azure AI Search.

## articles/search/search-pagination-page-layout.md{#item-115902}

<details>
<summary>Diff</summary>
````diff
@@ -90,7 +90,7 @@ The default page size is 50, while the maximum page size is 1,000. If you specif
 
 The top matches are determined by search score, assuming the query is full text search or semantic. Otherwise, the top matches are an arbitrary order for exact match queries (where uniform `@search.score=1.0` indicates arbitrary ranking).
 
-Set `top` to override the default of 50. In newer preview APIs, if you're using a hybrid query, you can [specify maxTextRecallSize](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode-preview) to return up to 10,000 documents.
+Set `top` to override the default of 50. In newer preview APIs, if you're using a hybrid query, you can [specify maxTextRecallSize](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) to return up to 10,000 documents.
 
 To control the paging of all documents returned in a result set, use `top` and `skip` together. This query returns the first set of 15 matching documents plus a count of total matches.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Correct link to documentation for maxTextRecallSize in pagination layout guide. Locale: en_US"
}
```

### Explanation
The code diff represents a minor update to the pagination and page layout guide in Azure AI Search documentation. The primary change involves the correction of a hyperlink to maintain accuracy and usability for users.

Key updates include:

1. **Correction of Link**: 
   - The hyperlink for `maxTextRecallSize` has been updated to reflect the correct path in the documentation. The previous version incorrectly referenced an older preview API, and this change ensures that users have access to the latest and most relevant information regarding how to utilize `maxTextRecallSize` and `countAndFacetMode`.

2. **Consistency in Instructions**:
   - The instruction about setting the `top` parameter remains intact, reaffirming its purpose of overriding the default page size of 50, while also stating the new capability in preview APIs to return a larger number of documents (up to 10,000) when using hybrid queries.

These modifications provide users with clearer and more accurate directions when working with pagination in Azure AI Search, contributing to an improved experience and ensuring that documentation remains up-to-date.

## articles/search/search-performance-analysis.md{#item-5032b3}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: mattgotteiner
 ms.author: magottei
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 06/06/2024
+ms.date: 01/16/2025
 ---
 
 # Analyze performance in Azure AI Search
@@ -17,7 +17,7 @@ This article describes the tools, behaviors, and approaches for analyzing query
 
 In any large implementation, it's critical to do a performance benchmarking test of your Azure AI Search service before you roll it into production. You should test both the search query load that you expect, but also the expected data ingestion workloads (if possible, run both workloads simultaneously). Having benchmark numbers helps to validate the proper [search tier](search-sku-tier.md), [service configuration](search-capacity-planning.md), and expected [query latency](search-performance-analysis.md#average-query-latency).
 
-To develop benchmarks, we recommend the [azure-search-performance-testing (GitHub)](https://github.com/Azure-Samples/azure-search-performance-testing) tool.
+<!-- To develop benchmarks, we recommend the [azure-search-performance-testing (GitHub)](https://github.com/Azure-Samples/azure-search-performance-testing) tool. -->
 
 To isolate the effects of a distributed service architecture, try testing on service configurations of one replica and one partition.
 
@@ -59,8 +59,8 @@ AzureDiagnostics
 Examining throttling over a specific time period can help you identify the times where throttling might occur more frequently. In the below example, a time series chart is used to show the number of throttled queries that occurred over a specified time frame. In this case, the throttled queries correlated with the times in with the performance benchmarking was performed.
 
 ```kusto
-let ['_startTime']=datetime('2021-02-25T20:45:07Z');
-let ['_endTime']=datetime('2021-03-03T20:45:07Z');
+let ['_startTime']=datetime('2024-02-25T20:45:07Z');
+let ['_endTime']=datetime('2024-03-03T20:45:07Z');
 let intervalsize = 1m; 
 AzureDiagnostics 
 | where TimeGenerated > ago(7d)
@@ -122,8 +122,8 @@ In the below query, an interval size of 1 minute is used to show the average lat
 
 ```kusto
 let intervalsize = 1m; 
-let _startTime = datetime('2021-02-23 17:40');
-let _endTime = datetime('2021-02-23 18:00');
+let _startTime = datetime('2024-02-23 17:40');
+let _endTime = datetime('2024-02-23 18:00');
 AzureDiagnostics
 | where TimeGenerated between(['_startTime']..['_endTime']) // Time range filtering
 | summarize AverageQueryLatency = avgif(DurationMs, OperationName in ("Query.Search", "Query.Suggest", "Query.Lookup", "Query.Autocomplete"))
@@ -139,8 +139,8 @@ The following query looks at the average number of queries per minute to ensure
 
 ```kusto
 let intervalsize = 1m; 
-let _startTime = datetime('2021-02-23 17:40');
-let _endTime = datetime('2021-02-23 18:00');
+let _startTime = datetime('2024-02-23 17:40');
+let _endTime = datetime('2024-02-23 18:00');
 AzureDiagnostics
 | where TimeGenerated between(['_startTime'] .. ['_endTime']) // Time range filtering
 | summarize QueriesPerMinute=bin(countif(OperationName in ("Query.Search", "Query.Suggest", "Query.Lookup", "Query.Autocomplete"))/(intervalsize/1m), 0.01)
@@ -158,8 +158,8 @@ From this insight, we can see that it took about 3 minutes for the search servic
 
 ```kusto
 let intervalsize = 1m; 
-let _startTime = datetime('2021-02-23 17:40');
-let _endTime = datetime('2021-02-23 18:00');
+let _startTime = datetime('2024-02-23 17:40');
+let _endTime = datetime('2024-02-23 18:00');
 AzureDiagnostics
 | where TimeGenerated between(['_startTime'] .. ['_endTime']) // Time range filtering
 | summarize IndexingOperationsPerSecond=bin(countif(OperationName == "Indexing.Index")/ (intervalsize/1m), 0.01)
@@ -171,7 +171,7 @@ AzureDiagnostics
 
 ## Background service processing
 
-It isn't unusual to see periodic spikes in query or indexing latency. Spikes might occur in response to indexing or high query rates, but could also occur during merge operations. Search indexes are stored in chunks - or shards. Periodically, the system merges smaller shards into large shards, which can help optimize service performance. This merge process also cleans up documents that have previously been marked for deletion from the index, resulting in the recovery of storage space. 
+It's common to see occasional spikes in query or indexing latency. Spikes might occur in response to indexing or high query rates, but could also occur during merge operations. Search indexes are stored in chunks - or shards. Periodically, the system merges smaller shards into large shards, which can help optimize service performance. This merge process also cleans up documents that have previously been marked for deletion from the index, resulting in the recovery of storage space. 
 
 Merging shards is fast, but also resource intensive and thus has the potential to degrade service performance. If you notice short bursts of query latency, and those bursts coincide with recent changes to indexed content, you can assume the latency is due to shard merge operations. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date references and improve language clarity in performance analysis document. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the performance analysis documentation for Azure AI Search. This update involves several date adjustments and refinements in the language used within the document.

Key points of the update include:

1. **Date Updates**:
   - The document's date metadata has been changed from June 6, 2024, to January 16, 2025, signifying the most recent update or revision of the document.
   - Several Kusto query date references within the examples have been updated from February 2021 to February 2024, ensuring the examples reflect a more current timeframe for users.

2. **Language Refinements**:
   - Certain phrases have been enhanced for clarity and consistency. For example, the line “It isn’t unusual to see periodic spikes” has been changed to “It's common to see occasional spikes,” improving the tone and readability.
   - Adjustments to the text help streamline instructions and explanations, ensuring users better understand the performance analysis processes and the context in which they operate.

These updates serve to enhance the accuracy and clarity of the performance analysis documentation, ensuring users have the most relevant information and can effectively analyze search performance metrics in Azure AI Search.

## articles/search/search-security-api-keys.md{#item-d8c908}

<details>
<summary>Diff</summary>
````diff
@@ -2,24 +2,23 @@
 title: Connect using API keys
 titleSuffix: Azure AI Search
 description: Learn how to use an admin or query API key for inbound access to an Azure AI Search service endpoint.
-
 manager: nitinme
 author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 10/30/2024
+ms.date: 1/16/2025
+#customer intent: I want to learn how to connect to Azure AI Search using API keys so that I can authenticate inbound requests to my search service.
 ---
 
 # Connect to Azure AI Search using keys
 
-Azure AI Search offers key-based authentication for connections to your search service. An API key is a unique string composed of 52 randomly generated numbers and letters. In your source code, you can specify it as an [environment variable](/azure/ai-services/cognitive-services-environment-variables) or as an app setting in your project, and then reference the variable on the request. A request made to a search service endpoint is accepted if both the request and the API key are valid.
-
-Key-based authentication is the default. 
+Azure AI Search supports both keyless and key-based authentication for connections to your search service. An API key is a unique string composed of 52 randomly generated numbers and letters. In your source code, you can specify it as an [environment variable](/azure/ai-services/cognitive-services-environment-variables) or as an app setting in your project, and then reference the variable on the request. A request made to a search service endpoint is accepted if both the request and the API key are valid.
 
-You can replace it with [role-based access](search-security-enable-roles.md), which eliminates the need for hardcoded keys in your codebase.
+> [!IMPORTANT]
+> When you create a search service, key-based authentication is the default, but it's not the most secure option. We recommend that you replace it with [role-based access](search-security-enable-roles.md).
 
 ## Types of API keys
 
@@ -36,11 +35,11 @@ Visually, there's no distinction between an admin key or query key. Both keys ar
 
 ## Use API keys on connections
 
-API keys are used for data plane (content) requests, such as creating or accessing an index or, any other request that's represented in the [Search REST APIs](/rest/api/searchservice/). Upon service creation, an API key is the only authentication mechanism for data plane operations, but you can replace or supplement key authentication with [Azure roles](search-security-rbac.md) if you can't use hard-coded keys in your code.
-
-Admin keys are used for creating, modifying, or deleting objects. Admin keys are also used to GET object definitions and system information.
+API keys are used for data plane (content) requests, such as creating or accessing an index or, any other request that's represented in the [Search REST APIs](/rest/api/searchservice/). 
 
-Query keys are typically distributed to client applications that issue queries.
+You can use either an API key or [Azure roles](search-security-rbac.md) for management plane (service) requests. When you use an API key:
+- Admin keys are used for creating, modifying, or deleting objects. Admin keys are also used to GET object definitions and system information.
+- Query keys are typically distributed to client applications that issue queries.
 
 ### [**REST API**](#tab/rest-use)
 
@@ -241,11 +240,13 @@ It's not possible to use [customer-managed key encryption](search-security-manag
 
 ## Best practices
 
++ For production workloads, switch to [Microsoft Entra ID and role-based access](keyless-connections.md). Or, if you want to continue using API keys, be sure to always monitor [who has access to your API keys](#secure-api-keys) and [regenerate API keys](#regenerate-admin-keys) on a regular cadence.
+
 + Only use API keys if data disclosure isn't a risk (for example, when using sample data) and if you're operating behind a firewall. Exposure of API keys is a risk to both data and to unauthorized use of your search service. 
 
-+ Always check code, samples, and training material before publishing to make sure you didn't leave valid API keys behind.
++ If you use an API key, store it securely somewhere else, such as in [Azure Key Vault](/azure/key-vault/general/overview). Don't include the API key directly in your code, and never post it publicly.
 
-+ For production workloads, switch to [Microsoft Entra ID and role-based access](keyless-connections.md). Or, if you want to continue using API keys, be sure to always monitor [who has access to your API keys](#secure-api-keys) and [regenerate API keys](#regenerate-admin-keys) on a regular cadence.
++ Always check code, samples, and training material before publishing to make sure you don't inadvertently expose an API key.
 
 ## See also
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to API key security practices and connection methods in Azure AI Search documentation. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the Azure AI Search documentation, specifically concerning the use of API keys for authentication and best security practices. This update enhances both the clarity and the security recommendations associated with API key usage.

Key changes include:

1. **Expanded Authentication Options**:
   - The documentation now specifies that Azure AI Search supports both keyless and key-based authentication, broadening the understanding of available security methods. This emphasizes security while providing clear instructions for implementation.

2. **Security Recommendations**:
   - The update includes a highlighted note advising against relying solely on key-based authentication due to its lesser security compared to role-based access. This important caution stresses the need for users to consider safer alternatives.
   - Additional best practices are provided, such as avoiding hardcoded API keys in code, storing them securely (e.g., using Azure Key Vault), and monitoring access to API keys.

3. **Organized Information**:
   - The section on types of API keys has been restructured for clarity, detailing the functions of both admin and query keys. It also clarifies which types of requests can use either API keys or Azure roles, improving understanding for the reader.

4. **Emphasis on Caution**:
   - Increased emphasis is placed on the precautions necessary when using API keys, highlighting the risks associated with their disclosure and encouraging users to routinely regenerate keys and review who has access.

These updates collectively improve the documentation's usability and reinforce security, ensuring users are better informed about best practices in connection methods for Azure AI Search services.

## articles/search/search-security-enable-roles.md{#item-4985c4}

<details>
<summary>Diff</summary>
````diff
@@ -2,19 +2,21 @@
 title: Enable role-based access control
 titleSuffix: Azure AI Search
 description: Enable or disable role-based access control for token authentication using Microsoft Entra ID on Azure AI Search.
-
 manager: nitinme
 author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 10/30/2024
-
+ms.date: 1/16/2025
+#customer intent: As a developer, I want to enable role-based access control for token authentication using Microsoft Entra ID on Azure AI Search so that I can secure my search service.
 ---
 
 # Enable or disable role-based access control in Azure AI Search
 
-Azure AI Search uses [key-based authentication](search-security-api-keys.md) by default, but it fully supports Microsoft Entra ID authentication and authorization for all control plane and data plane operations through Azure role-based access control (RBAC).
+Azure AI Search supports both keyless and [key-based authentication](search-security-api-keys.md) for for all control plane and data plane operations. You can use Microsoft Entra ID authentication and authorization for all control plane and data plane operations through Azure role-based access control (RBAC).
+
+> [!IMPORTANT]
+> When you create a search service, key-based authentication is the default, but it's not the most secure option. We recommend that you replace it with role-based access as described in this article.
 
 Before you can assign roles for authorized data plane access to Azure AI Search, you must enable role-based access control on your search service. Roles for service administration (control plane) are built in and can't be enabled or disabled. 
 
@@ -37,18 +39,18 @@ The default failure mode for unauthorized requests is `http401WithBearerChalleng
 
 ### [**Azure portal**](#tab/config-svc-portal)
 
-1. Sign in to the [Azure portal](https://portal.azure.com) and open the search service page.
+1. Sign in to the [Azure portal](https://portal.azure.com) and navigate to your search service.
 
 1. Select **Settings** and then select **Keys** in the left navigation pane.
 
    :::image type="content" source="media/search-security-rbac/search-security-enable-roles.png" lightbox="media/search-security-rbac/search-security-enable-roles.png" alt-text="Screenshot of the keys page with authentication options." border="true":::
 
-1. Choose **Role-based control** or **Both** if you're currently using keys and need time to transition clients to role-based access control. 
+1. Choose **Role-based control**. Only choose **Both** if you're currently using keys and need time to transition clients to role-based access control. 
 
    | Option | Description |
    |--------|--------------|
-   | API Key | (default). Requires [API keys](search-security-api-keys.md) on the request header for authorization. |
-   | Role-based access control | Requires membership in a role assignment to complete the task. It also requires an authorization header on the request. |
+   | API Key (default) | Requires [API keys](search-security-api-keys.md) on the request header for authorization. |
+   | Role-based access control (recommended) | Requires membership in a role assignment to complete the task. It also requires an authorization header on the request. |
    | Both | Requests are valid using either an API key or role-based access control, but if you provide both in the same request, the API key is used. |
 
 1. As an administrator, if you choose a roles-only approach, [assign data plane roles](search-security-rbac.md) to your user account to restore full administrative access over data plane operations in the Azure portal. Roles include Search Service Contributor, Search Index Data Contributor, and Search Index Data Reader. You need the first two roles if you want equivalent access.
@@ -140,7 +142,7 @@ All calls to the Management REST API are authenticated through Microsoft Entra I
 
 It's possible to disable role-based access control for data plane operations and use key-based authentication instead. You might do this as part of a test workflow, for example to rule out permission issues.
 
-Reverse the steps you followed previously to enable role-based access.
+To disable role-based access control in the Azure portal:
 
 1. Sign in to the [Azure portal](https://portal.azure.com) and open the search service page.
 
@@ -221,12 +223,6 @@ To re-enable key authentication, set "disableLocalAuth" to false. The search ser
 
 ---
 
-## Effects of role-based access control
-
-+ Role-based access control can increase the latency of some requests. Each unique combination of service resource (index, indexer, skillsets and so forth) and service principal triggers an authorization check. These authorization checks can add up to 200 milliseconds of latency per request. 
-
-+ In rare cases where requests originate from a high number of different service principals, all targeting different service resources (indexes, indexers, and so forth), it's possible for the authorization checks to result in throttling. Throttling would only happen if hundreds of unique combinations of search service resource and service principal were used within a second.
-
 ## Next steps
 
 > [!div class="nextstepaction"]
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to role-based access control documentation for Azure AI Search. Locale: en_US"
}
```

### Explanation
The code diff showcases a minor update to the documentation for enabling role-based access control (RBAC) in Azure AI Search. This update provides clearer guidance on authentication methods and reinforces security recommendations surrounding the use of Microsoft Entra ID.

Key modifications include:

1. **Clarification of Authentication Methods**:
   - The documentation has been revised to emphasize that Azure AI Search supports both keyless and key-based authentication options for all control plane and data plane operations, thereby offering users clearer choices for securing their services.
   - An important note has been added urging users to consider replacing key-based authentication, which is the default, with role-based access for improved security.

2. **Improved Instructions**:
   - Specific steps within the Azure portal process have been refined for clarity and greater usability. For instance, users are advised explicitly to select "Role-based control" with the option to choose "Both" only if they require time to transition from existing key-based access.
   - Descriptions in the settings options have been reformulated to enhance understanding of what each choice entails, which aids users in making more informed decisions.

3. **Removal of Redundant Content**:
   - Previous content discussing the effects and potential latencies related to role-based access control has been removed, streamlining the document and focusing on essential information.

4. **Structured Guidance for Disabling RBAC**:
   - The section on disabling role-based access control has been clarified, providing step-by-step instructions to guide users through the process effectively.

These enhancements contribute to a more user-friendly and comprehensible approach to implementing role-based access control in Azure AI Search, ensuring users can better secure their setups with Microsoft Entra ID.

## articles/search/search-security-rbac.md{#item-a5d129}

<details>
<summary>Diff</summary>
````diff
@@ -606,12 +606,6 @@ To enable a Conditional Access policy for Azure AI Search, follow these steps:
 > [!IMPORTANT]
 > If your search service has a managed identity assigned to it, the specific search service will show up as a cloud app that can be included or excluded as part of the Conditional Access policy. Conditional Access policies can't be enforced on a specific search service. Instead make sure you select the general **Azure AI Search** cloud app.
 
-## Limitations
-
-+ Role-based access control can increase the latency of some requests. Each unique combination of service resource (index, indexer, etc.) and service principal triggers an authorization check. These authorization checks can add up to 200 milliseconds of latency per request. 
-
-+ In rare cases where requests originate from a high number of different service principals, all targeting different service resources (indexes, indexers, etc.), it's possible for the authorization checks to result in throttling. Throttling would only happen if hundreds of unique combinations of search service resource and service principal were used within a second.
-
 ## Troubleshooting role-based access control issues
 
 When developing applications that use role-based access control for authentication, some common issues might occur:
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Removal of limitations section in RBAC documentation for Azure AI Search. Locale: en_US"
}
```

### Explanation
The code diff reflects a minor update to the role-based access control (RBAC) documentation for Azure AI Search, specifically involving the removal of a section outlining limitations. This change focuses on streamlining the content to make it more direct and actionable for users.

Key changes include:

1. **Removal of Limitations**:
   - The section detailing the potential increase in latency due to authorization checks has been removed. This content previously noted that every unique combination of service resources and service principals could trigger latency issues and discussions about throttling under specific conditions.
   - By eliminating this information, the documentation becomes less cumbersome, allowing users to concentrate on implementing RBAC without being concerned about theoretical limitations that may not affect most users.

2. **Continued Emphasis on Important Notes**:
   - The existing note regarding Conditional Access policies remains intact, ensuring that users are still informed about key considerations when managing access in the Azure AI Search environment.

Overall, this update to the documentation potentially reduces confusion and enhances readability, enabling users to focus on the core functionalities and best practices of implementing role-based access control without being distracted by less relevant limitations.

## articles/search/tutorial-csharp-create-load-index.md{#item-0a6ffd}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ author: diberry
 ms.author: diberry
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 08/16/2024
+ms.date: 01/17/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-azurecli
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update of publication date in C# tutorial for Azure AI Search. Locale: en_US"
}
```

### Explanation
The code diff represents a minor update to the publication date in the C# tutorial for creating and loading an index in Azure AI Search. This change reflects an adjustment in the document's metadata without altering the content or technical details of the tutorial itself.

Key modifications include:

1. **Updated Date**:
   - The metadata field for the publication date has been changed from "08/16/2024" to "01/17/2025". This change indicates a revision of the document's release schedule and ensures that users accessing the tutorial have the most up-to-date information regarding its publication.

This update demonstrates a routine maintenance action to keep the documentation current with respect to its publication timeline, which is important for user trust and reference accuracy. 

Overall, the change does not affect the technical specifications of the tutorial but ensures that the document reflects its correct status in the versioning system.

## articles/search/tutorial-csharp-deploy-static-web-app.md{#item-a2300f}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ author: diberry
 ms.author: diberry
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 08/16/2024
+ms.date: 01/17/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update of publication date in C# tutorial for deploying static web apps. Locale: en_US"
}
```

### Explanation
The code diff illustrates a minor update focusing on changing the publication date in the C# tutorial for deploying static web apps within Azure AI Search. This change pertains solely to the document's metadata, ensuring that the timing of the tutorial's availability is correctly reflected.

Key modifications include:

1. **Updated Date**:
   - The publication date metadata has been updated from "08/16/2024" to "01/17/2025". This revision suggests that the tutorial's release has been postponed or rescheduled, which informs users about the anticipated timing for when they can expect this resource.

This minor update is crucial as it assists users in keeping track of when the content was published, thereby promoting transparency and reliance on current documentation. 

Overall, the change does not involve any alterations to the tutorial's technical content, but it ensures that users are aware of its most accurate release status.

## articles/search/tutorial-csharp-overview.md{#item-57fa0d}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ author: diberry
 ms.author: diberry
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 10/21/2024
+ms.date: 01/17/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
@@ -48,7 +48,7 @@ The [sample code](https://github.com/Azure-Samples/azure-search-static-web-app)
 Create services and install the following software for your local development environment. 
 
 - [Azure AI Search](search-create-service-portal.md), any region or tier
-- [.NET 6](https://dotnet.microsoft.com/download/dotnet/6.0) or later
+- [.NET 9](https://dotnet.microsoft.com/download/dotnet) or latest version
 - [Git](https://git-scm.com/downloads)
 - [Visual Studio Code](https://code.visualstudio.com/)
 - [C# Dev Tools extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update of publication date and .NET version in C# overview tutorial. Locale: en_US"
}
```

### Explanation
The code diff reflects a minor update to the C# overview tutorial in Azure AI Search, encompassing both a change in the publication date and an update to the required .NET version. These modifications enhance the tutorial's accuracy and relevance.

Key modifications include:

1. **Updated Date**:
   - The publication date has been changed from "10/21/2024" to "01/17/2025". This implies a rescheduling of the tutorial’s release and helps users stay informed about the most current version of the document.

2. **Updated .NET Version**:
   - The recommended .NET framework version is updated from ".NET 6" to ".NET 9" (or the latest version). This change ensures that users are directed to the most up-to-date software needed for their development environment, thus increasing compatibility and security.

These updates are significant for maintaining the tutorial’s relevance and ensuring that users have access to the best practices and tools required for their development tasks. Overall, the changes do not alter the core content of the tutorial but refine the information critical for users to successfully utilize the resources provided.

## articles/search/tutorial-csharp-search-query-integration.md{#item-8ad6b5}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ author: diberry
 ms.author: diberry
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 08/16/2024
+ms.date: 01/17/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update of publication date in C# search query integration tutorial. Locale: en_US"
}
```

### Explanation
The code diff indicates a minor update to the C# search query integration tutorial within Azure AI Search. The primary change involves the adjustment of the publication date, which is critical for keeping the tutorial's information up-to-date and relevant to users.

Key modifications include:

1. **Updated Date**:
   - The publication date has been modified from "08/16/2024" to "01/17/2025". This reflects a postponement or rescheduling of the tutorial’s release, allowing users to be aware of when they can expect access to the content.

This update is important as it maintains the clarity and accuracy of the documentation, ensuring users have the most current information regarding publication timelines. No other content changes were made, so the tutorial's existing information and guidance remain intact and unchanged.

## articles/search/vector-search-how-to-query.md{#item-9bb93c}

<details>
<summary>Diff</summary>
````diff
@@ -136,7 +136,7 @@ api-key: {{admin-api-key}}
 This preview adds:
 
 + [`threshold`](#set-thresholds-to-exclude-low-scoring-results-preview) for excluding low scoring results.
-+ [`Hybridsearch.MaxTextRecallSize`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode-preview) for more control over the inputs to a [hybrid query](hybrid-search-how-to-query.md).
++ [`Hybridsearch.MaxTextRecallSize`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) for more control over the inputs to a [hybrid query](hybrid-search-how-to-query.md).
 
 In the following example, the vector is a representation of this string: "what Azure services support full text search". The query targets the `contentVector` field. The query returns `k` results. The actual vector has 1536 embeddings, so it's trimmed in this example for readability.
 
@@ -489,7 +489,7 @@ POST https://[service-name].search.windows.net/indexes/[index-name]/docs/search?
     } 
 ```
 
-Vector weighting applies to vectors only. The text query in this example ("hello world") has an implicit weight of 1.0 or neutral weight. However, in a hybrid query, you can increase or decrease the importance of text fields by setting [maxTextRecallSize](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode-preview).
+Vector weighting applies to vectors only. The text query in this example ("hello world") has an implicit weight of 1.0 or neutral weight. However, in a hybrid query, you can increase or decrease the importance of text fields by setting [maxTextRecallSize](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode).
 
 ## Set thresholds to exclude low-scoring results (preview)
 
@@ -522,13 +522,13 @@ POST https://[service-name].search.windows.net/indexes/[index-name]/docs/search?
  <!-- Keep H2 as-is. Direct link from a blog post. Bulk of maxtextsizerecall has moved to hybrid query doc-->
 ## MaxTextSizeRecall for hybrid search (preview)
 
-Vector queries are often used in hybrid constructs that include nonvector fields. If you discover that BM25-ranked results are over or under represented in a hybrid query results, you can [set `maxTextRecallSize`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode-preview) to increase or decrease the BM25-ranked results provided for hybrid ranking.
+Vector queries are often used in hybrid constructs that include nonvector fields. If you discover that BM25-ranked results are over or under represented in a hybrid query results, you can [set `maxTextRecallSize`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) to increase or decrease the BM25-ranked results provided for hybrid ranking.
 
 You can only set this property in hybrid requests that include both "search" and "vectorQueries" components.
 
 This parameter is still in preview. We recommend preview REST API version [2024-05-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-05-01-preview&preserve-view=true).
 
-For more information, see [Set maxTextRecallSize - Create a hybrid query](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode-preview).
+For more information, see [Set maxTextRecallSize - Create a hybrid query](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode).
 
 ## Next steps
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Adjustment of references and details in vector search how-to query tutorial. Locale: en_US"
}
```

### Explanation
The code diff showcases a minor update to the "Vector Search How to Query" tutorial in Azure AI Search. This update primarily involves correcting the reference links related to the `maxTextRecallSize` parameter and introducing a new `threshold` feature, aimed at enhancing the tutorial's clarity and accuracy.

Key modifications include:

1. **Introduction of Threshold Parameter**:
   - The tutorial now includes a mention of a new feature, `threshold`, which can be used to exclude low-scoring results. This addition enhances the usability of the query process and provides additional control for users.

2. **Corrections in References**:
   - The link references for `Hybridsearch.MaxTextRecallSize` have been adjusted by removing the outdated "preview" suffix in the link. This change aligns the documentation with the current state of the API and ensures that users are directed to the most relevant and accurate information source.

3. **Consistent Use of Terminology**:
   - The updates emphasize a consistent description of parameters and features, improving the overall readability and professional presentation of the tutorial.

These modifications do not introduce any new functionalities but enhance existing features and provide clearer guidance for users working with vector search queries within hybrid scenarios. Overall, these changes work towards keeping the documentation precise and user-friendly.

## articles/search/whats-new.md{#item-fa71b4}

<details>
<summary>Diff</summary>
````diff
@@ -89,7 +89,7 @@ ms.custom:
 |-----------------------------|------|--------------|
 | [Higher capacity and more vector quota at every tier (same billing rate)](search-limits-quotas-capacity.md#service-limits) | Infrastructure | For most regions, partition sizes are now even larger for Standard 2 (S2), Standard 3 (S3), and Standard 3 High Density (S3 HD) for services created after April 3, 2024. To get the larger partitions, create a new service in a [region that provides newer infrastructure](search-region-support.md). <br><br>Storage Optimized tiers (L1 and L2) also have more capacity. L1 and L2 customers must create a new service to benefit from the higher capacity. There's no in-place upgrade at this time. <br><br>Extra capacity is now available in [more regions](search-limits-quotas-capacity.md#service-limits): Germany North​, Germany West Central​, South Africa North​, Switzerland West​, and Azure Government (Texas, Arizona, and Virginia).|
 | [OneLake integration (preview)](search-how-to-index-onelake-files.md) | Feature | New indexer for OneLake files and OneLake shortcuts. If you use Microsoft Fabric and OneLake for data access to Amazon Web Services (AWS) and Google data sources, use this indexer to import external data into a search index. This indexer is available through the Azure portal, the [2024-05-01-preview REST API](/rest/api/searchservice/data-sources/create-or-update?view=rest-searchservice-2024-05-01-preview&preserve-view=true), and Azure SDK beta packages. |
-| [Vector relevance](vector-search-how-to-query.md) <br>[hybrid query relevance](hybrid-search-how-to-query.md) | Feature | Four enhancements improve vector and hybrid search relevance. <br><br>First, you can now set thresholds on vector search results to exclude low-scoring results. <br><br>Second, changes in the query architecture apply scoring profiles at the end of the query pipeline for every query type. Document boosting is a common scoring profile, and it now works as expected on vector and hybrid queries.<br><br>Third, you can set `MaxTextRecallSize` and `countAndFacetMode` in hybrid queries to control the quantity of BM25-ranked search results that flow into the hybrid ranking model. <br><br>Fourth, for vector and hybrid search, you can weight a vector query to have boost or diminish its importance in a multiquery request. |
+| [Vector relevance](vector-search-how-to-query.md) <br>[hybrid query relevance](hybrid-search-how-to-query.md) | Feature | Four enhancements improve vector and hybrid search relevance. <br><br>First, you can now set thresholds on vector search results to exclude low-scoring results. <br><br>Second, changes in the query architecture apply scoring profiles at the end of the query pipeline for every query type. Document boosting is a common scoring profile, and it now works as expected on vector and hybrid queries.<br><br>Third, you can set [`MaxTextRecallSize` and `countAndFacetMode`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) in hybrid queries to control the quantity of BM25-ranked search results that flow into the hybrid ranking model. <br><br>Fourth, for vector and hybrid search, you can weight a vector query to have boost or diminish its importance in a multiquery request. |
 | [Binary vectors support](/rest/api/searchservice/supported-data-types) | Feature | `Collection(Edm.Byte)` is a new supported data type. This data type opens up integration with the [Cohere v3 binary embedding models](https://cohere.com/blog/int8-binary-embeddings) and custom binary quantization. Narrow data types lower the cost of large vector datasets. See [Index binary data for vector search](vector-search-how-to-index-binary-data.md) for more information.| 
 | [Azure AI Vision multimodal embeddings skill (preview)](cognitive-search-skill-vision-vectorize.md) | Skill | New skill that's bound to the [multimodal embeddings API of Azure AI Vision](/azure/ai-services/computer-vision/concept-image-retrieval). You can generate embeddings for text or images during indexing. This skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true).|
 | [Azure AI Vision vectorizer (preview)](vector-search-vectorizer-ai-services-vision.md) | Vectorizer | New vectorizer connects to an Azure AI Vision resource using the [multimodal embeddings API](/azure/ai-services/computer-vision/concept-image-retrieval) to generate embeddings at query time. This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update of vector relevance features in what's new documentation. Locale: en_US"
}
```

### Explanation
The code diff reflects a minor update to the "What's New" documentation for Azure AI Search. This update entails clarifying details related to the vector relevance features and enhancements that improve both vector and hybrid search capabilities.

Key changes include:

1. **Enhanced Description of Vector Relevance Features**:
   - The section detailing the improvements to vector and hybrid search relevance has been modified for clarity. The text now explicitly mentions the `MaxTextRecallSize` and `countAndFacetMode` parameters, which help users understand how to control the quantity of BM25-ranked search results flowing into the hybrid ranking model better.

2. **Consistent Formatting**:
   - The syntax around the mention of `MaxTextRecallSize` and `countAndFacetMode` has been standardized by employing code formatting, making it more visually distinct and easier to identify in the context of the documentation.

Overall, the modifications serve to enhance the accuracy and usability of the documentation, allowing users to better comprehend the new functionalities available in vector and hybrid searches within Azure AI. The updates ensure that users are informed about the capabilities they can leverage to improve search relevance effectively.


