---
date: '2025-07-30'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:e0fd9e5...MicrosoftDocs:c2bb090
summary: 此次更新主要聚焦于对多个Azure认知搜索文档的轻微调整与优化，旨在提升文档的可读性、准确性和实用性。更新内容包括添加新功能说明、更新技能使用教程链接，以及优化文档结构。其中，尤其突出的是增加了生成式AI相关功能的描述，显著改善了用户体验和文档的一致性。此外，更新还包含多个标题和内容格式的改进，确保文档主题与内容相符，方便用户查找所需信息。整体而言，此次优化致力于增强用户对Azure认知搜索技术的理解和应用效率。
title: '[zh_CN] Diff Insight Report - search'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:e0fd9e5...MicrosoftDocs:c2bb090){target="_blank"}

# Highlights
此次更新主要聚焦于对多个Azure认知搜索文档的轻微调整与优化，提升文档的可读性、准确性和实用性。这些更新包含添加新功能说明、更新技能使用教程链接及优化文档结构等改动。特别突出的是增加了生成式AI（GenAI）相关的功能描述，显著提升了用户体验和文档内容的一致性。

## New features
- 添加了关于“GenAI Prompt”功能的详情，支持图像内容的描述。
- 增加了对新生成式AI提示技能文档的更新，提供关于如何结合数据分块技能的详细指导。

## Breaking changes
无重大破坏性更改。

## Other updates
- 多个文档更新了标题及内容格式，以提高可读性。
- 更新教程链接，增加了与生成式AI和多模态数据相关的新标题及引导。
- 在责任AI最佳实践文档中，新增了角色描述及使用GenAI Prompt技能时的注意事项。
- 改进了目录结构，使用户更容易导航到所需内容。

# Insights
本次代码更新通过一系列轻微但重要的文档调整，显著改善了Azure认知搜索相关文档的整体用户体验。这些调整体现了几个关键方向：

1. **增强功能覆盖与描述**：更新展示了强大的生成式AI能力，特别是在图像语言化和文本摘要方面。这不仅帮助用户理解如何配置和使用这些AI技能，还提供了实用的教程链接，方便快速上手。

2. **文档一致性与清晰性**：一系列标题及链接的更新，确保了文档的主题与内容相符。这在提高文档的一致性及用户查找文献的便利性方面起到了关键作用。

3. **最佳实践指南的完善**：在负责任AI的实践上，此次更新增加了关于透明度、内容质量检查和安全性过滤等主题的指导。这一信息对于开发者而言，是在应用GenAI时必要的参考，确保在创新的同时保证合规。

4. **多模态索引与处理**：通过详细解释如何在Azure AI中进行多模态内容的索引和搜索，修订文本指导用户按步骤操作，并提高了使用过程的效率。

总体来看，此次文档优化旨在通过对内容精简、标题更新和新增案例等多方面改进，提高Azure认知搜索的用户学习曲线及实际应用体验。用户通过更富信息量的文档，将能更全面地理解并迅速采纳最新技术及其优势。

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [chat-completion-skill-example-usage.md](#item-21e090) | minor update | 更新聊天完成技能示例用法文档 | modified | 10 | 10 | 20 | 
| [cognitive-search-concept-image-scenarios.md](#item-606953) | minor update | 添加GenAI提示功能说明 | modified | 1 | 0 | 1 | 
| [cognitive-search-skill-document-extraction.md](#item-072b72) | minor update | 更新文档提取技能的教程链接 | modified | 2 | 2 | 4 | 
| [cognitive-search-skill-document-intelligence-layout.md](#item-62c06f) | minor update | 更新文档智能布局技能的说明和教程 | modified | 6 | 7 | 13 | 
| [cognitive-search-skill-genai-prompt.md](#item-384bf9) | minor update | 更新GenAI提示技能文档 | modified | 15 | 7 | 22 | 
| [knowledge-store-projection-example-long.md](#item-e18999) | minor update | 更新知识库投影示例文档 | modified | 11 | 8 | 19 | 
| [multimodal-search-overview.md](#item-d82192) | minor update | 更新多模态搜索概述文档 | modified | 4 | 4 | 8 | 
| [responsible-ai-best-practices-genai-prompt-skill.md](#item-2a7b97) | minor update | 更新负责任的 AI 最佳实践文档 | modified | 25 | 17 | 42 | 
| [search-get-started-portal-image-search.md](#item-438b9b) | minor update | 更新开始使用门户图像搜索的文档 | modified | 4 | 4 | 8 | 
| [search-relevance-overview.md](#item-cb0e09) | minor update | 更新Azure AI搜索相关性概述文档 | modified | 10 | 5 | 15 | 
| [toc.yml](#item-c4768f) | minor update | 更新搜索文档的目录结构 | modified | 21 | 15 | 36 | 
| [tutorial-document-extraction-image-verbalization.md](#item-398a90) | minor update | 更新文档提取和图像语言化教程 | modified | 24 | 28 | 52 | 
| [tutorial-document-extraction-multimodal-embeddings.md](#item-a3db59) | minor update | 更新多模态嵌入和文档提取教程 | modified | 17 | 29 | 46 | 
| [tutorial-document-layout-image-verbalization.md](#item-f5de57) | minor update | 更新文档布局和图像语言化教程 | modified | 20 | 23 | 43 | 
| [tutorial-document-layout-multimodal-embeddings.md](#item-f67371) | minor update | 更新文档布局多模态嵌入教程 | modified | 17 | 25 | 42 | 


# Modified Contents
## articles/search/chat-completion-skill-example-usage.md{#item-21e090}

<details>
<summary>Diff</summary>
````diff
@@ -2,11 +2,11 @@
 title: Utilize the content generation capabilities of language models as part of content ingestion pipeline
 titleSuffix: Azure AI Search
 description: Use language models to caption your images and facilitate an image search through your data.
-author: amitkalay
-ms.author: amitkalay
+author: gmndrg
+ms.author: gimondra
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 05/05/2025
+ms.date: 07/28/2025
 ms.custom:
   - devx-track-csharp
   - build-2025
@@ -22,20 +22,20 @@ The GenAI Prompt skill (preview) generates a description of each image in your d
 
 To work with image content in a skillset, you need:
 
-+ A supported data source
-+ Files or blobs containing images
-+ Read access on the supported data source. This article uses key-based authentication, but indexers can also connect using the search service identity and Microsoft Entra ID authentication. For role-based access control, assign roles on the data source to allow read access by the service identity. If you're testing on a local development machine, make sure you also have read access on the supported data source.
-+ A search indexer, configured for image actions
-+ A skillset with the new custom genAI prompt skill
-+ A search index with fields to receive the verbalized text output, plus output field mappings in the indexer that establish association
++ A [supported data source](search-indexer-overview.md#supported-data-sources). We recommend Azure Storage.
++ Files or blobs containing images.
++ Read access to the supported data source. This article uses key-based authentication, but indexers can also connect using the search service identity and Microsoft Entra ID authentication. For role-based access control, assign roles on the data source to allow read access by the service identity. If you're testing on a local development machine, make sure you also have read access on the supported data source.
++ A [search indexer](search-how-to-create-indexers.md), configured for image actions.
++ A skillset with the new custom genAI prompt skill.
++ A search index with fields to receive the verbalized text output, plus output field mappings in the indexer that establish association.
 
 Optionally, you can define projections to accept image-analyzed output into a [knowledge store](knowledge-store-concept-intro.md) for data mining scenarios.
 
 <a name="get-normalized-images"></a>
 
 ## Configure indexers for image processing
 
-After the source files are set up, enable image normalization by setting the `imageAction` parameter in indexer configuration. Image normalization helps make images more uniform for downstream processing. Image normalization includes the following operations:
+After the source files are set up, enable image normalization by setting the `imageAction` parameter in the indexer configuration. Image normalization helps make images more uniform for downstream processing. Image normalization includes the following operations:
 
 + Large images are resized to a maximum height and width to make them uniform.
 + For images that have metadata that specifies orientation, image rotation is adjusted for vertical loading.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新聊天完成技能示例用法文档"
}
```

### Explanation
该代码差异展示了对文档`chat-completion-skill-example-usage.md`的修改。主要进行了作者信息和日期的更新，以及对内容的格式进行了优化，增强了可读性。

具体而言，文档的作者从“amitkalay”更改为“gmndrg”，同时更新了“ms.date”字段的日期，从“2025年5月5日”改为“2025年7月28日”。此外，文档中的某些条目格式得到了改进，原本的无链接文本被更新为包含链接的文本，这使得读者可以直接访问相关文档和资源，例如对支持数据源和搜索索引器的链接进行了明确的注释。

这些修改虽然不涉及重大的功能变动，但通过清晰的结构和最新的信息，提升了文档的实用性和用户体验。

## articles/search/cognitive-search-concept-image-scenarios.md{#item-606953}

<details>
<summary>Diff</summary>
````diff
@@ -16,6 +16,7 @@ ms.custom:
 
 Images often contain useful information that's relevant in search scenarios. You can [vectorize images](search-get-started-portal-image-search.md) to represent visual content in your search index. Or, you can use [AI enrichment and skillsets](cognitive-search-concept-intro.md) to create and extract searchable *text* from images, including:
 
+ + [GenAI Prompt](cognitive-search-skill-genai-prompt.md) to pass a prompt to a chat completion skill, requesting a description of image content.
 + [OCR](cognitive-search-skill-ocr.md) for optical character recognition of text and digits
 + [Image Analysis](cognitive-search-skill-image-analysis.md) that describes images through visual features
 + [Custom skills](#passing-images-to-custom-skills) to invoke any external image processing that you want to provide
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "添加GenAI提示功能说明"
}
```

### Explanation
该代码差异反映了对文档`cognitive-search-concept-image-scenarios.md`的轻微更新，具体是在描述图像处理技能的列表中新增了一项内容。

本次修改在现有的技能描述中，增加了一行关于“GenAI Prompt”的说明。该功能允许用户向聊天完成技能传递提示，以请求图像内容的描述。这一修改旨在提供更完整的信息，帮助用户理解如何利用这一新功能进行图像内容的搜索和处理。

通过这一小的更新，文档在指导用户充分利用图像搜索的能力时显得更加全面，提升了用户对相关技能组合使用的认识。

## articles/search/cognitive-search-skill-document-extraction.md{#item-072b72}

<details>
<summary>Diff</summary>
````diff
@@ -19,9 +19,9 @@ The **Document Extraction** skill extracts content from a file within the enrich
 
 For [vector](vector-search-overview.md) and [multimodal search](multimodal-search-overview.md), Document Extraction combined with the [Text Split skill](cognitive-search-skill-textsplit.md) is more affordable than other [data chunking approaches](vector-search-how-to-chunk-documents.md). The following tutorials demonstrate skill usage for different scenarios:
 
-+ [Tutorial: Index mixed content using multimodal embeddings and the Document Extraction skill](tutorial-document-extraction-multimodal-embeddings.md)
++ [Tutorial: Vectorize images and text](tutorial-document-extraction-multimodal-embeddings.md)
 
-+ [Tutorial: Index mixed content using image verbalizations and the Document Extraction skill](tutorial-document-extraction-image-verbalization.md)
++ [Tutorial: Verbalize images using generative AI](tutorial-document-extraction-image-verbalization.md)
 
 > [!NOTE]
 > This skill isn't bound to Azure AI services and has no Azure AI services key requirement.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新文档提取技能的教程链接"
}
```

### Explanation
该代码差异显示了对文档`cognitive-search-skill-document-extraction.md`的修改，主要是对教程链接的更新。

在此次更新中，原本的两个教程标题被重新命名，以更加准确地反映其内容。第一个教程从“使用多模态嵌入和文档提取技能索引混合内容”修改为“对图像和文本进行向量化”，第二个教程从“使用图像语言化和文档提取技能索引混合内容”改为“使用生成性AI进行图像语言化”。这种调整使得教程的标题更加匹配其实际内容，提高了文档的可理解性和实用性。

这些细微的修改虽然不涉及重大的功能变动，但通过改善标题的表述，提升了用户获取信息的效率，帮助用户更好地利用文档提取技能。

## articles/search/cognitive-search-skill-document-intelligence-layout.md{#item-62c06f}

<details>
<summary>Diff</summary>
````diff
@@ -22,16 +22,15 @@ The **Document Layout** skill analyzes a document to detect structure and charac
 
 This article is the reference documentation for the Document Layout skill. For usage information, see [How to chunk and vectorize by document layout](search-how-to-semantic-chunking.md). 
 
-It's common to use this skill on content such as PDFs that have structure and images. The following tutorials demonstrate several scenarios: 
+This skill uses the [Document Intelligence layout model](/azure/ai-services/document-intelligence/concept-layout) provided in [Azure AI Document Intelligence](/azure/ai-services/document-intelligence/overview).
 
-+ [Tutorial: Index mixed content using image verbalizations and the Document Layout skill](tutorial-document-layout-image-verbalization.md)
+This skill is bound to a [billable Azure AI multi-service resource](cognitive-search-attach-cognitive-services.md) for transactions that exceed 20 documents per indexer per day. Execution of built-in skills is charged at the existing [Azure AI services Standard price](https://azure.microsoft.com/pricing/details/cognitive-services/).
 
-+ [Tutorial: Index mixed content using multimodal embeddings and the Document Layout skill](tutorial-document-layout-multimodal-embeddings.md)
-
-> [!NOTE]
-> This skill uses the [Document Intelligence layout model](/azure/ai-services/document-intelligence/concept-layout) provided in [Azure AI Document Intelligence](/azure/ai-services/document-intelligence/overview).
+> [!TIP]
+> It's common to use this skill on content such as PDFs that have structure and images. The following tutorials demonstrate image verbalization with two different data chunking techniques:
 >
-> This skill is bound to a [billable Azure AI multi-service resource](cognitive-search-attach-cognitive-services.md) for transactions that exceed 20 documents per indexer per day. Execution of built-in skills is charged at the existing [Azure AI services Standard price](https://azure.microsoft.com/pricing/details/cognitive-services/).
+> - [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md)
+> - [Tutorial: Vectorize from a structured document layout](tutorial-document-layout-multimodal-embeddings.md)
 >
 
 ## Limitations
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新文档智能布局技能的说明和教程"
}
```

### Explanation
该代码差异显示了对文档`cognitive-search-skill-document-intelligence-layout.md`的轻微更新，主要集中在文档智能布局技能的描述及其相关教程的链接。

在此次修改中，原有的描述部分被重新组织，以更清晰地介绍文档布局技能如何使用Azure AI文档智能布局模型。这一更新强调了该技能对于PDF等具有结构和图像的内容的常见应用。

此外，教程部分也经过调整，提供了更明确和相关的链接。原文中的教程标题被更改为新的描述，包括“从结构化文档布局中语言化图像”和“从结构化文档布局中进行向量化”。这种变化使得用户能够更好地理解如何利用该技能在不同场景下进行操作。

整体上，这一修改提升了文档的流畅度和可读性，帮助用户更有效地利用文档智能布局技能，增强了指引的准确性和适用性。

## articles/search/cognitive-search-skill-genai-prompt.md{#item-384bf9}

<details>
<summary>Diff</summary>
````diff
@@ -1,29 +1,37 @@
 ---
 title: GenAI Prompt skill (Preview)
 titleSuffix: Azure AI Search
-description: Invokes Chat Completion models from Azure OpenAI or other Azure AI Foundry-hosted models at indexing time.
+description: Invokes chat completion models from Azure OpenAI or other Azure AI Foundry-hosted models to create content at indexing time.
 author: gmndrg
 ms.author: gimondra
 ms.service: azure-ai-search
 ms.custom:
   - build-2025
 ms.topic: reference
-ms.date: 05/27/2025
+ms.date: 07/28/2025
 ---
 
 # GenAI Prompt skill
 
 [!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
 
-The **GenAI (Generative AI) Prompt** skill executes a *chat completion* request against a Large Language Model (LLM) deployed in Azure AI Foundry or Azure OpenAI in Azure AI Foundry Models.  
+The **GenAI (Generative AI) Prompt** skill executes a *chat completion* request against a Large Language Model (LLM) deployed in Azure AI Foundry or Azure OpenAI in Azure AI Foundry Models. Use this capability to create new information that can be indexed and stored as searchable content.
 
-Use this capability to create new information that can be indexed and stored as searchable content. Examples include verbalize images, summarize larger passages, simplify complex content, or any other task that an LLM can perform. The skill supports text, image, and multimodal content such as a PDF that contains text and images. It's common to use this skill combined with a data chunking skill. The following tutorials demonstrate the image verbalization scenarios with two different data chunking techniques: 
+Here are some examples of how the GenAI prompt skill can help you create content:
 
-- [Tutorial: Index mixed content using image verbalizations and the Document Layout skill](tutorial-document-layout-image-verbalization.md)
+- Verbalize images
+- Summarize large passages of text
+- Simplify complex content
+- Perform any other task that you can articulate in a prompt
 
-- [Tutorial: Index mixed content using image verbalizations and the Document Extraction skill](tutorial-document-extraction-image-verbalization.md)
+The GenAI Prompt skill is available in the [2025-05-01-preview REST API](/rest/api/searchservice/skillsets/create?view=rest-searchservice-2025-05-01-preview&preserve-view=true) only. The skill supports text, image, and multimodal content such as a PDF that contains text and images. 
 
-The GenAI Prompt skill is available in the [2025-05-01-preview REST API](/rest/api/searchservice/skillsets/create?view=rest-searchservice-2025-05-01-preview&preserve-view=true) only. 
+> [!TIP]
+> It's common to use this skill combined with a data chunking skill. The following tutorials demonstrate image verbalization with two different data chunking techniques:
+>
+> - [Tutorial: Verbalize images using generative AI](tutorial-document-extraction-image-verbalization.md)
+> - [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md)
+>
 
 ## Supported models
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GenAI提示技能文档"
}
```

### Explanation
该代码差异展示了对文档`cognitive-search-skill-genai-prompt.md`的更新，旨在提升GenAI（生成性AI）提示技能的描述和指导信息。

在此次修改中，文档的描述部分进行了精简和改进，强调了该技能如何调用Azure OpenAI或其他Azure AI Foundry托管的模型来生成新的信息，包括图像语言化、文本摘要、内容简化等示例。这些更改能够帮助用户更好地理解该技能的用途以及其工作方式。

此外，文档中还新增了一些提示，指向相关的教程，帮助用户掌握如何使用该技能与数据分块技能的结合。更新后的教程列表更为清晰，指出了具体的使用案例，例如“使用生成AI进行图像语言化”。

这些变化提升了文档的实用性和易读性，使用户更容易找到所需的信息并有效地应用GenAI提示技能。整体而言，这次修改加强了用户对功能的认识，并提供了更好的学习资源。

## articles/search/knowledge-store-projection-example-long.md{#item-e18999}

<details>
<summary>Diff</summary>
````diff
@@ -7,22 +7,22 @@ manager: nitinme
 author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
-ms.topic: conceptual
-ms.date: 06/17/2025
+ms.topic: concept-article
+ms.date: 07/28/2025
 ms.custom:
   - ignite-2023
   - sfi-ropc-nochange
 ---
 
-# Detailed example of shapes and projections in a knowledge store
+# Example of shapes and projections in a knowledge store
 
-This article provides a detailed example that supplements [high-level concepts](knowledge-store-projection-overview.md) and [syntax-based articles](knowledge-store-projections-examples.md) by walking you through the shaping and projection steps required for fully expressing the output of a rich skillset in a [knowledge store](knowledge-store-concept-intro.md).
+This article provides a detailed example that supplements [high-level concepts](knowledge-store-projection-overview.md) and [syntax-based articles](knowledge-store-projections-examples.md) by walking you through the shaping and projection steps required for fully expressing the output of a rich skillset in a [knowledge store](knowledge-store-concept-intro.md) in Azure Storage.
 
-If your application requirements call for multiple skills and projections, this example can give you a better idea of how shapes and projections intersect.
+If your application requirements call for multiple skills and projections, this example can give you a better idea of how shapes and projections interact.
 
 ## Set up sample data
 
-Sample documents aren't included with the Projections collection, but the [AI enrichment demo data files](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/ai-enrichment-mixed-media) contain text and images that work with the projections described in this example.
+Sample documents aren't included with the Projections collection, but the [AI enrichment demo data files](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/ai-enrichment-mixed-media) contain text and images that work with the projections described in this example. If you use this sample data, you can skip step that [attaches an Azure AI multi-service account](cognitive-search-attach-cognitive-services.md) because you stay under the daily indexer limit for free enrichments.
 
 Create a blob container in Azure Storage and upload all 14 items.
 
@@ -39,7 +39,7 @@ Pay close attention to skill outputs (targetNames). Outputs written to the enric
 ```json
 {
     "name": "projections-demo-ss",
-    "description": "Skillset that enriches blob data found in "merged_content". The enrichment granularity is a document.",
+    "description": "Skillset that enriches blob data found in the merged_content field. The enrichment granularity is a document.",
     "skills": [
         {
             "@odata.type": "#Microsoft.Skills.Text.V3.EntityRecognitionSkill",
@@ -182,12 +182,15 @@ Pay close attention to skill outputs (targetNames). Outputs written to the enric
     "cognitiveServices": {
         "@odata.type": "#Microsoft.Azure.Search.CognitiveServicesByKey",
         "description": "An Azure AI services resource in the same region as Search.",
-        "key": "<Azure AI services All-in-ONE KEY>"
+        "key": ""
     },
     "knowledgeStore": null
 }
 ```
 
+> [!NOTE]
+> Under `"cognitiveServices"`, the key field is unspecified because the indexer can use an Azure AI multi-service account in the same region as your search service and process up to 20 transactions daily at no charge. The sample data for this example stays under the 20 transaction limit.
+
 ## Example Shaper skill
 
 A [Shaper skill](cognitive-search-skill-shaper.md) is a utility for working with existing enriched content instead of creating new enriched content. Adding a Shaper to a skillset lets you create a custom shape that you can project into table or blob storage. Without a custom shape, projections are limited to referencing a single node (one projection per output), which isn't suitable for tables. Creating a custom shape aggregates various elements into a new logical whole that can be projected as a single table, or sliced and distributed across a collection of tables. 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新知识库投影示例文档"
}
```

### Explanation
该代码差异展示了对文档`knowledge-store-projection-example-long.md`的轻微更新，旨在提高知识库投影示例的清晰度和实用性。

在此次修改中，文档的标题和部分内容被修改，以使其更加简洁和易于理解。例如，标题从“详细示例”更改为“示例”，并且对文章的描述进行了优化，明确指出该示例与Azure Storage中的知识库输出相关。这些调整有助于用户快速捕捉到文档的重点和应用场景。

此外，文档中增加了有关样本数据的说明，明确提到使用样本数据可以省略与Azure AI多服务账户相关的设置步骤，因为这可以确保用户在免费丰富内容的每日限制内工作。这是一个实用的提示，能够帮助用户更有效地利用示例而无需承担额外的配置工作。

文章中还增加了有关“Shaper技能”的新节，介绍了它的作用和使用方法，使得用户能够更好地理解如何处理已经丰富的内容。

总体而言，这次修改加强了文档的清晰度和可操作性，帮助用户更好地理解知识库投影的过程及其应用。

## articles/search/multimodal-search-overview.md{#item-d82192}

<details>
<summary>Diff</summary>
````diff
@@ -116,8 +116,8 @@ To help you get started with multimodal search in Azure AI Search, here's a coll
 | Content | Description |
 |--|--|
 | [Quickstart: Multimodal search in the Azure portal](search-get-started-portal-image-search.md) | Create and test a multimodal index in the Azure portal using the wizard and Search Explorer. |
-| [Tutorial: Image verbalization and Document Extraction skill](tutorial-document-extraction-image-verbalization.md) | Extract text and images, verbalize diagrams, and embed the resulting descriptions and text into a searchable index. |
-| [Tutorial: Multimodal embeddings and Document Extraction skill](tutorial-document-extraction-multimodal-embeddings.md) | Use a vision-text model to embed both text and images directly, enabling visual-similarity search over scanned PDFs. |
-| [Tutorial: Image verbalization and Document Layout skill](tutorial-document-layout-image-verbalization.md) | Apply layout-aware chunking and diagram verbalization, capture location metadata, and store cropped images for precise citations and page highlights. |
-| [Tutorial: Multimodal embeddings and Document Layout skill](tutorial-document-layout-multimodal-embeddings.md) | Combine layout-aware chunking with unified embeddings for hybrid semantic and keyword search that returns exact hit locations. |
+| [Tutorial: Verbalize images using generative AI](tutorial-document-extraction-image-verbalization.md) | Extract text and images, verbalize diagrams, and embed the resulting descriptions and text into a searchable index. |
+| [Tutorial: Vectorize images and text](tutorial-document-extraction-multimodal-embeddings.md) | Use a vision-text model to embed both text and images directly, enabling visual-similarity search over scanned PDFs. |
+| [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md) | Apply layout-aware chunking and diagram verbalization, capture location metadata, and store cropped images for precise citations and page highlights. |
+| [Tutorial: Vectorize from a structured document layout](tutorial-document-layout-multimodal-embeddings.md) | Combine layout-aware chunking with unified embeddings for hybrid semantic and keyword search that returns exact hit locations. |
 | [Sample app: Multimodal RAG GitHub repository](https://aka.ms/azs-multimodal-sample-app-repo) | An end-to-end, code-ready RAG application with multimodal capabilities that surfaces both text snippets and image annotations. Ideal for jump-starting enterprise copilots. |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新多模态搜索概述文档"
}
```

### Explanation
该代码差异展示了对文档`multimodal-search-overview.md`的轻微更新，旨在调整和优化相关教程的标题以增强清晰性和一致性。

在此次修改中，几个教程标题被重新命名，使其更符合内容主题。例如，“Image verbalization and Document Extraction skill”被更改为“Verbalize images using generative AI”，而“Multimodal embeddings and Document Extraction skill”改为“Vectorize images and text”。这样的更改旨在确保标题更为简洁，并准确反映出各自的功能。

此外，文档中的所有相关教程保持了其内容完整性，依然涵盖文本和图像的提取、结构化文件布局下的描述以及关键字搜索能力等。这些教程为用户提供了实践操作的实例，帮助他们更好地理解如何在Azure AI Search中实现多模态搜索。

总体而言，这次修改优化了文档的可读性与一致性，使得用户在查阅教程时能更加直观地理解每个教程的主题与应用场景。

## articles/search/responsible-ai-best-practices-genai-prompt-skill.md{#item-2a7b97}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.service: azure-ai-search
 ms.custom:
   - build-2025
 ms.topic: concept-article
-ms.date: 04/28/2025
+ms.date: 07/28/2025
 ---
 
 # Best practices - GenAI Prompt skill
@@ -25,38 +25,46 @@ The content generation capabilities of language models are continuing to evolve
 
 In order to list out the various challenges in incorporating AI content generation capabilities into an Azure AI Search indexer pipeline, it's important to understand the various personas that interact with the RAG application as each of them might carry a different set of challenges.
 
-* End-user: This persona is the one that is asking questions to the RAG application, expecting a well cited answer to their question based on results from the source document. In addition to accuracy of the answer, the end-user expects that any citations provided by the application make it clear if it was from verbatim content in a file from the data source or if it was based off say an AI powered summary of content from the file.
-* RAG application developer/search index admin: This persona is responsible for configuring the search index schema, and setting up the indexer and skillset to ingest language model augmented data into the index. GenAI Prompt custom skill allows developers to configure free-form prompts to several models hosted in AI foundry, thereby offering significant flexibility to light up various scenarios. However, developers need to ensure that the combination of data + skill configuration used in the pipeline doesn't produce harmful or unsafe content. Developers also need to evaluate the content generated by the language models for bias, inaccuracies, and incorrect information. This becomes particularly challenging to do for documents at a large scale and should be one of the first steps when building a RAG application, along with the index schema definition.
-* Data authority: This persona is expected to be the key subject matter expert (SME) for the content from the data source. The SME is expected to be the best judge of language model powered enrichments ingested into the index and the answer generated by the language model in the RAG application. The key role for the data authority to be able to get a representative sample and verify the quality of the enrichments and the answer, which can be challenging if dealing with data at large scale.
+| Persona | Description |
+|---------|-------------|
+| End user | The person asking questions of the RAG application, expecting a well-cited answer to their question based on results from the source document. In addition to accuracy of the answer, the end-user expects that any citations provided by the application make it clear if it was from verbatim content from a source file or an AI-powered summary from the model. |
+| RAG application developer/search index admin | The person responsible for configuring the search index schema, and setting up the indexer and skillset to ingest language model augmented data into the index. GenAI Prompt custom skill allows developers to configure free-form prompts to several models hosted in AI foundry, thereby offering significant flexibility to light up various scenarios. However, developers need to ensure that the combination of data and skills used in the pipeline doesn't produce harmful or unsafe content. Developers also need to evaluate the content generated by the language models for bias, inaccuracies, and incorrect information. Although this task can be challenging for documents at a large scale, it should be one of the first steps when building a RAG application, along with the index schema definition. |
+| Data authority | The person expected to be the key subject matter expert (SME) for the content from the data source. The SME is expected to be the best judge of language model powered enrichments ingested into the index and the answer generated by the language model in the RAG application. The key role for the data authority to be able to get a representative sample and verify the quality of the enrichments and the answer, which can be challenging if dealing with data at large scale. |
 
 The rest of this document lists out these various challenges along with tips and best practices that RAG application developers can follow to mitigate any risks.
 
 ## Challenges
 
-The following are the key challenges faced by the various personas that interact with a RAG systems that utilize language models to augment content ingested into a search index (using the GenAI Prompt custom skill) and to formulate answers for questions:
+The following challenges are faced by the various personas that interact with a RAG systems that utilize language models to augment content ingested into a search index (using the GenAI Prompt custom skill) and to formulate answers for questions:
+
+* Transparency: Users of RAG systems should understand that AI models might not always produce accurate or well-formulated answers. Azure AI Search has a robustly documented [Transparency Note](/legal/search/transparency-note) that developers should read through to understand the various ways in which AI is used to augment the capabilities of the core search engine. It's recommended that developers who build RAG applications share the transparency note to users of their applications, since they might be unaware of how AI interfaces with various aspects of the application being used. Additionally, when utilizing the GenAI Prompt custom skill developers should note that only part of the content ingested into the search index is generated by the language model and should highlight this to users of their applications.
 
-* Transparency: Users of RAG systems should understand that many the system is powered by AI models that might not always be accurate in the content ingested or the answer formulated. Azure AI Search has a robustly documented [Transparency Note](/legal/search/transparency-note) that developers should read through to understand the various ways in which AI is used to augment the capabilities of the core search engine. It's recommended that developers who build RAG applications share the transparency note to users of their applications, since they might be unaware of how AI interfaces with various aspects of the application being used. Additionally, when utilizing the GenAI Prompt custom skill developers should note that only part of the content ingested into the search index is generated by the language model and should highlight this to users of their applications.
 * Content sampling/inspection of content quality: Developers and data SMEs should consider sampling some of the content ingested into the search index after being augmented by the GenAI Prompt custom skill in order to inspect the quality of the enrichment performed by their language model. [Debug sessions](cognitive-search-debug-session.md) and [search explorer](search-explorer.md) on the Azure portal can be used for this purpose.
-* Content safety filtering and evaluations: It's important for developers to ensure that the language models they use with the GenAI Prompt custom skill have appropriate filters to ensure safety of the content generated and after ingested into the search index. Developers and data SMEs should also make sure they evaluate the content generated by the language model on various metrics such as accuracy, task specific performance, bias, and risk. Azure AI Foundry offers a robust set of tools for developers to add [content safety filters](../ai-foundry/ai-services/content-safety-overview.md) and [clear guidance for evaluation approaches](../ai-foundry/concepts/evaluation-approach-gen-ai.md)
-* Being agile in rolling back changes or modifying skill configuration: It's possible for the language model that is used with the GenAI Prompt custom skill to have issues over time (such as producing low-quality content). Developers should be prepared to roll back these changes either by altering their indexer and skillset configuration or by excluding index fields with AI generated content from search queries.
+
+* Content safety filtering and evaluations: It's important for developers to ensure that the language models they use with the GenAI Prompt custom skill have appropriate filters to ensure safety of the content generated and after ingested into the search index. Developers and data SMEs should also make sure they evaluate the content generated by the language model on various metrics such as accuracy, task specific performance, bias, and risk. Azure AI Foundry offers a robust set of tools for developers to add [content safety filters](../ai-foundry/ai-services/content-safety-overview.md) and [clear guidance for evaluation approaches](../ai-foundry/concepts/evaluation-approach-gen-ai.md).
+
+* Agility in rolling back changes or modifying skill configuration: It's possible for the language model used with the GenAI Prompt custom skill to have issues over time (such as producing low-quality content). Developers should be prepared to roll back these changes either by altering their indexer and skillset configuration or by excluding index fields with AI generated content from search queries.
 
 ## Best practices to mitigate risks
 
 When utilizing the GenAI Prompt custom skill to power RAG applications, there's a risk of over-reliance on AI as outlined in the challenges from the previous section. In this part of the document, we present some patterns and strategies to use to mitigate the risks and overcome the challenges.
 
 ### Content sampling and inspection before ingestion into the search index
 
-[Debug sessions](cognitive-search-debug-session.md) is an Azure AI Search feature available to customers who utilize the Azure portal to inspect the state of enrichment for a single document. To utilize a debug session, Azure AI Search customers need to create a skillset, and an indexer and have the indexer complete one run. We recommend customers that have an indexer utilizing the GenAI Prompt custom skill to initially ingest content into a "development" index - such an indexer can be used with a debug session to inspect the entire structure and contents of the enriched document that will be written into the index. A single run of a debug session works with one specific live document, and will have the content generated by the language model show up in a specific part of the enriched document. Developers can utilize several runs of their debug session, pointing to different documents from their data source to get a reasonable idea of the state of the content produced by their language model (and its relationship to the enriched document structure). The images below show how developers can inspect both the configuration of a skill and the values produced by the skill after calling the language model.
+[Debug sessions](cognitive-search-debug-session.md) is a tool built into the Azure portal. You can use it to inspect the state of enrichment for a single document. To start a debug session, create a skillset, and an indexer and have the indexer complete one run. We recommend that you begin with a "development" index before moving forward with solution. While the index is in development, use a debug session to inspect the entire structure and contents of the enriched document that will be written into the index. A single run of a debug session works with one specific live document, and will have the content generated by the language model show up in a specific part of the enriched document. Developers can utilize several runs of their debug session, pointing to different documents from their data source to get a reasonable idea of the state of the content produced by their language model (and its relationship to the enriched document structure).
 
-#### Inspecting the configuration of the GenAI Prompt skill
+ The screenshots below show how developers can inspect both the configuration of a skill and the values produced by the skill after calling the language model.
 
-[ ![Screenshot of a debug session showing the GenAI Prompt custom skill configuration.](./media/responsible-ai-practices-genai-prompt-skill/debug-session-skill-inspection.png) ](./media/responsible-ai-practices-genai-prompt-skill/debug-session-skill-inspection.png#lightbox)
+#### Example: Inspect the configuration of the GenAI Prompt skill
 
+[ ![Screenshot of a debug session showing the GenAI Prompt custom skill configuration.](./media/responsible-ai-practices-genai-prompt-skill/debug-session-skill-inspection.png) ](./media/responsible-ai-practices-genai-prompt-skill/debug-session-skill-inspection.png#lightbox)
 
-#### Inspecting the output from the GenAI Prompt skill
+#### Example: Inspect the output from the GenAI Prompt skill
 
 [ ![Screenshot of a debug session showing the GenAI Prompt custom skill output from the language model.](./media/responsible-ai-practices-genai-prompt-skill/debug-session-skill-output-inspection.png) ](./media/responsible-ai-practices-genai-prompt-skill/debug-session-skill-output-inspection.png#lightbox)
 
+#### Use Search Explorer to inspect output
+
 In addition to debug sessions, Azure AI Search also offers the ability to explore multiple documents at once by querying the search index via the Azure portal [search explorer](search-explorer.md). Developers can issue a broad query to retrieve a large number of documents from their search index and can inspect the fields which have their content generated by the GenAI Prompt custom skill. To be able to view the contents of the field, when defining the index schema it needs to be configured with the "Retrievable" property. For the same document that was inspected via the debug session, the image below shows the full contents of the search document that ends up into the index.
 
 [ ![Screenshot of search explorer page showing the full state of an indexed document.](./media/responsible-ai-practices-genai-prompt-skill/search-explorer-inspect-document.png) ](./media/responsible-ai-practices-genai-prompt-skill/search-explorer-inspect-document.png#lightbox)
@@ -79,15 +87,14 @@ The previous two sections stressed the importance for developers to have a "deve
 
 Once the evaluation in the development environment is satisfactory, developers should transition the ingestion process to a production environment, where the indexer operates on the full customer data. However, it's possible for there to be unexpected drops in quality or performance when operating on this data set. It's also possible for the model to be updated without undergoing evaluation in the development environment - both these cases can result in a suboptimal experience for users interacting with RAG applications, and developers need to be agile in detecting and mitigating such conditions. To catch such situations, developers should ensure that they also have a constant monitoring of their "production" index and be ready to modify configurations as needed. The following sections describe some patterns developers could adopt to be responsive to such scenarios.
 
-#### Primary-Seconday index powering RAG applications
+#### Primary-Secondary index powering RAG applications
 
 Developers should consider having a primary and a secondary index to power their RAG applications. The primary and secondary indexes would be similar in the configuration of fields - the only difference would be that the primary index will have an extra (searchable and retrievable) field which will contain content generated from the language model through the GenAI Prompt custom skill. Developers should configure their RAG applications such that the AI model being augmented can use either the primary or the secondary index as it's knowledge source. The primary index should be preferred, but if the quality of the results produced by the RAG application seems to be adversely impacted, the application should swap to using the secondary index which doesn't have generated content as part of the knowledge source. This can be achieved without needing any code change/redeployment of the RAG app by utilizing the [index alias feature](search-how-to-alias.md) and having the RAG application query the alias, and then swapping the indexes that map to the alias if necessary.
 
 The following diagram illustrates this pattern.
 
 [ ![Diagram depicting the primary-secondary index pattern.](./media/responsible-ai-practices-genai-prompt-skill/fallback-index-pattern.png) ](./media/responsible-ai-practices-genai-prompt-skill/fallback-index-pattern.png#lightbox)
 
-
 #### Dropping use of generated field in search queries
 
 A lighter weight alternative to having two copies of the search index, is to ensure that the RAG application can modify the search query issued to Azure AI Search easily. By default when a search query is issued, all searchable fields are scanned, however Azure AI Search allows specifying which fields must be analyzed to produce a set of search results. 
@@ -104,6 +111,7 @@ POST https://[service-name].search.windows.net/indexes/[index-name]?api-version=
     "queryType": "full"  
 } 
 ```
+
 The RAG application can fall back to this specific query (might require a code change/redeployment), if the default query starts to degrade in performance or evaluation metrics, illustrated by the following diagram.
 
 [ ![Diagram depicting the fall back query pattern.](./media/responsible-ai-practices-genai-prompt-skill/fallback-query-pattern.png) ](./media/responsible-ai-practices-genai-prompt-skill/fallback-query-pattern.png#lightbox)
@@ -128,6 +136,6 @@ Given the scale of data ingestion, it might not be feasible to have a human in t
 
 ## Learn more about Azure AI Search
 
-*   [Introduction to Azure AI Search](search-what-is-azure-search.md)
-*   [AI enrichment concepts](cognitive-search-concept-intro.md)
-*   [Retrieval Augmented Generation (RAG) in Azure AI Search](retrieval-augmented-generation-overview.md)
\ No newline at end of file
+* [Introduction to Azure AI Search](search-what-is-azure-search.md)
+* [AI enrichment concepts](cognitive-search-concept-intro.md)
+* [Retrieval Augmented Generation (RAG) in Azure AI Search](retrieval-augmented-generation-overview.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新负责任的 AI 最佳实践文档"
}
```

### Explanation
此次代码差异展示了对文档`responsible-ai-best-practices-genai-prompt-skill.md`的轻微更新，旨在优化内容结构和确保信息更加清晰易懂。

文档中添加了角色（Personas）的描述，并通过表格的形式呈现，包括终端用户、RAG应用开发者/搜索索引管理员和数据权威等。这种重新组织的信息架构，提供了更直观的界面，使读者能够更好地理解每种角色在RAG应用中的不同挑战和职责。

在挑战部分，强调了透明性、内容质量检查和内容安全过滤等主题，确保开发者在使用GenAI Prompt自定义技能时认识到潜在的风险。在文本的更新中，对内容的描述进行了精简，并补充了有关如何进行调试和内容检查的具体建议。

此外，引入了新的最佳实践建议，包括如何在进行数据摄取时进行内容抽样和检查，以确保信息的质量。文档中强调了在生产环境中的敏捷能力，建议开发者采取主-次索引的模式以便于在结果不理想时迅速调整。

这些变化提升了文档的实用性，帮助开发者在构建负责任的AI应用时，能够更合理地管理和评估生成的内容质量，确保合规和安全性。总体来说，这次修改增强了用户对方法论的理解，并提供了具体的可操作建议。

## articles/search/search-get-started-portal-image-search.md{#item-438b9b}

<details>
<summary>Diff</summary>
````diff
@@ -465,7 +465,7 @@ This quickstart uses billable Azure resources. If you no longer need the resourc
 
 This quickstart introduced you to the **Import and vectorize data** wizard, which creates all of the necessary objects for multimodal search. To explore each step in detail, see the following tutorials:
 
-+ [Tutorial: Image verbalization and Document Extraction skill](tutorial-document-extraction-image-verbalization.md)
-+ [Tutorial: Image verbalization and Document Layout skill](tutorial-document-layout-image-verbalization.md)
-+ [Tutorial: Multimodal embeddings and Document Extraction skill](tutorial-document-extraction-multimodal-embeddings.md)
-+ [Tutorial: Multimodal embeddings and Document Layout skill](tutorial-document-layout-multimodal-embeddings.md)
++ [Tutorial: Verbalize images using generative AI](tutorial-document-extraction-image-verbalization.md)
++ [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md)
++ [Tutorial: Vectorize images and text](tutorial-document-extraction-multimodal-embeddings.md)
++ [Tutorial: Vectorize from a structured document layout](tutorial-document-layout-multimodal-embeddings.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新开始使用门户图像搜索的文档"
}
```

### Explanation
该代码差异显示了对文档`search-get-started-portal-image-search.md`的轻微更新，具体改变了教程的标题以提高其准确性和一致性。

在此次更新中，四个教程的标题进行了重新命名，从原来的“Image verbalization and Document Extraction skill”和“Multimodal embeddings and Document Extraction skill”等，修改为“Verbalize images using generative AI”和“Vectorize images and text”等。这种变更不仅反映了更清晰的功能描述，而且也使得与生成式AI和多模态数据的整合逻辑更加契合。

这些修改旨在明确教程内容的具体主题，以便用户能够更快速、更有效地找到与其需求相匹配的指导材料。这对于初次接触多模态搜索的用户尤其重要，可以帮助他们更顺利地理解相关操作流程并克服学习曲线。总体来说，这次小幅更新增强了文档的可读性和实用性。

## articles/search/search-relevance-overview.md{#item-cb0e09}

<details>
<summary>Diff</summary>
````diff
@@ -13,14 +13,19 @@ ms.date: 07/23/2025
 
 # Relevance in Azure AI Search
 
-In a query operation, the relevance of any given result is determined by a ranking algorithm that evaluates the strength of a match based on how closely the indexed content and the query align. An algorithm assigns a score, and results are ranked by that score and returned in the response. 
+In a query operation, the relevance of any given result is determined by a ranking algorithm that evaluates the strength of a match based on how closely the query corresponds to an indexed document. When a match is found, an algorithm assigns a score, and results are ranked by that score and the topmost results are returned in the response. 
 
 Ranking occurs whenever the query request includes full text or vector queries. It doesn't occur if the query invokes strict pattern matching, such as a filter-only query or a specialized query form like autocomplete, suggestions, geospatial search, fuzzy search, or regular expression search. A uniform search score of 1.0 indicates the absence of a ranking algorithm.
 
-***Relevance tuning*** can be used to boost search scores based on extra criteria such as freshness or proximity. In Azure AI Search, relevance tuning is primarily directed at textual and numeric (nonvector) content when you apply a [scoring profile](#custom-boosting-logic-using-scoring-profiles) or invoke the [semantic ranker](semantic-search-overview.md). 
+## Relevance tuning
 
-> [!NOTE]
-> In Azure AI Search, there's no explicit relevance tuning capabilities for vector content, but you can experiment between Hierarchical Navigable Small World (HNSW) and exhaustive K-nearest neighbors (KNN) to see if one algorithm outperforms the other for your scenario. HNSW graphing with an exhaustive KNN override at query time is the most flexible approach for comparison testing. You can also experiment with various embedding models to see which ones produce higher quality results.
+***Relevance tuning*** is a technique for boosting search scores based on extra criteria such as weighted fields, freshness, or proximity. In Azure AI Search, relevance tuning options vary based on query type:
+
++ For textual and numeric (nonvector) content in keyword or hybrid search, you can tune relevance through [scoring profiles](#custom-boosting-logic-using-scoring-profiles) or invoking the [semantic ranker](semantic-search-overview.md).
+
++ For vector content in a hybrid query, you can [weight a vector field](hybrid-search-ranking.md#weighted-scores) to boost the importance of the vector component relative to the text component of the hybrid query.
+
++ For pure vector queries, you can experiment between Hierarchical Navigable Small World (HNSW) and exhaustive K-nearest neighbors (KNN) to see if one algorithm outperforms the other for your scenario. HNSW graphing with an exhaustive KNN override at query time is the most flexible approach for comparison testing. You can also experiment with various embedding models to see which ones produce higher quality results. Finally, remember that a hybrid query or a vector query on documents that include nonvector fields are in-scope for relevance tuning, so it's just the vector fields themselves that can't participate in a relevance tuning effort.
 
 ## Levels of ranking
 
@@ -42,7 +47,7 @@ Scoring logic applies to text and numeric nonvector content. You can use scoring
 
 + [Text (keyword) search](search-query-create.md)
 + [Pure vector queries](vector-search-how-to-query.md)
-+ [Hybrid queries](hybrid-search-how-to-query.md), with text and vector subqueries execute in parallel
++ [Hybrid queries](hybrid-search-how-to-query.md), where text and vector subqueries execute in parallel
 + [Semantically ranked queries](semantic-how-to-query-request.md)
 
 For standalone text queries, scoring profiles identify the top 1,000 matches in a [BM25-ranked search](index-similarity-and-scoring.md), with the top 50 matches returned in the response.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新Azure AI搜索相关性概述文档"
}
```

### Explanation
此次代码差异展示了对文档`search-relevance-overview.md`的轻微更新，以确保内容更加准确和易于理解。

在更新中，文档对相关性如何影响查询结果的描述进行了精炼和澄清。具体来说，将“query corresponds to an indexed document”改为“query corresponds to an indexed document”增强了表述的精确性，并明确了查询与索引文档之间的关系。此外，文中引入了一个新的小节“相关性调优”（Relevance tuning），详细介绍了如何在Azure AI Search中根据文本和数字内容的查询类型进行调整。

更新内容中还增加了针对向量数据的相关性调优说明，包括如何通过加权向量字段来提高相关性，以及针对算法（如HNSW和KNN）进行实验的建议。这些补充信息使用户在面对不同类型的查询时能够更好地理解如何优化搜索性能。

此外，文档结构进行了小幅调整，提高了可读性并便于用户快速定位信息。整体来看，这次轻微更新增加了文档的实用性，使得开发者能够更有效地应用Azure AI Search的相关性调优功能。

## articles/search/toc.yml{#item-c4768f}

<details>
<summary>Diff</summary>
````diff
@@ -138,14 +138,14 @@ items:
       href: tutorial-adls-gen2-indexer-acls.md
   - name: Multimodal indexing tutorials
     items:
-    - name: Use document extraction and multimodal embeddings
+    - name: Vectorize images and text
       href: tutorial-document-extraction-multimodal-embeddings.md
-    - name: Use document extraction and image verbalizations
-      href: tutorial-document-extraction-image-verbalization.md
-    - name: Use semantic chunking and multimodal embeddings
+    - name: Vectorize from a structured document layout
       href: tutorial-document-layout-multimodal-embeddings.md
-    - name: Use semantic chunking and image verbalizations
-      href: tutorial-document-layout-image-verbalization.md      
+    - name: Verbalize images using generative AI
+      href: tutorial-document-extraction-image-verbalization.md
+    - name: Verbalize images from a structured document layout
+      href: tutorial-document-layout-image-verbalization.md
   - name: RAG tutorials
     items:
     - name: Build a RAG solution
@@ -364,22 +364,28 @@ items:
         href: cognitive-search-output-field-mapping.md
       - name: Process image files
         href: cognitive-search-concept-image-scenarios.md
-      - name: Configure an enrichment cache
-        href: enrichment-cache-how-to-configure.md
-      - name: Manage an enrichment cache
-        href: enrichment-cache-how-to-manage.md
-      - name: Best practices - GenAI Prompt skill
-        href: responsible-ai-best-practices-genai-prompt-skill.md
-      - name: GenAI Prompt Skill - Example Usage Guide
-        href: chat-completion-skill-example-usage.md
+      - name: Enrichment cache
+        items:
+        - name: Configure an enrichment cache
+          href: enrichment-cache-how-to-configure.md
+        - name: Manage an enrichment cache
+          href: enrichment-cache-how-to-manage.md
+      - name: Generative AI skills
+        items:
+        - name: Add AI-generated content (GenAI Prompt skill)
+          href: chat-completion-skill-example-usage.md
+        - name: Best practices using GenAI Prompt skill
+          href: responsible-ai-best-practices-genai-prompt-skill.md
       - name: Custom skills
         items:
-        - name: Integrate custom skills
+        - name: Add custom skills
           href: cognitive-search-custom-skill-interface.md
         - name: Scale out custom skills
           href: cognitive-search-custom-skill-scale.md
         - name: Example - Bing Entity Search
           href: cognitive-search-create-custom-skill-example.md
+        - name: Azure AI Search Power Skills
+          href: https://github.com/Azure-Samples/azure-search-power-skills
   - name: Retrieval
     items:
     - name: Agentic retrieval
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新搜索文档的目录结构"
}
```

### Explanation
此次代码差异显示了对`toc.yml`文件的更新，涉及到搜索文档的目录结构的调整和内容的扩充。

在此次更新中，多个教程的名称和链接进行了修改与优化。例如，“Use document extraction and multimodal embeddings”被更名为“Vectorize images and text”，同时几个新教程被引入，如“Vectorize from a structured document layout”和“Verbalize images using generative AI”。这样的变动使得目录更加直观和易于理解，有助于用户快速找到所需的资源。

同时，原有的关于“enrichment cache”的部分被重新组织，采用了层次结构，使得相关信息更为清晰。此外，新增的“Generative AI skills”小节包含了关于生成式AI提示技能的最佳实践以及使用示例，进一步丰富了文档的内容。

整体而言，此次修改使得文档目录更具结构性和可读性，帮助用户更好地导航并访问各类资料，同时也反映出对最新技术的适应和内容的持续更新。

## articles/search/tutorial-document-extraction-image-verbalization.md{#item-398a90}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Use Image Verbalization and Document Extraction Skill for Multimodal Indexing'
+title: 'Tutorial: Verbalize images using generative AI'
 titleSuffix: Azure AI Search
 description: Learn how to extract, index, and search multimodal content using the Document Extraction skill for chunking and GenAI Prompt skill for image verbalizations.
 
@@ -14,49 +14,42 @@ ms.date: 05/29/2025
 
 ---
 
-# Tutorial: Index mixed content using image verbalizations and the Document Extraction skill
+# Tutorial: Verbalize images using generative AI
 
-Azure AI Search can extract and index both text and images from PDF documents stored in Azure Blob Storage. This tutorial shows you how to build a multimodal indexing pipeline by describing visual content in natural language and embedding it alongside document text.
+Azure AI Search can extract and index both text and images from PDF documents stored in Azure Blob Storage. This tutorial shows you how to build a multimodal indexing pipeline that includes steps for describing visual content in natural language and using the generated descriptions in your searchable index.
 
-From the source document, each image is passed to the [GenAI Prompt skill (preview)](cognitive-search-skill-genai-prompt.md) to generate a concise textual description. These descriptions, along with the original document text, are then embedded into vector representations using Azure OpenAI’s text-embedding-3-large model. The result is a single index containing semantically searchable content from both modalities: text and verbalized images.
+From the source document, each image is passed to the [GenAI Prompt skill (preview)](cognitive-search-skill-genai-prompt.md) that calls a chat completion model to generate a concise textual description. These descriptions, along with the original document text, are then embedded into vector representations using Azure OpenAI’s text-embedding-3-large model. The result is a single index containing semantically searchable content from both modalities: text and verbalized images.
 
 In this tutorial, you use:
 
 + A 36-page PDF document that combines rich visual content, such as charts, infographics, and scanned pages, with traditional text.
 
-+ The [Document Extraction skill](cognitive-search-skill-document-extraction.md) for extracting normalized images and text.
++ An indexer and skillset to create an indexing pipeline that includes AI enrichment through skills.
 
-+ The [GenAI Prompt skill (preview)](cognitive-search-skill-genai-prompt.md) to generate image captions, which are text-based descriptions of visual content, for search and grounding.
++ The [Document Extraction skill](cognitive-search-skill-document-extraction.md) for extracting normalized images and text.
 
-+ A search index configured to store text and image embeddings and support for vector-based similarity search.
++ The [GenAI Prompt skill (preview)](cognitive-search-skill-genai-prompt.md) that calls a chat completion model to create descriptions of visual content.
 
-This tutorial demonstrates a lower-cost approach for indexing multimodal content using Document Extraction skill and image captioning. It enables extraction and search over both text and images from documents in Azure Blob Storage. However, it doesn't include locational metadata for text, such as page numbers or bounding regions.
++ A search index configured to store text and image verbalizations.
 
-For a more comprehensive solution that includes structured text layout and spatial metadata, see [Indexing blobs with text and images for multimodal RAG scenarios using image verbalization and Document Layout skill](tutorial-document-layout-image-verbalization.md).
+This tutorial demonstrates a lower-cost approach for indexing multimodal content using the Document Extraction skill and image captioning. It enables extraction and search over both text and images from documents in Azure Blob Storage. However, it doesn't include locational metadata for text, such as page numbers or bounding regions. For a more comprehensive solution that includes structured text layout and spatial metadata, see [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md).
 
 > [!NOTE]
-> Setting `imageAction` to `generateNormalizedImages` is required for this tutorial and incurs an additional charge for image extraction according to [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/).
-
-Using a REST client and the [Search REST APIs](/rest/api/searchservice/) you will:
-
-> [!div class="checklist"]
-> + Set up sample data and configure an `azureblob` data source
-> + Create an index with support for text and image embeddings
-> + Define a skillset with extraction, captioning, and embedding steps
-> + Create and run an indexer to process and index content
-> + Search the index you just created
+> Setting `imageAction` to `generateNormalizedImages` results in image extraction, which is an extra charge. For more information, see [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/) for image extraction.
 
 ## Prerequisites
 
-+ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
++ [Azure AI Search](search-create-service-portal.md). [Configure your search service](search-manage.md) for role-based access control and a managed identity. Your service must be on the Basic tier or higher. This tutorial isn't supported on the Free tier. It must also be in the same region as your multi-service account.
 
-+ [Azure Storage](/azure/storage/common/storage-account-create).
++ [Azure Storage](/azure/storage/common/storage-account-create), used for storing sample data and for creating a [knowledge store](knowledge-store-concept-intro.md).
 
-+ [Azure AI Search](search-what-is-azure-search.md), Basic pricing tier or higher, with a managed identity. [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.  
++ A chat completion model hosted in Azure AI Foundry or another source. The model is used to verbalize image content. You provide the URI to the hosted model in the GenAI Prompt skill definition.
+
++ A text embedding model deployed in Azure AI Foundry. The model is used to vectorize text content pull from source documents and the image descriptions generated by the chat completion model. For integrated vectorization, the embedding model must be located in Azure AI Foundry, and it must be either text-embedding-ada-002, text-embedding-3-large, or text-embedding-3-small. If you want to use an external embedding model, use a custom skill instead of the Azure OpenAI embedding skill.
 
 + [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
-### Download files
+## Prepare data
 
 Download the following sample PDF:
 
@@ -68,7 +61,7 @@ Download the following sample PDF:
 
 1. [Upload the sample data file](/azure/storage/blobs/storage-quickstart-blobs-portal).
 
-1. [Create a role assignment in Azure Storage and Specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
+1. [Create a **Storage Blob Data Reader** role assignment and specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
 
 1. For connections made using a system-assigned managed identity. Provide a connection string that contains a ResourceId, with no account key or password. The ResourceId must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name. The connection string is similar to the following example:
 
@@ -77,6 +70,7 @@ Download the following sample PDF:
         "connectionString" : "ResourceId=/subscriptions/00000000-0000-0000-0000-00000000/resourceGroups/MY-DEMO-RESOURCE-GROUP/providers/Microsoft.Storage/storageAccounts/MY-DEMO-STORAGE-ACCOUNT/;" 
     }
     ```
+
 1. For connections made using a user-assigned managed identity. Provide a connection string that contains a ResourceId, with no account key or password. The ResourceId must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name. Provide an identity using the syntax shown in the following example. Set userAssignedIdentity to the user-assigned managed identity The connection string is similar to the following example:
 
     ```json
@@ -339,7 +333,9 @@ Key points:
 
 ## Create a skillset
 
-[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a search index on your search service. An index specifies all the parameters and their attributes.
+[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a skillset on your search service. A skillset defines the operations that chunk and embed content prior to indexing. This skillset uses the built-in Document Extraction skill to extract text and images. It uses Text Split skill to chunk large text. It uses Azure OpenAI Embedding skill to vectorize text content.
+
+The skillset also performs actions specific to images. It uses the GenAI Prompt skill to generate image descriptions. It also creates a knowledge store that stores intact images so that you can return them in a query.
 
 ```http
 ### Create a skillset
@@ -354,7 +350,7 @@ POST {{baseUrl}}/skillsets?api-version=2025-05-01-preview   HTTP/1.1
     {
       "@odata.type": "#Microsoft.Skills.Util.DocumentExtractionSkill",
       "name": "document-extraction-skill",
-      "description": "Document extraction skill to exract text and images from documents",
+      "description": "Document extraction skill to extract text and images from documents",
       "parsingMode": "default",
       "dataToExtract": "contentAndMetadata",
       "configuration": {
@@ -458,7 +454,7 @@ POST {{baseUrl}}/skillsets?api-version=2025-05-01-preview   HTTP/1.1
     },    
     {
     "@odata.type": "#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill",
-    "name": "verblized-image-embedding-skill",
+    "name": "verbalized-image-embedding-skill",
     "description": "Embedding skill for verbalized images",
     "context": "/document/normalized_images/*",
     "inputs": [
@@ -752,4 +748,4 @@ Now that you're familiar with a sample implementation of a multimodal indexing s
 * [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md)
 * [Vectors in Azure AI Search](vector-search-overview.md)
 * [Semantic ranking in Azure AI Search](semantic-search-overview.md)
-* [Indexing blobs with text and images for multimodal RAG scenarios using image verbalization and Document Layout skill](tutorial-document-layout-image-verbalization.md)
+* [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新文档提取和图像语言化教程"
}
```

### Explanation
此次代码差异反映了对`tutorial-document-extraction-image-verbalization.md`文档的轻微更新，主要目的是提高内容的准确性和可读性，增强用户体验。

在标题方面，文档从“使用图像语言化和文档提取技能进行多模态索引的教程”更改为“使用生成式AI进行图像语言化的教程”，更强化了文档的主题。此外，文档内容中关于如何使用Azure AI Search从PDF文档中提取和索引文本与图像的说明也进行了精简，并对描述的自然语言生成进行了更详细的阐述。

更新后的文档明确了在构建多模态索引管道时所需的步骤和工具，例如强调生成图像描述所使用的聊天完成模型的作用。此外，新增了关于如何使用技能集中的AI增强选项的细节，提供了更强的指导。

此外，文档中对先决条件的描述也进行了优化，明确了Azure AI Search服务的配置要求，以及必须使用的模型和环境，确保读者能够顺利实施教程中的步骤。

整体而言，这些改动使得文档结构更加清晰，信息更加准确，用户能更有效地应用这些教程进行多模态内容的索引和搜索。

## articles/search/tutorial-document-extraction-multimodal-embeddings.md{#item-a3db59}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Use Multimodal Embeddings and Document Extraction Skill for Multimodal Indexing'
+title: 'Tutorial: Vectorize images and text'
 titleSuffix: Azure AI Search
 description: Learn how to extract, index, and search multimodal content using the Document Extraction skill for chunking and Azure AI Vision for embeddings.
 
@@ -13,51 +13,39 @@ ms.topic: tutorial
 ms.date: 06/11/2025
 
 ---
-
-# Tutorial: Index mixed content using multimodal embeddings and the Document Extraction skill
+<!-- # Tutorial: Index mixed content using multimodal embeddings and the Document Extraction skill -->
+# Tutorial: Vectorize images and text
 
 Azure AI Search can extract and index both text and images from PDF documents stored in Azure Blob Storage. This tutorial shows you how to build a multimodal indexing pipeline by embedding both text and images into a unified semantic search index.
 
 In this tutorial, you use:
 
 + A 36-page PDF document that combines rich visual content, such as charts, infographics, and scanned pages, with traditional text.
 
-+ The [Document Extraction skill](cognitive-search-skill-document-extraction.md) for extracting text and normalized images.
++ An indexer and skillset to create an indexing pipeline that includes AI enrichment through skills.
 
-+ Vectorization using the [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md), which generates embeddings for both text and images.
++ The [Document Extraction skill](cognitive-search-skill-document-extraction.md) for extracting normalized images and text.
 
-+ A search index configured to store text and image embeddings and support for vector-based similarity search.
++ The [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) to vectorize text and images.
 
-This tutorial demonstrates a lower-cost approach for indexing multimodal content using Document Extraction skill and image captioning. It enables extraction and search over both text and images from documents in Azure Blob Storage. However, it doesn't include locational metadata for text, such as page numbers or bounding regions.
++ A search index configured to store text and image embeddings and support for vector-based similarity search.
 
-For a more comprehensive solution that includes structured text layout and spatial metadata, see [Indexing blobs with text and images for multimodal RAG scenarios using image verbalization and Document Layout skill](tutorial-document-layout-image-verbalization.md).
+This tutorial demonstrates a lower-cost approach for indexing multimodal content using the Document Extraction skill and image captioning. It enables extraction and search over both text and images from documents in Azure Blob Storage. However, it doesn't include locational metadata for text, such as page numbers or bounding regions. For a more comprehensive solution that includes structured text layout and spatial metadata, see [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md).
 
 > [!NOTE]
-> Setting `imageAction` to `generateNormalizedImages` as is required for this tutorial incurs an additional charge for image extraction according to [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/).
-
-Using a REST client and the [Search REST APIs](/rest/api/searchservice/) you will:
-
-> [!div class="checklist"]
-> + Set up sample data and configure an `azureblob` data source
-> + Create an index with support for text and image embeddings
-> + Define a skillset with extraction and embedding steps
-> + Create and run an indexer to process and index content
-> + Search the index you just created
+> Setting `imageAction` to `generateNormalizedImages` results in image extraction, which is an extra charge. For more information, see [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/) for image extraction.
 
 ## Prerequisites
 
-+ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
-
-+ [Azure Storage](/azure/storage/common/storage-account-create).
++ [Azure AI Search](search-create-service-portal.md). [Configure your search service](search-manage.md) for role-based access control and a managed identity. Your service must be on the Basic tier or higher. This tutorial isn't supported on the Free tier. It must also be in the same region as your multi-service account.
 
-+ An [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) for image vectorization. Image vectorization requires Azure AI Vision multimodal embeddings. For an updated list of regions, see the [Azure AI Vision documentation](/azure/ai-services/computer-vision/overview-image-analysis#region-availability).
++ [Azure Storage](/azure/storage/common/storage-account-create), used for storing sample data and for creating a [knowledge store](knowledge-store-concept-intro.md).
 
-+ [Azure AI Search](search-what-is-azure-search.md), with a managed identity. [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.  
-  > Your service must be on the Basic tier or higher—this tutorial isn't supported on the Free tier. It must also be in the same region as your multi-service account.
++ An [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) that provides Azure AI Vision for multimodal embeddings. You must use an Azure AI multi-service account for this task. For an updated list of regions that provide multimodal embeddings, see the [Azure AI Vision documentation](/azure/ai-services/computer-vision/overview-image-analysis#region-availability).
 
 + [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
-### Download files
+## Prepare data
 
 Download the following sample PDF:
 
@@ -69,7 +57,7 @@ Download the following sample PDF:
 
 1. [Upload the sample data file](/azure/storage/blobs/storage-quickstart-blobs-portal).
 
-1. [Create a role assignment in Azure Storage and Specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
+1. [Create a **Storage Blob Data Reader** role assignment and specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
 
 1. For connections made using a system-assigned managed identity. Provide a connection string that contains a ResourceId, with no account key or password. The ResourceId must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name. The connection string is similar to the following example:
 
@@ -339,7 +327,7 @@ Key points:
 
 ## Create a skillset
 
-[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a search index on your search service. An index specifies all the parameters and their attributes.
+[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a skillset on your search service. A skillset defines the operations that chunk and embed content prior to indexing. This skillset uses the built-in Document Extraction skill to extract text and images. It uses Text Split skill to chunk large text. It uses Azure AI Vision multimodal embeddings skill to vectorize image and text content.
 
 ```http
 ### Create a skillset
@@ -354,7 +342,7 @@ POST {{baseUrl}}/skillsets?api-version=2025-05-01-preview   HTTP/1.1
     {
       "@odata.type": "#Microsoft.Skills.Util.DocumentExtractionSkill",
       "name": "document-extraction-skill",
-      "description": "Document extraction skill to exract text and images from documents",
+      "description": "Document extraction skill to extract text and images from documents",
       "parsingMode": "default",
       "dataToExtract": "contentAndMetadata",
       "configuration": {
@@ -712,4 +700,4 @@ Now that you're familiar with a sample implementation of a multimodal indexing s
 * [AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md)
 * [Vectors in Azure AI Search](vector-search-overview.md)
 * [Semantic ranking in Azure AI Search](semantic-search-overview.md)
-* [Indexing blobs with text and images for multimodal RAG scenarios using image verbalization and Document Layout skill](tutorial-document-layout-image-verbalization.md)
+* [Tutorial: Verbalize images from a structured document layout](tutorial-document-layout-image-verbalization.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新多模态嵌入和文档提取教程"
}
```

### Explanation
此次代码差异展示了对`tutorial-document-extraction-multimodal-embeddings.md`文档的轻微更新，目的是提升内容清晰度和准确性，同时更好地引导用户进行多模态索引的操作。

文档标题已由“使用多模态嵌入和文档提取技能进行多模态索引的教程”更改为“向量化图像和文本的教程”，更明确地指出了教程的核心主题。此外，文档结构和内容也进行了相应的调整，以优化信息呈现。

在描述中，明确了如何构建多模态索引管道，包括提取文本和图像的步骤，以及使用AI富集过程创建技能集的说明。例如，提到了利用“文档提取技能”提取标准化的图像和文本，并通过“Azure AI Vision 多模态嵌入技能”进行向量化。此更新加强了对工具和流程的深入解释，使得用户在执行过程中更加清晰。

同时，相关链接与步骤也进行了更新，提供了更简洁的操作指导，包括创建角色分配和管理身份的步骤。整体来看，此次修改旨在为用户提供更加流畅和易于理解的教程，使其在索引和搜索多模态内容时更为高效。

## articles/search/tutorial-document-layout-image-verbalization.md{#item-f5de57}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Use Image Verbalization and Document Layout Skill for Multimodal Indexing'
+title: 'Tutorial: Verbalize images from a structured document layout'
 titleSuffix: Azure AI Search
 description: Learn how to extract, index, and search multimodal content using the Document Layout skill for chunking and GenAI Prompt skill for image verbalizations.
 
@@ -14,7 +14,7 @@ ms.date: 05/29/2025
 
 ---
 
-# Tutorial: Index mixed content using image verbalizations and the Document Layout skill
+# Tutorial: Verbalize images from a structured document layout
 
 In this Azure AI Search tutorial, learn how to build a multimodal indexing pipeline that chunks data based on document structure and uses image verbalization to describe images. Cropped images are stored in a knowledge store, and visual content is described in natural language and ingested alongside text in a searchable index.
 
@@ -24,37 +24,31 @@ In this tutorial, you use:
 
 + A 36-page PDF document that combines rich visual content, such as charts, infographics, and scanned pages, with traditional text.
 
-+ The [Document Layout skill (preview)](cognitive-search-skill-document-intelligence-layout.md) for extracting text and normalized images with its locationMetadata from various documents, such as page numbers or bounding regions.
++ An indexer and skillset to create an indexing pipeline that includes AI enrichment through skills.
 
-  The [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) has limited regional availability, is bound to Azure AI services, and requires a [billable resource](cognitive-search-attach-cognitive-services.md) for transactions that exceed 20 documents per indexer per day. For a lower-cost solution to indexing multimodal content, see [Index multimodal content using image verbalization and Document Extraction skill](tutorial-document-extraction-image-verbalization.md).
++ The [Document Layout skill (preview)](cognitive-search-skill-document-intelligence-layout.md) for extracting text and normalized images with its `locationMetadata` from various documents, such as page numbers or bounding regions.
 
-+ The [GenAI Prompt skill (preview)](cognitive-search-skill-genai-prompt.md) to generate image captions, which are text-based descriptions of visual content, for search and grounding.
++ The [GenAI Prompt skill (preview)](cognitive-search-skill-genai-prompt.md) that calls a chat completion model to create descriptions of visual content.
 
-+ A search index configured to store text and image embeddings and support for vector-based similarity search.
++ A search index configured to store extracted text and image verbalizations. Some content is vectorized for vector-based similarity search.
 
-> [!NOTE]
-> Setting `imageAction` to `generateNormalizedImages` is required for this tutorial and incurs an additional charge for image extraction according to [Azure AI Search pricing](https://azure.microsoft.com/pricing/details/search/).
+## Prerequisites
 
-Using a REST client and the [Search REST APIs](/rest/api/searchservice/), you will:
++ [Azure AI Search](search-create-service-portal.md). [Configure your search service](search-manage.md) for role-based access control and a managed identity. Your service must be on the Basic tier or higher. This tutorial isn't supported on the Free tier. It must also be in the same region as your multi-service account.
 
-> [!div class="checklist"]
-> + Set up sample data and configure an `azureblob` data source
-> + Create an index with support for text and image embeddings
-> + Define a skillset with extraction, captioning, embedding and knowleage store file projection steps
-> + Create and run an indexer to process and index content
-> + Search the index you just created
++ [Azure Storage](/azure/storage/common/storage-account-create), used for storing sample data and for creating a [knowledge store](knowledge-store-concept-intro.md).
 
-## Prerequisites
++ A chat completion model hosted in Azure AI Foundry or another source. The model is used to verbalize image content. You provide the URI to the hosted model in the GenAI Prompt skill definition.
 
-+ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
++ A text embedding model deployed in Azure AI Foundry. The model is used to vectorize text content pull from source documents and the image descriptions generated by the chat completion model. For integrated vectorization, the embedding model must be located in Azure AI Foundry, and it must be either text-embedding-ada-002, text-embedding-3-large, or text-embedding-3-small. If you want to use an external embedding model, use a custom skill instead of the Azure OpenAI embedding skill.
 
-+ [Azure Storage](/azure/storage/common/storage-account-create).
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
-+ [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. Your service must be on the Basic tier or higher. This tutorial isn't supported on the Free tier.
+## Limitations
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
+The [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) has limited regional availability, is bound to Azure AI services, and requires a [billable resource](cognitive-search-attach-cognitive-services.md) for transactions that exceed 20 documents per indexer per day. For a lower-cost solution to indexing multimodal content, see [Tutorial: Verbalize images using generative AI](tutorial-document-extraction-image-verbalization.md).
 
-### Download files
+## Prepare data
 
 Download the following sample PDF:
 
@@ -66,7 +60,7 @@ Download the following sample PDF:
 
 1. [Upload the sample data file](/azure/storage/blobs/storage-quickstart-blobs-portal).
 
-1. [Create a role assignment in Azure Storage and Specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
+1. [Create a **Storage Blob Data Reader** role assignment and specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
 
    1. For connections made using a system-assigned managed identity. Provide a connection string that contains a ResourceId, with no account key or password. The ResourceId must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name. The connection string is similar to the following example:
 
@@ -302,7 +296,10 @@ Key points:
 
 ## Create a skillset
 
-[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a search index on your search service. An index specifies all the parameters and their attributes.
+[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a skillset on your search service. A skillset defines the operations that chunk and embed content prior to indexing. This skillset uses the Document Layout skill to extract text and images, preserving location metadata which is useful for citations in RAG applications. It uses Azure OpenAI Embedding skill to vectorize text content.
+
+The skillset also performs actions specific to images. It uses the GenAI Prompt skill to generate image descriptions. It also creates a knowledge store that stores intact images so that you can return them in a query.
+
 
 ```http
 ### Create a skillset
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新文档布局和图像语言化教程"
}
```

### Explanation
此次代码差异显示了对`tutorial-document-layout-image-verbalization.md`文档的轻微更新，旨在提升内容的清晰度和用户体验，特别是针对如何从结构化文档布局中进行图像语言化的指南。

文档标题已更改为“从结构化文档布局中语言化图像的教程”，更加精准地描述了教程内容。整体结构经过优化，强调如何利用文档布局技能创建多模态索引管道，并通过自然语言描述图像。

具体内容上，文档详细介绍了所使用的工具和技能，包括：
- 使用“文档布局技能”提取文本和图像，以及获取相关位置元数据（如页码和边界区域）。
- 使用“GenAI Prompt技能”生成视觉内容的描述，以增强搜索和数据的可用性。

此外，提供了有关如何设置AI富集过程的指导，以及创建和调用技能集的步骤。文档还更新了先决条件部分，说明了所需的Azure服务和资源，包括聊天完成模型和文本嵌入模型的配置方法。

整体而言，这些改动旨在使用户更容易理解和应用教程内容，提升多模态内容的索引和搜索能力。文档中还新增了有关“文档布局技能”的可用性限制信息，确保用户了解相关条件和成本，更好地规划实施方案。

## articles/search/tutorial-document-layout-multimodal-embeddings.md{#item-f67371}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Use Multimodal Embeddings and Document Layout Skill for Multimodal Indexing'
+title: 'Tutorial: Vectorize from a structured document layout'
 titleSuffix: Azure AI Search
 description: Learn how to extract, index, and search multimodal content using the Document Layout skill for chunking and Azure AI Vision for embeddings.
 
@@ -14,7 +14,7 @@ ms.date: 06/11/2025
 
 ---
 
-# Tutorial: Index mixed content using multimodal embeddings and the Document Layout skill
+# Tutorial: Vectorize from a structured document layout
 
 <!-- Multimodal plays an essential role in generative AI apps and the user experience as it enables the extraction of information not only from text but also from complex images embedded within documents.  -->
 In this Azure AI Search tutorial, learn how to build a multimodal indexing pipeline that chunks data based on document structure, and uses a multimodal embedding model to vectorize text and images in a searchable index.
@@ -23,49 +23,41 @@ In this tutorial, you use:
 
 + A 36-page PDF document that combines rich visual content, such as charts, infographics, and scanned pages, with traditional text.
 
-+ The [Document Layout skill (preview)](cognitive-search-skill-document-intelligence-layout.md) for extracting text and normalized images with its locationMetadata from various documents, such as page numbers or bounding regions.
++ An indexer and skillset to create an indexing pipeline that includes AI enrichment through skills.
 
-  The [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) has limited regional availability, is bound to Azure AI services, and requires a [billable resource](cognitive-search-attach-cognitive-services.md) for transactions that exceed 20 documents per indexer per day. For a lower-cost solution to indexing multimodal content, see [Index multimodal content using image verbalization and Document Extraction skill](tutorial-document-extraction-image-verbalization.md).
++ The [Document Layout skill (preview)](cognitive-search-skill-document-intelligence-layout.md) for extracting text and normalized images with its `locationMetadata` from various documents, such as page numbers or bounding regions.
 
-+ Vectorization using the [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md), which generates embeddings for both text and images.
++ The [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) to vectorize text and images.
 
-+ A search index configured to store text and image embeddings and support for vector-based similarity search.
-
-Using a REST client and the [Search REST APIs](/rest/api/searchservice/), you will:
-
-> [!div class="checklist"]
-> + Set up sample data and configure an `azureblob` data source
-> + Create an index with support for text and image embeddings
-> + Define a skillset with extraction, embedding and knowleage store file projection steps
-> + Create and run an indexer to process and index content
-> + Search the index you just created
++ A search index configured to store extracted text and image verbalizations. Some content is vectorized for vector-based similarity search.
 
 ## Prerequisites
 
-+ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
-
-+ [Azure Storage](/azure/storage/common/storage-account-create).
++ [Azure AI Search](search-create-service-portal.md). [Configure your search service](search-manage.md) for role-based access control and a managed identity. Your service must be on the Basic tier or higher. This tutorial isn't supported on the Free tier. It must also be in the same region as your multi-service account.
 
-+ An [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) for image vectorization. Image vectorization requires Azure AI Vision multimodal embeddings. For an updated list of regions, see the [Azure AI Vision documentation](/azure/ai-services/computer-vision/overview-image-analysis#region-availability).
++ [Azure Storage](/azure/storage/common/storage-account-create), used for storing sample data and for creating a [knowledge store](knowledge-store-concept-intro.md).
 
-+ [Azure AI Search](search-what-is-azure-search.md), with a managed identity. [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. Your service must be on the Basic tier or higher—this tutorial isn't supported on the Free tier. It must also be in the same region as your multi-service account.
++ An [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) that provides Azure AI Vision for multimodal embeddings. You must use an Azure AI multi-service account for this task. For an updated list of regions that provide multimodal embeddings, see the [Azure AI Vision documentation](/azure/ai-services/computer-vision/overview-image-analysis#region-availability).
 
 + [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
-### Download files
+## Limitations
+
+The [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) has limited regional availability, is bound to Azure AI services, and requires a [billable resource](cognitive-search-attach-cognitive-services.md) for transactions that exceed 20 documents per indexer per day. For a lower-cost solution to indexing multimodal content, see [Tutorial: Verbalize images using generative AI](tutorial-document-extraction-image-verbalization.md).
+
+## Prepare data 
 
 Download the following sample PDF:
 
 + [sustainable-ai-pdf](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Accelerating-Sustainability-with-AI-2025.pdf)
 
-
 ### Upload sample data to Azure Storage
 
 1. In Azure Storage, create a new container named **doc-intelligence-multimodality-container**.
 
 1. [Upload the sample data file](/azure/storage/blobs/storage-quickstart-blobs-portal).
 
-1. [Create a role assignment in Azure Storage and specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
+1. [Create a **Storage Blob Data Reader** role assignment and specify a managed identity in a connection string](search-howto-managed-identities-storage.md)
 
    1. For connections made using a system-assigned managed identity, provide a connection string that contains a ResourceId, with no account key or password. The ResourceId must include the subscription ID of the storage account, the resource group of the storage account, and the storage account name. The connection string is similar to the following example:
 
@@ -299,7 +291,7 @@ Key points:
 
 ## Create a skillset
 
-[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a search index on your search service. An index specifies all the parameters and their attributes.
+[Create Skillset (REST)](/rest/api/searchservice/skillsets/create) creates a skillset on your search service. A skillset defines the operations that chunk and embed content prior to indexing. This skillset uses the Document Layout skill to extract text and images, preserving location metadata which is useful for citations in RAG applications. It uses Azure AI Vision multimodal embeddings skill to vectorize image and text content.
 
 ```http
 ### Create a skillset
@@ -614,4 +606,4 @@ Now that you're familiar with a sample implementation of a multimodal indexing s
 + [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md)
 + [Vectors in Azure AI Search](vector-search-overview.md)
 + [Semantic ranking in Azure AI Search](semantic-search-overview.md)
-+ [Index multimodal content using embeddings and Document Extraction skill](tutorial-document-extraction-multimodal-embeddings.md)
++ [Tutorial: Vectorize images and text](tutorial-document-extraction-multimodal-embeddings.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新文档布局多模态嵌入教程"
}
```

### Explanation
此次代码差异展示了对`tutorial-document-layout-multimodal-embeddings.md`文档的轻微更新，目的是改善内容准确性和用户体验，以便更好地指导用户从结构化文档布局中进行向量化操作。

文档标题已修改为“从结构化文档布局中向量化的教程”，更加精准地反映了教程的内容。整体结构进行了调整，使得如何构建多模态索引管道的过程展现得更加清晰，尤其是如何在可搜索索引中向量化文本和图像。

更新内容包括：
- 引入了操作技能集和索引器的定义，以创建一个包括AI富集的索引管道。
- 详细描述了使用“文档布局技能”提取文本、图像及其位置信息（如页码和边界区域）的过程。
- 更新了教程对组件的描述，使其涵盖新的向量化技术和工具，具体包括“Azure AI Vision多模态嵌入技能”。

除此之外，也改进了先决条件部分的信息，确保用户了解所需的Azure服务和资源。新增加了包括可用性限制的信息，帮助用户在实施时作出合适的决策。

整体来看，此次修改的目标是提高该教程的可读性和实用性，让用户在进行多模态内容索引时能够更加高效、顺利地完成相关操作。文档更新确保了信息的完整性，并为用户提供了更为清晰的指南。


