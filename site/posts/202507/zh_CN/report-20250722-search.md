---
date: '2025-07-22'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:043ab30...MicrosoftDocs:8cabf74
summary: æ­¤æ¬¡æ–‡æ¡£æ›´æ–°ä¸»è¦å¢åŠ äº†ç”¨äºä»£ç†æ£€ç´¢çš„ Java å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ï¼Œå¹¶å¯¹ä¸€ç³»åˆ—æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ã€‚è¿™äº›æ›´æ–°åŒ…æ‹¬å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„æ”¯æŒæ‰©å±•ã€Azure ç®—æ³•å’Œæ¨¡å‹çš„ä½¿ç”¨å»ºè®®ã€æ–‡æ¡£ç»“æ„çš„æ”¹å–„ä»¥åŠå†…å®¹ä¿®æ­£ï¼Œä»¥ç¡®ä¿ç”¨æˆ·èƒ½è·å–åˆ°æœ€æ–°å’Œå‡†ç¡®çš„ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨
  Azure çš„ AI åŠŸèƒ½ã€‚åŒæ—¶ï¼Œæ›´æ–°æ²¡æœ‰é‡å¤§å˜åŒ–ï¼Œä½†å¤šä¸ªæ–‡æ¡£çš„ä¿®æ”¹æ—¥æœŸå·²æ›´æ–°ï¼Œä»¥ä¿è¯å†…å®¹çš„æ—¶æ•ˆæ€§ã€‚æ–°çš„ `Cohere-embed-v4` æ¨¡å‹çš„æ”¯æŒä¿¡æ¯è¢«æ–°å¢æˆ–è°ƒæ•´ï¼Œä¸ºç”¨æˆ·æä¾›æ›´åŠ ä¸°å¯Œçš„å¤šæ¨¡æ€åµŒå…¥å¤„ç†é€‰é¡¹ã€‚æ›´æ–°è¿˜é’ˆå¯¹æ¨¡å‹ä½¿ç”¨çš„å»ºè®®è¿›è¡Œäº†è¯¦ç»†è¯´æ˜ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨
  Azure çš„æœ€æ–°åŠŸèƒ½ï¼Œæé«˜æœç´¢è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™æ¬¡æ›´æ–°è‡´åŠ›äºæå‡å†…å®¹çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ï¼Œå¢å¼ºç”¨æˆ·åœ¨å¼€å‘é«˜çº§æœç´¢å’Œæ•°æ®å¤„ç†åŠŸèƒ½æ—¶çš„ä¿¡å¿ƒã€‚
title: '[zh_CN] Diff Insight Report - search'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:043ab30...MicrosoftDocs:8cabf74){target="_blank"}

<format>
# äº®ç‚¹
æ­¤æ¬¡æ–‡æ¡£æ›´æ–°ä¸»è¦åŒ…æ‹¬æ–°å¢ä»£ç†æ£€ç´¢çš„ Java å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ä»¥åŠä¸€ç³»åˆ—æ–‡æ¡£çš„å°å¹…æ›´æ–°ã€‚è¿™äº›æ›´æ–°æ¶‰åŠå¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„æ”¯æŒæ‹“å±•ã€Azure ç®—æ³•å’Œæ¨¡å‹çš„å…·ä½“ä½¿ç”¨å»ºè®®ã€æ–‡æ¡£ç»“æ„çš„æ”¹å–„ä»¥åŠå†…å®¹çš„ä¿®æ­£ï¼Œä»¥ç¡®ä¿ç”¨æˆ·è·å¾—æœ€æ–°ã€å‡†ç¡®çš„ä¿¡æ¯ï¼Œæ›´å¥½åœ°è¿ç”¨ Azure çš„ AI åŠŸèƒ½ã€‚

## æ–°åŠŸèƒ½
- æ–°å¢äº†ç”¨äºä»£ç†æ£€ç´¢åŠŸèƒ½çš„ Java å¿«é€Ÿå…¥é—¨æ–‡æ¡£ï¼Œå¸®åŠ©ç”¨æˆ·ä½¿ç”¨ Java å®ç° Azure AI Search ä¸­çš„ä»£ç†æ£€ç´¢åŠŸèƒ½ã€‚

## é‡å¤§å˜åŒ–
- æ— é‡å¤§å˜åŒ–ã€‚

## å…¶ä»–æ›´æ–°
- å¤šä¸ªæ–‡æ¡£æ›´æ–°äº†æœ€åä¿®æ”¹æ—¥æœŸï¼Œä¿è¯ç”¨æˆ·è·å–æœ€æ–°ä¿¡æ¯ã€‚
- æ–°å¢å’Œè°ƒæ•´äº†å¤šæ¨¡æ€æ¨¡å‹ `Cohere-embed-v4` çš„æ”¯æŒä¿¡æ¯ã€‚
- æ·»åŠ å’Œæ”¹å–„äº†æ¨¡å‹ä½¿ç”¨çš„å»ºè®®å’Œé™åˆ¶è¯´æ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ `embed-v-4-0` æ¨¡å‹ã€‚
- å„æ–‡æ¡£çš„å†…å®¹ã€æ ¼å¼ä»¥åŠæè¿°æ€§æ–‡å­—è¿›è¡Œäº†ä¼˜åŒ–ä¸æ›´æ–°ã€‚

# è§è§£
è¿™æ¬¡æ–‡æ¡£æ›´æ–°ä¸ºç”¨æˆ·æä¾›äº†æ–°çš„ç¼–ç¨‹è¯­è¨€ï¼ˆJavaï¼‰è¿›è¡Œä»£ç†æ£€ç´¢å…¥é—¨çš„é€”å¾„ï¼Œä½¿å¾— Azure AI Search çš„å…¥é—¨æ–‡æ¡£æ›´å…·åŒ…å®¹æ€§ï¼Œç¬¦åˆå¼€å‘è€…çš„å¤šæ ·éœ€æ±‚ã€‚æ–°å¢åŠ çš„ `Cohere-embed-v4` æ¨¡å‹æ”¯æŒæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€åµŒå…¥ï¼Œå°†ä¸ºç”¨æˆ·æä¾›æ›´å¼ºå¤§çš„å¤„ç†é€‰é¡¹ï¼Œå¹¶æ˜¾è‘—æå‡åº”ç”¨çš„æ™ºèƒ½æ€§ã€‚

é€šè¿‡å¯¹å„ä¸ªæ”¯æŒæ¨¡å‹çš„è¯¦ç»†æè¿°ï¼Œç”¨æˆ·èƒ½å¤Ÿæ›´æ¸…æ™°åœ°äº†è§£å¦‚ä½•åˆ©ç”¨ Azure å¹³å°çš„æœ€æ–°åŠŸèƒ½æ¥æé«˜æœç´¢è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ã€‚è¿™äº›æ›´æ–°ä¸ä»…ä¿è¯äº†å¼€å‘è€…èƒ½å¤Ÿä½¿ç”¨æœ€æ–°çš„æ¨¡å‹è¿›è¡Œå‘é‡åŒ–ï¼Œè¿˜æä¾›äº†æ›´ä¸ºç»†è‡´ã€æ˜ç¡®çš„æ“ä½œæµç¨‹å’Œä½¿ç”¨å»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·é¿å…æ½œåœ¨çš„è¯¯ç”¨å’Œé™·å…¥æ¨¡å‹é€‰æ‹©çš„å›°å¢ƒã€‚

æ–‡æ¡£çš„æ›´æ–°è¿˜ç‰¹åˆ«å…³æ³¨äº†ç»“æ„å’Œè¯­æ³•ä¸Šçš„æ”¹è¿›ï¼Œç¡®ä¿æä¾›çš„ç¤ºä¾‹é“¾æ¥å’Œè¿‡ç¨‹æŒ‡å¯¼ç¬¦åˆå®é™…æ“ä½œéœ€æ±‚ï¼Œè¿™ä¸ä»…å¢å¼ºäº†å­¦ä¹ æ•ˆæœï¼Œä¹Ÿå‡å°‘äº†å¼€å‘è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„éšœç¢ï¼Œæé«˜äº†ç”¨æˆ·ä½“éªŒã€‚

æ€»ä½“æ¥è¯´ï¼Œè¿™æ¬¡æ›´æ–°å±•ç¤ºäº†æ–‡æ¡£å›¢é˜Ÿåœ¨ä¿æŒå†…å®¹å‡†ç¡®æ€§å’Œå®ç”¨æ€§æ–¹é¢çš„æŒç»­åŠªåŠ›ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿæ›´åŠ è‡ªä¿¡åœ°è¿ç”¨ Azure çš„æŠ€æœ¯æ¥å¼€å‘é«˜çº§æœç´¢å’Œæ•°æ®å¤„ç†åŠŸèƒ½ã€‚
</format>

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [agentic-retrieval-java.md](#item-4e2c55) | new feature | ä»£ç†æ£€ç´¢ Java å¿«é€Ÿå…¥é—¨ | added | 1412 | 0 | 1412 | 
| [search-get-started-agentic-retrieval.md](#item-4a40f4) | minor update | å¢åŠ  Java å¿«é€Ÿå…¥é—¨é“¾æ¥ | modified | 4 | 0 | 4 | 
| [search-get-started-portal-image-search.md](#item-438b9b) | minor update | æ›´æ–°å›¾åƒæœç´¢å¿«é€Ÿå…¥é—¨æ–‡æ¡£ | modified | 4 | 2 | 6 | 
| [search-get-started-portal-import-vectors.md](#item-7dae77) | minor update | æ›´æ–°çŸ¢é‡å¯¼å…¥æ–‡æ¡£ | modified | 4 | 2 | 6 | 
| [search-how-to-integrated-vectorization.md](#item-86fb1e) | minor update | æ›´æ–°é›†æˆå‘é‡åŒ–æ–‡æ¡£ | modified | 2 | 2 | 4 | 
| [tutorial-adls-gen2-indexer-acls.md](#item-6881a0) | minor update | æ›´æ–°ADLS Gen2ç´¢å¼•å™¨ACLæ–‡æ¡£ä¸­çš„æ•°æ®é“¾æ¥ | modified | 1 | 1 | 2 | 
| [tutorial-rag-build-solution-models.md](#item-6796c9) | minor update | æ›´æ–°RAGæ„å»ºè§£å†³æ–¹æ¡ˆæ¨¡å‹æ–‡æ¡£ | modified | 7 | 5 | 12 | 
| [vector-search-how-to-configure-vectorizer.md](#item-30ffd8) | minor update | æ›´æ–°å‘é‡æœç´¢é…ç½®æ–‡æ¡£ä¸­çš„æ¨¡å‹å’Œä¿¡æ¯ | modified | 5 | 3 | 8 | 
| [vector-search-integrated-vectorization-ai-studio.md](#item-353fcc) | minor update | æ›´æ–°é›†æˆå‘é‡åŒ–æ–‡æ¡£ä¸­çš„åµŒå…¥æ¨¡å‹ä¿¡æ¯ | modified | 12 | 15 | 27 | 
| [vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md](#item-ebe7a3) | minor update | æ›´æ–° Azure AI Foundry æ¨¡å‹ç›®å½•å‘é‡åŒ–å™¨æ–‡æ¡£ | modified | 8 | 6 | 14 | 


# Modified Contents
## articles/search/includes/quickstarts/agentic-retrieval-java.md{#item-4e2c55}

<details>
<summary>Diff</summary>
````diff
@@ -0,0 +1,1412 @@
+---
+manager: nitinme
+author: haileytap
+ms.author: haileytapia
+ms.service: azure-ai-search
+ms.topic: include
+ms.date: 7/21/2025
+---
+[!INCLUDE [Feature preview](../previews/preview-generic.md)]
+
+In this quickstart, you use [agentic retrieval](../../search-agentic-retrieval-concept.md) to create a conversational search experience powered by large language models (LLMs) and your proprietary data. Agentic retrieval breaks down complex user queries into subqueries, runs the subqueries in parallel, and extracts grounding data from documents indexed in Azure AI Search. The output is intended for integration with agentic and custom chat solutions.
+
+Although you can provide your own data, this quickstart uses [sample JSON documents](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/nasa-e-book/earth-at-night-json) from NASA's Earth at Night e-book. The documents describe general science topics and images of Earth at night as observed from space.
+
+## Prerequisites
+
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+
++ An [Azure AI Search service](../../search-create-service-portal.md) on the Basic tier or higher with [semantic ranker enabled](../../semantic-how-to-enable-disable.md).
+
++ An [Azure AI Foundry project](/azure/ai-foundry/how-to/create-projects). You get an Azure AI Foundry resource (that you need for model deployments) when you create an Azure AI Foundry project.
+
++ The [Azure CLI](/cli/azure/install-azure-cli) for keyless authentication with Microsoft Entra ID.
+
+[!INCLUDE [Setup](./agentic-retrieval-setup.md)]
+## Set up
+
+The sample in this quickstart works with the Java Runtime. Install a Java Development Kit such as [Azul Zulu OpenJDK](https://www.azul.com/downloads/?package=jdk). The [Microsoft Build of OpenJDK](https://www.microsoft.com/openjdk) or your preferred JDK should also work.
+
+1. Install [Apache Maven](https://maven.apache.org/install.html). Then run `mvn -v` to confirm successful installation.
+1. Create a new folder `quickstart-agentic-retrieval` to contain the application and open Visual Studio Code in that folder with the following command:
+
+    ```shell
+    mkdir quickstart-agentic-retrieval && cd quickstart-agentic-retrieval
+    ```
+1. Create a new `pom.xml` file in the root of your project, and copy the following code into it:
+
+    ```xml
+    <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+        <modelVersion>4.0.0</modelVersion>
+        <groupId>azure.search.sample</groupId>
+        <artifactId>azuresearchquickstart</artifactId>
+        <version>1.0.0-SNAPSHOT</version>
+        <build>
+            <sourceDirectory>src</sourceDirectory>
+            <plugins>
+            <plugin>
+                <artifactId>maven-compiler-plugin</artifactId>
+                <version>3.7.0</version>
+                <configuration>
+                <source>1.8</source>
+                <target>1.8</target>
+                </configuration>
+            </plugin>
+            </plugins>
+        </build>
+        <dependencies>
+            <dependency>
+                <groupId>junit</groupId>
+                <artifactId>junit</artifactId>
+                <version>4.11</version>
+                <scope>test</scope>
+            </dependency>
+            <dependency>
+                <groupId>com.azure</groupId>
+                <artifactId>azure-search-documents</artifactId>
+                <version>11.8.0-beta.7</version>
+            </dependency>
+            <dependency>
+                <groupId>com.azure</groupId>
+                <artifactId>azure-core</artifactId>
+                <version>1.53.0</version>
+            </dependency>
+            <dependency>
+                <groupId>com.azure</groupId>
+                <artifactId>azure-identity</artifactId>
+                <version>1.15.1</version>
+            </dependency>
+            <dependency>
+                <groupId>com.azure</groupId>
+                <artifactId>azure-ai-openai</artifactId>
+                <version>1.0.0-beta.16</version>
+            </dependency>
+            <dependency>
+                <groupId>com.fasterxml.jackson.core</groupId>
+                <artifactId>jackson-databind</artifactId>
+                <version>2.16.1</version>
+            </dependency>
+            <dependency>
+                <groupId>io.github.cdimascio</groupId>
+                <artifactId>dotenv-java</artifactId>
+                <version>3.0.0</version>
+            </dependency>
+            <dependency>
+                <groupId>org.apache.httpcomponents.client5</groupId>
+                <artifactId>httpclient5</artifactId>
+                <version>5.3.1</version>
+            </dependency>
+        </dependencies>
+    </project>
+    ```
+
+1. Install the dependencies including the Azure AI Search client library ([Azure.Search.Documents](/java/api/overview/azure/search)) for Java and [Azure Identity client library for Java](https://mvnrepository.com/artifact/com.azure/azure-identity) with:
+
+   ```console
+   mvn clean dependency:copy-dependencies
+   ```
+
+## Create the index and knowledge agent
+
+1. Create a new file named `.env` in the `quickstart-agentic-retrieval` folder and add the following environment variables:
+
+    ```plaintext
+    AZURE_OPENAI_ENDPOINT=https://<your-ai-foundry-resource-name>.openai.azure.com/
+    AZURE_OPENAI_GPT_DEPLOYMENT=gpt-4.1-mini
+    AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-large
+    AZURE_SEARCH_ENDPOINT=https://<your-search-service-name>.search.windows.net
+    AZURE_SEARCH_INDEX_NAME=agentic-retrieval-sample
+    ```
+
+    Replace `<your-search-service-name>` and `<your-ai-foundry-resource-name>` with your actual Azure AI Search service name and Azure AI Foundry resource name.
+
+1. Paste the following code into a new file named `AgenticRetrievalQuickstart.java` in the `quickstart-agentic-retrieval` folder:
+
+    ```java
+    import com.azure.ai.openai.OpenAIAsyncClient;
+    import com.azure.ai.openai.OpenAIClientBuilder;
+    import com.azure.ai.openai.models.*;
+    import com.azure.core.credential.TokenCredential;
+    import com.azure.core.http.HttpClient;
+    import com.azure.core.http.HttpHeaders;
+    import com.azure.core.http.HttpMethod;
+    import com.azure.core.http.HttpRequest;
+    import com.azure.core.http.HttpResponse;
+    import com.azure.core.util.BinaryData;
+    import com.azure.identity.DefaultAzureCredential;
+    import com.azure.identity.DefaultAzureCredentialBuilder;
+    import com.azure.search.documents.SearchClient;
+    import com.azure.search.documents.SearchClientBuilder;
+    import com.azure.search.documents.SearchDocument;
+    import com.azure.search.documents.indexes.SearchIndexClient;
+    import com.azure.search.documents.indexes.SearchIndexClientBuilder;
+    import com.azure.search.documents.indexes.models.*;
+    import com.azure.search.documents.agents.SearchKnowledgeAgentClient;
+    import com.azure.search.documents.agents.SearchKnowledgeAgentClientBuilder;
+    import com.azure.search.documents.agents.models.*;
+    import com.fasterxml.jackson.databind.JsonNode;
+    import com.fasterxml.jackson.databind.ObjectMapper;
+    import com.fasterxml.jackson.databind.node.ObjectNode;
+    import io.github.cdimascio.dotenv.Dotenv;
+    
+    import java.io.IOException;
+    import java.net.URI;
+    import java.net.http.HttpRequest.Builder;
+    import java.time.Duration;
+    import java.util.*;
+    import java.util.concurrent.TimeUnit;
+    
+    public class AgenticRetrievalQuickstart {
+        
+        // Configuration - Update these values for your environment
+        private static final String SEARCH_ENDPOINT;
+        private static final String AZURE_OPENAI_ENDPOINT;
+        private static final String AZURE_OPENAI_GPT_DEPLOYMENT;
+        private static final String AZURE_OPENAI_GPT_MODEL = "gpt-4.1-mini";
+        private static final String AZURE_OPENAI_EMBEDDING_DEPLOYMENT;
+        private static final String AZURE_OPENAI_EMBEDDING_MODEL = "text-embedding-3-large";
+        private static final String INDEX_NAME = "earth_at_night";
+        private static final String AGENT_NAME = "earth-search-agent";
+        private static final String SEARCH_API_VERSION = "2025-05-01-Preview";
+        
+        static {
+            // Load environment variables from .env file
+            Dotenv dotenv = Dotenv.configure().ignoreIfMissing().load();
+            
+            SEARCH_ENDPOINT = getEnvVar(dotenv, "AZURE_SEARCH_ENDPOINT", 
+                "https://contoso-agentic-search-service.search.windows.net");
+            AZURE_OPENAI_ENDPOINT = getEnvVar(dotenv, "AZURE_OPENAI_ENDPOINT",
+                "https://contoso-proj-agentic-foundry-res.openai.azure.com/");
+            AZURE_OPENAI_GPT_DEPLOYMENT = getEnvVar(dotenv, "AZURE_OPENAI_GPT_DEPLOYMENT", "gpt-4.1-mini");
+            AZURE_OPENAI_EMBEDDING_DEPLOYMENT = getEnvVar(dotenv, "AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-large");
+        }
+        
+        private static String getEnvVar(Dotenv dotenv, String key, String defaultValue) {
+            String value = dotenv.get(key);
+            return (value != null && !value.isEmpty()) ? value : defaultValue;
+        }
+        
+        public static void main(String[] args) {
+            try {
+                System.out.println("Starting Azure AI Search agentic retrieval quickstart...\n");
+                
+                // Initialize Azure credentials using managed identity (recommended)
+                TokenCredential credential = new DefaultAzureCredentialBuilder().build();
+                
+                // Create search clients
+                SearchIndexClient searchIndexClient = new SearchIndexClientBuilder()
+                    .endpoint(SEARCH_ENDPOINT)
+                    .credential(credential)
+                    .buildClient();
+                    
+                SearchClient searchClient = new SearchClientBuilder()
+                    .endpoint(SEARCH_ENDPOINT)
+                    .indexName(INDEX_NAME)
+                    .credential(credential)
+                    .buildClient();
+                
+                // Create Azure OpenAI client
+                OpenAIAsyncClient openAIClient = new OpenAIClientBuilder()
+                    .endpoint(AZURE_OPENAI_ENDPOINT)
+                    .credential(credential)
+                    .buildAsyncClient();
+                
+                // Step 1: Create search index with vector and semantic capabilities
+                createSearchIndex(searchIndexClient);
+                
+                // Step 2: Upload documents
+                uploadDocuments(searchClient);
+                
+                // Step 3: Create knowledge agent
+                createKnowledgeAgent(credential);
+                
+                // Step 4: Run agentic retrieval with conversation
+                runAgenticRetrieval(credential, openAIClient);
+                
+                // Step 5: Clean up - Delete knowledge agent and search index
+                deleteKnowledgeAgent(credential);
+                deleteSearchIndex(searchIndexClient);
+                
+                System.out.println("[DONE] Quickstart completed successfully!");
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error in main execution: " + e.getMessage());
+                e.printStackTrace();
+            }
+        }
+        
+        private static void createSearchIndex(SearchIndexClient indexClient) {
+            System.out.println("[WAIT] Creating search index...");
+            
+            try {
+                // Delete index if it exists
+                try {
+                    indexClient.deleteIndex(INDEX_NAME);
+                    System.out.println("[DELETE] Deleted existing index '" + INDEX_NAME + "'");
+                } catch (Exception e) {
+                    // Index doesn't exist, which is fine
+                }
+                
+                // Define fields
+                List<SearchField> fields = Arrays.asList(
+                    new SearchField("id", SearchFieldDataType.STRING)
+                        .setKey(true)
+                        .setFilterable(true)
+                        .setSortable(true)
+                        .setFacetable(true),
+                    new SearchField("page_chunk", SearchFieldDataType.STRING)
+                        .setSearchable(true)
+                        .setFilterable(false)
+                        .setSortable(false)
+                        .setFacetable(false),
+                    new SearchField("page_embedding_text_3_large", SearchFieldDataType.collection(SearchFieldDataType.SINGLE))
+                        .setSearchable(true)
+                        .setFilterable(false)
+                        .setSortable(false)
+                        .setFacetable(false)
+                        .setVectorSearchDimensions(3072)
+                        .setVectorSearchProfileName("hnsw_text_3_large"),
+                    new SearchField("page_number", SearchFieldDataType.INT32)
+                        .setFilterable(true)
+                        .setSortable(true)
+                        .setFacetable(true)
+                );
+                
+                // Create vectorizer
+                AzureOpenAIVectorizer vectorizer = new AzureOpenAIVectorizer("azure_openai_text_3_large")
+                    .setParameters(new AzureOpenAIVectorizerParameters()
+                        .setResourceUrl(AZURE_OPENAI_ENDPOINT)
+                        .setDeploymentName(AZURE_OPENAI_EMBEDDING_DEPLOYMENT)
+                        .setModelName(AzureOpenAIModelName.TEXT_EMBEDDING_3_LARGE));
+                
+                // Create vector search configuration
+                VectorSearch vectorSearch = new VectorSearch()
+                    .setProfiles(Arrays.asList(
+                        new VectorSearchProfile("hnsw_text_3_large", "alg")
+                            .setVectorizerName("azure_openai_text_3_large")
+                    ))
+                    .setAlgorithms(Arrays.asList(
+                        new HnswAlgorithmConfiguration("alg")
+                    ))
+                    .setVectorizers(Arrays.asList(vectorizer));
+                
+                // Create semantic search configuration
+                SemanticSearch semanticSearch = new SemanticSearch()
+                    .setDefaultConfigurationName("semantic_config")
+                    .setConfigurations(Arrays.asList(
+                        new SemanticConfiguration("semantic_config",
+                            new SemanticPrioritizedFields()
+                                .setContentFields(Arrays.asList(
+                                    new SemanticField("page_chunk")
+                                ))
+                        )
+                    ));
+                
+                // Create the index
+                SearchIndex index = new SearchIndex(INDEX_NAME)
+                    .setFields(fields)
+                    .setVectorSearch(vectorSearch)
+                    .setSemanticSearch(semanticSearch);
+                
+                indexClient.createOrUpdateIndex(index);
+                System.out.println("[DONE] Index '" + INDEX_NAME + "' created successfully.");
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error creating index: " + e.getMessage());
+                throw new RuntimeException(e);
+            }
+        }
+        
+        private static void uploadDocuments(SearchClient searchClient) {
+            System.out.println("[WAIT] Uploading documents...");
+            
+            try {
+                // Fetch documents from GitHub
+                List<SearchDocument> documents = fetchEarthAtNightDocuments();
+                
+                searchClient.uploadDocuments(documents);
+                System.out.println("[DONE] Uploaded " + documents.size() + " documents successfully.");
+                
+                // Wait for indexing to complete
+                System.out.println("[WAIT] Waiting for document indexing to complete...");
+                Thread.sleep(5000);
+                System.out.println("[DONE] Document indexing completed.");
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error uploading documents: " + e.getMessage());
+                throw new RuntimeException(e);
+            }
+        }
+        
+        private static List<SearchDocument> fetchEarthAtNightDocuments() {
+            System.out.println("[WAIT] Fetching Earth at Night documents from GitHub...");
+            
+            String documentsUrl = "https://raw.githubusercontent.com/Azure-Samples/azure-search-sample-data/refs/heads/main/nasa-e-book/earth-at-night-json/documents.json";
+            
+            try {
+                java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+                java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+                    .uri(URI.create(documentsUrl))
+                    .build();
+                
+                java.net.http.HttpResponse<String> response = httpClient.send(request, 
+                    java.net.http.HttpResponse.BodyHandlers.ofString());
+                
+                if (response.statusCode() != 200) {
+                    throw new IOException("Failed to fetch documents: " + response.statusCode());
+                }
+                
+                ObjectMapper mapper = new ObjectMapper();
+                JsonNode jsonArray = mapper.readTree(response.body());
+                
+                List<SearchDocument> documents = new ArrayList<>();
+                for (int i = 0; i < jsonArray.size(); i++) {
+                    JsonNode doc = jsonArray.get(i);
+                    SearchDocument searchDoc = new SearchDocument();
+                    
+                    searchDoc.put("id", doc.has("id") ? doc.get("id").asText() : String.valueOf(i + 1));
+                    searchDoc.put("page_chunk", doc.has("page_chunk") ? doc.get("page_chunk").asText() : "");
+                    
+                    // Handle embeddings
+                    if (doc.has("page_embedding_text_3_large") && doc.get("page_embedding_text_3_large").isArray()) {
+                        List<Double> embeddings = new ArrayList<>();
+                        for (JsonNode embedding : doc.get("page_embedding_text_3_large")) {
+                            embeddings.add(embedding.asDouble());
+                        }
+                        searchDoc.put("page_embedding_text_3_large", embeddings);
+                    } else {
+                        // Fallback embeddings
+                        List<Double> fallbackEmbeddings = new ArrayList<>();
+                        for (int j = 0; j < 3072; j++) {
+                            fallbackEmbeddings.add(0.1);
+                        }
+                        searchDoc.put("page_embedding_text_3_large", fallbackEmbeddings);
+                    }
+                    
+                    searchDoc.put("page_number", doc.has("page_number") ? doc.get("page_number").asInt() : i + 1);
+                    
+                    documents.add(searchDoc);
+                }
+                
+                System.out.println("[DONE] Fetched " + documents.size() + " documents from GitHub");
+                return documents;
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error fetching documents from GitHub: " + e.getMessage());
+                System.out.println("ğŸ”„ Falling back to sample documents...");
+                
+                // Fallback to sample documents
+                List<SearchDocument> fallbackDocs = new ArrayList<>();
+                
+                SearchDocument doc1 = new SearchDocument();
+                doc1.put("id", "1");
+                doc1.put("page_chunk", "The Earth at night reveals the patterns of human settlement and economic activity. City lights trace the contours of civilization, creating a luminous map of where people live and work.");
+                List<Double> embeddings1 = new ArrayList<>();
+                for (int i = 0; i < 3072; i++) {
+                    embeddings1.add(0.1);
+                }
+                doc1.put("page_embedding_text_3_large", embeddings1);
+                doc1.put("page_number", 1);
+                
+                SearchDocument doc2 = new SearchDocument();
+                doc2.put("id", "2");
+                doc2.put("page_chunk", "From space, the aurora borealis appears as shimmering curtains of green and blue light dancing across the polar regions.");
+                List<Double> embeddings2 = new ArrayList<>();
+                for (int i = 0; i < 3072; i++) {
+                    embeddings2.add(0.2);
+                }
+                doc2.put("page_embedding_text_3_large", embeddings2);
+                doc2.put("page_number", 2);
+                
+                fallbackDocs.add(doc1);
+                fallbackDocs.add(doc2);
+                
+                return fallbackDocs;
+            }
+        }
+        
+        private static void createKnowledgeAgent(TokenCredential credential) {
+            System.out.println("[WAIT] Creating knowledge agent...");
+            
+            // Delete agent if it exists
+            deleteKnowledgeAgent(credential);
+            
+            try {
+                ObjectMapper mapper = new ObjectMapper();
+                ObjectNode agentDefinition = mapper.createObjectNode();
+                agentDefinition.put("name", AGENT_NAME);
+                agentDefinition.put("description", "Knowledge agent for Earth at Night e-book content");
+                
+                ObjectNode model = mapper.createObjectNode();
+                model.put("kind", "azureOpenAI");
+                ObjectNode azureOpenAIParams = mapper.createObjectNode();
+                azureOpenAIParams.put("resourceUri", AZURE_OPENAI_ENDPOINT);
+                azureOpenAIParams.put("deploymentId", AZURE_OPENAI_GPT_DEPLOYMENT);
+                azureOpenAIParams.put("modelName", AZURE_OPENAI_GPT_MODEL);
+                model.set("azureOpenAIParameters", azureOpenAIParams);
+                agentDefinition.set("models", mapper.createArrayNode().add(model));
+                
+                ObjectNode targetIndex = mapper.createObjectNode();
+                targetIndex.put("indexName", INDEX_NAME);
+                targetIndex.put("defaultRerankerThreshold", 2.5);
+                agentDefinition.set("targetIndexes", mapper.createArrayNode().add(targetIndex));
+                
+                String token = getAccessToken(credential, "https://search.azure.com/.default");
+                
+                java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+                java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+                    .uri(URI.create(SEARCH_ENDPOINT + "/agents/" + AGENT_NAME + "?api-version=" + SEARCH_API_VERSION))
+                    .header("Content-Type", "application/json")
+                    .header("Authorization", "Bearer " + token)
+                    .PUT(java.net.http.HttpRequest.BodyPublishers.ofString(mapper.writeValueAsString(agentDefinition)))
+                    .build();
+                
+                java.net.http.HttpResponse<String> response = httpClient.send(request,
+                    java.net.http.HttpResponse.BodyHandlers.ofString());
+                
+                if (response.statusCode() >= 400) {
+                    throw new RuntimeException("Failed to create knowledge agent: " + response.statusCode() + " " + response.body());
+                }
+                
+                System.out.println("[DONE] Knowledge agent '" + AGENT_NAME + "' created successfully.");
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error creating knowledge agent: " + e.getMessage());
+                throw new RuntimeException(e);
+            }
+        }
+        
+        private static void runAgenticRetrieval(TokenCredential credential, OpenAIAsyncClient openAIClient) {
+            System.out.println("[SEARCH] Running agentic retrieval...");
+            
+            // Initialize messages with system instructions
+            List<Map<String, String>> messages = new ArrayList<>();
+            
+            Map<String, String> systemMessage = new HashMap<>();
+            systemMessage.put("role", "system");
+            systemMessage.put("content", "A Q&A agent that can answer questions about the Earth at night.\n" +
+                "Sources have a JSON format with a ref_id that must be cited in the answer.\n" +
+                "If you do not have the answer, respond with \"I don't know\".");
+            messages.add(systemMessage);
+            
+            Map<String, String> userMessage = new HashMap<>();
+            userMessage.put("role", "user");
+            userMessage.put("content", "Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown? Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?");
+            messages.add(userMessage);
+            
+            try {
+                // Call agentic retrieval API (excluding system message)
+                List<Map<String, String>> userMessages = messages.stream()
+                    .filter(m -> !"system".equals(m.get("role")))
+                    .collect(java.util.stream.Collectors.toList());
+                
+                String retrievalResponse = callAgenticRetrieval(credential, userMessages);
+                
+                // Add assistant response to conversation history
+                Map<String, String> assistantMessage = new HashMap<>();
+                assistantMessage.put("role", "assistant");
+                assistantMessage.put("content", retrievalResponse);
+                messages.add(assistantMessage);
+                
+                System.out.println(retrievalResponse);
+                
+                // Now do chat completion with full conversation history
+                generateFinalAnswer(openAIClient, messages);
+                
+                // Continue conversation with second question
+                continueConversation(credential, openAIClient, messages);
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error in agentic retrieval: " + e.getMessage());
+                throw new RuntimeException(e);
+            }
+        }
+        
+        private static String callAgenticRetrieval(TokenCredential credential, List<Map<String, String>> messages) {
+            try {
+                ObjectMapper mapper = new ObjectMapper();
+                ObjectNode retrievalRequest = mapper.createObjectNode();
+                
+                // Convert messages to the correct format expected by the Knowledge agent
+                com.fasterxml.jackson.databind.node.ArrayNode agentMessages = mapper.createArrayNode();
+                for (Map<String, String> msg : messages) {
+                    ObjectNode agentMessage = mapper.createObjectNode();
+                    agentMessage.put("role", msg.get("role"));
+                    
+                    com.fasterxml.jackson.databind.node.ArrayNode content = mapper.createArrayNode();
+                    ObjectNode textContent = mapper.createObjectNode();
+                    textContent.put("type", "text");
+                    textContent.put("text", msg.get("content"));
+                    content.add(textContent);
+                    agentMessage.set("content", content);
+                    
+                    agentMessages.add(agentMessage);
+                }
+                retrievalRequest.set("messages", agentMessages);
+                
+                com.fasterxml.jackson.databind.node.ArrayNode targetIndexParams = mapper.createArrayNode();
+                ObjectNode indexParam = mapper.createObjectNode();
+                indexParam.put("indexName", INDEX_NAME);
+                indexParam.put("rerankerThreshold", 2.5);
+                indexParam.put("maxDocsForReranker", 100);
+                indexParam.put("includeReferenceSourceData", true);
+                targetIndexParams.add(indexParam);
+                retrievalRequest.set("targetIndexParams", targetIndexParams);
+                
+                String token = getAccessToken(credential, "https://search.azure.com/.default");
+                
+                java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+                java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+                    .uri(URI.create(SEARCH_ENDPOINT + "/agents/" + AGENT_NAME + "/retrieve?api-version=" + SEARCH_API_VERSION))
+                    .header("Content-Type", "application/json")
+                    .header("Authorization", "Bearer " + token)
+                    .POST(java.net.http.HttpRequest.BodyPublishers.ofString(mapper.writeValueAsString(retrievalRequest)))
+                    .build();
+                
+                java.net.http.HttpResponse<String> response = httpClient.send(request,
+                    java.net.http.HttpResponse.BodyHandlers.ofString());
+                
+                if (response.statusCode() >= 400) {
+                    throw new RuntimeException("Agentic retrieval failed: " + response.statusCode() + " " + response.body());
+                }
+                
+                JsonNode responseJson = mapper.readTree(response.body());
+                
+                // Log activities and results
+                logActivitiesAndResults(responseJson);
+                
+                // Extract response content
+                if (responseJson.has("response") && responseJson.get("response").isArray()) {
+                    com.fasterxml.jackson.databind.node.ArrayNode responseArray = (com.fasterxml.jackson.databind.node.ArrayNode) responseJson.get("response");
+                    if (responseArray.size() > 0) {
+                        JsonNode firstResponse = responseArray.get(0);
+                        if (firstResponse.has("content") && firstResponse.get("content").isArray()) {
+                            com.fasterxml.jackson.databind.node.ArrayNode contentArray = (com.fasterxml.jackson.databind.node.ArrayNode) firstResponse.get("content");
+                            if (contentArray.size() > 0) {
+                                JsonNode textContent = contentArray.get(0);
+                                if (textContent.has("text")) {
+                                    return textContent.get("text").asText();
+                                }
+                            }
+                        }
+                    }
+                }
+                
+                return "No response content available";
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error in agentic retrieval call: " + e.getMessage());
+                throw new RuntimeException(e);
+            }
+        }
+        
+        private static void logActivitiesAndResults(JsonNode responseJson) {
+            ObjectMapper mapper = new ObjectMapper();
+            
+            // Log activities
+            System.out.println("\nActivities:");
+            if (responseJson.has("activity") && responseJson.get("activity").isArray()) {
+                for (JsonNode activity : responseJson.get("activity")) {
+                    String activityType = "UnknownActivityRecord";
+                    if (activity.has("InputTokens")) {
+                        activityType = "KnowledgeAgentModelQueryPlanningActivityRecord";
+                    } else if (activity.has("TargetIndex")) {
+                        activityType = "KnowledgeAgentSearchActivityRecord";
+                    } else if (activity.has("QueryTime")) {
+                        activityType = "KnowledgeAgentSemanticRankerActivityRecord";
+                    }
+                    
+                    System.out.println("Activity Type: " + activityType);
+                    try {
+                        System.out.println(mapper.writerWithDefaultPrettyPrinter().writeValueAsString(activity));
+                    } catch (Exception e) {
+                        System.out.println(activity.toString());
+                    }
+                }
+            }
+            
+            // Log results
+            System.out.println("Results");
+            if (responseJson.has("references") && responseJson.get("references").isArray()) {
+                for (JsonNode reference : responseJson.get("references")) {
+                    String referenceType = "KnowledgeAgentAzureSearchDocReference";
+                    
+                    System.out.println("Reference Type: " + referenceType);
+                    try {
+                        System.out.println(mapper.writerWithDefaultPrettyPrinter().writeValueAsString(reference));
+                    } catch (Exception e) {
+                        System.out.println(reference.toString());
+                    }
+                }
+            }
+        }
+        
+        private static void generateFinalAnswer(OpenAIAsyncClient openAIClient, List<Map<String, String>> messages) {
+            System.out.println("\n[ASSISTANT]: ");
+            
+            try {
+                List<ChatRequestMessage> chatMessages = new ArrayList<>();
+                for (Map<String, String> msg : messages) {
+                    String role = msg.get("role");
+                    String content = msg.get("content");
+                    
+                    switch (role) {
+                        case "system":
+                            chatMessages.add(new ChatRequestSystemMessage(content));
+                            break;
+                        case "user":
+                            chatMessages.add(new ChatRequestUserMessage(content));
+                            break;
+                        case "assistant":
+                            chatMessages.add(new ChatRequestAssistantMessage(content));
+                            break;
+                    }
+                }
+                
+                ChatCompletionsOptions chatOptions = new ChatCompletionsOptions(chatMessages)
+                    .setMaxTokens(1000)
+                    .setTemperature(0.7);
+                
+                ChatCompletions completion = openAIClient.getChatCompletions(AZURE_OPENAI_GPT_DEPLOYMENT, chatOptions).block();
+                
+                if (completion != null && completion.getChoices() != null && !completion.getChoices().isEmpty()) {
+                    String answer = completion.getChoices().get(0).getMessage().getContent();
+                    System.out.println(answer.replace(".", "\n"));
+                    
+                    // Add this response to conversation history
+                    Map<String, String> assistantResponse = new HashMap<>();
+                    assistantResponse.put("role", "assistant");
+                    assistantResponse.put("content", answer);
+                    messages.add(assistantResponse);
+                }
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error generating final answer: " + e.getMessage());
+                throw new RuntimeException(e);
+            }
+        }
+        
+        private static void continueConversation(TokenCredential credential, OpenAIAsyncClient openAIClient, List<Map<String, String>> messages) {
+            System.out.println("\n === Continuing Conversation ===");
+            
+            // Add follow-up question
+            String followUpQuestion = "How do I find lava at night?";
+            System.out.println("[QUESTION] Follow-up question: " + followUpQuestion);
+            
+            Map<String, String> userMessage = new HashMap<>();
+            userMessage.put("role", "user");
+            userMessage.put("content", followUpQuestion);
+            messages.add(userMessage);
+            
+            try {
+                // FILTER OUT SYSTEM MESSAGE - only send user/assistant messages to agentic retrieval
+                List<Map<String, String>> userAssistantMessages = messages.stream()
+                    .filter(m -> !"system".equals(m.get("role")))
+                    .collect(java.util.stream.Collectors.toList());
+                
+                String newRetrievalResponse = callAgenticRetrieval(credential, userAssistantMessages);
+                
+                // Add assistant response to conversation history
+                Map<String, String> assistantMessage = new HashMap<>();
+                assistantMessage.put("role", "assistant");
+                assistantMessage.put("content", newRetrievalResponse);
+                messages.add(assistantMessage);
+                
+                System.out.println(newRetrievalResponse);
+                
+                // Generate final answer for follow-up
+                generateFinalAnswer(openAIClient, messages);
+                
+                System.out.println("\n === Conversation Complete ===");
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error in conversation continuation: " + e.getMessage());
+                throw new RuntimeException(e);
+            }
+        }
+        
+        private static void deleteKnowledgeAgent(TokenCredential credential) {
+            System.out.println("[DELETE] Deleting knowledge agent...");
+            
+            try {
+                String token = getAccessToken(credential, "https://search.azure.com/.default");
+                
+                java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+                java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+                    .uri(URI.create(SEARCH_ENDPOINT + "/agents/" + AGENT_NAME + "?api-version=" + SEARCH_API_VERSION))
+                    .header("Authorization", "Bearer " + token)
+                    .DELETE()
+                    .build();
+                
+                java.net.http.HttpResponse<String> response = httpClient.send(request,
+                    java.net.http.HttpResponse.BodyHandlers.ofString());
+                
+                if (response.statusCode() == 404) {
+                    System.out.println("[INFO] Knowledge agent '" + AGENT_NAME + "' does not exist or was already deleted.");
+                    return;
+                }
+                
+                if (response.statusCode() >= 400) {
+                    throw new RuntimeException("Failed to delete knowledge agent: " + response.statusCode() + " " + response.body());
+                }
+                
+                System.out.println("[DONE] Knowledge agent '" + AGENT_NAME + "' deleted successfully.");
+                
+            } catch (Exception e) {
+                System.err.println("[ERROR] Error deleting knowledge agent: " + e.getMessage());
+                // Don't throw - this is cleanup
+            }
+        }
+        
+        private static void deleteSearchIndex(SearchIndexClient indexClient) {
+            System.out.println("[DELETE] Deleting search index...");
+            
+            try {
+                indexClient.deleteIndex(INDEX_NAME);
+                System.out.println("[DONE] Search index '" + INDEX_NAME + "' deleted successfully.");
+                
+            } catch (Exception e) {
+                if (e.getMessage() != null && (e.getMessage().contains("404") || e.getMessage().contains("IndexNotFound"))) {
+                    System.out.println("[INFO] Search index '" + INDEX_NAME + "' does not exist or was already deleted.");
+                    return;
+                }
+                System.err.println("[ERROR] Error deleting search index: " + e.getMessage());
+                // Don't throw - this is cleanup
+            }
+        }
+        
+        private static String getAccessToken(TokenCredential credential, String scope) {
+            try {
+                return credential.getToken(new com.azure.core.credential.TokenRequestContext().addScopes(scope)).block().getToken();
+            } catch (Exception e) {
+                throw new RuntimeException("Failed to get access token", e);
+            }
+        }
+    }
+    ```
+    
+1. Sign in to Azure with the following command:
+
+    ```shell
+    az login
+    ```
+
+1. Run your new console application:
+
+    ```console
+    javac Address.java App.java Hotel.java -cp ".;target\dependency\*"
+    java -cp ".;target\dependency\*" App
+    ```
+
+## Output
+
+The output of the application should look similar to the following:
+
+```plaintext
+Starting Azure AI Search agentic retrieval quickstart...
+
+[WAIT] Creating search index...
+[DELETE] Deleted existing index 'earth_at_night'
+[DONE] Index 'earth_at_night' created successfully.
+[WAIT] Uploading documents...
+[WAIT] Fetching Earth at Night documents from GitHub...
+[DONE] Fetched 194 documents from GitHub
+[DONE] Uploaded 194 documents successfully.
+[WAIT] Waiting for document indexing to complete...
+[DONE] Document indexing completed.
+[WAIT] Creating knowledge agent...
+[DELETE] Deleting knowledge agent...
+[INFO] Knowledge agent 'earth-search-agent' does not exist or was already deleted.
+[DONE] Knowledge agent 'earth-search-agent' created successfully.
+[SEARCH] Running agentic retrieval...
+
+Activities:
+Activity Type: UnknownActivityRecord
+{
+  "type" : "ModelQueryPlanning",
+  "id" : 0,
+  "inputTokens" : 1379,
+  "outputTokens" : 545
+}
+Activity Type: UnknownActivityRecord
+{
+  "type" : "AzureSearchQuery",
+  "id" : 1,
+  "targetIndex" : "earth_at_night",
+  "query" : {
+    "search" : "Why do suburban areas show greater December brightening compared to urban cores despite higher absolute light levels downtown?",
+    "filter" : null
+  },
+  "queryTime" : "2025-07-21T15:07:04.024Z",
+  "count" : 0,
+  "elapsedMs" : 2609
+}
+Activity Type: UnknownActivityRecord
+{
+  "type" : "AzureSearchQuery",
+  "id" : 2,
+  "targetIndex" : "earth_at_night",
+  "query" : {
+    "search" : "Why is the Phoenix nighttime street grid sharply visible from space, while large stretches of interstate highways between Midwestern cities appear comparatively dim?",
+    "filter" : null
+  },
+  "queryTime" : "2025-07-21T15:07:04.267Z",
+  "count" : 0,
+  "elapsedMs" : 243
+}
+Activity Type: UnknownActivityRecord
+{
+  "type" : "AzureSearchSemanticRanker",
+  "id" : 3,
+  "inputTokens" : 48602
+}
+Results
+[]
+
+[ASSISTANT]: 
+The suburban belts display larger December brightening than urban cores despite higher absolute light levels downtown likely because suburban areas have more seasonal variation in lighting usage, such as increased outdoor and holiday lighting in December
+ Urban cores, being brightly lit throughout the year, show less relative change
+
+
+Regarding Phoenix's nighttime street grid visibility, it is sharply visible from space due to the structured and continuous lighting of the city's streets
+ In contrast, large stretches of interstate highways between Midwestern cities are comparatively dim because highways typically have less intense and less frequent lighting compared to urban street grids
+
+
+[Note: This explanation is based on general knowledge; no specific source with ref_id was provided
+]
+
+ === Continuing Conversation ===
+[QUESTION] Follow-up question: How do I find lava at night?
+
+Activities:
+Activity Type: UnknownActivityRecord
+{
+  "type" : "ModelQueryPlanning",
+  "id" : 0,
+  "inputTokens" : 1545,
+  "outputTokens" : 127
+}
+Activity Type: UnknownActivityRecord
+{
+  "type" : "AzureSearchQuery",
+  "id" : 1,
+  "targetIndex" : "earth_at_night",
+  "query" : {
+    "search" : "How can I find lava at night?",
+    "filter" : null
+  },
+  "queryTime" : "2025-07-21T15:07:15.445Z",
+  "count" : 6,
+  "elapsedMs" : 370
+}
+Activity Type: UnknownActivityRecord
+{
+  "type" : "AzureSearchSemanticRanker",
+  "id" : 2,
+  "inputTokens" : 22994
+}
+Results
+Reference Type: KnowledgeAgentAzureSearchDocReference
+{
+  "type" : "AzureSearchDoc",
+  "id" : "0",
+  "activitySource" : 1,
+  "docKey" : "earth_at_night_508_page_44_verbalized",
+  "sourceData" : {
+    "id" : "earth_at_night_508_page_44_verbalized",
+    "page_chunk" : "## Nature's Light Shows\n\nAt night, with the light of the Sun removed, nature's brilliant glow from Earth's surface becomes visible to the naked eye from space. Some of Earth's most spectacular light shows are natural, like the aurora borealis, or Northern Lights, in the Northern Hemisphere (aurora australis, or Southern Lights, in the Southern Hemisphere). The auroras are natural electrical phenomena caused by charged particles that race from the Sun toward Earth, inducing chemical reactions in the upper atmosphere and creating the appearance of streamers of reddish or greenish light in the sky, usually near the northern or southern magnetic pole. Other natural lights can indicate danger, like a raging forest fire encroaching on a city, town, or community, or lava spewing from an erupting volcano.\n\nWhatever the source, the ability of humans to monitor nature's light shows at night has practical applications for society. For example, tracking fires during nighttime hours allows for continuous monitoring and enhances our ability to protect humans and other animals, plants, and infrastructure. Combined with other data sources, our ability to observe the light of fires at night allows emergency managers to more efficiently and accurately issue warnings and evacuation orders and allows firefighting efforts to continue through the night. With enough moonlight (e.g., full-Moon phase), it's even possible to track the movement of smoke plumes at night, which can impact air quality, regardless of time of day.\n\nAnother natural source of light at night is emitted from glowing lava flows at the site of active volcanoes. Again, with enough moonlight, these dramatic scenes can be tracked and monitored for both scientific research and public safety.\n\n\n### Figure: The Northern Lights Viewed from Space\n\n**September 17, 2011**\n\nThis photo, taken from the International Space Station on September 17, 2011, shows a spectacular display of the aurora borealis (Northern Lights) as green and reddish light in the night sky above Earth. In the foreground, part of a Soyuz spacecraft is visible, silhouetted against the bright auroral light. The green glow is generated by energetic charged particles from the Sun interacting with Earth's upper atmosphere, exciting oxygen and nitrogen atoms, and producing characteristic colors. The image demonstrates the vividness and grandeur of natural night-time light phenomena as seen from orbit."
+  }
+}
+Reference Type: KnowledgeAgentAzureSearchDocReference
+{
+  "type" : "AzureSearchDoc",
+  "id" : "1",
+  "activitySource" : 1,
+  "docKey" : "earth_at_night_508_page_65_verbalized",
+  "sourceData" : {
+    "id" : "earth_at_night_508_page_65_verbalized",
+    "page_chunk" : "# Volcanoes\n\n## Figure: Satellite Image of Sicily and Mount Etna Lava, March 16, 2017\n\nThe annotated satellite image below shows the island of Sicily and the surrounding region at night, highlighting city lights and volcanic activity.\n\n**Description:**\n\n- **Date of image:** March 16, 2017\n- **Geographical locations labeled:**\n    - Major cities: Palermo (northwest Sicily), Marsala (western Sicily), Catania (eastern Sicily)\n    - Significant feature: Mount Etna, labeled with an adjacent \"hot lava\" region showing the glow from active lava flows\n    - Surrounding water body: Mediterranean Sea\n    - Island: Malta to the south of Sicily\n- **Other details:** \n    - The image is shown at night, with bright spots indicating city lights.\n    - The position of \"hot lava\" near Mount Etna is distinctly visible as a bright spot different from other city lights, indicating volcanic activity.\n    - A scale bar is included showing a reference length of 50 km.\n    - North direction is indicated with an arrow.\n    - Cloud cover is visible in the southwest part of the image, partially obscuring the view near Marsala and Malta.\n\n**Summary of Features Visualized:**\n\n| Feature          | Description                                           |\n|------------------|------------------------------------------------------|\n| Cities           | Bright clusters indicating locations: Palermo, Marsala, Catania |\n| Mount Etna       | Marked on the map, located on the eastern side of Sicily, with visible hot lava activity |\n| Malta            | Clearly visible to the south of Sicily               |\n| Water bodies     | Mediterranean Sea labeled                            |\n| Scale & Direction| 50 km scale bar and North indicator                  |\n| Date             | March 16, 2017                                       |\n| Cloud Cover      | Visible in the lower left (southern) part of the image |\n\nThis figure demonstrates the visibility of volcanic activity at Mount Etna from space at night, distinguishing the light from hot lava against the background city lights of Sicily and Malta."
+  }
+}
+Reference Type: KnowledgeAgentAzureSearchDocReference
+{
+  "type" : "AzureSearchDoc",
+  "id" : "2",
+  "activitySource" : 1,
+  "docKey" : "earth_at_night_508_page_64_verbalized",
+  "sourceData" : {
+    "id" : "earth_at_night_508_page_64_verbalized",
+    "page_chunk" : "<!-- PageHeader=\"Volcanoes\" -->\n\n### Nighttime Glow at Mount Etna - Italy\n\nAt about 2:30 a.m. local time on March 16, 2017, the VIIRS DNB on the Suomi NPP satellite captured this nighttime image of lava flowing on Mount Etna in Sicily, Italy. Etna is one of the world's most active volcanoes.\n\n#### Figure: Location of Mount Etna\nA world globe is depicted, with a marker indicating the location of Mount Etna in Sicily, Italy, in southern Europe near the center of the Mediterranean Sea.\n\n<!-- PageFooter=\"Earth at Night\" -->\n<!-- PageNumber=\"48\" -->"
+  }
+}
+Reference Type: KnowledgeAgentAzureSearchDocReference
+{
+  "type" : "AzureSearchDoc",
+  "id" : "3",
+  "activitySource" : 1,
+  "docKey" : "earth_at_night_508_page_66_verbalized",
+  "sourceData" : {
+    "id" : "earth_at_night_508_page_66_verbalized",
+    "page_chunk" : "# Volcanoes\n\n---\n\n### Mount Etna Erupts - Italy\n\nThe highly active Mount Etna in Italy sent red lava rolling down its flank on March 19, 2017. An astronaut onboard the ISS took the photograph below of the volcano and its environs that night. City lights surround the mostly dark volcanic area.\n\n---\n\n#### Figure 1: Location of Mount Etna, Italy\n\nA world map highlighting the location of Mount Etna in southern Italy. The marker indicates its geographic placement on the east coast of Sicily, Italy, in the Mediterranean region, south of mainland Europe and north of northern Africa.\n\n---\n\n#### Figure 2: Nighttime View of Mount Etna's Eruption and Surrounding Cities\n\nThis is a nighttime satellite image taken on March 19, 2017, showing the eruption of Mount Etna (southeastern cone) with visible bright red and orange coloring indicating flowing lava from a lateral vent. The surrounding areas are illuminated by city lights, with the following geographic references labeled:\n\n| Location        | Position in Image         | Visible Characteristics                    |\n|-----------------|--------------------------|--------------------------------------------|\n| Mt. Etna (southeastern cone) | Top center-left | Bright red/orange lava flow                |\n| Lateral vent    | Left of the volcano       | Faint red/orange flow extending outwards   |\n| Resort          | Below the volcano, to the left   | Small cluster of lights                    |\n| Giarre          | Top right                 | Bright cluster of city lights              |\n| Acireale        | Center right              | Large, bright area of city lights          |\n| Biancavilla     | Bottom left               | Smaller cluster of city lights             |\n\nAn arrow pointing north is shown on the image for orientation.\n\n---\n\n<!-- Earth at Night Page Footer -->\n<!-- Page Number: 50 -->"
+  }
+}
+Reference Type: KnowledgeAgentAzureSearchDocReference
+{
+  "type" : "AzureSearchDoc",
+  "id" : "4",
+  "activitySource" : 1,
+  "docKey" : "earth_at_night_508_page_46_verbalized",
+  "sourceData" : {
+    "id" : "earth_at_night_508_page_46_verbalized",
+    "page_chunk" : "For the first time in perhaps a decade, Mount Etna experienced a \"flank eruption\"ï¿½erupting from its side instead of its summitï¿½on December 24, 2018. The activity was accompanied by 130 earthquakes occurring over three hours that morning. Mount Etna, Europeï¿½s most active volcano, has seen periodic activity on this part of the mountain since 2013. The Operational Land Imager (OLI) on the Landsat 8 satellite acquired the main image of Mount Etna on December 28, 2018.\n\nThe inset image highlights the active vent and thermal infrared signature from lava flows, which can be seen near the newly formed fissure on the southeastern side of the volcano. The inset was created with data from OLI and the Thermal Infrared Sensor (TIRS) on Landsat 8. Ash spewing from the fissure cloaked adjacent villages and delayed aircraft from landing at the nearby Catania airport. Earthquakes occurred in the subsequent days after the initial eruption and displaced hundreds of people from their homes.\n\nFor nighttime images of Mount Etnaï¿½s March 2017 eruption, see pages 48ï¿½51.\n\n---\n\n### Hazards of Volcanic Ash Plumes and Satellite Observation\n\nWith the help of moonlight, satellite instruments can track volcanic ash plumes, which present significant hazards to airplanes in flight. The volcanic ashï¿½composed of tiny pieces of glass and rockï¿½is abrasive to engine turbine blades, and can melt on the blades and other engine parts, causing damage and even engine stalls. This poses a danger to both the planeï¿½s integrity and passenger safety. Volcanic ash also reduces visibility for pilots and can cause etching of windshields, further reducing pilotsï¿½ ability to see. Nightlight images can be combined with thermal images to provide a more complete view of volcanic activity on Earthï¿½s surface.\n\nThe VIIRS Day/Night Band (DNB) on polar-orbiting satellites uses faint light sources such as moonlight, airglow (the atmosphereï¿½s self-illumination through chemical reactions), zodiacal light (sunlight scattered by interplanetary dust), and starlight from the Milky Way. Using these dim light sources, the DNB can detect changes in clouds, snow cover, and sea ice:\n\n#### Table: Light Sources Used by VIIRS DNB\n\n| Light Source         | Description                                                                  |\n|----------------------|------------------------------------------------------------------------------|\n| Moonlight            | Reflected sunlight from the Moon, illuminating Earth's surface at night      |\n| Airglow              | Atmospheric self-illumination from chemical reactions                        |\n| Zodiacal Light       | Sunlight scattered by interplanetary dust                                    |\n| Starlight/Milky Way  | Faint illumination provided by stars in the Milky Way                        |\n\nGeostationary Operational Environmental Satellites (GOES), managed by NOAA, orbit over Earthï¿½s equator and offer uninterrupted observations of North America. High-latitude areas such as Alaska benefit from polar-orbiting satellites like Suomi NPP, which provide overlapping coverage at the poles, enabling more data collection in these regions. During polar darkness (winter months), VIIRS DNB data allow scientists to:\n\n- Observe sea ice formation\n- Monitor snow cover extent at the highest latitudes\n- Detect open water for ship navigation\n\n#### Table: Satellite Coverage Overview\n\n| Satellite Type          | Orbit           | Coverage Area         | Special Utility                              |\n|------------------------|-----------------|----------------------|----------------------------------------------|\n| GOES                   | Geostationary   | Equatorial/North America | Continuous regional monitoring              |\n| Polar-Orbiting (e.g., Suomi NPP) | Polar-orbiting    | Poles/high latitudes      | Overlapping passes; useful during polar night|\n\n---\n\n### Weather Forecasting and Nightlight Data\n\nThe use of nightlight data by weather forecasters is growing as the VIIRS instrument enables observation of clouds at night illuminated by sources such as moonlight and lightning. Scientists use these data to study the nighttime behavior of weather systems, including severe storms, which can develop and strike populous areas at night as well as during the day. Combined with thermal data, visible nightlight data allow the detection of clouds at various heights in the atmosphere, such as dense marine fog. This capability enables weather forecasters to issue marine advisories with higher confidence, leading to greater utility. (See \"Marine Layer Cloudsï¿½California\" on page 56.)\n\nIn this section of the book, you will see how nightlight data are used to observe natureï¿½s spectacular light shows across a wide range of sources.\n\n---\n\n#### Notable Data from Mount Etna Flank Eruption (December 2018)\n\n| Event/Observation                  | Details                                                                    |\n|-------------------------------------|----------------------------------------------------------------------------|\n| Date of Flank Eruption              | December 24, 2018                                                          |\n| Number of Earthquakes               | 130 earthquakes within 3 hours                                              |\n| Image Acquisition                   | December 28, 2018 by Landsat 8 OLI                                         |\n| Location of Eruption                | Southeastern side of Mount Etna                                            |\n| Thermal Imaging Data                | From OLI and TIRS (Landsat 8), highlighting active vent and lava flows     |\n| Impact on Villages/Air Transport    | Ash covered villages; delayed aircraft at Catania airport                  |\n| Displacement                        | Hundreds of residents displaced                                            |\n| Ongoing Seismic Activity            | Earthquakes continued after initial eruption                               |\n\n---\n\n<!-- PageFooter=\"Earth at Night\" -->\n<!-- PageNumber=\"30\" -->"
+  }
+}
+Reference Type: KnowledgeAgentAzureSearchDocReference
+{
+  "type" : "AzureSearchDoc",
+  "id" : "5",
+  "activitySource" : 1,
+  "docKey" : "earth_at_night_508_page_60_verbalized",
+  "sourceData" : {
+    "id" : "earth_at_night_508_page_60_verbalized",
+    "page_chunk" : "<!-- PageHeader=\"Volcanoes\" -->\n\n## Volcanoes\n\n### The Infrared Glows of Kilauea's Lava Flowsï¿½Hawaii\n\nIn early May 2018, an eruption on Hawaii's Kilauea volcano began to unfold. The eruption took a dangerous turn on May 3, 2018, when new fissures opened in the residential neighborhood of Leilani Estates. During the summer-long eruptive event, other fissures emerged along the East Rift Zone. Lava from vents along the rift zone flowed downslope, reaching the ocean in several areas, and filling in Kapoho Bay.\n\nA time series of Landsat 8 imagery shows the progression of the lava flows from May 16 to August 13. The night view combines thermal, shortwave infrared, and near-infrared wavelengths to tease out the very hot lava (bright white), cooling lava (red), and lava flows obstructed by clouds (purple).\n\n#### Figure: Location of Kilauea Volcano, Hawaii\n\nA globe is shown centered on North America, with a marker placed in the Pacific Ocean indicating the location of Hawaii, to the southwest of the mainland United States.\n\n<!-- PageFooter=\"Earth at Night\" -->\n<!-- PageNumber=\"44\" -->"
+  }
+}
+[{"ref_id":0,"content":"## Nature's Light Shows\n\nAt night, with the light of the Sun removed, nature's brilliant glow from Earth's surface becomes visible to the naked eye from space. Some of Earth's most spectacular light shows are natural, like the aurora borealis, or Northern Lights, in the Northern Hemisphere (aurora australis, or Southern Lights, in the Southern Hemisphere). The auroras are natural electrical phenomena caused by charged particles that race from the Sun toward Earth, inducing chemical reactions in the upper atmosphere and creating the appearance of streamers of reddish or greenish light in the sky, usually near the northern or southern magnetic pole. Other natural lights can indicate danger, like a raging forest fire encroaching on a city, town, or community, or lava spewing from an erupting volcano.\n\nWhatever the source, the ability of humans to monitor nature's light shows at night has practical applications for society. For example, tracking fires during nighttime hours allows for continuous monitoring and enhances our ability to protect humans and other animals, plants, and infrastructure. Combined with other data sources, our ability to observe the light of fires at night allows emergency managers to more efficiently and accurately issue warnings and evacuation orders and allows firefighting efforts to continue through the night. With enough moonlight (e.g., full-Moon phase), it's even possible to track the movement of smoke plumes at night, which can impact air quality, regardless of time of day.\n\nAnother natural source of light at night is emitted from glowing lava flows at the site of active volcanoes. Again, with enough moonlight, these dramatic scenes can be tracked and monitored for both scientific research and public safety.\n\n\n### Figure: The Northern Lights Viewed from Space\n\n**September 17, 2011**\n\nThis photo, taken from the International Space Station on September 17, 2011, shows a spectacular display of the aurora borealis (Northern Lights) as green and reddish light in the night sky above Earth. In the foreground, part of a Soyuz spacecraft is visible, silhouetted against the bright auroral light. The green glow is generated by energetic charged particles from the Sun interacting with Earth's upper atmosphere, exciting oxygen and nitrogen atoms, and producing characteristic colors. The image demonstrates the vividness and grandeur of natural night-time light phenomena as seen from orbit."},{"ref_id":1,"content":"# Volcanoes\n\n## Figure: Satellite Image of Sicily and Mount Etna Lava, March 16, 2017\n\nThe annotated satellite image below shows the island of Sicily and the surrounding region at night, highlighting city lights and volcanic activity.\n\n**Description:**\n\n- **Date of image:** March 16, 2017\n- **Geographical locations labeled:**\n    - Major cities: Palermo (northwest Sicily), Marsala (western Sicily), Catania (eastern Sicily)\n    - Significant feature: Mount Etna, labeled with an adjacent \"hot lava\" region showing the glow from active lava flows\n    - Surrounding water body: Mediterranean Sea\n    - Island: Malta to the south of Sicily\n- **Other details:** \n    - The image is shown at night, with bright spots indicating city lights.\n    - The position of \"hot lava\" near Mount Etna is distinctly visible as a bright spot different from other city lights, indicating volcanic activity.\n    - A scale bar is included showing a reference length of 50 km.\n    - North direction is indicated with an arrow.\n    - Cloud cover is visible in the southwest part of the image, partially obscuring the view near Marsala and Malta.\n\n**Summary of Features Visualized:**\n\n| Feature          | Description                                           |\n|------------------|------------------------------------------------------|\n| Cities           | Bright clusters indicating locations: Palermo, Marsala, Catania |\n| Mount Etna       | Marked on the map, located on the eastern side of Sicily, with visible hot lava activity |\n| Malta            | Clearly visible to the south of Sicily               |\n| Water bodies     | Mediterranean Sea labeled                            |\n| Scale & Direction| 50 km scale bar and North indicator                  |\n| Date             | March 16, 2017                                       |\n| Cloud Cover      | Visible in the lower left (southern) part of the image |\n\nThis figure demonstrates the visibility of volcanic activity at Mount Etna from space at night, distinguishing the light from hot lava against the background city lights of Sicily and Malta."},{"ref_id":2,"content":"<!-- PageHeader=\"Volcanoes\" -->\n\n### Nighttime Glow at Mount Etna - Italy\n\nAt about 2:30 a.m. local time on March 16, 2017, the VIIRS DNB on the Suomi NPP satellite captured this nighttime image of lava flowing on Mount Etna in Sicily, Italy. Etna is one of the world's most active volcanoes.\n\n#### Figure: Location of Mount Etna\nA world globe is depicted, with a marker indicating the location of Mount Etna in Sicily, Italy, in southern Europe near the center of the Mediterranean Sea.\n\n<!-- PageFooter=\"Earth at Night\" -->\n<!-- PageNumber=\"48\" -->"},{"ref_id":3,"content":"# Volcanoes\n\n---\n\n### Mount Etna Erupts - Italy\n\nThe highly active Mount Etna in Italy sent red lava rolling down its flank on March 19, 2017. An astronaut onboard the ISS took the photograph below of the volcano and its environs that night. City lights surround the mostly dark volcanic area.\n\n---\n\n#### Figure 1: Location of Mount Etna, Italy\n\nA world map highlighting the location of Mount Etna in southern Italy. The marker indicates its geographic placement on the east coast of Sicily, Italy, in the Mediterranean region, south of mainland Europe and north of northern Africa.\n\n---\n\n#### Figure 2: Nighttime View of Mount Etna's Eruption and Surrounding Cities\n\nThis is a nighttime satellite image taken on March 19, 2017, showing the eruption of Mount Etna (southeastern cone) with visible bright red and orange coloring indicating flowing lava from a lateral vent. The surrounding areas are illuminated by city lights, with the following geographic references labeled:\n\n| Location        | Position in Image         | Visible Characteristics                    |\n|-----------------|--------------------------|--------------------------------------------|\n| Mt. Etna (southeastern cone) | Top center-left | Bright red/orange lava flow                |\n| Lateral vent    | Left of the volcano       | Faint red/orange flow extending outwards   |\n| Resort          | Below the volcano, to the left   | Small cluster of lights                    |\n| Giarre          | Top right                 | Bright cluster of city lights              |\n| Acireale        | Center right              | Large, bright area of city lights          |\n| Biancavilla     | Bottom left               | Smaller cluster of city lights             |\n\nAn arrow pointing north is shown on the image for orientation.\n\n---\n\n<!-- Earth at Night Page Footer -->\n<!-- Page Number: 50 -->"},{"ref_id":4,"content":"For the first time in perhaps a decade, Mount Etna experienced a \"flank eruption\"ï¿½erupting from its side instead of its summitï¿½on December 24, 2018. The activity was accompanied by 130 earthquakes occurring over three hours that morning. Mount Etna, Europeï¿½s most active volcano, has seen periodic activity on this part of the mountain since 2013. The Operational Land Imager (OLI) on the Landsat 8 satellite acquired the main image of Mount Etna on December 28, 2018.\n\nThe inset image highlights the active vent and thermal infrared signature from lava flows, which can be seen near the newly formed fissure on the southeastern side of the volcano. The inset was created with data from OLI and the Thermal Infrared Sensor (TIRS) on Landsat 8. Ash spewing from the fissure cloaked adjacent villages and delayed aircraft from landing at the nearby Catania airport. Earthquakes occurred in the subsequent days after the initial eruption and displaced hundreds of people from their homes.\n\nFor nighttime images of Mount Etnaï¿½s March 2017 eruption, see pages 48ï¿½51.\n\n---\n\n### Hazards of Volcanic Ash Plumes and Satellite Observation\n\nWith the help of moonlight, satellite instruments can track volcanic ash plumes, which present significant hazards to airplanes in flight. The volcanic ashï¿½composed of tiny pieces of glass and rockï¿½is abrasive to engine turbine blades, and can melt on the blades and other engine parts, causing damage and even engine stalls. This poses a danger to both the planeï¿½s integrity and passenger safety. Volcanic ash also reduces visibility for pilots and can cause etching of windshields, further reducing pilotsï¿½ ability to see. Nightlight images can be combined with thermal images to provide a more complete view of volcanic activity on Earthï¿½s surface.\n\nThe VIIRS Day/Night Band (DNB) on polar-orbiting satellites uses faint light sources such as moonlight, airglow (the atmosphereï¿½s self-illumination through chemical reactions), zodiacal light (sunlight scattered by interplanetary dust), and starlight from the Milky Way. Using these dim light sources, the DNB can detect changes in clouds, snow cover, and sea ice:\n\n#### Table: Light Sources Used by VIIRS DNB\n\n| Light Source         | Description                                                                  |\n|----------------------|------------------------------------------------------------------------------|\n| Moonlight            | Reflected sunlight from the Moon, illuminating Earth's surface at night      |\n| Airglow              | Atmospheric self-illumination from chemical reactions                        |\n| Zodiacal Light       | Sunlight scattered by interplanetary dust                                    |\n| Starlight/Milky Way  | Faint illumination provided by stars in the Milky Way                        |\n\nGeostationary Operational Environmental Satellites (GOES), managed by NOAA, orbit over Earthï¿½s equator and offer uninterrupted observations of North America. High-latitude areas such as Alaska benefit from polar-orbiting satellites like Suomi NPP, which provide overlapping coverage at the poles, enabling more data collection in these regions. During polar darkness (winter months), VIIRS DNB data allow scientists to:\n\n- Observe sea ice formation\n- Monitor snow cover extent at the highest latitudes\n- Detect open water for ship navigation\n\n#### Table: Satellite Coverage Overview\n\n| Satellite Type          | Orbit           | Coverage Area         | Special Utility                              |\n|------------------------|-----------------|----------------------|----------------------------------------------|\n| GOES                   | Geostationary   | Equatorial/North America | Continuous regional monitoring              |\n| Polar-Orbiting (e.g., Suomi NPP) | Polar-orbiting    | Poles/high latitudes      | Overlapping passes; useful during polar night|\n\n---\n\n### Weather Forecasting and Nightlight Data\n\nThe use of nightlight data by weather forecasters is growing as the VIIRS instrument enables observation of clouds at night illuminated by sources such as moonlight and lightning. Scientists use these data to study the nighttime behavior of weather systems, including severe storms, which can develop and strike populous areas at night as well as during the day. Combined with thermal data, visible nightlight data allow the detection of clouds at various heights in the atmosphere, such as dense marine fog. This capability enables weather forecasters to issue marine advisories with higher confidence, leading to greater utility. (See \"Marine Layer Cloudsï¿½California\" on page 56.)\n\nIn this section of the book, you will see how nightlight data are used to observe natureï¿½s spectacular light shows across a wide range of sources.\n\n---\n\n#### Notable Data from Mount Etna Flank Eruption (December 2018)\n\n| Event/Observation                  | Details                                                                    |\n|-------------------------------------|----------------------------------------------------------------------------|\n| Date of Flank Eruption              | December 24, 2018                                                          |\n| Number of Earthquakes               | 130 earthquakes within 3 hours                                              |\n| Image Acquisition                   | December 28, 2018 by Landsat 8 OLI                                         |\n| Location of Eruption                | Southeastern side of Mount Etna                                            |\n| Thermal Imaging Data                | From OLI and TIRS (Landsat 8), highlighting active vent and lava flows     |\n| Impact on Villages/Air Transport    | Ash covered villages; delayed aircraft at Catania airport                  |\n| Displacement                        | Hundreds of residents displaced                                            |\n| Ongoing Seismic Activity            | Earthquakes continued after initial eruption                               |\n\n---\n\n<!-- PageFooter=\"Earth at Night\" -->\n<!-- PageNumber=\"30\" -->"},{"ref_id":5,"content":"<!-- PageHeader=\"Volcanoes\" -->\n\n## Volcanoes\n\n### The Infrared Glows of Kilauea's Lava Flowsï¿½Hawaii\n\nIn early May 2018, an eruption on Hawaii's Kilauea volcano began to unfold. The eruption took a dangerous turn on May 3, 2018, when new fissures opened in the residential neighborhood of Leilani Estates. During the summer-long eruptive event, other fissures emerged along the East Rift Zone. Lava from vents along the rift zone flowed downslope, reaching the ocean in several areas, and filling in Kapoho Bay.\n\nA time series of Landsat 8 imagery shows the progression of the lava flows from May 16 to August 13. The night view combines thermal, shortwave infrared, and near-infrared wavelengths to tease out the very hot lava (bright white), cooling lava (red), and lava flows obstructed by clouds (purple).\n\n#### Figure: Location of Kilauea Volcano, Hawaii\n\nA globe is shown centered on North America, with a marker placed in the Pacific Ocean indicating the location of Hawaii, to the southwest of the mainland United States.\n\n<!-- PageFooter=\"Earth at Night\" -->\n<!-- PageNumber=\"44\" -->"}]
+
+[ASSISTANT]: 
+To find lava at night, you can look for the visible glow of active lava flows from erupting volcanoes, which emit light detectable from space during nighttime
+ For example:
+
+- The active lava flows of Mount Etna in Sicily, Italy, have been clearly observed at night by satellites and astronauts aboard the International Space Station
+ The bright red and orange glow of lava distinguishes it from surrounding city lights (refs 1, 3)
+
+
+- Similarly, the Kilauea volcano in Hawaii emits an infrared glow from its lava flows, which can be captured in nighttime satellite imagery combining thermal and near-infrared wavelengths (ref 5)
+
+
+- Nighttime satellite instruments like the VIIRS Day/Night Band (DNB) on the Suomi NPP satellite use faint light sources such as moonlight to detect the glow of lava and volcanic activity even when direct sunlight is absent (refs 2, 4)
+
+
+Therefore, to find lava at night, monitoring nighttime satellite imagery over active volcanic regions is effective, as the glowing lava stands out distinctly against the dark landscape and city lights
+
+
+References: [1], [2], [3], [4], [5]
+
+ === Conversation Complete ===
+[DELETE] Deleting knowledge agent...
+[DONE] Knowledge agent 'earth-search-agent' deleted successfully.
+[DELETE] Deleting search index...
+[DONE] Search index 'earth_at_night' deleted successfully.
+[DONE] Quickstart completed successfully!
+```
+
+## Explaining the code
+
+Now that you have the code, let's break down the key components:
+
+- [Create a search index](#create-a-search-index)
+- [Upload documents to the index](#upload-documents-to-the-index)
+- [Create a knowledge agent](#create-a-knowledge-agent)
+- [Set up messages](#set-up-messages)
+- [Run the retrieval pipeline](#run-the-retrieval-pipeline)
+- [Review the response, activity, and results](#review-the-response-activity-and-results)
+- [Create the Azure OpenAI client](#create-the-azure-openai-client)
+- [Use the Chat Completions API to generate an answer](#use-the-chat-completions-api-to-generate-an-answer)
+- [Continue the conversation](#continue-the-conversation)
+
+### Create a search index
+
+In Azure AI Search, an index is a structured collection of data. The following code defines an index named `earth_at_night` to contain plain text and vector content. You can use an existing index, but it must meet the criteria for [agentic retrieval workloads](../../search-agentic-retrieval-how-to-index.md). 
+
+```java
+List<SearchField> fields = Arrays.asList(
+    new SearchField("id", SearchFieldDataType.STRING)
+        .setKey(true)
+        .setFilterable(true)
+        .setSortable(true)
+        .setFacetable(true),
+    new SearchField("page_chunk", SearchFieldDataType.STRING)
+        .setSearchable(true)
+        .setFilterable(false)
+        .setSortable(false)
+        .setFacetable(false),
+    new SearchField("page_embedding_text_3_large", SearchFieldDataType.collection(SearchFieldDataType.SINGLE))
+        .setSearchable(true)
+        .setFilterable(false)
+        .setSortable(false)
+        .setFacetable(false)
+        .setVectorSearchDimensions(3072)
+        .setVectorSearchProfileName("hnsw_text_3_large"),
+    new SearchField("page_number", SearchFieldDataType.INT32)
+        .setFilterable(true)
+        .setSortable(true)
+        .setFacetable(true)
+);
+
+// Create vectorizer
+AzureOpenAIVectorizer vectorizer = new AzureOpenAIVectorizer("azure_openai_text_3_large")
+    .setParameters(new AzureOpenAIVectorizerParameters()
+        .setResourceUrl(AZURE_OPENAI_ENDPOINT)
+        .setDeploymentName(AZURE_OPENAI_EMBEDDING_DEPLOYMENT)
+        .setModelName(AzureOpenAIModelName.TEXT_EMBEDDING_3_LARGE));
+
+// Create vector search configuration
+VectorSearch vectorSearch = new VectorSearch()
+    .setProfiles(Arrays.asList(
+        new VectorSearchProfile("hnsw_text_3_large", "alg")
+            .setVectorizerName("azure_openai_text_3_large")
+    ))
+    .setAlgorithms(Arrays.asList(
+        new HnswAlgorithmConfiguration("alg")
+    ))
+    .setVectorizers(Arrays.asList(vectorizer));
+
+// Create semantic search configuration
+SemanticSearch semanticSearch = new SemanticSearch()
+    .setDefaultConfigurationName("semantic_config")
+    .setConfigurations(Arrays.asList(
+        new SemanticConfiguration("semantic_config",
+            new SemanticPrioritizedFields()
+                .setContentFields(Arrays.asList(
+                    new SemanticField("page_chunk")
+                ))
+        )
+    ));
+
+// Create the index
+SearchIndex index = new SearchIndex(INDEX_NAME)
+    .setFields(fields)
+    .setVectorSearch(vectorSearch)
+    .setSemanticSearch(semanticSearch);
+
+indexClient.createOrUpdateIndex(index);
+```
+
+The index schema contains fields for document identification and page content, embeddings, and numbers. It also includes configurations for semantic ranking and vector queries, which use the `text-embedding-3-large` model you previously deployed.
+
+### Upload documents to the index
+
+Currently, the `earth_at_night` index is empty. Run the following code to populate the index with JSON documents from [NASA's Earth at Night e-book](https://raw.githubusercontent.com/Azure-Samples/azure-search-sample-data/refs/heads/main/nasa-e-book/earth-at-night-json/documents.json). As required by Azure AI Search, each document conforms to the fields and data types defined in the index schema.
+
+```java
+String documentsUrl = "https://raw.githubusercontent.com/Azure-Samples/azure-search-sample-data/refs/heads/main/nasa-e-book/earth-at-night-json/documents.json";
+        
+try {
+    java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+    java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+        .uri(URI.create(documentsUrl))
+        .build();
+    
+    java.net.http.HttpResponse<String> response = httpClient.send(request, 
+        java.net.http.HttpResponse.BodyHandlers.ofString());
+    
+    if (response.statusCode() != 200) {
+        throw new IOException("Failed to fetch documents: " + response.statusCode());
+    }
+    
+    ObjectMapper mapper = new ObjectMapper();
+    JsonNode jsonArray = mapper.readTree(response.body());
+    
+    List<SearchDocument> documents = new ArrayList<>();
+    for (int i = 0; i < jsonArray.size(); i++) {
+        JsonNode doc = jsonArray.get(i);
+        SearchDocument searchDoc = new SearchDocument();
+        
+        searchDoc.put("id", doc.has("id") ? doc.get("id").asText() : String.valueOf(i + 1));
+        searchDoc.put("page_chunk", doc.has("page_chunk") ? doc.get("page_chunk").asText() : "");
+        
+        // Handle embeddings
+        if (doc.has("page_embedding_text_3_large") && doc.get("page_embedding_text_3_large").isArray()) {
+            List<Double> embeddings = new ArrayList<>();
+            for (JsonNode embedding : doc.get("page_embedding_text_3_large")) {
+                embeddings.add(embedding.asDouble());
+            }
+            searchDoc.put("page_embedding_text_3_large", embeddings);
+        } else {
+            // Fallback embeddings
+            List<Double> fallbackEmbeddings = new ArrayList<>();
+            for (int j = 0; j < 3072; j++) {
+                fallbackEmbeddings.add(0.1);
+            }
+            searchDoc.put("page_embedding_text_3_large", fallbackEmbeddings);
+        }
+        
+        searchDoc.put("page_number", doc.has("page_number") ? doc.get("page_number").asInt() : i + 1);
+        
+        documents.add(searchDoc);
+    }
+    
+    System.out.println("[DONE] Fetched " + documents.size() + " documents from GitHub");
+    return documents;
+    
+}
+```
+
+### Create a knowledge agent
+
+To connect Azure AI Search to your `gpt-4.1-mini` deployment and target the `earth_at_night` index at query time, you need a knowledge agent. The following code defines a knowledge agent named `earth-search-agent` that uses the agent definition to process queries and retrieve relevant documents from the `earth_at_night` index.
+
+To ensure relevant and semantically meaningful responses, `defaultRerankerThreshold` is set to exclude responses with a reranker score of `2.5` or lower.
+
+```java
+ObjectMapper mapper = new ObjectMapper();
+ObjectNode agentDefinition = mapper.createObjectNode();
+agentDefinition.put("name", AGENT_NAME);
+agentDefinition.put("description", "Knowledge agent for Earth at Night e-book content");
+
+ObjectNode model = mapper.createObjectNode();
+model.put("kind", "azureOpenAI");
+ObjectNode azureOpenAIParams = mapper.createObjectNode();
+azureOpenAIParams.put("resourceUri", AZURE_OPENAI_ENDPOINT);
+azureOpenAIParams.put("deploymentId", AZURE_OPENAI_GPT_DEPLOYMENT);
+azureOpenAIParams.put("modelName", AZURE_OPENAI_GPT_MODEL);
+model.set("azureOpenAIParameters", azureOpenAIParams);
+agentDefinition.set("models", mapper.createArrayNode().add(model));
+
+ObjectNode targetIndex = mapper.createObjectNode();
+targetIndex.put("indexName", INDEX_NAME);
+targetIndex.put("defaultRerankerThreshold", 2.5);
+agentDefinition.set("targetIndexes", mapper.createArrayNode().add(targetIndex));
+
+String token = getAccessToken(credential, "https://search.azure.com/.default");
+
+java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+    .uri(URI.create(SEARCH_ENDPOINT + "/agents/" + AGENT_NAME + "?api-version=" + SEARCH_API_VERSION))
+    .header("Content-Type", "application/json")
+    .header("Authorization", "Bearer " + token)
+    .PUT(java.net.http.HttpRequest.BodyPublishers.ofString(mapper.writeValueAsString(agentDefinition)))
+    .build();
+
+java.net.http.HttpResponse<String> response = httpClient.send(request,
+    java.net.http.HttpResponse.BodyHandlers.ofString());
+```
+
+### Set up messages
+
+Messages are the input for the retrieval route and contain the conversation history. Each message includes a role that indicates its origin, such as assistant or user, and content in natural language. The LLM you use determines which roles are valid.
+
+A user message represents the query to be processed, while an assistant message guides the knowledge agent on how to respond. During the retrieval process, these messages are sent to an LLM to extract relevant responses from indexed documents.
+
+This assistant message instructs `earth-search-agent` to answer questions about the Earth at night, cite sources using their `ref_id`, and respond with "I don't know" when answers are unavailable.
+
+```java
+List<Map<String, String>> messages = new ArrayList<>();
+
+Map<String, String> systemMessage = new HashMap<>();
+systemMessage.put("role", "system");
+systemMessage.put("content", "A Q&A agent that can answer questions about the Earth at night.\n" +
+    "Sources have a JSON format with a ref_id that must be cited in the answer.\n" +
+    "If you do not have the answer, respond with \"I don't know\".");
+messages.add(systemMessage);
+
+Map<String, String> userMessage = new HashMap<>();
+userMessage.put("role", "user");
+userMessage.put("content", "Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown? Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?");
+messages.add(userMessage);
+```
+
+### Run the retrieval pipeline
+
+This step runs the retrieval pipeline to extract relevant information from your search index. Based on the messages and parameters on the retrieval request, the LLM:
+1. Analyzes the entire conversation history to determine the underlying information need.
+1. Breaks down the compound user query into focused subqueries.
+1. Runs each subquery simultaneously against text fields and vector embeddings in your index.
+1. Uses semantic ranker to rerank the results of all subqueries.
+1. Merges the results into a single string.
+
+The following code sends a two-part user query to `earth-search-agent`, which deconstructs the query into subqueries, runs the subqueries against both text fields and vector embeddings in the `earth_at_night` index, and ranks and merges the results. The response is then appended to the `messages` list.
+
+```java
+ObjectMapper mapper = new ObjectMapper();
+ObjectNode retrievalRequest = mapper.createObjectNode();
+
+// Convert messages to the correct format expected by the Knowledge agent
+com.fasterxml.jackson.databind.node.ArrayNode agentMessages = mapper.createArrayNode();
+for (Map<String, String> msg : messages) {
+    ObjectNode agentMessage = mapper.createObjectNode();
+    agentMessage.put("role", msg.get("role"));
+    
+    com.fasterxml.jackson.databind.node.ArrayNode content = mapper.createArrayNode();
+    ObjectNode textContent = mapper.createObjectNode();
+    textContent.put("type", "text");
+    textContent.put("text", msg.get("content"));
+    content.add(textContent);
+    agentMessage.set("content", content);
+    
+    agentMessages.add(agentMessage);
+}
+retrievalRequest.set("messages", agentMessages);
+
+com.fasterxml.jackson.databind.node.ArrayNode targetIndexParams = mapper.createArrayNode();
+ObjectNode indexParam = mapper.createObjectNode();
+indexParam.put("indexName", INDEX_NAME);
+indexParam.put("rerankerThreshold", 2.5);
+indexParam.put("maxDocsForReranker", 100);
+indexParam.put("includeReferenceSourceData", true);
+targetIndexParams.add(indexParam);
+retrievalRequest.set("targetIndexParams", targetIndexParams);
+
+String token = getAccessToken(credential, "https://search.azure.com/.default");
+
+java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+    .uri(URI.create(SEARCH_ENDPOINT + "/agents/" + AGENT_NAME + "/retrieve?api-version=" + SEARCH_API_VERSION))
+    .header("Content-Type", "application/json")
+    .header("Authorization", "Bearer " + token)
+    .POST(java.net.http.HttpRequest.BodyPublishers.ofString(mapper.writeValueAsString(retrievalRequest)))
+    .build();
+
+java.net.http.HttpResponse<String> response = httpClient.send(request,
+    java.net.http.HttpResponse.BodyHandlers.ofString());
+```
+
+### Review the response, activity, and results
+
+Now you want to display the response, activity, and results of the retrieval pipeline.
+
+Each retrieval response from Azure AI Search includes:
+
++ A unified string that represents grounding data from the search results.
+
++ The query plan.
+
++ Reference data that shows which chunks of the source documents contributed to the unified string.
+
+```java
+ObjectMapper mapper = new ObjectMapper();
+        
+// Log activities
+System.out.println("\nActivities:");
+if (responseJson.has("activity") && responseJson.get("activity").isArray()) {
+    for (JsonNode activity : responseJson.get("activity")) {
+        String activityType = "UnknownActivityRecord";
+        if (activity.has("InputTokens")) {
+            activityType = "KnowledgeAgentModelQueryPlanningActivityRecord";
+        } else if (activity.has("TargetIndex")) {
+            activityType = "KnowledgeAgentSearchActivityRecord";
+        } else if (activity.has("QueryTime")) {
+            activityType = "KnowledgeAgentSemanticRankerActivityRecord";
+        }
+        
+        System.out.println("Activity Type: " + activityType);
+        try {
+            System.out.println(mapper.writerWithDefaultPrettyPrinter().writeValueAsString(activity));
+        } catch (Exception e) {
+            System.out.println(activity.toString());
+        }
+    }
+}
+
+// Log results
+System.out.println("Results");
+if (responseJson.has("references") && responseJson.get("references").isArray()) {
+    for (JsonNode reference : responseJson.get("references")) {
+        String referenceType = "KnowledgeAgentAzureSearchDocReference";
+        
+        System.out.println("Reference Type: " + referenceType);
+        try {
+            System.out.println(mapper.writerWithDefaultPrettyPrinter().writeValueAsString(reference));
+        } catch (Exception e) {
+            System.out.println(reference.toString());
+        }
+    }
+}
+```
+
+The output should include:
+
++ `Response` provides a text string of the most relevant documents (or chunks) in the search index based on the user query. As shown later in this quickstart, you can pass this string to an LLM for answer generation.
+
++ `Activity` tracks the steps that were taken during the retrieval process, including the subqueries generated by your `gpt-4.1-mini` deployment and the tokens used for query planning and execution.
+
++ `Results` lists the documents that contributed to the response, each one identified by their `DocKey`.
+
+### Create the Azure OpenAI client
+
+To extend the retrieval pipeline from answer *extraction* to answer *generation*, set up the Azure OpenAI client to interact with your `gpt-4.1-mini` deployment.
+
+```java
+OpenAIAsyncClient openAIClient = new OpenAIClientBuilder()
+    .endpoint(AZURE_OPENAI_ENDPOINT)
+    .credential(credential)
+    .buildAsyncClient();
+```
+
+### Use the Chat Completions API to generate an answer
+
+One option for answer generation is the Chat Completions API, which passes the conversation history to the LLM for processing.
+
+```java
+List<ChatRequestMessage> chatMessages = new ArrayList<>();
+for (Map<String, String> msg : messages) {
+    String role = msg.get("role");
+    String content = msg.get("content");
+    
+    switch (role) {
+        case "system":
+            chatMessages.add(new ChatRequestSystemMessage(content));
+            break;
+        case "user":
+            chatMessages.add(new ChatRequestUserMessage(content));
+            break;
+        case "assistant":
+            chatMessages.add(new ChatRequestAssistantMessage(content));
+            break;
+    }
+}
+
+ChatCompletionsOptions chatOptions = new ChatCompletionsOptions(chatMessages)
+    .setMaxTokens(1000)
+    .setTemperature(0.7);
+
+ChatCompletions completion = openAIClient.getChatCompletions(AZURE_OPENAI_GPT_DEPLOYMENT, chatOptions).block();
+```
+
+### Continue the conversation
+
+Continue the conversation by sending another user query to `earth-search-agent`. The following code reruns the retrieval pipeline, fetching relevant content from the `earth_at_night` index and appending the response to the `messages` list. However, unlike before, you can now use the Azure OpenAI client to generate an answer based on the retrieved content.
+
+```java
+String followUpQuestion = "How do I find lava at night?";
+System.out.println("[QUESTION] Follow-up question: " + followUpQuestion);
+
+Map<String, String> userMessage = new HashMap<>();
+userMessage.put("role", "user");
+userMessage.put("content", followUpQuestion);
+messages.add(userMessage);
+```
+
+## Clean up resources
+
+When working in your own subscription, it's a good idea to finish a project by determining whether you still need the resources you created. Resources that are left running can cost you money. You can delete resources individually, or you can delete the resource group to delete the entire set of resources.
+
+In the Azure portal, you can find and manage resources by selecting **All resources** or **Resource groups** from the left pane. You can also run the following code to delete the objects you created in this quickstart.
+
+### Delete the knowledge agent
+
+The knowledge agent created in this quickstart was deleted using the following code snippet:
+
+```java
+String token = getAccessToken(credential, "https://search.azure.com/.default");
+            
+java.net.http.HttpClient httpClient = java.net.http.HttpClient.newHttpClient();
+java.net.http.HttpRequest request = java.net.http.HttpRequest.newBuilder()
+    .uri(URI.create(SEARCH_ENDPOINT + "/agents/" + AGENT_NAME + "?api-version=" + SEARCH_API_VERSION))
+    .header("Authorization", "Bearer " + token)
+    .DELETE()
+    .build();
+
+java.net.http.HttpResponse<String> response = httpClient.send(request,
+    java.net.http.HttpResponse.BodyHandlers.ofString());
+```
+
+### Delete the search index
+
+The search index created in this quickstart was deleted using the following code snippet:
+
+```java
+indexClient.deleteIndex(INDEX_NAME);
+System.out.println("[DONE] Search index '" + INDEX_NAME + "' deleted successfully.");
+```
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "ä»£ç†æ£€ç´¢ Java å¿«é€Ÿå…¥é—¨"
}
```

### Explanation
æœ¬æ¬¡ä»£ç ä¿®æ”¹å¢åŠ äº†ä¸€ä¸ªæ–°çš„å¿«é€Ÿå…¥é—¨ç¤ºä¾‹æ–‡æ¡£ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨ Java å®ç°ä»£ç†æ£€ç´¢åŠŸèƒ½ã€‚è¯¥æ–‡æ¡£æ˜¯å…³äº Azure AI Search çš„ï¼Œé‡ç‚¹åœ¨äºå¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œç”¨æˆ·çš„ä¸“æœ‰æ•°æ®åˆ›å»ºå¯¹è¯æœç´¢ä½“éªŒã€‚é€šè¿‡è¯¥ç¤ºä¾‹ç”¨æˆ·å°†å­¦ä¹ å¦‚ä½•è¿ç”¨ä»£ç†æ£€ç´¢æŠ€æœ¯ï¼Œå°†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºå­æŸ¥è¯¢å¹¶å¹¶è¡Œæ‰§è¡Œï¼Œä»ä¸­æå–æ•°æ®ï¼Œå¹¶å°†ç»“æœç”¨äºè‡ªå®šä¹‰èŠå¤©è§£å†³æ–¹æ¡ˆã€‚

æ–‡æ¡£åŒ…å«äº†è®¾ç½®ä»£ç†æ£€ç´¢çš„è¯¦ç»†æ­¥éª¤ï¼ŒåŒ…æ‹¬å‰ææ¡ä»¶ï¼ˆå¦‚ Azure è´¦æˆ·å’Œç›¸åº”çš„æœåŠ¡é…ç½®ï¼‰ã€é¡¹ç›®ç¯å¢ƒçš„åˆ›å»ºä¸é…ç½®ï¼Œå¦‚å®‰è£…æ‰€éœ€çš„ Java å¼€å‘å·¥å…·åŠç›¸åº”çš„ä¾èµ–åº“ã€‚æ­¤å¤–ï¼Œä»£ç ç¤ºä¾‹æ¶µç›–äº†ä»åˆ›å»ºæœç´¢ç´¢å¼•ã€ä¸Šä¼ æ–‡æ¡£åˆ°åˆ›å»ºçŸ¥è¯†ä»£ç†ã€è¿è¡Œæ£€ç´¢è¯·æ±‚çš„å…¨è¿‡ç¨‹ã€‚

ç”¨æˆ·å°†äº†è§£åˆ°å¦‚ä½•è°ƒç”¨ Azure OpenAI æœåŠ¡ä»¥è·å–ç­”æ¡ˆï¼Œå¹¶ä¸”å¦‚ä½•ç»§ç»­å¯¹è¯ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚è¿™ä¸€ä¿®æ”¹æå¤§åœ°ä¸°å¯Œäº†ä»£ç†æ£€ç´¢çš„ Java å®ç°ç¤ºä¾‹ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°åœ¨ä»–ä»¬çš„é¡¹ç›®ä¸­åº”ç”¨è¿™ä¸€æŠ€æœ¯ã€‚

## articles/search/search-get-started-agentic-retrieval.md{#item-4a40f4}

<details>
<summary>Diff</summary>
````diff
@@ -18,6 +18,10 @@ zone_pivot_groups: search-get-started-agentic-retrieval
 [!INCLUDE [C# quickstart](includes/quickstarts/agentic-retrieval-csharp.md)]
 ::: zone-end
 
+::: zone pivot="programming-language-java"
+[!INCLUDE [Java quickstart](includes/quickstarts/agentic-retrieval-java.md)]
+::: zone-end
+
 ::: zone pivot="programming-language-javascript"
 [!INCLUDE [JavaScript quickstart](includes/quickstarts/agentic-retrieval-javascript.md)]
 ::: zone-end
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "å¢åŠ  Java å¿«é€Ÿå…¥é—¨é“¾æ¥"
}
```

### Explanation
æ­¤ä»£ç ä¿®æ”¹å¯¹ `search-get-started-agentic-retrieval.md` æ–‡æ¡£è¿›è¡Œäº†æ›´æ–°ï¼Œæ·»åŠ äº†å¯¹ Java ç¼–ç¨‹è¯­è¨€çš„å¿«é€Ÿå…¥é—¨é“¾æ¥ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨åŸæœ‰çš„æ–‡æ¡£ä¸­å¢åŠ äº†ä¸€æ®µæ–°ä»£ç ï¼Œç”¨äºå¼•å…¥ Java å¿«é€Ÿå…¥é—¨çš„å†…å®¹ã€‚è¿™ä½¿å¾—æ–‡æ¡£ç°åœ¨åŒæ—¶æ”¯æŒ C#ã€Java å’Œ JavaScript çš„å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿæ›´æ–¹ä¾¿åœ°æ ¹æ®è‡ªå·±çš„ç¼–ç¨‹è¯­è¨€é€‰æ‹©å¹¶å­¦ä¹ å¦‚ä½•å®ç°ä»£ç†æ£€ç´¢åŠŸèƒ½ã€‚

ä¿®æ”¹å†…å®¹åŒ…æ‹¬åœ¨é€‚å½“çš„åŒºåŸŸæ·»åŠ ä»¥ä¸‹ä»£ç ï¼Œä»è€Œå¼•å…¥é’ˆå¯¹ Java çš„å¿«é€Ÿå…¥é—¨ï¼š
```
::: zone pivot="programming-language-java"
[!INCLUDE [Java quickstart](includes/quickstarts/agentic-retrieval-java.md)]
::: zone-end
```
è¿™ä¸€æ›´æ–°æœ‰æ•ˆåœ°å¢å¼ºäº†æ–‡æ¡£çš„å¯ç”¨æ€§å’Œä¿¡æ¯å®Œæ•´æ€§ï¼Œä½¿å¾—æ›´å¤šç”¨æˆ·èƒ½å¤Ÿæ‰¾åˆ°ç›¸å…³çš„å­¦ä¹ èµ„æºã€‚

## articles/search/search-get-started-portal-image-search.md{#item-438b9b}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ ms.author: haileytapia
 ms.service: azure-ai-search
 ms.update-cycle: 90-days
 ms.topic: quickstart
-ms.date: 06/11/2025
+ms.date: 07/16/2025
 ms.custom:
   - references_regions
 ---
@@ -52,7 +52,7 @@ For content embedding, you can choose either image verbalization (followed by te
 | Method | Description | Supported models |
 |--|--|--|
 | Image verbalization | Uses an LLM to generate natural-language descriptions of images, and then uses an embedding model to vectorize plain text and verbalized images.<br><br>Requires an [Azure OpenAI resource](/azure/ai-services/openai/how-to/create-resource) <sup>1, 2</sup> or [Azure AI Foundry project](/azure/ai-foundry/how-to/create-projects).<br><br>For text vectorization, you can also use an [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> in a [supported region](cognitive-search-skill-vision-vectorize.md). | LLMs:<br>GPT-4o<br>GPT-4o-mini<br>phi-4 <sup>4</sup><br><br>Embedding models:<br>text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large |
-| Multimodal embeddings | Uses an embedding model to directly vectorize both text and images.<br><br>Requires an [Azure AI Foundry project](/azure/ai-foundry/how-to/create-projects) or [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> in a [supported region](cognitive-search-skill-vision-vectorize.md). | Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual |
+| Multimodal embeddings | Uses an embedding model to directly vectorize both text and images.<br><br>Requires an [Azure AI Foundry project](/azure/ai-foundry/how-to/create-projects) or [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> in a [supported region](cognitive-search-skill-vision-vectorize.md). | Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br>Cohere-embed-v4 <sup>5</sup> |
 
 <sup>1</sup> The endpoint of your Azure OpenAI resource must have a [custom subdomain](/azure/ai-services/cognitive-services-custom-subdomains), such as `https://my-unique-name.openai.azure.com`. If you created your resource in the [Azure portal](https://portal.azure.com/), this subdomain was automatically generated during resource setup.
 
@@ -62,6 +62,8 @@ For content embedding, you can choose either image verbalization (followed by te
 
 <sup>4</sup> `phi-4` is only available to Azure AI Foundry projects.
 
+<sup>5</sup> The Azure portal doesn't support `embed-v-4-0` for vectorization, so don't use it for this quickstart. Instead, use the [AML skill](cognitive-search-aml-skill.md) or [Azure AI Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) to programmatically specify this model. You can then use the portal to manage the skillset or vectorizer.
+
 ### Public endpoint requirements
 
 All of the preceding resources must have public access enabled so that the Azure portal nodes can access them. Otherwise, the wizard fails. After the wizard runs, you can enable firewalls and private endpoints on the integration components for security. For more information, see [Secure connections in the import wizards](search-import-data-portal.md#secure-connections).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–°å›¾åƒæœç´¢å¿«é€Ÿå…¥é—¨æ–‡æ¡£"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `search-get-started-portal-image-search.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹é¢çš„å˜åŒ–ï¼š

1. **æ–‡ä»¶çš„æœ€åæ›´æ–°æ—¥æœŸæ›´æ–°**ï¼šå°†æ–‡æ¡£ä¸­çš„æ›´æ–°æ—¥æœŸä» `06/11/2025` æ›´æ”¹ä¸º `07/16/2025`ï¼Œç¡®ä¿ç”¨æˆ·è·å¾—æœ€æ–°ä¿¡æ¯ã€‚
   
2. **ä¿®æ”¹äº†æè¿°ä¸­çš„ä¿¡æ¯**ï¼šåœ¨å…³äºå¤šæ¨¡æ€åµŒå…¥çš„éƒ¨åˆ†æ·»åŠ äº†æ–°çš„æ¨¡å‹ `Cohere-embed-v4`ã€‚åŸæœ‰çš„æè¿°åªåŒ…æ‹¬äº† `Cohere-embed-v3-english` å’Œ `Cohere-embed-v3-multilingual`ï¼Œæ›´æ–°åæ–°å¢çš„æ¨¡å‹ä¸ºä»¥ä¸‹å†…å®¹ï¼š
   ```
   | Multimodal embeddings | Uses an embedding model to directly vectorize both text and images.<br><br>Requires an [Azure AI Foundry project](/azure/ai-foundry/how-to/create-projects) or [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> in a [supported region](cognitive-search-skill-vision-vectorize.md). | Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br>Cohere-embed-v4 <sup>5</sup> |
   ```

3. **æ·»åŠ ä½¿ç”¨å»ºè®®**ï¼šåœ¨æ–‡æ¡£æœ«å°¾å¢åŠ äº†å…³äºä¸æ”¯æŒ `embed-v-4-0` è¿›è¡Œå‘é‡åŒ–çš„æŒ‡å¼•ï¼Œå¹¶æ¨èç”¨æˆ·è½¬è€Œä½¿ç”¨ `AML skill` æˆ– `Azure AI Foundry model catalog vectorizer` æ¥ç¼–ç¨‹æŒ‡å®šè¿™ä¸€æ¨¡å‹ã€‚è¯¥ä¿¡æ¯æœ‰æ•ˆåœ°æŒ‡å¯¼ç”¨æˆ·è¿›è¡Œæ›´å®‰å…¨å’Œé«˜æ•ˆçš„æ“ä½œã€‚

4. **å…¶ä»–å°è°ƒæ•´**ï¼šæ–‡æ¡£çš„æ ¼å¼è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥ç¡®ä¿æ ¼å¼çš„ä¸€è‡´æ€§å’Œå¯è¯»æ€§ã€‚

è¿™äº›æ›´æ–°æå‡äº†æ–‡æ¡£çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè·å–æ›´ä¸°å¯Œçš„æ¨¡å‹ä¿¡æ¯å¹¶åœ¨ä½¿ç”¨ Azure è¿›è¡Œå›¾åƒæœç´¢æ—¶åšå‡ºæœ‰æ•ˆå†³ç­–ã€‚

## articles/search/search-get-started-portal-import-vectors.md{#item-7dae77}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: quickstart
-ms.date: 06/11/2025
+ms.date: 07/17/2025
 ---
 
 # Quickstart: Vectorize text in the Azure portal
@@ -49,7 +49,7 @@ For integrated vectorization, you must use one of the following embedding models
 |--|--|
 | [Azure OpenAI in Azure AI Foundry Models](/azure/ai-services/openai/how-to/create-resource) <sup>1, 2</sup> | text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large |
 | [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> | For text and images: [Azure AI Vision multimodal](/azure/ai-services/computer-vision/how-to/image-retrieval) <sup>4</sup></li> |
-| [Azure AI Foundry model catalog](/azure/ai-foundry/what-is-azure-ai-foundry) | For text:<br>Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br><br>For images:<br>Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant |
+| [Azure AI Foundry model catalog](/azure/ai-foundry/what-is-azure-ai-foundry) | For text:<br>Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br><br>For images:<br>Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant<br><br>For text and images:<br>Cohere-embed-v4 <sup>5</sup> |
 
 <sup>1</sup> The endpoint of your Azure OpenAI resource must have a [custom subdomain](/azure/ai-services/cognitive-services-custom-subdomains), such as `https://my-unique-name.openai.azure.com`. If you created your resource in the [Azure portal](https://portal.azure.com/), this subdomain was automatically generated during resource setup.
 
@@ -59,6 +59,8 @@ For integrated vectorization, you must use one of the following embedding models
 
 <sup>4</sup> The Azure AI Vision multimodal embedding model is available in [select regions](/azure/ai-services/computer-vision/overview-image-analysis#region-availability).
 
+<sup>5</sup> The Azure portal doesn't support `embed-v-4-0` for vectorization, so don't use it for this quickstart. Instead, use the [AML skill](cognitive-search-aml-skill.md) or [Azure AI Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) to programmatically specify this model. You can then use the portal to manage the skillset or vectorizer.
+
 ### Public endpoint requirements
 
 For the purposes of this quickstart, all of the preceding resources must have public access enabled so that the Azure portal nodes can access them. Otherwise, the wizard fails. After the wizard runs, you can enable firewalls and private endpoints on the integration components for security. For more information, see [Secure connections in the import wizards](search-import-data-portal.md#secure-connections).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–°çŸ¢é‡å¯¼å…¥æ–‡æ¡£"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `search-get-started-portal-import-vectors.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š

1. **æ›´æ–°æ–‡æ¡£çš„æœ€åä¿®æ”¹æ—¥æœŸ**ï¼šå°†æ–‡æ¡£çš„æ›´æ–°æ—¥æœŸä» `06/11/2025` ä¿®æ”¹ä¸º `07/17/2025`ï¼Œç¡®ä¿ç”¨æˆ·è·å–åˆ°æœ€æ–°çš„æ—¥æœŸä¿¡æ¯ã€‚

2. **æ–°å¢æ”¯æŒæ¨¡å‹**ï¼šåœ¨åµŒå…¥æ¨¡å‹çš„éƒ¨åˆ†ï¼Œä¸ºæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€åµŒå…¥æ·»åŠ äº† `Cohere-embed-v4` æ¨¡å‹ã€‚è¿™ä¸ªæ›´æ”¹å¢å¼ºäº†æ–‡æ¡£çš„å®Œæ•´æ€§ï¼Œä½¿å¾—ç”¨æˆ·åœ¨çŸ¢é‡åŒ–è¿‡ç¨‹ä¸­æœ‰æ›´å¤šçš„é€‰é¡¹ï¼š
   ```
   | [Azure AI Foundry model catalog](/azure/ai-foundry/what-is-azure-ai-foundry) | For text:<br>Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br><br>For images:<br>Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant<br><br>For text and images:<br>Cohere-embed-v4 <sup>5</sup> |
   ```

3. **æä¾›ä½¿ç”¨å»ºè®®**ï¼šæ–°å¢é’ˆå¯¹ `embed-v-4-0` æ¨¡å‹çš„ä½¿ç”¨é™åˆ¶è¯´æ˜ï¼Œå¼ºè°ƒç”¨æˆ·åº”è¯¥ä½¿ç”¨ `AML skill` æˆ– `Azure AI Foundry model catalog vectorizer` æ¥ç¼–ç¨‹æŒ‡å®šæ¨¡å‹ï¼Œä»è€Œé¿å…æ½œåœ¨çš„é—®é¢˜ã€‚

4. **å¢åŠ å…¬å…±ç«¯ç‚¹è¦æ±‚çš„è¯´æ˜**ï¼šæ˜ç¡®åœ¨ä½¿ç”¨è¿™äº›èµ„æºæ—¶ï¼Œéœ€ç¡®ä¿æ‰€æœ‰èµ„æºå…·æœ‰å…¬å…±è®¿é—®æƒé™ï¼Œä»¥ä¾¿ Azure é—¨æˆ·èŠ‚ç‚¹èƒ½å¤Ÿè®¿é—®ã€‚è¿™ä¸€è¯´æ˜å¸®åŠ©ç”¨æˆ·è®¾ç½®æ›´å®‰å…¨çš„ç¯å¢ƒã€‚

è¿™äº›æ›´æ–°æå‡äº†æ–‡æ¡£çš„å®ç”¨æ€§å’Œå¯ç”¨æ€§ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å¦‚ä½•åœ¨ Azure é—¨æˆ·ä¸­è¿›è¡ŒçŸ¢é‡å¯¼å…¥ï¼Œå¹¶ç¡®ä¿ä»–ä»¬èƒ½å¤Ÿè·å¾—æœ€æ–°çš„ä¿¡æ¯å’Œæœ€ä½³å®è·µã€‚

## articles/search/search-how-to-integrated-vectorization.md{#item-86fb1e}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ author: haileytap
 ms.author: haileytapia
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 06/11/2025
+ms.date: 07/17/2025
 ---
 
 # Set up integrated vectorization in Azure AI Search using REST
@@ -48,7 +48,7 @@ For integrated vectorization, you must use one of the following embedding models
 |--|--|
 | [Azure OpenAI in Azure AI Foundry Models](/azure/ai-services/openai/how-to/create-resource) <sup>1, 2</sup> | text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large |
 | [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) <sup>3</sup> | For text and images: [Azure AI Vision multimodal](/azure/ai-services/computer-vision/how-to/image-retrieval) <sup>4</sup></li> |
-<!--| [Azure AI Foundry model catalog](/azure/ai-foundry/what-is-azure-ai-foundry) | For text:<br>Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br><br>For images:<br>Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant |-->
+<!--| [Azure AI Foundry model catalog](/azure/ai-foundry/what-is-azure-ai-foundry) | For text:<br>Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br><br>For images:<br>Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant<br><br>For text and images:<br>Cohere-embed-v4 |-->
 
 <sup>1</sup> The endpoint of your Azure OpenAI resource must have a [custom subdomain](/azure/ai-services/cognitive-services-custom-subdomains), such as `https://my-unique-name.openai.azure.com`. If you created your resource in the [Azure portal](https://portal.azure.com/), this subdomain was automatically generated during resource setup.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–°é›†æˆå‘é‡åŒ–æ–‡æ¡£"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `search-how-to-integrated-vectorization.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹é¢çš„å˜åŒ–ï¼š

1. **æ›´æ–°æ–‡æ¡£çš„æœ€åä¿®æ”¹æ—¥æœŸ**ï¼šå°†æ–‡æ¡£çš„æ›´æ–°æ—¥æœŸä» `06/11/2025` ä¿®æ”¹ä¸º `07/17/2025`ï¼Œç¡®ä¿ç”¨æˆ·è·å–åˆ°æœ€æ–°çš„ä¿¡æ¯ã€‚

2. **è°ƒæ•´åµŒå…¥æ¨¡å‹çš„æè¿°**ï¼šåœ¨åµŒå…¥æ¨¡å‹ä½¿ç”¨éƒ¨åˆ†ï¼Œå°†åŸå…ˆå¯¹ `Azure AI Foundry model catalog` çš„æè¿°è¿›è¡Œäº†ä¸€äº›æ ¼å¼è°ƒæ•´ï¼Œæ·»åŠ äº† `Cohere-embed-v4` æ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°åæ˜ å½“å‰å¯ç”¨çš„æ¨¡å‹é€‰é¡¹ã€‚è¿™åŒ…æ‹¬æ–‡æœ¬æ¨¡å‹å’Œå›¾åƒæ¨¡å‹çš„æ›´æ–°ï¼Œä½¿å¾—ç”¨æˆ·åœ¨é€‰æ‹©æ¨¡å‹æ—¶å…·å¤‡æ›´å¤šçš„çµæ´»æ€§ã€‚

3. **åˆ é™¤äº†åŸæœ‰çš„æ³¨é‡Šè¡Œ**ï¼šåŸå…ˆçš„å…³äº `Azure AI Foundry model catalog` çš„è¡Œè¢«æ³¨é‡Šæ‰ï¼Œæ–°çš„è¡Œåˆ™é‡‡çº³äº†æ›´ä¸ºå…¨é¢çš„æ¨¡å‹æè¿°ã€‚æ­¤æ›´æ–°ç›®çš„æ˜¯ä¸ºäº†æ¸…æ™°åŒ–ä¿¡æ¯ä¼ é€’ï¼Œé¿å…å†—ä½™ã€‚

è¿™æ¬¡æ›´æ–°æå‡äº†æ–‡æ¡£çš„å‡†ç¡®æ€§ï¼Œä½¿ç”¨æˆ·åœ¨è®¾ç½® Azure AI æœç´¢çš„é›†æˆå‘é‡åŒ–æ—¶èƒ½å¤Ÿè·å–æ›´å…¨é¢çš„ä¿¡æ¯å’Œæœ€æ–°çš„æ¨¡å‹é€‰é¡¹ï¼Œä»è€Œæé«˜äº†ä½¿ç”¨ä½“éªŒã€‚

## articles/search/tutorial-adls-gen2-indexer-acls.md{#item-6881a0}

<details>
<summary>Diff</summary>
````diff
@@ -45,7 +45,7 @@ Use a REST client to complete this tutorial and the [2025-05-01-preview](/rest/a
 
 ## Prepare sample data
 
-Upload the [state parks sample data](https://github.com/Azure-Samples/azure-search-sample-data/state-parks) to a container in ADLS Gen2. The container name should be "parks" and it should have two folders: "Oregon" and "Washington".
+Upload the [state parks sample data](https://github.com/Azure-Samples/azure-search-sample-data) to a container in ADLS Gen2. The container name should be "parks" and it should have two folders: "Oregon" and "Washington".
 
 ## Check search service configuration
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–°ADLS Gen2ç´¢å¼•å™¨ACLæ–‡æ¡£ä¸­çš„æ•°æ®é“¾æ¥"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `tutorial-adls-gen2-indexer-acls.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦æ¶‰åŠåˆ°ä»¥ä¸‹æ–¹é¢çš„å†…å®¹ï¼š

1. **æ›´æ–°ç¤ºä¾‹æ•°æ®é“¾æ¥**ï¼šå°†ä¸Šä¼ ç¤ºä¾‹æ•°æ®çš„é“¾æ¥ä»åŸæ¥çš„ `state-parks` å­ç›®å½•æ›´æ–°ä¸ºæŒ‡å‘æ•´ä¸ª `azure-search-sample-data` ä»“åº“ã€‚è¿™ä¸€ä¿®æ”¹ç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿç›´æ¥è®¿é—®æœ€æ–°çš„ç¤ºä¾‹æ•°æ®èµ„æºï¼Œè€Œæ— éœ€ç‰¹æŒ‡æŸä¸ªå­ç›®å½•ï¼Œæå‡äº†è®¿é—®çš„æ–¹ä¾¿æ€§å’Œèµ„æºçš„æ˜“ç”¨æ€§ã€‚

2. **è¯­å¥ç»“æ„è½»å¾®è°ƒæ•´**ï¼šåŒæ—¶ï¼Œæ­¤æ¬¡æ›´æ–°åœ¨æè¿°ä¸­è¿›è¡Œäº†ä¸€ç‚¹è¯­è¨€ä¸Šçš„å¾®è°ƒï¼Œä»¥ç¡®ä¿æŒ‡å‘å†…å®¹çš„å‡†ç¡®æ€§ã€‚

è¿™æ¬¡æ›´æ–°æ—¨åœ¨å¢å¼ºæ–‡æ¡£çš„å‡†ç¡®æ€§ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿé¡ºåˆ©è®¿é—®åˆ°æ‰€éœ€çš„æ ·æœ¬æ•°æ®ï¼Œä»è€Œæ”¯æŒä»–ä»¬åœ¨ ADLS Gen2 ä¸­è¿›è¡Œç´¢å¼•å™¨è®¾ç½®çš„ç›¸å…³æ“ä½œã€‚

## articles/search/tutorial-rag-build-solution-models.md{#item-6796c9}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.update-cycle: 180-days
 ms.topic: tutorial
 ms.custom: references_regions
-ms.date: 06/11/2025
+ms.date: 07/17/2025
 
 ---
 
@@ -52,13 +52,15 @@ Azure AI Search provides skill and vectorizer support for the following embeddin
 
 | Client | Embedding models | Skill | Vectorizer |
 |--------|------------------|-------|------------|
-| Azure OpenAI | text-embedding-ada-002, <br>text-embedding-3-large, <br>text-embedding-3-small | [AzureOpenAIEmbedding](cognitive-search-skill-azure-openai-embedding.md) | [AzureOpenAIEmbedding](vector-search-vectorizer-azure-open-ai.md) |
+| Azure OpenAI | text-embedding-ada-002<br>text-embedding-3-large<br>text-embedding-3-small | [AzureOpenAIEmbedding](cognitive-search-skill-azure-openai-embedding.md) | [AzureOpenAIEmbedding](vector-search-vectorizer-azure-open-ai.md) |
 | Azure AI Vision | multimodal 4.0 <sup>1</sup> | [AzureAIVision](cognitive-search-skill-vision-vectorize.md) | [AzureAIVision](vector-search-vectorizer-ai-services-vision.md) |
-| Azure AI Foundry model catalog | Facebook-DinoV2-Image-Embeddings-ViT-Base, <br>Facebook-DinoV2-Image-Embeddings-ViT-Giant, <br>Cohere-embed-v3-english, <br>Cohere-embed-v3-multilingual | [AML](cognitive-search-aml-skill.md) <sup>2</sup>  | [Azure AI Foundry model catalog](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) |
+| Azure AI Foundry model catalog | Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant<br>Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual<br>Cohere-embed-v4 <sup>1, 2</sup> | [AML](cognitive-search-aml-skill.md) <sup>3</sup> | [Azure AI Foundry model catalog](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) |
 
-<sup>1</sup> Supports image and text vectorization.
+<sup>1</sup> Supports text and image vectorization.
 
-<sup>2</sup> Deployed models in the model catalog are accessed over an AML endpoint. We use the existing AML skill for this connection.
+<sup>2</sup> At this time, you can only specify `embed-v-4-0` programmatically through the [AML skill](cognitive-search-aml-skill.md) or [Azure AI Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md), not through the Azure portal. However, you can use the portal to manage the skillset or vectorizer afterward.
+
+<sup>3</sup> Deployed models in the model catalog are accessed over an AML endpoint. We use the existing AML skill for this connection.
 
 You can use other models besides the ones listed here. For more information, see [Use non-Azure models for embeddings](#use-non-azure-models-for-embeddings) in this article.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–°RAGæ„å»ºè§£å†³æ–¹æ¡ˆæ¨¡å‹æ–‡æ¡£"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `tutorial-rag-build-solution-models.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢çš„æ”¹åŠ¨ï¼š

1. **æ›´æ–°æ—¶é—´æ›´æ–°**ï¼šæ–‡æ¡£çš„æœ€åä¿®æ”¹æ—¥æœŸå·²ä» `06/11/2025` æ›´æ–°ä¸º `07/17/2025`ï¼Œç¡®ä¿ç”¨æˆ·è·å¾—æœ€æ–°æ–‡æ¡£ä¿¡æ¯ã€‚

2. **åµŒå…¥æ¨¡å‹åˆ—è¡¨æ ¼å¼è°ƒæ•´**ï¼šåœ¨åµŒå…¥æ¨¡å‹ä¸€æ ä¸­ï¼Œå¯¹å¤šä¸ªæ¨¡å‹åç§°çš„æ’åˆ—æ ¼å¼è¿›è¡Œäº†è°ƒæ•´ï¼Œå°†åŸå…ˆä»¥é€—å·å’Œæ¢è¡Œåˆ†éš”çš„æ ¼å¼æ”¹ä¸ºé€è¡Œåˆ—å‡ºã€‚è¿™æé«˜äº†å¯è¯»æ€§ï¼Œä½¿å¾—ç”¨æˆ·æ›´å®¹æ˜“è¾¨è¯†ä¸åŒçš„æ¨¡å‹ç±»å‹ã€‚

3. **æ–°æ¨¡å‹çš„æ·»åŠ **ï¼šåœ¨ `Azure AI Foundry model catalog` çš„åµŒå…¥æ¨¡å‹ä¸­åŠ å…¥äº† `Cohere-embed-v4`ï¼Œå¢åŠ äº†ç”¨æˆ·åœ¨é€‰æ‹©æ¨¡å‹æ—¶çš„é€‰é¡¹ï¼Œæå‡äº†æ–‡æ¡£çš„å®ç”¨æ€§ã€‚

4. **è¯´æ˜æ–‡å­—çš„ä¿®æ”¹**ï¼šå¯¹ä¸€äº›è¯´æ˜è¿›è¡Œäº†æ›´æ”¹ï¼Œä½¿å…¶æ›´å‡†ç¡®ã€‚ä¾‹å¦‚ï¼Œå°†å¯¹å›¾ç‰‡å’Œæ–‡æœ¬å‘é‡åŒ–çš„æ”¯æŒæè¿°ç”±â€œæ”¯æŒå›¾åƒå’Œæ–‡æœ¬å‘é‡åŒ–â€æ›´æ­£ä¸ºâ€œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒå‘é‡åŒ–â€ï¼Œä»¥ç¡®ä¿ä¿¡æ¯ä¸€è‡´æ€§ã€‚

5. **å¢åŠ äº†å…³äºç¨‹åºåŒ–æŒ‡å®šæ¨¡å‹çš„è¯´æ˜**ï¼šæ˜ç¡®æŒ‡å‡º `embed-v-4-0` æ¨¡å‹åªèƒ½é€šè¿‡ç¨‹åºåŒ–æ–¹å¼è¿›è¡ŒæŒ‡å®šï¼Œè€Œæ— æ³•é€šè¿‡ Azure é—¨æˆ·ç›´æ¥æ“ä½œï¼Œå¹¶è¡¥å……äº†ä½¿ç”¨é—¨æˆ·ç®¡ç†æŠ€èƒ½é›†æˆ–å‘é‡åŒ–å™¨çš„ç›¸å…³ä¿¡æ¯ã€‚

è¿™æ¬¡æ›´æ–°æ—¨åœ¨æå‡æ–‡æ¡£çš„å‡†ç¡®æ€§å’Œå¯è¯»æ€§ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿè·å¾—æœ€æ–°çš„ä¿¡æ¯å’Œæ›´å¥½çš„ä½¿ç”¨ä½“éªŒï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ„å»ºå…¶è§£å†³æ–¹æ¡ˆæ¨¡å‹ã€‚

## articles/search/vector-search-how-to-configure-vectorizer.md{#item-30ffd8}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.update-cycle: 180-days
 ms.custom:
   - build-2024
 ms.topic: how-to
-ms.date: 05/08/2025
+ms.date: 07/17/2025
 ---
 
 # Configure a vectorizer in a search index
@@ -43,11 +43,13 @@ The following table lists the embedding models that can be used with a vectorize
 
 | Vectorizer kind | Model names | Model provider | Associated skill |
 |-----------------|------------|----------------|------------------|
-| [`azureOpenAI`](vector-search-vectorizer-azure-open-ai.md) | text-embedding-ada-002, text-embedding-3 | Azure OpenAI | [AzureOpenAIEmbedding skill](cognitive-search-skill-azure-openai-embedding.md) |
-| [`aml`](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) | Facebook-DinoV2-Image-Embeddings, Cohere-embed-v3 | [Azure AI Foundry model catalog](vector-search-integrated-vectorization-ai-studio.md)  | [AML skill](cognitive-search-aml-skill.md) |
+| [`azureOpenAI`](vector-search-vectorizer-azure-open-ai.md) | text-embedding-ada-002<br>text-embedding-3 | Azure OpenAI | [AzureOpenAIEmbedding skill](cognitive-search-skill-azure-openai-embedding.md) |
+| [`aml`](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) | Facebook-DinoV2-Image-Embeddings<br>Cohere-embed-v3<br>Cohere-embed-v4 <sup>1</sup> | [Azure AI Foundry model catalog](vector-search-integrated-vectorization-ai-studio.md)  | [AML skill](cognitive-search-aml-skill.md) |
 | [`aiServicesVision`](vector-search-vectorizer-ai-services-vision.md) | [Multimodal embeddings 4.0 API](/azure/ai-services/computer-vision/concept-image-retrieval) | Azure AI Vision (through an Azure AI services multi-service account) | [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) |
 | [`customWebApi`](vector-search-vectorizer-custom-web-api.md) | Any embedding model | Hosted externally | [Custom Web API skill](cognitive-search-custom-skill-web-api.md) |
 
+<sup>1</sup> At this time, you can only specify `embed-v-4-0` programmatically through the [AML skill](cognitive-search-aml-skill.md) or [Azure AI Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md), not through the Azure portal. However, you can use the portal to manage the skillset or vectorizer afterward.
+
 ## Try a vectorizer with sample data
 
 The [Import and vectorize data wizard](search-get-started-portal-import-vectors.md) reads files from Azure Blob storage, creates an index with chunked and vectorized fields, and adds a vectorizer. By design, the vectorizer that's created by the wizard is set to the same embedding model used to index the blob content.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–°å‘é‡æœç´¢é…ç½®æ–‡æ¡£ä¸­çš„æ¨¡å‹å’Œä¿¡æ¯"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `vector-search-how-to-configure-vectorizer.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦æ¶µç›–äº†ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

1. **æ›´æ–°æ—¶é—´æ›´æ–°**ï¼šæ–‡æ¡£çš„æœ€åä¿®æ”¹æ—¥æœŸå·²ä» `05/08/2025` æ›´æ–°ä¸º `07/17/2025`ï¼Œä»¥ç¡®ä¿ç”¨æˆ·è·å–åˆ°æœ€æ–°çš„å†…å®¹ã€‚

2. **åµŒå…¥æ¨¡å‹åˆ—è¡¨æ ¼å¼è°ƒæ•´**ï¼šåœ¨å‘é‡åŒ–å™¨æ”¯æŒçš„åµŒå…¥æ¨¡å‹çš„åˆ—è¡¨ä¸­ï¼Œå¤šä¸ªæ¨¡å‹åç§°çš„æ’ç‰ˆæ ¼å¼è¿›è¡Œäº†æ”¹å˜ã€‚åŸæœ¬ä»¥é€—å·åˆ†éš”çš„æ¨¡å‹åç§°ç°åœ¨ä»¥é€è¡Œåˆ—å‡ºæ–¹å¼å‘ˆç°ï¼Œè¿™æ ·çš„æ ¼å¼æé«˜äº†å¯è¯»æ€§ï¼Œä½¿å¾—ä¿¡æ¯æ›´åŠ æ¸…æ™°ã€‚

3. **æ–°æ¨¡å‹çš„æ·»åŠ **ï¼šåœ¨ `aml` çš„æ¨¡å‹ä¸€æ ä¸­æ–°å¢äº† `Cohere-embed-v4`ï¼Œä¸ºç”¨æˆ·æä¾›äº†æ›´å¤šçš„é€‰æ‹©ï¼Œå¢å¼ºäº†æ–‡æ¡£çš„å®ç”¨æ€§ã€‚

4. **ç¨‹åºåŒ–æŒ‡å®šæ¨¡å‹çš„è¯´æ˜**ï¼šæ˜ç¡®æŒ‡å‡º `embed-v-4-0` æ¨¡å‹åªèƒ½é€šè¿‡ç¨‹åºåŒ–æ–¹å¼æŒ‡å®šï¼Œè€Œä¸èƒ½é€šè¿‡ Azure é—¨æˆ·ç›´æ¥é…ç½®ï¼Œå¹¶è¡¥å……äº†ç”¨æˆ·å¯ä»¥ä½¿ç”¨é—¨æˆ·è¿›è¡ŒæŠ€èƒ½é›†æˆ–å‘é‡åŒ–å™¨ç®¡ç†çš„ä¿¡æ¯ã€‚

5. **ç¤ºä¾‹æ•°æ®ä½¿ç”¨è¯´æ˜**ï¼šæ·»åŠ äº†å…³äºå¦‚ä½•ä½¿ç”¨ç¤ºä¾‹æ•°æ®æ¥å°è¯•å‘é‡åŒ–å™¨çš„ç›¸å…³ä¿¡æ¯ï¼Œä»‹ç»äº†å¦‚ä½•åˆ©ç”¨ `Import and vectorize data wizard` å·¥å…·ä» Azure Blob å­˜å‚¨è¯»å–æ–‡ä»¶å¹¶åˆ›å»ºç´¢å¼•ï¼Œè¿™å¢å¼ºäº†ç”¨æˆ·çš„æ“ä½œæŒ‡å¯¼ï¼Œå¸®åŠ©ä»–ä»¬æ›´æœ‰æ•ˆåœ°ä½¿ç”¨å‘é‡åŒ–å™¨ã€‚

æ­¤æ¬¡æ›´æ–°çš„ç›®æ ‡æ˜¯å¢å¼ºæ–‡æ¡£çš„å®ç”¨æ€§å’Œå¯è¯»æ€§ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿè·å–å‡†ç¡®çš„ä¿¡æ¯ï¼Œä»è€Œæ›´è½»æ¾åœ°é…ç½®å‘é‡åŒ–å™¨å¹¶ä½¿ç”¨ç›¸å…³åŠŸèƒ½ã€‚

## articles/search/vector-search-integrated-vectorization-ai-studio.md{#item-353fcc}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.service: azure-ai-search
 ms.custom:
   - build-2024
 ms.topic: how-to
-ms.date: 07/07/2025
+ms.date: 07/17/2025
 ---
 
 # Use embedding models from Azure AI Foundry model catalog for integrated vectorization
@@ -35,15 +35,13 @@ After the model is deployed, you can use it for [integrated vectorization](vecto
 
 Integrated vectorization and the [Import and vectorize data wizard](search-import-data-portal.md) support the following embedding models in the model catalog:
 
-For text embeddings:
+| Embedding type | Supported models |
+|--|--|
+| Text | Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual |
+| Image | Facebook-DinoV2-Image-Embeddings-ViT-Base<br>Facebook-DinoV2-Image-Embeddings-ViT-Giant |
+| Text and image (multimodal) | Cohere-embed-v4 <sup>1</sup> |
 
-+ Cohere-embed-v3-english
-+ Cohere-embed-v3-multilingual
-
-For image embeddings:
-
-+ Facebook-DinoV2-Image-Embeddings-ViT-Base
-+ Facebook-DinoV2-Image-Embeddings-ViT-Giant
+<sup>1</sup> At this time, you can only specify `embed-v-4-0` programmatically through the [AML skill](cognitive-search-aml-skill.md) or [Azure AI Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md), not through the Azure portal. However, you can use the portal to manage the skillset or vectorizer afterward.
 
 ## Deploy an embedding model from the Azure AI Foundry model catalog
 
@@ -174,19 +172,18 @@ The URI and key are generated when you deploy the model from the catalog. For mo
 
 ### [**Cohere embedding models**](#tab/cohere)
 
-This AML skill payload works with the following text embedding models from Azure AI Foundry:
+This AML skill payload works with the following embedding models from Azure AI Foundry:
 
 + Cohere-embed-v3-english
 + Cohere-embed-v3-multilingual
++ Cohere-embed-v4
 
 It assumes that you're chunking your content using the Text Split skill and therefore your text to be vectorized is in the `/document/pages/*` path. If your text comes from a different path, update all references to the `/document/pages/*` path accordingly.
 
-You must add the `/v1/embed` path onto the end of the URL that you copied from your Azure AI Foundry deployment. You might also change the values for the `input_type`, `truncate` and `embedding_types` inputs to better fit your use case. For more information on the available options, review the [Cohere Embed API reference](/azure/ai-foundry/how-to/deploy-models-cohere-embed).
+You must add the `/v1/embed` path onto the end of the URL that you copied from your Azure AI Foundry deployment. You might also change the values for the `input_type`, `truncate`, and `embedding_types` inputs to better fit your use case. For more information on the available options, review the [Cohere Embed API reference](/azure/ai-foundry/how-to/deploy-models-cohere-embed).
 
 The URI and key are generated when you deploy the model from the catalog. For more information about these values, see [How to deploy Cohere Embed models with Azure AI Foundry](/azure/ai-foundry/how-to/deploy-models-cohere-embed).
 
-Note that image URIs aren't supported by this integration at this time.
-
 ```json
 {
   "@odata.type": "#Microsoft.Skills.Custom.AmlSkill",
@@ -220,9 +217,9 @@ Note that image URIs aren't supported by this integration at this time.
 }
 ```
 
-In addition, the output of the Cohere model isn't the embeddings array directly, but rather a JSON object that contains it. You need to select it appropriately when mapping it to the index definition via `indexProjections` or `outputFieldMappings`. Here's a sample `indexProjections` payload that would allow you to do implement this mapping. 
+In addition, the output of the Cohere model isn't the embeddings array directly, but rather a JSON object that contains it. You need to select it appropriately when mapping it to the index definition via `indexProjections` or `outputFieldMappings`. Here's a sample `indexProjections` payload that would allow you to do implement this mapping.
 
-If you selected a different `embedding_types` in your skill definition that you have to change `float` in the `source` path to the appropriate type that you did select instead.
+If you selected a different `embedding_types` in your skill definition, change `float` in the `source` path to the type you selected.
 
 ```json
 "indexProjections": {
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–°é›†æˆå‘é‡åŒ–æ–‡æ¡£ä¸­çš„åµŒå…¥æ¨¡å‹ä¿¡æ¯"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `vector-search-integrated-vectorization-ai-studio.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ–¹é¢çš„æ”¹åŠ¨ï¼š

1. **æ›´æ–°æ—¶é—´æ›´æ–°**ï¼šæ–‡æ¡£çš„æœ€åä¿®æ”¹æ—¥æœŸå·²ä» `07/07/2025` æ›´æ–°ä¸º `07/17/2025`ï¼Œç¡®ä¿ç”¨æˆ·è·å–åˆ°æœ€æ–°çš„å†…å®¹å’Œæ›´æ–°ä¿¡æ¯ã€‚

2. **æ”¯æŒçš„åµŒå…¥æ¨¡å‹åˆ—è¡¨ä¿®æ”¹**ï¼šæ•´åˆäº†æ”¯æŒçš„åµŒå…¥æ¨¡å‹ï¼Œé‡‡ç”¨è¡¨æ ¼å½¢å¼åˆ—å‡ºæ–‡æœ¬å’Œå›¾åƒåµŒå…¥æ¨¡å‹ï¼Œå¢å¼ºäº†ä¿¡æ¯çš„å¯è¯»æ€§ã€‚æ–°å¢äº†å¯¹ `Cohere-embed-v4` çš„æ”¯æŒè¯´æ˜ï¼Œè¯¥æ¨¡å‹å¯ä»¥å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼ˆå¤šæ¨¡æ€ï¼‰åµŒå…¥ã€‚

3. **æ¨¡å‹ä½¿ç”¨è¯´æ˜æ›´æ˜ç¡®**ï¼šå¯¹é€šè¿‡ç¨‹åºåŒ–æ–¹å¼æŒ‡å®š `embed-v-4-0` æ¨¡å‹çš„è¯´æ˜åšäº†æ›´è¯¦ç»†çš„é˜è¿°ï¼Œæ˜ç¡®æŒ‡å‡ºè¯¥æ¨¡å‹ä¸æ”¯æŒé€šè¿‡ Azure é—¨æˆ·ç›´æ¥é…ç½®ï¼Œè€Œä»…èƒ½é€šè¿‡å·²æœ‰çš„æŠ€èƒ½ï¼ˆå¦‚ AML æŠ€èƒ½ï¼‰æˆ–å‘é‡åŒ–å™¨è¿›è¡Œè®¾ç½®ã€‚

4. **æ–‡æ¡£å†…å®¹ç»“æ„ä¼˜åŒ–**ï¼šåˆ é™¤äº†ä¸€äº›å¤šä½™çš„è¡Œï¼Œç®€åŒ–æ–‡æ¡£å†…å®¹ï¼Œæ”¹å–„äº†æ•´ä½“ç»“æ„ï¼Œä½¿ä¿¡æ¯æ›´åŠ æµç•…æ˜“æ‡‚ã€‚

5. **è¯æ±‡å’Œæœ¯è¯­æ›´æ–°**ï¼šåœ¨æåˆ°ä½¿ç”¨çš„æ¨¡å‹æ—¶ï¼Œå»æ‰äº†ä¸€äº›å†—ä½™æè¿°ï¼Œå¹¶è¿›è¡Œäº†ç”¨è¯ä¼˜åŒ–ï¼Œä¾‹å¦‚å°†â€œæ–‡æœ¬åµŒå…¥â€ç®€åŒ–ä¸ºâ€œåµŒå…¥æ¨¡å‹â€ï¼Œä»¥æé«˜ä¸“ä¸šæ€§å’Œç®€æ´æ€§ã€‚

æ­¤æ¬¡æ›´æ–°æ—¨åœ¨æå‡æ–‡æ¡£çš„å‡†ç¡®æ€§å’Œå¯è¯»æ€§ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿæ›´åŠ æ¸…æ™°åœ°ç†è§£å¹¶æœ‰æ•ˆåœ°å®æ–½é›†æˆå‘é‡åŒ–ï¼ŒåŒæ—¶ä¿è¯è·å–åˆ°æœ€æ–°çš„æ¨¡å‹å’Œä½¿ç”¨è¯´æ˜ã€‚

## articles/search/vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md{#item-ebe7a3}

<details>
<summary>Diff</summary>
````diff
@@ -8,15 +8,15 @@ ms.service: azure-ai-search
 ms.custom:
   - build-2024
 ms.topic: reference
-ms.date: 12/03/2024
+ms.date: 07/17/2024
 ---
 
 #	Azure AI Foundry model catalog vectorizer
 
 > [!IMPORTANT]
-> This vectorizer is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [2024-05-01-Preview REST API](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-05-01-Preview&preserve-view=true) supports this feature.
+> This vectorizer is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). To use this feature, we recommend the latest preview version of [Indexes - Create Or Update](/rest/api/searchservice/indexes/create-or-update) (REST API).
 
-The **Azure AI Foundry model catalog** vectorizer connects to an embedding model that was deployed via [the Azure AI Foundry model catalog](/azure/ai-foundry/how-to/model-catalog-overview) to an Azure Machine Learning endpoint. Your data is processed in the [Geo](https://azure.microsoft.com/explore/global-infrastructure/data-residency/) where your model is deployed. 
+The **Azure AI Foundry model catalog** vectorizer connects to an embedding model that was deployed via the [Azure AI Foundry model catalog](/azure/ai-foundry/how-to/model-catalog-overview) to an Azure Machine Learning endpoint. Your data is processed in the [Geo](https://azure.microsoft.com/explore/global-infrastructure/data-residency/) where your model is deployed. 
 
 If you used integrated vectorization to create the vector arrays, the skillset should include an [AML skill pointing to the model catalog in Azure AI Foundry portal](cognitive-search-aml-skill.md).
 
@@ -27,7 +27,7 @@ Parameters are case-sensitive. Which parameters you choose to use depends on wha
 | Parameter name | Description |
 |--------------------|-------------|
 | `uri` | (Required) The [URI of the AML online endpoint](../machine-learning/how-to-authenticate-online-endpoint.md) to which the _JSON_ payload is sent. Only the **https** URI scheme is allowed. |
-| `modelName` | (Required) The model ID from the Azure AI Foundry model catalog that is deployed at the provided endpoint. Supported models are: <ul><li>Facebook-DinoV2-Image-Embeddings-ViT-Base </li><li>Facebook-DinoV2-Image-Embeddings-ViT-Giant </li><li>Cohere-embed-v3-english </li><li>Cohere-embed-v3-multilingual</ul> |
+| `modelName` | (Required) The model ID from the Azure AI Foundry model catalog that is deployed at the provided endpoint. Supported models are:<p><ul><li>Facebook-DinoV2-Image-Embeddings-ViT-Base </li><li>Facebook-DinoV2-Image-Embeddings-ViT-Giant </li><li>Cohere-embed-v3-english </li><li>Cohere-embed-v3-multilingual</li><li>Cohere-embed-v4</li></ul> |
 | `key` | (Required for [key authentication](#WhatParametersToUse)) The [key for the AML online endpoint](../machine-learning/how-to-authenticate-online-endpoint.md). |
 | `resourceId` | (Required for [token authentication](#WhatParametersToUse)). The Azure Resource Manager resource ID of the AML online endpoint. It should be in the format subscriptions/{guid}/resourceGroups/{resource-group-name}/Microsoft.MachineLearningServices/workspaces/{workspace-name}/onlineendpoints/{endpoint_name}. |
 | `region` | (Optional for [token authentication](#WhatParametersToUse)). The [region](https://azure.microsoft.com/global-infrastructure/regions/) the AML online endpoint is deployed in. Needed if the region is different from the region of the search service. |
@@ -53,8 +53,9 @@ Which vector query types are supported by the Azure AI Foundry model catalog vec
 |--------------------|-------------|-------------|-------------|
 | Facebook-DinoV2-Image-Embeddings-ViT-Base |  | X | X |
 | Facebook-DinoV2-Image-Embeddings-ViT-Giant |  | X | X |
-| Cohere-embed-v3-english | X |  |  |
-| Cohere-embed-v3-multilingual | X |  |  |
+| Cohere-embed-v3-english | X |  | X |
+| Cohere-embed-v3-multilingual | X |  | X |
+| Cohere-embed-v4 | X |  | X |
 
 ## Expected field dimensions
 
@@ -66,6 +67,7 @@ The expected field dimensions for a vector field configured with an Azure AI Fou
 | Facebook-DinoV2-Image-Embeddings-ViT-Giant | 1536 |
 | Cohere-embed-v3-english | 1024 |
 | Cohere-embed-v3-multilingual | 1024 |
+| Cohere-embed-v4 | 256â€“1536 |
 
 ## Sample definition
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "æ›´æ–° Azure AI Foundry æ¨¡å‹ç›®å½•å‘é‡åŒ–å™¨æ–‡æ¡£"
}
```

### Explanation
æ­¤æ¬¡ä»£ç ä¿®æ”¹å¯¹ `vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md` æ–‡æ¡£è¿›è¡Œäº†å°å¹…æ›´æ–°ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ æ–¹é¢çš„å˜åŒ–ï¼š

1. **æ›´æ–°æ—¶é—´æ›´æ–°**ï¼šæ–‡æ¡£çš„æœ€åä¿®æ”¹æ—¥æœŸå·²ä» `12/03/2024` æ›´æ–°ä¸º `07/17/2024`ï¼Œä»¥ç¡®ä¿ç”¨æˆ·è·å–æœ€æ–°çš„ä¿¡æ¯ã€‚

2. **é‡è¦æç¤ºä¿®æ”¹**ï¼šå°†æœ‰å…³å‘é‡åŒ–å™¨åœ¨å…¬å…±é¢„è§ˆçŠ¶æ€çš„æç¤ºè¿›è¡Œäº†æ›´æ–°ï¼Œæ¨èç”¨æˆ·ä½¿ç”¨æœ€æ–°çš„é¢„è§ˆç‰ˆæœ¬çš„ REST API æ¥åˆ©ç”¨æ­¤åŠŸèƒ½ï¼Œç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚

3. **æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨æ‰©å±•**ï¼šåœ¨æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ä¸­ï¼Œæ–°å¢äº† `Cohere-embed-v4` æ¨¡å‹ã€‚æ­¤æ¨¡å‹çš„åŠ å…¥ä¸ºç”¨æˆ·æä¾›äº†æ›´å¤šçš„é€‰æ‹©ï¼Œå¢å¼ºäº†æ–‡æ¡£çš„å®ç”¨æ€§ã€‚

4. **æ–‡æ¡£ä¿¡æ¯ç»“æ„ä¼˜åŒ–**ï¼šå¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œäº†æ ¼å¼åŒ–ä¼˜åŒ–ï¼Œä¾‹å¦‚åœ¨æ¨¡å‹åç§°çš„æ”¯æŒåˆ—è¡¨ä¸­ä¿æŒä¸€è‡´çš„ HTML æ ¼å¼ï¼Œæ”¹è¿›äº†æ•´ä½“çš„å¯è¯»æ€§ã€‚

5. **è¡¨æ ¼å†…å®¹å¾®è°ƒ**ï¼šåœ¨æè¿°å‚æ•°å’Œæ”¯æŒçš„å‘é‡æŸ¥è¯¢ç±»å‹çš„è¡¨æ ¼ä¸­ï¼Œè¿›è¡Œäº†è°ƒæ•´å’Œæ’ç‰ˆä¿®æ”¹ï¼Œä½¿ä¿¡æ¯æ›´åŠ æ¸…æ™°æ˜“æ‡‚ã€‚å¯¹äºæ¨¡å‹æ”¯æŒçš„å‘é‡æŸ¥è¯¢ç±»å‹ä¹Ÿè¿›è¡Œäº†é€‚å½“çš„æ ‡æ³¨æ›´æ–°ã€‚

6. **å­—æ®µç»´åº¦æ‰©å±•**ï¼šåœ¨æœŸå¾…çš„å­—æ®µç»´åº¦éƒ¨åˆ†ï¼Œæ–°å¢äº† `Cohere-embed-v4` æ¨¡å‹çš„ç»´åº¦ä¿¡æ¯ï¼Œæå‡å¯¹ä½¿ç”¨è€…çš„æŒ‡å¯¼ä»·å€¼ã€‚

æ­¤æ¬¡æ›´æ–°çš„ç›®çš„æ˜¯ä¸ºäº†æå‡æ–‡æ¡£çš„å‡†ç¡®æ€§ã€å¯è¯»æ€§å’Œå®ç”¨æ€§ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿæ›´å¥½åœ°äº†è§£å¦‚ä½•ä½¿ç”¨ Azure AI Foundry çš„å‘é‡åŒ–å™¨åŠŸèƒ½ã€‚


