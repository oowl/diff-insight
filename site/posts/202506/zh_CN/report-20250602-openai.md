---
date: '2025-06-02'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:ca0291a...MicrosoftDocs:178e7a4
summary: 此次更新主要涉及文档日期信息的更新和多处内容的优化，同时新增了多张图像以提升可视化效果。目标是提高文档的准确性、可读性和用户体验。新功能包括引入图像文件以帮助用户更好地进行评估操作以及在部分文档中增加新章节以详细介绍使用方法。无重大破坏性更改，主要进行了正向改进。所有文件的日期信息已更新至2025年5月31日，并对文档表达进行了优化，确保用户能够轻松获取所需信息。整体而言，更新旨在提升用户体验，使Azure
  AI服务的使用更加便利。
title: '[zh_CN] Diff Insight Report - openai'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:ca0291a...MicrosoftDocs:178e7a4){target="_blank"}

<format>
# Highlights
此次更新主要包括文档中日期信息的更新和多处内容的优化。此外，还新增了一系列图像文件，提升了文档的可视化表现。这些修改旨在提高文档的准确性、可读性和用户体验。

## New features
- 新增了多张图像文件，以协助用户更好地进行评估相关操作。这些图像包括评估生成、语义相似度评估、提交作业和测试标准的说明。
- 在部分文档中引入了新的章节或内容，以便更详细介绍相关内容和使用方法，如输入限制说明等。

## Breaking changes
- 无重大破坏性更改。本次更新主要采取了正向改进，提升文档的准确性和可理解性，没有引入与现有功能不兼容的变化。

## Other updates
- 所有涉及文件的日期信息均已更新至 `05/31/2025`，以反映当前的文档更新时效性。
- 优化了一些文档的书面表达，使其更为简洁明了，同时通过小幅内容删减增强可读性。
- 章节标题的变更如"Next steps"变更为"Related content"，更准确地反映内容的性质。

# Insights
本次更新覆盖范围广泛，主要集中在文档性能和内容表达的改进上。通过统一更新日期信息，确保了用户获取到的是最新的文档状态，这对于长期维护文档的时效性及一致性至关重要。

新增加的图像为用户提供了直观的使用指导，特别是在操作复杂或新功能的说明上，借助这些图像，用户可以更高效地执行相应任务并理解其步骤。这种图文并茂的方式提升了用户学习新工具和功能的效率。

细节性的内容优化和语言调整则展现出了对用户体验的重视。此类文档经常是技术人员和用户操作的首选参考，故需保持高度的简洁和可读性。通过细致的语言润色和结构优化，用户在查阅时能够轻松获得需要的信息而不至于因过多冗余内容导致理解偏差。

总之，此次更新不仅仅是文档上的简单修订，而是一次面向用户体验的全面优化，助力Azure AI服务用户更为便利地使用相关功能，增强其业务能力。
</format>

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [default-safety-policies.md](#item-39b6a0) | minor update | 更新默认安全政策文档的日期信息 | modified | 1 | 1 | 2 | 
| [gpt-4-v-prompt-engineering.md](#item-fd7772) | minor update | 更新GPT-4视觉提示工程文档的日期和内容 | modified | 2 | 2 | 4 | 
| [gpt-with-vision.md](#item-991388) | minor update | 更新GPT与视觉相关的文档内容和日期 | modified | 12 | 12 | 24 | 
| [prompt-transformation.md](#item-21e047) | minor update | 更新提示转换文档日期 | modified | 1 | 1 | 2 | 
| [dall-e-quickstart.md](#item-fcd528) | minor update | 更新DALL·E快速入门文档日期 | modified | 1 | 1 | 2 | 
| [gpt-v-quickstart.md](#item-2a6183) | minor update | 更新GPT-V快速入门文档内容 | modified | 3 | 7 | 10 | 
| [evaluations.md](#item-dfaa1c) | minor update | 更新评估文档以增加内容和清晰度 | modified | 72 | 36 | 108 | 
| [gpt-with-vision.md](#item-4d8502) | minor update | 更新GPT与视觉模型文档内容 | modified | 3 | 3 | 6 | 
| [risks-safety-monitor.md](#item-b2be0b) | minor update | 更新风险与安全监控文档内容 | modified | 3 | 3 | 6 | 
| [use-blocklists.md](#item-e99db7) | minor update | 更新使用阻止列表文档内容 | modified | 5 | 5 | 10 | 
| [create-new-eval.png](#item-d7bc67) | new feature | 新增创建新评估的图像 | added | 0 | 0 | 0 | 
| [eval-generate-1.png](#item-5be1c8) | new feature | 新增评估生成的图像 | added | 0 | 0 | 0 | 
| [eval-generate-2.png](#item-efeac5) | new feature | 新增评估生成的第二张图像 | added | 0 | 0 | 0 | 
| [eval-semantic-similarity-1.png](#item-8729c1) | new feature | 新增语义相似度评估的图像 | added | 0 | 0 | 0 | 
| [eval-semantic-similarity-2.png](#item-a316c2) | new feature | 新增第二张语义相似度评估的图像 | added | 0 | 0 | 0 | 
| [eval-semantic-similarity-3.png](#item-0c0125) | new feature | 新增第三张语义相似度评估的图像 | added | 0 | 0 | 0 | 
| [eval-submit-job-2.png](#item-d8110c) | new feature | 新增第二张提交作业的图像 | added | 0 | 0 | 0 | 
| [eval-submit-job.png](#item-a9a1f5) | new feature | 新增提交作业的图像 | added | 0 | 0 | 0 | 
| [eval-testing-criteria-2.png](#item-220e27) | new feature | 新增测试标准的图像 | added | 0 | 0 | 0 | 
| [eval-testing-criteria.png](#item-268f4b) | new feature | 新增评估标准的图像 | added | 0 | 0 | 0 | 
| [upload-data-1.png](#item-cd5af0) | new feature | 新增上传数据的图像 | added | 0 | 0 | 0 | 
| [upload-data-2.png](#item-de63da) | new feature | 新增上传数据的第二张图像 | added | 0 | 0 | 0 | 


# Modified Contents
## articles/ai-services/openai/concepts/default-safety-policies.md{#item-39b6a0}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新默认安全政策文档的日期信息"
}
```

### Explanation
此次代码差异修改涉及对文件 `default-safety-policies.md` 的小幅更新，主要是将文档的日期从原来的 `02/20/2025` 修改为 `05/31/2025`。这一改动是满足最新的文档更新要求，确保读者能获得正确的日期信息。修改引入了1行新增内容，同时删除了1行，体现了文档在格式或内容上的必要性调整。该变更不会影响文档的其他内容或结构，仅仅是对时间信息的更新。具体变更可以查看[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/concepts/default-safety-policies.md)。

## articles/ai-services/openai/concepts/gpt-4-v-prompt-engineering.md{#item-fd7772}

<details>
<summary>Diff</summary>
````diff
@@ -6,13 +6,13 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
 # Image prompt engineering techniques
 
-To unlock the full potential of vision-enabled chat models like GPT-4 Turbo with Vision and GPT-4o, it's essential to tailor the prompts to your specific needs. Here are some guidelines to enhance the accuracy and efficiency of your prompts.
+To unlock the full potential of vision-enabled chat models, it's essential to tailor the prompts to your specific needs. Here are some guidelines to enhance the accuracy and efficiency of your prompts.
 
 ## Fundamentals of writing an image prompt
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GPT-4视觉提示工程文档的日期和内容"
}
```

### Explanation
此次代码差异修改涉及对文件 `gpt-4-v-prompt-engineering.md` 的更新，主要包含以下两个方面的变动：

1. **日期更新**：文档中的日期信息已从 `02/20/2025` 更新为 `05/31/2025`，这确保了文档的时效性和相关性。
   
2. **内容调整**：对文档中的一段文字进行了修改，去掉了“像 GPT-4 Turbo with Vision 和 GPT-4o”这部分描述，使得文本更加简洁流畅，更加专注于视觉启用的聊天模型的提示工程相关内容。 

整体而言，此次修改属于小幅更新，增加了2行内容，同时删除了2行，体现了在内容精简和信息更新上的必要性。详细的更改可以查看[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/concepts/gpt-4-v-prompt-engineering.md)。

## articles/ai-services/openai/concepts/gpt-with-vision.md{#item-991388}

<details>
<summary>Diff</summary>
````diff
@@ -6,13 +6,13 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
 # Vision-enabled chat model concepts
 
-Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are GPT-4 Turbo with Vision, GPT-4o, and GPT-4o-mini. This guide provides details on their capabilities and limitations.
+Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. This guide provides details on their capabilities and limitations. To see which models support image input, see the [Models page](./models.md).
 
 To try out vision-enabled chat models, see the [quickstart](/azure/ai-services/openai/gpt-v-quickstart).
 
@@ -21,6 +21,16 @@ To try out vision-enabled chat models, see the [quickstart](/azure/ai-services/o
 The vision-enabled models answer general questions about what's present in the images you upload.
 
 
+## Input limitations
+
+This section describes the limitations of vision-enabled chat models.
+
+### Image support
+
+- **Maximum input image size**: The maximum size for input images is restricted to 20 MB.
+- **Low resolution accuracy**: When images are analyzed using the "low resolution" setting, it allows for faster responses and uses fewer input tokens for certain use cases. However, this could impact the accuracy of object and text recognition within the image.
+- **Image chat restriction**: When you upload images in [Azure AI Foundry portal](https://ai.azure.com/) or the API, there is a limit of 10 images per chat call.
+
 ## Special pricing information
 
 > [!IMPORTANT]
@@ -70,16 +80,6 @@ For a typical use case, take a 3-minute video with a 100-token prompt input. The
 Additionally, there's a one-time indexing cost of $0.15 to generate the Video Retrieval index for this 3-minute video. This index can be reused across any number of Video Retrieval and GPT-4 Turbo with Vision API calls.
 -->
 
-## Input limitations
-
-This section describes the limitations of vision-enabled chat models.
-
-### Image support
-
-- **Maximum input image size**: The maximum size for input images is restricted to 20 MB.
-- **Low resolution accuracy**: When images are analyzed using the "low resolution" setting, it allows for faster responses and uses fewer input tokens for certain use cases. However, this could impact the accuracy of object and text recognition within the image.
-- **Image chat restriction**: When you upload images in [Azure AI Foundry portal](https://ai.azure.com/) or the API, there is a limit of 10 images per chat call.
-
 <!--
 ### Video support
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GPT与视觉相关的文档内容和日期"
}
```

### Explanation
此次代码差异修改发生在文件 `gpt-with-vision.md` 中，主要包括以下几个方面的变动：

1. **日期更新**：文档中的日期信息已从 `02/20/2025` 更新为 `05/31/2025`，以确保文档的时效性和准确性。

2. **内容调整**：文档中有关视觉启用聊天模型的描述进行了一些重要修改。移除了对特定模型（GPT-4 Turbo with Vision, GPT-4o 和 GPT-4o-mini）的直接提及，改为强调了这些模型的能力和限制。同时，为了方便用户获取支持图像输入的模型，增加了指向“模型页面”的链接。

3. **新增章节**：添加了一个关于输入限制的新章节，包括对最大输入图像大小、低分辨率准确性以及图像聊天限制的详细说明。特别地，描述了以下几点：
   - 最大输入图像大小限制为20 MB。
   - 使用“低分辨率”设置时，可能会影响物体和文本识别的准确性。
   - 在Azure AI Foundry门户或API中上传图像时，每次聊天调用限制为10张图片。

整体而言，此次修改涉及12行内容的新增与删除，体现了在信息更新和用户指导方面的重要性。具体变更可查看[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/concepts/gpt-with-vision.md)。

## articles/ai-services/openai/concepts/prompt-transformation.md{#item-21e047}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新提示转换文档日期"
}
```

### Explanation
此次代码差异修改发生在文件 `prompt-transformation.md` 中，主要包含了一个重要的更新：

1. **日期更新**：文档中的日期信息已从 `02/20/2025` 更新为 `05/31/2025`。这一修改旨在反映最近的文档时效性，确保用户获取到最新的信息。

此次修改总体上为小幅更新，仅涉及1行的新增与1行的删除。更多详细信息可以参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/concepts/prompt-transformation.md)。

## articles/ai-services/openai/dall-e-quickstart.md{#item-fcd528}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom: devx-track-python, devx-track-dotnet, devx-track-extended-java, devx-
 ms.topic: quickstart
 author: PatrickFarley
 ms.author: pafarley
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 zone_pivot_groups: openai-quickstart-dall-e
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新DALL·E快速入门文档日期"
}
```

### Explanation
此次代码差异修改发生在文件 `dall-e-quickstart.md` 中，主要包括以下变动：

1. **日期更新**：文档中的日期信息从 `02/20/2025` 更改为 `05/31/2025`。这一修改旨在确保文档反映最新的时效性，以便用户获得最新信息。

此次更新为小幅变动，仅涉及1行的修改，继续确保DALL·E相关内容的准确性和时效性。更多详细信息可参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/dall-e-quickstart.md)。

## articles/ai-services/openai/gpt-v-quickstart.md{#item-2a6183}

<details>
<summary>Diff</summary>
````diff
@@ -9,18 +9,14 @@ ms.custom: devx-track-python, devx-track-js, devx-track-ts
 ms.topic: quickstart
 author: PatrickFarley
 ms.author: pafarley
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 zone_pivot_groups: openai-quickstart-gpt-v
 ---
 
 # Quickstart: Use images in your AI chats
 
-Get started using GPT-4 Turbo with images with the Azure OpenAI in Azure AI Foundry Models.
+Get started using images in your chats with Azure OpenAI in Azure AI Foundry Models.
 
-> [!NOTE]
-> **Model choice**
->
-> The latest vision-capable models are `gpt-4o` and `gpt-4o mini`. These models are in public preview. The latest available GA model is `gpt-4` version `turbo-2024-04-09`.
 
 > [!IMPORTANT]
 > Extra usage fees might apply when using chat completion models with vision functionality.
@@ -62,7 +58,7 @@ Get started using GPT-4 Turbo with images with the Azure OpenAI in Azure AI Foun
 
 ::: zone-end
 
-## Next steps
+## Related content
 
 * [Get started with multimodal vision chat apps using Azure OpenAI AI App template](/azure/developer/ai/get-started-app-chat-vision?tabs=github-codespaces)
 * Learn more about these APIs in the [Vision-enabled models how-to guide](./gpt-v-quickstart.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GPT-V快速入门文档内容"
}
```

### Explanation
此次代码差异修改发生在文件 `gpt-v-quickstart.md` 中，主要包括以下变动：

1. **日期更新**：文档中的日期信息从 `02/20/2025` 更改为 `05/31/2025`，确保文档时效性。

2. **内容优化**：修改了部分文本，具体包括将原句“Get started using GPT-4 Turbo with images with the Azure OpenAI in Azure AI Foundry Models.” 简化为“Get started using images in your chats with Azure OpenAI in Azure AI Foundry Models.”，使其更加简洁明了。

3. **章节标题更改**：原先的章节标题“Next steps”更改为“Related content”，以更准确地反映该部分内容。

此次修改包含3行新增和7行删除，共造成10行的变化，通过这些调整，进一步提高了文档的可读性和清晰度。更多详细信息可参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/gpt-v-quickstart.md)。

## articles/ai-services/openai/how-to/evaluations.md{#item-dfaa1c}

<details>
<summary>Diff</summary>
````diff
@@ -18,14 +18,39 @@ The evaluation of large language models is a critical step in measuring their pe
 
 Azure OpenAI evaluation enables developers to create evaluation runs to test against expected input/output pairs, assessing the model’s performance across key metrics such as accuracy, reliability, and overall performance.
 
-## Evaluations support
+## Evaluation support
 
 ### Regional availability
 
-- East US2
+- Australia East
+- Brazil South
+- Canada Central
+- Central US
+- East US 2
+- France Central
+- Germany West Central
+- Italy North
+- Japan East
+- Japan West
+- Korea Central
 - North Central US
+- Norway East
+- Poland Central
+- South Africa North
+- Southeast Asia
+- Spain Central
 - Sweden Central
+- Switzerland North
 - Switzerland West
+- UAE North
+- UK South
+- UK West
+- West Europe
+- West US
+- West US 2
+- West US 3
+
+If your preferred region is missing, refer to [Azure OpenAI regions](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#global-standard-model-availability) and check if it is one of the Azure OpenAI regional availability zones.
 
 ### Supported deployment types
 
@@ -36,6 +61,10 @@ Azure OpenAI evaluation enables developers to create evaluation runs to test aga
 - Global provisioned-managed
 - Data zone provisioned-managed
 
+## Evaluation API (preview)
+
+Evaluation API lets you test model outputs directly through API calls, and programmatically assess model quality and performance. To use Evaluation API, check out the [REST API documentation](https://learn.microsoft.com/azure/ai-services/openai/authoring-reference-preview#evaluation---get-list). 
+
 ## Evaluation pipeline
 
 ### Test data
@@ -90,30 +119,40 @@ Outputs generated during the evaluation will be referenced in subsequent steps u
 
 ### Model deployment
 
-As part of creating evaluations you'll pick which models to use when generating responses (optional) as well as which models to use when grading models with specific testing criteria.  
-
-In Azure OpenAI you'll be assigning specific model deployments to use as part of your evaluations. You can compare multiple model deployments in single evaluation run.
+In Azure OpenAI, you need to create a model deployment to use for your evaluation. You can pick and deploy a single model, or multiple models, depending on your needs. These model deployments will be used when grading your base model or your fine-tuned model with the test criteria of your choice. You can also use the deployed models to auto-generate responses for your provided prompt. 
 
-You can evaluate either base or fine-tuned model deployments. The deployments available in your list depend on those you created within your Azure OpenAI resource. If you can't find the desired deployment, you can create a new one from the Azure OpenAI Evaluation page.
+The deployments available in your list depend on those you created within your Azure OpenAI resource. If you can't find the desired deployment, you can create a new one from the Azure OpenAI Evaluation page.
 
 ### Testing criteria
 
 Testing criteria is used to assess the effectiveness of each output generated by the target model. These tests compare the input data with the output data to ensure consistency. You have the flexibility to configure different criteria to test and measure the quality and relevance of the output at different levels.
 
-:::image type="content" source="../media/how-to/evaluations/testing-criteria.png" alt-text="Screenshot that shows the evaluations testing criteria options." lightbox="../media/how-to/evaluations/testing-criteria.png":::
+:::image type="content" source="../media/how-to/evaluations/eval-testing-criteria.png" alt-text="Screenshot that shows the evaluations testing criteria options." lightbox="../media/how-to/evaluations/eval-testing-criteria.png":::
+
+When you click into each testing criteria, you will see different types of graders as well as preset schemas that you can modify per your own evaluation dataset and criteria. 
+
+:::image type="content" source="../media/how-to/evaluations/eval-testing-criteria-2.png" alt-text="Screenshot that shows the evaluations testing criteria options." lightbox="../media/how-to/evaluations/eval-testing-criteria-2.png":::
 
 ## Getting started
 
 1. Select the **Azure OpenAI Evaluation (PREVIEW)** within [Azure AI Foundry portal](https://ai.azure.com/). To see this view as an option may need to first select an existing Azure OpenAI resource in a supported region.
-2. Select **New evaluation**
+2. Select **+ New evaluation**
 
     :::image type="content" source="../media/how-to/evaluations/new-evaluation.png" alt-text="Screenshot of the Azure OpenAI evaluation UX with new evaluation selected." lightbox="../media/how-to/evaluations/new-evaluation.png":::
 
-3. Enter a name of your evaluation. By default a random name is automatically generated unless you edit and replace it. Select **Upload new dataset**.
+3. Choose how you would like to provide test data for evaluation. You can import stored Chat Completions, create data using provided default templates, or upload your own data. Let's walk through uploading your own data. 
+
+    :::image type="content" source="../media/how-to/evaluations/create-new-eval.png" alt-text="Screenshot of the Azure OpenAI create new evaluation." lightbox="../media/how-to/evaluations/create-new-eval.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/upload.png" alt-text="Screenshot of the Azure OpenAI upload UX." lightbox="../media/how-to/evaluations/upload.png":::
+4. Select your evaluation data which will be in `.jsonl` format. If you already have an existing data, you can select one, or upload a new data.
 
-4. Select your evaluation which will be in `.jsonl` format. If you need a sample test file you can save these 10 lines to a file called `eval-test.jsonl`:
+    :::image type="content" source="../media/how-to/evaluations/upload-data-1.png" alt-text="Screenshot of data upload." lightbox="../media/how-to/evaluations/upload-data-1.png":::
+
+   When you upload new data, you'll see the first three lines of the file as a preview on the right side:
+
+    :::image type="content" source="../media/how-to/evaluations/upload-data-2.png" alt-text="Screenshot of data upload." lightbox="../media/how-to/evaluations/upload-data-2.png":::
+
+   If you need a sample test file, you can use this sample `.jsonl` text. This sample contains sentences of various technical content, and we are going to be assessing semantic similarity across these sentences.
 
     ```jsonl
     {"input": [{"role": "system", "content": "Provide a clear and concise summary of the technical content, highlighting key concepts and their relationships. Focus on the main ideas and practical implications."}, {"role": "user", "content": "Tokenization is a key step in preprocessing for natural language processing, involving the division of text into smaller components called tokens. These can be words, subwords, or characters, depending on the method chosen. Word tokenization divides text at word boundaries, while subword techniques like Byte Pair Encoding (BPE) or WordPiece can manage unknown words by breaking them into subunits. Character tokenization splits text into individual characters, useful for multiple languages and misspellings. The tokenization method chosen greatly affects model performance and its capacity to handle various languages and vocabularies."}], "output": "Tokenization divides text into smaller units (tokens) for NLP applications, using word, subword (e.g., BPE), or character methods. Each has unique benefits, impacting model performance and language processing capabilities."}      
@@ -127,48 +166,45 @@ Testing criteria is used to assess the effectiveness of each output generated by
     {"input": [{"role": "system", "content": "Generate a clear and focused summary that captures the essential technical details while maintaining their relationships and significance."}, {"role": "user", "content": "Transfer learning is a machine learning method where a model developed for one task is reused as the starting point for a model on a second task. This approach is powerful in deep learning, where pre-trained models on large datasets (like ImageNet for computer vision or BERT for NLP) are fine-tuned on specific downstream tasks. Transfer learning reduces the need for large amounts of task-specific training data and computational resources, as the model has already learned useful features from the source domain. Common strategies include feature extraction (freezing pre-trained layers) and fine-tuning (updating all or some pre-trained weights)."}], "output": "Transfer learning reuses models trained on one task for different tasks, particularly effective in deep learning. It leverages pre-trained models through feature extraction or fine-tuning, reducing data and computational needs for new tasks."}      
     {"input": [{"role": "system", "content": "Provide a precise and informative summary that distills the key technical concepts while maintaining their relationships and practical importance."}, {"role": "user", "content": "Ensemble methods combine multiple machine learning models to create a more robust and accurate predictor. Common techniques include bagging (training models on random data subsets), boosting (sequentially training models to correct earlier errors), and stacking (using a meta-model to combine base model predictions). Random Forests, a popular bagging method, create multiple decision trees using random feature subsets. Gradient Boosting builds trees sequentially, with each tree correcting the errors of previous ones. These methods often outperform single models by reducing overfitting and variance while capturing different data aspects."}], "output": "Ensemble methods enhance prediction accuracy by combining multiple models through techniques like bagging, boosting, and stacking. Popular implementations include Random Forests (using multiple trees with random features) and Gradient Boosting (sequential error correction), offering better performance than single models."}
     ```
+    
+5. If you would like to create new responses using inputs from your test data, you can select 'Generate new responses'. This will inject the input fields from our evaluation file into individual prompts for a model of your choice to generate output.
 
-    You'll see the first three lines of the file as a preview:
-
-    :::image type="content" source="../media/how-to/evaluations/preview.png" alt-text="Screenshot that shows a preview of an uploaded evaluation file." lightbox="../media/how-to/evaluations/preview.png":::
-
-5. Under **Responses** select the **Create** button. Select `{{item.input}}` from the **Create with a template** dropdown. This will inject the input fields from our evaluation file into individual prompts for a new model run that we want to able to compare against our evaluation dataset. The model will take that input and generate its own unique outputs which in this case will be stored in a variable called `{{sample.output_text}}`. We'll then use that sample output text later as part of our testing criteria. Alternatively you could provide your own custom system message and individual message examples manually.
-
-6. Select which model you want to generate responses based on your evaluation. If you don't have a model you can create one. For the purpose of this example we're using a standard deployment of `gpt-4o-mini`.
+:::image type="content" source="../media/how-to/evaluations/eval-generate-1.png" alt-text="Screenshot of the UX for generating model responses." lightbox="../media/how-to/evaluations/eval-generate-1.png":::
+   
+You will select the model of your choice. If you do not have a model, you can create a new model deployment. The selected model will take the input data and generate its own unique outputs, which in this case will be stored in a variable called `{{sample.output_text}}`. We'll then use that output later as part of our testing criteria. Alternatively, you could provide your own custom system message and individual message examples manually.
 
-    :::image type="content" source="../media/how-to/evaluations/item-input.png" alt-text="Screenshot of the UX for generating model responses with a model selected." lightbox="../media/how-to/evaluations/item-input.png":::
+:::image type="content" source="../media/how-to/evaluations/eval-generate-2.png" alt-text="Screenshot of the UX for generating model responses." lightbox="../media/how-to/evaluations/eval-generate-2.png":::
 
-    The settings/sprocket symbol controls the basic parameters that are passed to the model. Only the following parameters are supported at this time:
+6. For creating a test criteria, select **Add**. For the example file we provided, we are going to be assessing semantic similarity. Select **Model Scorer**, which contains test criteria presets for Semantic Similarity.
 
-- **Temperature**
-- **Maximum length**
-- **Top P**
+    :::image type="content" source="../media/how-to/evaluations/eval-semantic-similarity-1.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/eval-semantic-similarity-1.png":::
 
- Maximum length is currently capped at 2048 regardless of what model you select.
+   Select **Semantic Similarity** at the top. Scroll to the bottom, and in `User` section, specify `{{item.output}}` as `Ground truth`, and specify `{{sample.output_text}}` as `Output`. This will take the original reference output from your evaluation `.jsonl` file (the sample file provided) and compare it against the output that is generated by the model you chose in the previous step.
 
-7. Select **Add testing criteria** select **Add**.
+    :::image type="content" source="../media/how-to/evaluations/eval-semantic-similarity-2.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/eval-semantic-similarity-2.png":::
 
-8. Select **Semantic Similarity** >  Under **Compare** add `{{item.output}}` under **With** add ``{{sample.output_text}}``. This will take the original reference output from your evaluation `.jsonl` file and compare it against the output that will be generated by giving the model prompts based on your ``{{item.input}}``.
+:::image type="content" source="../media/how-to/evaluations/eval-semantic-similarity-3.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/eval-semantic-similarity-3.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/semantic-similarity-config.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/semantic-similarity-config.png":::
+7. Select **Add** to add this testing criteria. If you would like to add additional testing criteria, you can add them at this step.
 
-9. Select **Add** > at this point you can either add additional testing criteria or you select Create to initiate the evaluation job run.
+8. You are ready to create your Evaluation. Provide your Evaluation name, review everything looks correct, and **Submit** to create the Evaluation job. You'll be taken to a status page for your evaluation job, which will show the status as "Waiting".
 
-10. Once you select **Create** you'll be taken to a status page for your evaluation job.
+:::image type="content" source="../media/how-to/evaluations/eval-submit-job.png" alt-text="Screenshot of the evaluation job submit UX." lightbox="../media/how-to/evaluations/eval-submit-job.png":::
+:::image type="content" source="../media/how-to/evaluations/eval-submit-job-2.png" alt-text="Screenshot of the evaluation job submit UX." lightbox="../media/how-to/evaluations/eval-submit-job-2.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/status.png" alt-text="Screenshot of the evaluation status UX." lightbox="../media/how-to/evaluations/status.png":::
+9. Once your evaluation job has created, you can select the job to view the full details of the job:
 
-11. Once your evaluation job has created you can select the job to view the full details of the job:
+:::image type="content" source="../media/how-to/evaluations/test-complete.png" alt-text="Screenshot of a completed semantic similarity test with mix of pass and failures." lightbox="../media/how-to/evaluations/test-complete.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/test-complete.png" alt-text="Screenshot of a completed semantic similarity test with mix of pass and failures." lightbox="../media/how-to/evaluations/test-complete.png":::
+10. For semantic similarity **View output details** contains a JSON representation that you can copy/paste of your passing tests.
 
-12. For semantic similarity **View output details** contains a JSON representation that you can copy/paste of your passing tests.
+:::image type="content" source="../media/how-to/evaluations/output-details.png" alt-text="Screenshot of the evaluation status UX with output details." lightbox="../media/how-to/evaluations/output-details.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/output-details.png" alt-text="Screenshot of the evaluation status UX with output details." lightbox="../media/how-to/evaluations/output-details.png":::
+11. You can also add more Eval runs by selecting **+ Add Run** button at the top left corner of your evaluation job page.
 
-## Testing criteria details
+## Types of Testing Criteria
 
-Azure OpenAI Evaluation offers multiple testing criteria options. The section below provides additional details on each option.
+Azure OpenAI Evaluation offers various evaluation testing criteria on top of Semantic Similarity we saw in the provided example. This section provides information about each testing criteria at much more detail.
 
 ### Factuality
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新评估文档以增加内容和清晰度"
}
```

### Explanation
此次代码差异修改发生在文件 `evaluations.md` 中，主要包括以下内容的显著变化：

1. **新增内容**：共增加了72行，删除了36行，整体发生了108行的变化。更新后的文档信息更为详尽，包括支持的区域、API使用说明、可用的模型部署方式等，丰富了用户对于Azure OpenAI评估功能的理解。

2. **区域支持的更新**：在“Regional availability”部分，列出了新的地区，如澳大利亚东部、巴西南部、法国中部等，增强了对用户的地域支持指南。

3. **API功能介绍**：新增了“Evaluation API (preview)”的部分，描述如何通过API直接测试模型输出，提供了链接以获取REST API文档。这一点特别重要，使开发者能够以编程方式评估模型的质量和性能。

4. **内容组织优化**：在“Getting started”部分中，更新了步骤说明，指导用户如何选择和上传评估数据，提升了用户体验和操作便捷性。

5. **示例和截图的添加**：增添了多个示例和截图，以便用户更清晰地理解如何使用评估功能，特别是在创建测试数据和评估标准的过程中。

整体修改针对用户需求进行了全面优化，使得文档不仅更新了时间信息，还增强了可操作性和信息的准确性。更多详细信息可参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/how-to/evaluations.md)。

## articles/ai-services/openai/how-to/gpt-with-vision.md{#item-4d8502}

<details>
<summary>Diff</summary>
````diff
@@ -7,14 +7,14 @@ ms.author: pafarley #delegenz
 #customer intent: As a developer, I want to learn how to use vision-enabled chat models so that I can integrate image processing capabilities into my applications.
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
 # Use vision-enabled chat models
 
 
-Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are [o1](./reasoning.md), GPT-4o, GPT-4o-mini, and GPT-4 Turbo with Vision.
+Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are the [o-series reasoning models](./reasoning.md), GPT-4.1 series models, GPT-4.5, GPT-4o series, and GPT-4 Turbo with Vision.
 
 The vision-enabled models can answer general questions about what's present in the images you upload.
 
@@ -383,7 +383,7 @@ Every response includes a `"finish_reason"` field. It has the following possible
 [!INCLUDE [GPT-4 Turbo](../includes/gpt-4-turbo.md)]
 
 
-## Next steps
+## Related content
 
 * [Learn more about Azure OpenAI](../overview.md).
 * [Vision-enabled chats quickstart](../gpt-v-quickstart.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新GPT与视觉模型文档内容"
}
```

### Explanation
此次代码差异修改发生在文件 `gpt-with-vision.md` 中，主要包括以下内容调整：

1. **日期更新**：文档中的发布日期从 `02/20/2025` 更改为 `05/31/2025`，确保信息的时效性。

2. **模型系列描述更新**：在对视觉启用聊天模型的描述中，细化了模型的名称，将“当前视觉启用模型是[o1](./reasoning.md)、GPT-4o、GPT-4o-mini和GPT-4 Turbo with Vision。”替换为“当前视觉启用模型是[o系列推理模型](./reasoning.md)、GPT-4.1系列模型、GPT-4.5、GPT-4o系列和GPT-4 Turbo with Vision。” 这种修改提供了更清晰的模型分类。

3. **章节标题更改**：将“Next steps”章节标题更改为“Related content”，更好地反映出该部分内容的性质。

这些变动增强了文档的准确性和可读性，使用户对可视聊天模型的了解更加清晰。更多详细信息可参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/how-to/gpt-with-vision.md)。

## articles/ai-services/openai/how-to/risks-safety-monitor.md{#item-b2be0b}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley 
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
@@ -43,14 +43,14 @@ Adjust your content filter configuration to further align with business needs an
 
 ## Potentially abusive user detection   
 
-The **Potentially abusive user detection** pane leverages user-level abuse reporting to show information about users whose behavior has resulted in blocked content. The goal is to help you get a view of the sources of harmful content so you can take responsive actions to ensure the model is being used in a responsible way. 
+The **Potentially abusive user detection** pane shows information about users whose behavior has resulted in blocked content. The goal is to help you get a view of the sources of harmful content so you can take responsive actions to ensure the model is being used in a responsible way. 
 
 
 To use Potentially abusive user detection, you need:
 - A content filter configuration applied to your deployment.
 - You must be sending user ID information in your Chat Completion requests (see the _user_ parameter of the [Completions API](/azure/ai-services/openai/reference#completions), for example).
     > [!CAUTION]
-    > Use GUID strings to identify individual users. Do not include sensitive personal information in the _user_ field.
+    > Use GUID strings to identify individual users. Don't include sensitive personal information in the _user_ field.
 - An Azure Data Explorer database set up to store the user analysis results (instructions below).
 
 ### Set up your Azure Data Explorer database
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新风险与安全监控文档内容"
}
```

### Explanation
此次代码差异修改发生在文件 `risks-safety-monitor.md` 中，主要包含以下变化：

1. **日期更新**：文档中的发布日期从 `02/20/2025` 更改为 `05/31/2025`，保证了内容的时效性。

2. **潜在滥用用户检测部分的调整**：
   - 描述中删除了“leverages user-level abuse reporting”这一表述，使得这一部分的内容更为简洁，减少了冗余信息，明确了功能的核心：该面板显示用户行为信息，帮助识别导致内容被阻止的用户。
   
3. **语句优化**：将“Do not include sensitive personal information in the _user_ field.”中的“Do not”更改为“Don't”，使得语气更为口语化，提高了阅读的流畅性。

这些修改提升了文档的清晰度和可读性，同时保持了信息的完整性。更多详细信息可参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/how-to/risks-safety-monitor.md)。

## articles/ai-services/openai/how-to/use-blocklists.md{#item-e99db7}

<details>
<summary>Diff</summary>
````diff
@@ -5,7 +5,7 @@ description: Learn how to use blocklists with Azure OpenAI
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 author: PatrickFarley
 ms.author: pafarley
 ---
@@ -17,7 +17,7 @@ The [configurable content filters](/azure/ai-services/openai/how-to/content-filt
 ## Prerequisites
 
 - An Azure subscription. <a href="https://azure.microsoft.com/free/ai-services" target="_blank">Create one for free</a>.
-- Once you have your Azure subscription, create an Azure OpenAI resource in the Azure portal to get your token, key and endpoint. Enter a unique name for your resource, select the subscription you entered on the application form, select a resource group, supported region, and supported pricing tier. Then select **Create**.
+- Once you have your Azure subscription, create an Azure OpenAI resource in the Azure portal to get your token, key, and endpoint. Enter a unique name for your resource, select the subscription you entered on the application form, select a resource group, supported region, and supported pricing tier. Then select **Create**.
     - The resource takes a few minutes to deploy. After it finishes, select **go to resource**. In the left pane, under **Resource Management**, select **Subscription Key and Endpoint**. The endpoint and either of the keys are used to call APIs.
 - [Azure CLI](/cli/azure/install-azure-cli) installed
 - [cURL](https://curl.haxx.se/) installed
@@ -30,7 +30,7 @@ You can create blocklists with the Azure OpenAI API. The following steps help yo
 
 ### Get your token
 
-First, you need to get a token for accessing the APIs for creating, editing and deleting blocklists. You can get this token using the following Azure CLI command: 
+First, you need to get a token for accessing the APIs for creating, editing, and deleting blocklists. You can get this token using the following Azure CLI command: 
 
 ```bash
 az account get-access-token 
@@ -102,7 +102,7 @@ Copy the cURL command below to a text editor and make the following changes:
 1. Replace {raiBlocklistName} (in the URL) with a custom name for your list. Allowed characters: `0-9, A-Z, a-z, - . _ ~`. 
 1. Replace {raiBlocklistItemName} with a custom name for your list item. 
 1. Replace {token} with the token you got from the "Get your token" step above. 
-1. Replace the value of the `"blocking pattern"` field with the item you'd like to add to your blocklist. The maximum length of a blockItem is 1000 characters. Also specify whether the pattern is regex or exact match. 
+1. Replace the value of the `"blocking pattern"` field with the item you'd like to add to your blocklist. The maximum length of a blockItem is 1,000 characters. Also specify whether the pattern is regex or exact match. 
 
 ```bash
 curl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}/raiBlocklistItems/{raiBlocklistItemName}?api-version=2024-04-01-preview' \ 
@@ -117,7 +117,7 @@ curl --location --request PUT 'https://management.azure.com/subscriptions/{subsc
 ```
 
 > [!NOTE]
-> It can take around 5 minutes for a new term to be added to the blocklist. Please test after 5 minutes. 
+> It can take around 5 minutes for a new term to be added to the blocklist. Test the blocklist after 5 minutes. 
 
 The response code should be `200`. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新使用阻止列表文档内容"
}
```

### Explanation
此次代码差异修改发生在文件 `use-blocklists.md` 中，主要包含以下更改：

1. **日期更新**：文档发布日期从 `02/20/2025` 更改为 `05/31/2025`，以保持信息的时效性。

2. **条目格式规范化**：在说明中，逗号后添加了空格，例如“token, key and endpoint”更改为“token, key, and endpoint”，使整体句子结构更为清晰。

3. **步骤内容调整**：
   - 在指示创建 Azure OpenAI资源的步骤中，删除了“和”后面的空格，使句子更加规范。
   - 在给出创建、编辑和删除阻止列表的命令时，使用了“和”而不是“和，”，以提高语句的流畅性。

4. **文字表达优化**：内容中关于阻止列表的文字表达进行了微调，使得部分句子更为简洁明了，例如将“Please test after 5 minutes.”更改为“Test the blocklist after 5 minutes.”，增强了可读性。

这些更新有助于提升文档的可读性和专业性，同时确保文中信息的准确和一致。更多详细信息可参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/how-to/use-blocklists.md)。

## articles/ai-services/openai/media/how-to/evaluations/create-new-eval.png{#item-d7bc67}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增创建新评估的图像"
}
```

### Explanation
此次代码差异涉及文件 `create-new-eval.png` 的添加，这是一个新增图像文件，旨在补充如何创建新评估的相关内容。该图像将为用户提供可视化的指导，帮助他们更好地理解和执行创建评估的步骤。

此次修改没有任何代码行的添加或删除，仅为文档增添了重要的视觉元素，从而增强了用户体验和信息传达的效率。更多详细信息可查看[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/create-new-eval.png)。

## articles/ai-services/openai/media/how-to/evaluations/eval-generate-1.png{#item-5be1c8}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增评估生成的图像"
}
```

### Explanation
此次代码差异涉及文件 `eval-generate-1.png` 的添加，作为如何生成评估的示例图像。此新增图像旨在为用户提供直观的步骤指南，从而帮助他们更清晰地理解生成评估的过程。

该修改未涉及任何其他代码的增删，仅通过引入新的视觉元素来增强文档的可读性和实用性。图像的存在有助于用户掌握复杂的概念或操作，进一步提升了学习和使用体验。详细信息请参见[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-generate-1.png)。

## articles/ai-services/openai/media/how-to/evaluations/eval-generate-2.png{#item-efeac5}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增评估生成的第二张图像"
}
```

### Explanation
此代码差异记录了文件 `eval-generate-2.png` 的新增，这是一个用于展示评估生成过程的图像。该图像旨在补充用户对生成评估步骤的理解，为文档添加直观的视觉指导。

此次修改没有涉及任何代码行的添加或删除，仅通过添加新的图像来增强文档的功能性和可读性。该图像的加入将帮助用户更好地掌握相关概念，从而提高整体使用体验。更多信息请参考[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-generate-2.png)。

## articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-1.png{#item-8729c1}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增语义相似度评估的图像"
}
```

### Explanation
此次代码差异展示了文件 `eval-semantic-similarity-1.png` 的添加，该图像用于说明语义相似度评估过程。此新增图像为用户提供了可视化的参考，有助于他们更好地理解语义相似度评估的步骤和方法。

此次修改未涉及其他代码的增删，仅通过引入新的视觉内容来丰富文档的表现力，提升用户的学习体验。新增图像旨在为用户提供更清晰的指引，使复杂的概念变得更加易于理解。详细信息可以通过[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-1.png)获取。

## articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-2.png{#item-a316c2}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增第二张语义相似度评估的图像"
}
```

### Explanation
该代码差异显示了文件 `eval-semantic-similarity-2.png` 的新增，这张图像用于进一步说明语义相似度评估的过程和方法。这一新增内容为有关语义相似度的讨论提供了直观的视觉支持，帮助用户更容易地理解复杂概念。

此次修改没有涉及任何其他代码行的添加或删除，仅通过新增的图像来增强文档内容。新增图像旨在为用户提供更加全面的理解工具，使其更好地掌握有关语义相似度评估的知识。有关该图像的更多信息，可以访问[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-2.png)。

## articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-3.png{#item-0c0125}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增第三张语义相似度评估的图像"
}
```

### Explanation
此次代码差异展示了文件 `eval-semantic-similarity-3.png` 的添加，这张图像用于补充说明语义相似度评估的相关概念。这项修改通过引入新的视觉元素，旨在帮助用户更深入地理解语义相似度评估的流程及其应用。

此次更新没有对现有代码进行修改、添加或删除，仅通过新增图像来增强文档的可读性和信息量。这张图像为用户提供了更多的上下文，有助于他们掌握和运用语义相似度评估的知识。更多信息可以通过[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-3.png)访问。

## articles/ai-services/openai/media/how-to/evaluations/eval-submit-job-2.png{#item-d8110c}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增第二张提交作业的图像"
}
```

### Explanation
该代码差异展示了文件 `eval-submit-job-2.png` 的新增，这张图像旨在进一步说明提交作业的过程及相关操作。这一新增为文档提供了直观的视觉指导，帮助用户更清晰地理解如何在评估过程中提交作业。

此次更新没有涉及对现有内容的修改或删除，仅通过添加新图像来增强文档的结构和信息完整性。这张图像能够帮助用户更好地掌握提交作业的步骤和注意事项。有关该图像的进一步信息，可以通过[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-submit-job-2.png)访问。

## articles/ai-services/openai/media/how-to/evaluations/eval-submit-job.png{#item-a9a1f5}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增提交作业的图像"
}
```

### Explanation
此次代码差异显示了文件 `eval-submit-job.png` 的添加，这张图像用于展示如何在评估中提交作业。这一新特性为用户提供了必要的视觉指引，从而帮助他们更好地理解提交作业的步骤和相关要求。

此次修改并未对现有文件内容进行任何删改，专注于通过新增图像来提升文档的可读性和有效性。该图像是在文档中引入的，可为用户详细展示如何操作提交作业。欲查看该图像，可以通过[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-submit-job.png)访问。

## articles/ai-services/openai/media/how-to/evaluations/eval-testing-criteria-2.png{#item-220e27}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增测试标准的图像"
}
```

### Explanation
该代码差异显示了文件 `eval-testing-criteria-2.png` 的新增，此图像旨在展示评估过程中的测试标准。这一新特性为用户提供了视觉上的指导，使他们可以更轻松地理解和应用相关的评估标准。

在此次修改中，没有对任何现有内容进行删除或更改，专注于通过新增图像来增强文档的展示效果，便于用户在实际操作中参考。用户可通过[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-testing-criteria-2.png)查看该图像的具体内容。

## articles/ai-services/openai/media/how-to/evaluations/eval-testing-criteria.png{#item-268f4b}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增评估标准的图像"
}
```

### Explanation
此代码差异展示了文件 `eval-testing-criteria.png` 的添加，该图像用于阐述在评估过程中应遵循的测试标准。这一新增特性为用户提供了清晰的视觉指导，有助于他们更好地理解如何执行评估。

此次修改没有对现有的文件内容进行任何删除或更改，重点在于通过新增的图像来增强文档的使用体验。用户可以通过[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/eval-testing-criteria.png)访问该图像，获取更多信息。

## articles/ai-services/openai/media/how-to/evaluations/upload-data-1.png{#item-cd5af0}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增上传数据的图像"
}
```

### Explanation
该代码差异显示了文件 `upload-data-1.png` 的新增，这张图像用来说明如何上传数据的过程。这一新特性为用户提供了必要的视觉参考，旨在帮助他们更轻松地完成数据上传的操作。

此次修改没有对其他内容进行任何删除或更改，专注于通过新增图像来提升文档的可用性和理解。用户可以通过[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/upload-data-1.png)查看该图像的具体内容，以获取更详细的帮助和指导。

## articles/ai-services/openai/media/how-to/evaluations/upload-data-2.png{#item-de63da}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增上传数据的第二张图像"
}
```

### Explanation
此代码差异记录了 `upload-data-2.png` 文件的新增，该图像旨在进一步说明数据上传的过程。通过这种方式，用户可以获得更多的视觉指导，以帮助他们更有效地完成数据上传的操作。

此次修改没有影响到现有内容的删除或更改，完全是通过新增图像来提升文档的辅助性和教育性。用户可以访问[此链接](https://github.com/MicrosoftDocs/azure-ai-docs/blob/178e7a4063ff73d0c1f1b4471242bdce3e1b5787/articles/ai-services/openai/media/how-to/evaluations/upload-data-2.png)查看这张图像，获取有关上传数据的具体信息和指导。


