---
date: '2025-06-05'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:45b2d74...MicrosoftDocs:c6cc5fa
summary: 此次代码更新新增了多个文档图像，旨在增强用户理解和体验，同时进行了功能更新和改进。一些不再使用的图像被移除，以维持文档的准确性。更新包括增加了图像文件和对快速入门文档的详细说明，以及对文档描述和格式的轻微调整。通过这些更新，用户可以更清晰、直观地掌握多模态搜索和向量化处理的概念，进而提高工作效率。
title: '[zh_CN] Diff Insight Report - search'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:45b2d74...MicrosoftDocs:c6cc5fa){target="_blank"}

# Highlights
在这次代码更新中，主要新增了多个文档图像，以增强用户的理解和体验，并进行了一些功能的更新和改进。同时，也有一些已不再使用的图像被移除以保持文档的准确性。目录文件和文档内容的更新以反映这些新添加的功能和图像。

## New features
- 增加了多个图像文件（如 `doc-intelligence-options.png` 和 `multimodal-embedding-tile.png`），为文档内容提供了更多的视觉支持和引导。
- 更新了快速入门文档，详细说明了在Azure门户中进行多模态搜索及向量导入的过程。

## Breaking changes
- 移除了一些不再必要或不适用的图像，如 `extract-your-content.png` 和 `vectorize-enrich-images.png`。

## Other updates
- 文档的描述和格式进行了轻微的调整，以增强其可读性和准确性。
- 目录文件更新以准确反映新增功能和其路径。

# Insights
此次更新主要围绕增强用户体验而展开，新增的图像和文字描述为用户提供了更清晰、精确的指导，特别是在多模态搜索和向量化处理方面。在文档中引入新的视觉元素（图像）的目的在于通过示例和视觉支持来帮助用户更加快速直观地掌握复杂的概念和操作。

针对移除的图像，说明这些部分可能已被更新的图像所替代或其信息已在其他资源中得到了更好的呈现，从而不再必要。此外，小幅改动文档内容的部分，如描述和先决条件，反映了对文档整体可读性和易用性的不断优化，并确保用户能够顺利地找到所需信息。

目录文件及其更新意在通过更精确和具有描述性的名称，帮助用户快速识别并定位他们需要的工具和功能，这是提高用户工作效率的关键。综上，这些变更显示出文档不仅关注新增功能的介绍，同时也持续关注文档质量和实用性的提升。

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [doc-intelligence-options.png](#item-7b6fc5) | new feature | 新增文档图像: 智能选项 | added | 0 | 0 | 0 | 
| [extract-your-content-doc-extraction.png](#item-34b9cc) | new feature | 新增文档图像: 内容提取 | added | 0 | 0 | 0 | 
| [extract-your-content-doc-intelligence.png](#item-88fe13) | new feature | 新增文档图像: 内容智能 | added | 0 | 0 | 0 | 
| [extract-your-content.png](#item-8531c0) | breaking change | 移除文档图像: 内容提取 | removed | 0 | 0 | 0 | 
| [image-verbalization-tile.png](#item-e0825f) | minor update | 修改文档图像: 图像可视化瓦片 | modified | 0 | 0 | 0 | 
| [multimodal-embedding-tile.png](#item-6186ac) | new feature | 添加文档图像: 多模态嵌入瓦片 | added | 0 | 0 | 0 | 
| [multimodal-embeddings-options.png](#item-a53c43) | new feature | 添加文档图像: 多模态嵌入选项 | added | 0 | 0 | 0 | 
| [vectorize-enrich-images.png](#item-c4b2b6) | breaking change | 移除文档图像: 向量化增强图像 | removed | 0 | 0 | 0 | 
| [vectorize-your-content.png](#item-9fe468) | new feature | 添加文档图像: 向量化您的内容 | added | 0 | 0 | 0 | 
| [search-get-started-portal-image-search.md](#item-438b9b) | minor update | 更新快速入门文档: 在Azure门户中进行多模态搜索 | modified | 175 | 63 | 238 | 
| [search-get-started-portal-import-vectors.md](#item-7dae77) | minor update | 更新快速入门文档: 在Azure门户中导入向量 | modified | 8 | 12 | 20 | 
| [search-query-overview.md](#item-dcd5d6) | minor update | 更新查询概述文档 | modified | 1 | 1 | 2 | 
| [toc.yml](#item-c4768f) | minor update | 更新目录文件以反映新功能 | modified | 3 | 3 | 6 | 


# Modified Contents
## articles/search/media/search-get-started-portal-images/doc-intelligence-options.png{#item-7b6fc5}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增文档图像: 智能选项"
}
```

### Explanation
此次修改涉及新增一张图片，文件名为 `doc-intelligence-options.png`，该文件位于搜索获取入门门户图片的相关目录中。这个新的媒体文件将作为文档的一部分，旨在增强用户对智能选项的理解和体验。该图片的添加没有导致其他文件的更改或删除，确保了文档的原有结构和内容保持不变。您可以通过提供的链接访问该文件，以获取更多详细信息。

## articles/search/media/search-get-started-portal-images/extract-your-content-doc-extraction.png{#item-34b9cc}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增文档图像: 内容提取"
}
```

### Explanation
此次修改增加了一张新图片，文件名为 `extract-your-content-doc-extraction.png`，该文件位于搜索获取入门门户图像的相应目录中。这一新增的媒体文件将有助于用户更好地理解内容提取的过程和方法。此次更新没有影响到其他文件的状态，确保了文档的完整性和一致性。用户可以通过提供的链接来查看该图片的详细内容。

## articles/search/media/search-get-started-portal-images/extract-your-content-doc-intelligence.png{#item-88fe13}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增文档图像: 内容智能"
}
```

### Explanation
此次修改增加了一张新图片，文件名为 `extract-your-content-doc-intelligence.png`，该文件位于搜索获取入门门户图像的目录中。这张新增加的媒体文件旨在帮助用户理解内容智能的相关概念与操作。该更新确保了其他文件的状态没有变化，保持文档的结构和内容的完整性。用户可以通过提供的链接访问该图片，了解详细信息。

## articles/search/media/search-get-started-portal-images/extract-your-content.png{#item-8531c0}

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "移除文档图像: 内容提取"
}
```

### Explanation
此次修改移除了名为 `extract-your-content.png` 的图片，该文件位于搜索获取入门门户图像的相关目录中。这一移除意味着该媒体文件将不再用于支持相关文档内容，可能会影响用户对内容提取过程的理解。用户在访问相关文档时，可能需要寻找其他方式来获取此信息。此更新明确了文件的删除，并保持了其他文件的状态不变，确保文档内容的合理性和有效性。

## articles/search/media/search-get-started-portal-images/image-verbalization-tile.png{#item-e0825f}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "修改文档图像: 图像可视化瓦片"
}
```

### Explanation
此次修改对文件 `image-verbalization-tile.png` 进行了更新，该文件位于搜索获取入门门户图像的相关目录中。虽然此更新没有添加或删除任何内容，但可能涉及对图像的质量或内容的细微调整，以更好地支持文档中关于图像可视化的主题。此变化将有助于用户更清晰地理解所涉及的概念。用户仍然可以通过提供的链接访问更新后的图像，获取最新的信息和视觉示例。

## articles/search/media/search-get-started-portal-images/multimodal-embedding-tile.png{#item-6186ac}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加文档图像: 多模态嵌入瓦片"
}
```

### Explanation
此次修改新增了文件 `multimodal-embedding-tile.png`，该文件位于搜索获取入门门户图像的相关目录中。这个新图像为文档提供了额外的视觉支持，有助于用户更好地理解多模态嵌入的概念。通过引入该图像，文档内容的丰富性得到了提升，使得用户在学习相关主题时有更多的参考资料可用。用户可以通过提供的链接访问新添加的图像，以获得更直观的学习体验。

## articles/search/media/search-get-started-portal-images/multimodal-embeddings-options.png{#item-a53c43}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加文档图像: 多模态嵌入选项"
}
```

### Explanation
此次修改新增了文件 `multimodal-embeddings-options.png`，该文件同样位于搜索获取入门门户图像的相关目录中。此图像为文档提供了有关多模态嵌入选项的可视化信息，进一步丰富了内容，并帮助用户理解不同的嵌入选择及其应用场景。通过引入此图像，文档的视觉效果和教育价值得到了提升。用户可以通过提供的链接访问新添加的图像，以增强对多模态嵌入概念的理解。

## articles/search/media/search-get-started-portal-images/vectorize-enrich-images.png{#item-c4b2b6}

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "移除文档图像: 向量化增强图像"
}
```

### Explanation
此次修改移除了文件 `vectorize-enrich-images.png`，该文件位于搜索获取入门门户图像相关目录中。这一变更意味着与向量化增强图像相关的视觉信息不再可用，可能会影响用户对相关概念的理解和学习参考。经过评估，该图像被认为不再必要或不再适合当前文档内容，因此被移除。尽管此变更可能影响文档的完整性，但它有助于保持文档的准确性和相关性。

## articles/search/media/search-get-started-portal-images/vectorize-your-content.png{#item-9fe468}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加文档图像: 向量化您的内容"
}
```

### Explanation
此次修改新增了文件 `vectorize-your-content.png`，该文件被添加到搜索获取入门门户图像的相关目录中。此图像旨在帮助用户理解如何将其内容进行向量化处理，提供了重要的视觉参考，增强了文档的教育性和实用性。通过访问提供的链接，用户可以查看此新图像，从而更好地掌握向量化内容的相关概念和应用。此变更拓展了文档的内容，提升了用户的学习体验。

## articles/search/search-get-started-portal-image-search.md{#item-438b9b}

<details>
<summary>Diff</summary>
````diff
@@ -1,36 +1,66 @@
 ---
 title: "Quickstart: Multimodal Search in the Azure portal"
 titleSuffix: Azure AI Search
-description: Learn how to search for multimodal content on an Azure AI Search index in the Azure portal. Run a wizard to generate natural-language descriptions of images and vectorize both text and images, and then use Search Explorer to query your multimodal index.
+description: Learn how to index and search for multimodal content in the Azure portal. Run a wizard to extract and embed both text and images, and then use Search Explorer to query your multimodal index.
 author: haileytap
 ms.author: haileytapia
 ms.service: azure-ai-search
 ms.topic: quickstart
-ms.date: 05/22/2025
+ms.date: 06/04/2025
 ms.custom:
   - references_regions
 ---
 
 # Quickstart: Search for multimodal content in the Azure portal
 
-In this quickstart, you use the **Import and vectorize data** wizard in the Azure portal to get started with [multimodal search](multimodal-search-overview.md). The wizard simplifies the process of extracting page text and inline images from documents, describing images in natural language, vectorizing image descriptions and text, and storing images for later retrieval.
+In this quickstart, you use the **Import and vectorize data** wizard in the Azure portal to get started with [multimodal search](multimodal-search-overview.md). The wizard simplifies the process of extracting, chunking, vectorizing, and loading both text and images into a searchable index.
 
-The sample data consists of a multimodal PDF in the [azure-search-sample-data](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/sustainable-ai-pdf) repo, but you can use different files and still follow this quickstart.
+Unlike [Quickstart: Vector search in the Azure portal](search-get-started-portal-import-vectors.md), which processes simple text-containing images, this quickstart supports advanced image processing for multimodal RAG scenarios.
+
+This quickstart uses a multimodal PDF from the [azure-search-sample-data](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/sustainable-ai-pdf) repo. However, you can use different files and still complete this quickstart.
 
 ## Prerequisites
 
 + An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ An [Azure Storage account](/azure/storage/common/storage-account-create). Use Azure Blob Storage or Azure Data Lake Storage Gen2 (storage account with a hierarchical namespace) on a standard performance (general-purpose v2) account. Access tiers can be hot, cool, or cold.
++ An [Azure AI Search service](search-create-service-portal.md). We recommend the Basic tier or higher.
 
-+ An [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) in East US, West Europe, or North Central US.
++ An [Azure Storage account](/azure/storage/common/storage-account-create). Use Azure Blob Storage or Azure Data Lake Storage Gen2 (storage account with a hierarchical namespace) on a standard performance (general-purpose v2) account. Access tiers can be hot, cool, or cold.
 
-+ An [Azure AI Search service](search-create-service-portal.md) in the same region as your Azure AI multi-service account.
++ A [supported extraction method](#supported-extraction-methods).
 
-+ An [Azure OpenAI resource](/azure/ai-services/openai/how-to/create-resource).
++ A [supported embedding method](#supported-embedding-methods).
 
 + Familiarity with the wizard. See [Import data wizards in the Azure portal](search-import-data-portal.md).
 
+### Supported extraction methods
+
+For content extraction, you can choose either default extraction via Azure AI Search or enhanced extraction via [Azure AI Document Intelligence](/azure/ai-services/document-intelligence/overview). The following table describes both extraction methods.
+
+| Method | Description |
+|--|--|
+| Default extraction | Extracts location metadata from PDF images only. Doesn't require another Azure AI resource. |
+| Enhanced extraction | Extracts location metadata from text and images for multiple document types. Requires an [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>1</sup> in a [supported region](cognitive-search-skill-document-intelligence-layout.md#supported--regions). |
+
+<sup>1</sup> For billing purposes, you must [attach your multi-service resource](cognitive-search-attach-cognitive-services.md) to the skillset in your Azure AI Search service. Unless you use a [keyless connection](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection) to create the skillset, both resources must be in the same region.
+
+### Supported embedding methods
+
+For content embedding, you can choose either image verbalization (followed by text vectorization) or multimodal embeddings. Deployment instructions for the models are provided in a [later section](#deploy-models). The following table describes both embedding methods.
+
+| Method | Description | Supported models |
+|--|--|--|
+| Image verbalization | Uses an LLM to generate natural-language descriptions of images, and then uses an embedding model to vectorize plain text and verbalized images.<br><br>Requires an [Azure OpenAI resource](/azure/ai-services/openai/how-to/create-resource) <sup>1, 2</sup> or [Azure AI Foundry project](/azure/ai-foundry/how-to/create-projects).<br><br>For text vectorization, you can also use an [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> in a [supported region](cognitive-search-skill-vision-vectorize.md). | LLMs:<br>GPT-4o<br>GPT-4o-mini<br>phi-4 <sup>4</sup><br><br>Embedding models:<br>text-embedding-ada-002<br>text-embedding-3-small<br>text-embedding-3-large |
+| Multimodal embeddings | Uses an embedding model to directly vectorize both text and images.<br><br>Requires an [Azure AI Foundry project](/azure/ai-foundry/how-to/create-projects) or [Azure AI services multi-service resource](/azure/ai-services/multi-service-resource#azure-ai-multi-services-resource-for-azure-ai-search-skills) <sup>3</sup> in a [supported region](cognitive-search-skill-vision-vectorize.md). | Cohere-embed-v3-english<br>Cohere-embed-v3-multilingual |
+
+<sup>1</sup> The endpoint of your Azure OpenAI resource must have a [custom subdomain](/azure/ai-services/cognitive-services-custom-subdomains), such as `https://my-unique-name.openai.azure.com`. If you created your resource in the [Azure portal](https://portal.azure.com/), this subdomain was automatically generated during resource setup.
+
+<sup>2</sup> Azure OpenAI resources (with access to embedding models) that were created in the [Azure AI Foundry portal](https://ai.azure.com/?cid=learnDocs) aren't supported. You must create an Azure OpenAI resource in the Azure portal.
+
+<sup>3</sup> For billing purposes, you must [attach your multi-service resource](cognitive-search-attach-cognitive-services.md) to the skillset in your Azure AI Search service. Unless you use a [keyless connection (preview)](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection) to create the skillset, both resources must be in the same region.
+
+<sup>4</sup> `phi-4` is only available to Azure AI Foundry projects.
+
 ### Public endpoint requirements
 
 All of the preceding resources must have public access enabled so that the Azure portal nodes can access them. Otherwise, the wizard fails. After the wizard runs, you can enable firewalls and private endpoints on the integration components for security. For more information, see [Secure connections in the import wizards](search-import-data-portal.md#secure-connections).
@@ -45,7 +75,11 @@ If you're starting with the free service, you're limited to three indexes, three
 
 Before you begin, make sure you have permissions to access content and operations. We recommend Microsoft Entra ID authentication and role-based access for authorization. You must be an **Owner** or **User Access Administrator** to assign roles. If roles aren't feasible, you can use [key-based authentication](search-security-api-keys.md) instead.
 
-Configure access to each resource identified in this section.
+Configure the [required roles](#required-roles) and [conditional roles](#conditional-roles) identified in this section.
+
+### Required roles
+
+Azure AI Search and Azure Storage are required for all multimodal search scenarios.
 
 ### [**Azure AI Search**](#tab/search-perms)
 
@@ -67,28 +101,42 @@ On your Azure AI Search service:
 
 ### [**Azure Storage**](#tab/storage-perms)
 
-Azure Storage is both the data source for your documents and the destination for extracted images. Your search service requires access to these storage containers, which you create in the next section of this quickstart.
+Azure Storage is both the data source for your documents and the destination for extracted images. Your search service requires access to these storage containers, which you create in the next section.
 
 On your Azure Storage account:
 
 + Assign **Storage Blob Data Contributor** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
 
-### [**Azure AI services**](#tab/ai-services-perms)
-
-An Azure AI multi-service account provides multiple Azure AI services, including [Azure AI Document Intelligence](/azure/ai-services/document-intelligence/overview) for content extraction and semantic chunking. Your search service requires access to call the [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md).
+---
 
-On your Azure AI multi-service account:
+### Conditional roles
 
-+ Assign **Cognitive Services User** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
+The following tabs cover all wizard-compatible resources for multimodal search. Select only the tabs that apply to your chosen [extraction method](#supported-extraction-methods) and [embedding method](#supported-embedding-methods).
 
 ### [**Azure OpenAI**](#tab/openai-perms)
 
-Azure OpenAI provides large language models (LLMs) for image verbalization and embedding models for text and image vectorization. Your search service requires access to call the [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) and [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md).
+Azure OpenAI provides LLMs for image verbalization and embedding models for text and image vectorization. Your search service requires access to call the [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) and [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md).
 
 On your Azure OpenAI resource:
 
 + Assign **Cognitive Services OpenAI User** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
 
+### [**Azure AI Foundry**](#tab/ai-foundry-perms)
+
+The Azure AI Foundry model catalog provides LLMs for image verbalization and embedding models for text and image vectorization. Your search service requires access to call the [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) and [AML skill](cognitive-search-aml-skill.md).
+
+On your Azure AI Foundry project:
+
++ Assign **Azure AI Project Manager** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
+
+### [**Azure AI services**](#tab/ai-services-perms)
+
+An Azure AI multi-service resource provides multiple Azure AI services, including [Azure AI Document Intelligence](/azure/ai-services/document-intelligence/overview) for content extraction and [Azure AI Vision](/azure/ai-services/computer-vision/overview) for content embedding. Your search service requires access to call the [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) and [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md).
+
+On your Azure AI multi-service resource:
+
++ Assign **Cognitive Services User** to your [search service identity](search-howto-managed-identities-data-sources.md#create-a-system-managed-identity).
+
 ---
 
 ## Prepare sample data
@@ -107,27 +155,20 @@ To prepare the sample data for this quickstart:
 
 ## Deploy models
 
-The wizard requires an LLM to verbalize images and an embedding model to generate vector representations of text and verbalized text content. Both models are available through Azure OpenAI.
-
-To deploy the models for this quickstart:
-
-1. Sign in to the [Azure AI Foundry portal](https://ai.azure.com/?cid=learnDocs) and select your Azure OpenAI resource.
-
-1. From the left pane, select **Model catalog**.
+The wizard offers several options for content embedding. Image verbalization requires an LLM to describe images and an embedding model to vectorize text and image content, while direct multimodal embeddings only require an embedding model. These models are available through Azure OpenAI and Azure AI Foundry.
 
-1. Deploy one of the following LLMs:
+> [!NOTE]
+> If you're using Azure AI Vision, skip this step. The multimodal embeddings are built into your Azure AI multi-service resource and don't require model deployment.
 
-   + gpt-4o
-
-   + gpt-4o-mini
+To deploy the models for this quickstart:
 
-1. Deploy one of the following embedding models:
+1. Sign in to the [Azure AI Foundry portal](https://ai.azure.com/?cid=learnDocs).
 
-   + text-embedding-ada-002
+1. Select your Azure OpenAI resource or Azure AI Foundry project.
 
-   + text-embedding-3-small
+1. From the left pane, select **Model catalog**.
 
-   + text-embedding-3-large
+1. Deploy the models required for your chosen [embedding method](#supported-embedding-methods).
 
 ## Start the wizard
 
@@ -165,43 +206,65 @@ To connect to your data:
 
 ## Extract your content
 
-The next step is to select a method for document cracking and chunking.
+Depending on your chosen [extraction method](#supported-extraction-methods), the wizard provides configuration options for document cracking and chunking.
+
+### [**Default extraction**](#tab/document-extraction)
+
+The default method calls the [Document Extraction skill](cognitive-search-skill-document-extraction.md) to extract text content and generate normalized images from your documents. The [Text Split skill](cognitive-search-skill-textsplit.md) is then called to split the extracted text content into pages.
 
-Your Azure AI multi-service account provides access to the [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md), which extracts page numbers, bounding polygons, and other location metadata from both text and images. The Document Layout skill also breaks documents into smaller, more manageable chunks.
+To use the Document Extraction skill:
+
+1. On the **Content extraction** page, select **Default**.
+
+   :::image type="content" source="media/search-get-started-portal-images/extract-your-content-doc-extraction.png" alt-text="Screenshot of the wizard page with the default method selected for content extraction." border="true" lightbox="media/search-get-started-portal-images/extract-your-content-doc-extraction.png":::
+
+1. Select **Next**.
+
+### [**Enhanced extraction**](#tab/document-intelligence)
+
+Your Azure AI multi-service resource provides access to [Azure AI Document Intelligence](/azure/ai-services/document-intelligence/overview), which calls the [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) to recognize document structure and extract text and images relationally. It does so by attaching location metadata, such as page numbers and bounding polygons, to each image. The Document Layout skill also breaks text content into smaller, more manageable chunks.
 
 To use the Document Layout skill:
 
 1. On the **Content extraction** page, select **AI Document Intelligence**.
 
-1. Specify your Azure subscription and Azure AI multi-service account.
+   :::image type="content" source="media/search-get-started-portal-images/extract-your-content-doc-intelligence.png" alt-text="Screenshot of the wizard page with Azure AI Document Intelligence selected for content extraction." border="true" lightbox="media/search-get-started-portal-images/extract-your-content-doc-intelligence.png":::
+
+1. Specify your Azure subscription and multi-service resource.
 
 1. For the authentication type, select **System assigned identity**.
 
 1. Select the checkbox that acknowledges the billing effects of using these resources.
 
-   :::image type="content" source="media/search-get-started-portal-images/extract-your-content.png" alt-text="Screenshot of the wizard page for selecting a content extraction method." border="true" lightbox="media/search-get-started-portal-images/extract-your-content.png":::
+   :::image type="content" source="media/search-get-started-portal-images/doc-intelligence-options.png" alt-text="Screenshot of the wizard page with configuration options for Azure AI Document Intelligence selected." border="true" lightbox="media/search-get-started-portal-images/doc-intelligence-options.png":::
 
 1. Select **Next**.
 
+---
+
 ## Embed your content
 
-During this step, the wizard calls two skills to generate descriptive text for images (image verbalization) and vector embeddings for text and images.
+During this step, the wizard uses your chosen [embedding method](#supported-embedding-methods) to generate vector representations of both text and images.
+
+### [**Image verbalization**](#tab/image-verbalization)
 
-For image verbalization, the [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) uses the LLM you deployed to analyze each extracted image and produce a natural-language description.
+The wizard calls one skill to create descriptive text for images (image verbalization) and another skill to create vector embeddings for both text and images.
 
-For text and image embeddings, the [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) uses the embedding model you deployed to convert the text chunks and verbalized descriptions into high-dimensional vectors. These vectors enable similarity and hybrid retrieval.
+For image verbalization, the [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) uses your deployed LLM to analyze each extracted image and produce a natural-language description.
 
-To use the GenAI Prompt skill and Azure OpenAI Embedding skill:
+For embeddings, the [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md), [AML skill](cognitive-search-aml-skill.md), or [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) uses your deployed embedding model to convert text chunks and verbalized descriptions into high-dimensional vectors. These vectors enable similarity and hybrid retrieval.
+
+To use the skills for image verbalization:
 
 1. On the **Content embedding** page, select **Image Verbalization**.
 
    :::image type="content" source="media/search-get-started-portal-images/image-verbalization-tile.png" alt-text="Screenshot of the Image Verbalization tile in the wizard." border="true" lightbox="media/search-get-started-portal-images/image-verbalization-tile.png":::
 
 1. On the **Image Verbalization** tab:
 
-   1. For the kind, select **Azure OpenAI**.
+   1. For the kind, select your LLM provider: **Azure OpenAI** or **AI Foundry Hub catalog models**.
 
-   1. Specify your Azure subscription, Azure OpenAI resource, and LLM deployment.
+   1. Specify your Azure subscription, resource, and LLM deployment.
 
    1. For the authentication type, select **System assigned identity**.
 
@@ -211,9 +274,9 @@ To use the GenAI Prompt skill and Azure OpenAI Embedding skill:
 
 1. On the **Text Vectorization** tab:
 
-   1. For the kind, select **Azure OpenAI**.
+   1. For the kind, select your model provider: **Azure OpenAI**, **AI Foundry Hub catalog models**, or **AI Vision vectorization**.
 
-   1. Specify your Azure subscription, Azure OpenAI resource, and embedding model deployment.
+   1. Specify your Azure subscription, resource, and embedding model deployment.
 
    1. For the authentication type, select **System assigned identity**.
 
@@ -223,6 +286,32 @@ To use the GenAI Prompt skill and Azure OpenAI Embedding skill:
 
 1. Select **Next**.
 
+### [**Multimodal embeddings**](#tab/multimodal-embeddings)
+
+If the raw content of your data includes text, the wizard calls the [AML skill](cognitive-search-aml-skill.md) or [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) to vectorize it. The same embedding skill is used to generate vector representations of images.
+
+The wizard also calls the [Shaper skill](cognitive-search-skill-shaper.md) to enrich the output with metadata, such as page numbers. This metadata is useful for associating vectorized content with its original context in the document.
+
+To use the skills for multimodal embeddings:
+
+1. On the **Content embedding** page, select **Multimodal Embedding**.
+
+   :::image type="content" source="media/search-get-started-portal-images/multimodal-embedding-tile.png" alt-text="Screenshot of the Multimodal Embedding tile in the wizard." border="true" lightbox="media/search-get-started-portal-images/multimodal-embedding-tile.png":::
+
+1. For the kind, select your model provider: **AI Foundry Hub catalog models** or **AI Vision vectorization**.
+
+   <!-- If it's unavailable, make sure your Azure AI Search service and Azure AI multi-service account are both in a region that [supports the AI Vision multimodal APIs](/azure/ai-services/computer-vision/how-to/image-retrieval). -->
+
+1. Specify your Azure subscription, resource, and embedding model deployment.
+
+1. Select the checkbox that acknowledges the billing effects of using this resource.
+
+   :::image type="content" source="media/search-get-started-portal-images/multimodal-embeddings-options.png" alt-text="Screenshot of the wizard page for vectorizing text and images." border="true" lightbox="media/search-get-started-portal-images/multimodal-embeddings-options.png":::
+
+1. Select **Next**.
+
+---
+
 ## Store the extracted images
 
 The next step is to send images extracted from your documents to Azure Storage. In Azure AI Search, this secondary storage is known as a [knowledge store](knowledge-store-concept-intro.md).
@@ -245,14 +334,14 @@ On the **Advanced settings** page, you can optionally add fields to the index sc
 
 | Field | Applies to | Description | Attributes |
 |--|--|--|--|
-| content_id | Text and image vectors | String field. Document key for the index. | Searchable, retrievable, sortable, filterable, and facetable. |
-| document_title | Text and image vectors | String field. Human-readable document title, page title, or page number. | Searchable, retrievable, sortable, filterable, and facetable. |
+| content_id | Text and image vectors | String field. Document key for the index. | Retrievable, sortable, and searchable. |
+| document_title | Text and image vectors | String field. Human-readable document title. | Retrievable and searchable. |
 | text_document_id | Text vectors | String field. Identifies the parent document from which the text chunk originates. | Retrievable and filterable. |
-| image_document_id | Image vectors | String field. Identifies the parent document from which the image originates. | Searchable, retrievable, sortable, filterable, and facetable. |
-| content_text | Text vectors | String field. Human-readable version of the text chunk. | Searchable, retrievable, sortable, filterable, and facetable. |
-| content_embedding | Image vectors | Collection(Edm.Single). Vector representation of the image verbalization. | Searchable and retrievable. |
-| content_path | Text and image vectors | String field. Path to the content in the storage container. | Retrievable, sortable, filterable, and facetable. |
-| locationMetadata | Text and image vectors | Edm.ComplexType. Contains metadata about the content's location. | Varies by field. |
+| image_document_id | Image vectors | String field. Identifies the parent document from which the image originates. | Retrievable and filterable. |
+| content_text | Text vectors | String field. Human-readable version of the text chunk. | Retrievable and searchable. |
+| content_embedding | Text and image vectors | Collection(Edm.Single). Vector representation of text and images. | Retrievable and searchable. |
+| content_path | Text and image vectors | String field. Path to the content in the storage container. | Retrievable and searchable. |
+| locationMetadata | Image vectors | Edm.ComplexType. Contains metadata about the image's location in the documents. | Varies by field. |
 
 You can't modify the generated fields or their attributes, but you can add fields if your data source provides them. For example, Azure Blob Storage provides a collection of metadata fields.
 
@@ -264,9 +353,6 @@ To add fields to the index schema:
 
 1. Select a source field from the available fields, enter a field name for the index, and accept (or override) the default data type.
 
-   > [!NOTE]
-   > Metadata fields are searchable but not retrievable, filterable, facetable, or sortable.
-
 1. If you want to restore the schema to its original version, select **Reset**.
 
 ## Schedule indexing
@@ -303,13 +389,11 @@ When the wizard completes the configuration, it creates the following objects:
 
 + A skillset with the following skills:
 
-  + The [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) splits documents into text chunks and extracts images with location data.
+  + The [Document Extraction skill](cognitive-search-skill-document-extraction.md) or [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) extracts text and images from source documents. The [Text Split skill](cognitive-search-skill-textsplit.md) accompanies the Document Extraction skill for data chunking, while the Document Layout skill has built-in chunking.
 
-  + The [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) generates natural-language descriptions (verbalizations) of images.
+  + The [GenAI Prompt skill](cognitive-search-skill-genai-prompt.md) verbalizes images in natural language. If you're using direct multimodal embeddings, this skill is absent.
 
-  + The [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) vectorizes each text chunk.
-
-  + The [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) is called again to vectorize each image verbalization.
+  + The [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md), [AML skill](cognitive-search-aml-skill.md), or [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) is called once for text vectorization and once for image vectorization.
 
   + The [Shaper skill](cognitive-search-skill-shaper.md) enriches the output with metadata and creates new images with contextual information.
 
@@ -318,9 +402,9 @@ When the wizard completes the configuration, it creates the following objects:
 
 ## Check results
 
-This quickstart creates a multimodal index that supports [hybrid search](hybrid-search-overview.md) over both text and verbalized images. However, it doesn't support images as query inputs, which requires integrated vectorization using an embedding skill and an equivalent vectorizer. For more information, see [Query with Search explorer](search-explorer.md).
+This quickstart creates a multimodal index that supports [hybrid search](hybrid-search-overview.md) over both text and images. Unless you use direct multimodal embeddings, the index doesn't accept images as query inputs, which requires the [AML skill](cognitive-search-aml-skill.md) or [Azure AI Vision multimodal embeddings skill](cognitive-search-skill-vision-vectorize.md) with an equivalent vectorizer. For more information, see [Configure a vectorizer in a search index](vector-search-how-to-configure-vectorizer.md).
 
-Hybrid search is a combination of full-text queries and vector queries. When you issue a hybrid query, the search engine computes the semantic similarity between your query and the indexed vectors and ranks the results accordingly. For the index created in this quickstart, the results surface content from the `content_text` field that closely aligns with your query.
+Hybrid search combines full-text queries and vector queries. When you issue a hybrid query, the search engine computes the semantic similarity between your query and the indexed vectors and ranks the results accordingly. For the index created in this quickstart, the results surface content from the `content_text` field that closely aligns with your query.
 
 To query your multimodal index:
 
@@ -340,12 +424,40 @@ To query your multimodal index:
 
    :::image type="content" source="media/search-get-started-portal-images/search-button.png" alt-text="Screenshot of the Search button in Search Explorer." border="true" lightbox="media/search-get-started-portal-images/search-button.png":::
 
-   The results should include text and image content related to `energy` in your index. Highlights from relevant passages and image verbalizations appear in `@search.captions`, helping you quickly identify matches to your query.
+   The JSON results should include text and image content related to `energy` in your index. If you enabled semantic ranker, the `@search.answers` array provides concise, high-confidence [semantic answers](semantic-answers.md) to help you quickly identify relevant matches.
+
+   ```json
+   "@search.answers": [
+      {
+         "key": "a71518188062_aHR0cHM6Ly9oYWlsZXlzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9tdWx0aW1vZGFsLXNlYXJjaC9BY2NlbGVyYXRpbmctU3VzdGFpbmFiaWxpdHktd2l0aC1BSS0yMDI1LnBkZg2_normalized_images_7",
+         "text": "A vertical infographic consisting of three sections describing the roles of AI in sustainability:  1. **Measure, predict, and optimize complex systems**: AI facilitates analysis, modeling, and optimization in areas like energy distribution, resource allocation, and environmental monitoring. **Accelerate the development of sustainability solution...",
+         "highlights": "A vertical infographic consisting of three sections describing the roles of AI in sustainability:  1. **Measure, predict, and optimize complex systems**: AI facilitates analysis, modeling, and optimization in areas like<em> energy distribution, </em>resource<em> allocation, </em>and environmental monitoring. **Accelerate the development of sustainability solution...",
+         "score": 0.9950000047683716
+      },
+      {
+         "key": "1cb0754930b6_aHR0cHM6Ly9oYWlsZXlzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9tdWx0aW1vZGFsLXNlYXJjaC9BY2NlbGVyYXRpbmctU3VzdGFpbmFiaWxpdHktd2l0aC1BSS0yMDI1LnBkZg2_text_sections_5",
+         "text": "...cross-laminated timber.8 Through an agreement with Brookfield, we aim  10.5 gigawatts (GW) of renewable energy to the grid.910.5 GWof new renewable energy capacity to be developed across the United States and Europe.Play 4 Advance AI policy principles and governance for sustainabilityWe advocated for policies that accelerate grid decarbonization",
+         "highlights": "...cross-laminated timber.8 Through an agreement with Brookfield, we aim <em> 10.5 gigawatts (GW) of renewable energy </em>to the<em> grid.910.5 </em>GWof new<em> renewable energy </em>capacity to be developed across the United States and Europe.Play 4 Advance AI policy principles and governance for sustainabilityWe advocated for policies that accelerate grid decarbonization",
+         "score": 0.9890000224113464
+      },
+      {
+         "key": "1cb0754930b6_aHR0cHM6Ly9oYWlsZXlzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9tdWx0aW1vZGFsLXNlYXJjaC9BY2NlbGVyYXRpbmctU3VzdGFpbmFiaWxpdHktd2l0aC1BSS0yMDI1LnBkZg2_text_sections_50",
+         "text": "ForewordAct... Similarly, we have restored degraded stream ecosystems near our datacenters from Racine, Wisconsin120 to Jakarta, Indonesia.117INNOVATION SPOTLIGHTAI-powered Community Solar MicrogridsDeveloping energy transition programsWe are co-innovating with communities to develop energy transition programs that align their goals with broader s.",
+         "highlights": "ForewordAct... Similarly, we have restored degraded stream ecosystems near our datacenters from Racine, Wisconsin120 to Jakarta, Indonesia.117INNOVATION SPOTLIGHTAI-powered Community<em> Solar MicrogridsDeveloping energy transition programsWe </em>are co-innovating with communities to develop<em> energy transition programs </em>that align their goals with broader s.",
+          "score": 0.9869999885559082
+      }
+   ]
+   ```
 
 ## Clean up resources
 
 This quickstart uses billable Azure resources. If you no longer need the resources, delete them from your subscription to avoid charges.
 
-## Next step
+## Next steps
+
+This quickstart introduced you to the **Import and vectorize data** wizard, which creates all of the necessary objects for multimodal search. To explore each step in detail, see the following tutorials:
 
-This quickstart introduced you to the **Import and vectorize data wizard**, which creates all of the necessary objects for multimodal search. To explore each step in detail, see [Tutorial: Index mixed content using image verbalizations and the Document Layout skill](tutorial-document-layout-image-verbalization.md).
++ [Tutorial: Image verbalization and Document Extraction skill](tutorial-document-extraction-image-verbalization.md)
++ [Tutorial: Image verbalization and Document Layout skill](tutorial-document-layout-image-verbalization.md)
++ [Tutorial: Multimodal embeddings and Document Extraction skill](tutorial-document-extraction-multimodal-embeddings.md)
++ [Tutorial: Multimodal embeddings and Document Layout skill](tutorial-document-layout-multimodal-embeddings.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新快速入门文档: 在Azure门户中进行多模态搜索"
}
```

### Explanation
此次修改对文档 `search-get-started-portal-image-search.md` 进行了显著更新，新增了175行并删除了63行，内容更改总计238行。更新主要包括对多模态搜索的描述、先决条件的调整以及对使用向量化处理的详细说明。 

具体而言，增改内容澄清了如何在Azure门户中索引和搜索多模态内容，替换了描述中的某些术语，以突出强调提取和嵌入文本和图像的过程。同时，引入了对支持的提取和嵌入方法的更详细说明，帮助用户理解在不同场景下的选项。

此外，更新的文档调整了必要的角色和权限要求，以确保用户在设置和使用多模态搜索时能获取适当的访问权限。整体而言，此次更新旨在为用户提供更清晰、详尽的操作流程和配置指导，提升用户的整体体验。

## articles/search/search-get-started-portal-import-vectors.md{#item-7dae77}

<details>
<summary>Diff</summary>
````diff
@@ -9,14 +9,14 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: quickstart
-ms.date: 05/22/2025
+ms.date: 06/04/2025
 ---
 
 # Quickstart: Vectorize text in the Azure portal
 
-In this quickstart, you use the **Import and vectorize data** wizard in the Azure portal to get started with [integrated vectorization](vector-search-integrated-vectorization.md). The wizard chunks your content and calls an embedding model to vectorize content during indexing and for queries.
+In this quickstart, you use the **Import and vectorize data** wizard in the Azure portal to get started with [integrated vectorization](vector-search-integrated-vectorization.md). The wizard chunks your content and calls an embedding model to vectorize the chunks at indexing and query time.
 
-The sample data for this quickstart consists of text-based PDFs, but you can also use images and follow this quickstart to vectorize them.
+This quickstart uses text-based PDFs from the [azure-search-sample-data](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/sustainable-ai-pdf) repo. However, you can use images and still complete this quickstart.
 
 ## Prerequisites
 
@@ -52,9 +52,9 @@ For integrated vectorization, you must use one of the following embedding models
 
 <sup>1</sup> The endpoint of your Azure OpenAI resource must have a [custom subdomain](/azure/ai-services/cognitive-services-custom-subdomains), such as `https://my-unique-name.openai.azure.com`. If you created your resource in the [Azure portal](https://portal.azure.com/), this subdomain was automatically generated during resource setup.
 
-<sup>2</sup> Azure OpenAI resources (with access to embedding models) that were created in the [Azure AI Foundry portal](https://ai.azure.com/?cid=learnDocs) aren't supported. Only Azure OpenAI resources created in the Azure portal are compatible with the [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) integration.
+<sup>2</sup> Azure OpenAI resources (with access to embedding models) that were created in the [Azure AI Foundry portal](https://ai.azure.com/?cid=learnDocs) aren't supported. Only Azure OpenAI resources created in the Azure portal are compatible with the [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md).
 
-<sup>3</sup> For billing purposes, you must [attach your Azure AI multi-service resource](cognitive-search-attach-cognitive-services.md) to the skillset in your Azure AI Search service. Unless you use a [keyless connection (preview)](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection) to create the skillset, both resources must be in the same region.
+<sup>3</sup> For billing purposes, you must [attach your multi-service resource](cognitive-search-attach-cognitive-services.md) to the skillset in your Azure AI Search service. Unless you use a [keyless connection (preview)](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection) to create the skillset, both resources must be in the same region.
 
 <sup>4</sup> The Azure AI Vision multimodal embedding model is available in [select regions](/azure/ai-services/computer-vision/overview-image-analysis#region-availability).
 
@@ -265,19 +265,15 @@ For the model catalog, you should have an [Azure AI Foundry project](/azure/ai-f
 
 ## Start the wizard
 
+To start the wizard for vector search:
+
 1. Sign in to the [Azure portal](https://portal.azure.com/) and select your Azure AI Search service.
 
 1. On the **Overview** page, select **Import and vectorize data**.
 
    :::image type="content" source="media/search-get-started-portal-import-vectors/command-bar.png" alt-text="Screenshot of the command to open the wizard for importing and vectorizing data.":::
 
-1. Select your data source:
-
-   + Azure Blob Storage
-
-   + ADLS Gen2
-
-   + OneLake
+1. Select your data source: **Azure Blob Storage**, **ADLS Gen2**, or **OneLake**.
 
 1. Select **RAG**.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新快速入门文档: 在Azure门户中导入向量"
}
```

### Explanation
此次修改对文档 `search-get-started-portal-import-vectors.md` 进行了更新，包括新增8行和删除12行，总体更改20行。更新的内容主要集中在文档的描述和格式化上，以增强可读性和清晰度。

首先，修改了描述来清楚地阐明如何使用Azure门户中的**导入和向量化数据**向导，该向导现已明确提到它在索引和查询时会对内容进行分块并调用嵌入模型。示例数据部分也进行了简化，突出了可以使用的文本基础PDF和图像。

此外，在先决条件部分，更新了一些描述，使得信息更加简洁明了。同时，简化了选择数据源的步骤，以便用户更快速地找到所需的信息。这些更改旨在提升用户在使用Azure AI Search服务时的体验和理解。

## articles/search/search-query-overview.md{#item-dcd5d6}

<details>
<summary>Diff</summary>
````diff
@@ -96,4 +96,4 @@ For a closer look at query implementation, review the examples for each syntax.
 
 + [Simple query examples](search-query-simple-examples.md)
 + [Lucene syntax query examples for building advanced queries](search-query-lucene-examples.md)
-+ [How full text search works in Azure AI Search](search-lucene-query-architecture.md)git
++ [How full text search works in Azure AI Search](search-lucene-query-architecture.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新查询概述文档"
}
```

### Explanation
此次修改对文档 `search-query-overview.md` 进行了轻微的调整，具体为添加和删除各1行，共计修改2行。主要更改集中在文档末尾引用链接的格式。

具体来说，更新添加了两个引用链接，分别提供了简单查询示例和基于Lucene语法的高级查询示例。这有助于用户更好地理解和实现查询。此外，修改还确保了对“如何在Azure AI Search中实现全文搜索”的引用保持一致性。

这些细微的更改旨在改善文档的整体组织和可用性，使用户在寻找查询示例时只需轻松访问相关链接，以便于获取所需的知识和信息。

## articles/search/toc.yml{#item-c4768f}

<details>
<summary>Diff</summary>
````diff
@@ -32,11 +32,11 @@ items:
     href: search-get-started-rbac.md
   - name: Azure portal
     items:
-    - name: Keyword search wizard
+    - name: Create a search index
       href: search-get-started-portal.md
-    - name: RAG wizard
+    - name: Create a vector index
       href: search-get-started-portal-import-vectors.md
-    - name: Multimodal RAG wizard
+    - name: Create a multimodal index
       href: search-get-started-portal-image-search.md
     - name: Create a demo app
       href: search-create-app-portal.md
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新目录文件以反映新功能"
}
```

### Explanation
此次修改对文件 `toc.yml` 进行了更新，包括新增3行和删除3行，总计更改6行。主要修改集中在Azure门户下的搜索相关工具的名称和链接上，以反映新功能的变化。

具体来说，文档中的几个项名称被重新命名，以增强其描述性和准确性。例如，将“关键词搜索向导”更新为“创建搜索索引”，同时还调整了其他几个项的名称，将“RAG向导”更改为“创建向量索引”，以及将“多模态RAG向导”更改为“创建多模态索引”。这些更改确保了文档目录能够更清晰地指向当前可用的功能和引导用户进行相关操作。

此更新旨在提升文档的可用性，使用户能够更方便地找到适合他们需求的工具和资源。


