---
date: '2025-06-02'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:ca0291a...MicrosoftDocs:178e7a4
summary: 'Summary: Recent updates to Azure AI OpenAI documentation include minor date
  changes, language clarifications, new sections, removal of redundant content, and
  the addition of images to enhance understanding. A major overhaul has occurred in
  the "gpt-with-vision.md" document, improving its structure and content to clarify
  vision-enabled chat models. This update aims to enhance clarity, usability, and
  relevance across the documentation. The changes also involve improved visual support,
  more coherent guidance on input limitations, and updated metadata to reflect current
  information, ensuring users find the documentation helpful and easy to navigate.'
title: '[en_US] Diff Insight Report - openai'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:ca0291a...MicrosoftDocs:178e7a4){target="_blank"}

# Highlights
The diff across various Azure AI OpenAI documentation files includes notable updates such as: minor updates for date modifications, clarifications in language, introduction of new sections and removal of redundant content, along with new image additions to improve visual comprehension. A significant revamp has been noted in the "gpt-with-vision.md" document, indicating a structural and content overhaul to enhance clarity on vision-enabled chat models.

## New features
- Several new images have been added to the documentation, enhancing visual support for processes such as evaluation creation, semantic similarity evaluation, and data upload.
- Updated content structure in the "gpt-with-vision.md" document, providing more cohesive and clear guidance on input limitations and reference links for supported models.

## Breaking changes
- A major revamp in the "gpt-with-vision.md" file where significant structural and content changes have been executed, including new sections on input limitations.

## Other updates
- Metadata dates updated across multiple documents to reflect a new effective date.
- Text edits to improve readability and clarity, such as streamlined sentences and revised section headers.
- Visual aids added in multiple evaluation-related documents to support better understanding of processes.

# Insights
The modifications across the Azure AI OpenAI documentation serve several purposes aimed at improving clarity, accuracy, and usability. The frequent updates of metadata dates across multiple documents ensure that the content remains relevant and up-to-date with anticipated changes or ongoing developments within Azure services. 

The notable overhaul in the "gpt-with-vision.md" document reflects a significant step to refine the reader's understanding of vision-enabled chat models by detailing input constraints and linking to reference models. This revamp not only removes redundant content but introduces a cohesive layout that emphasizes clear articulation of model capabilities and limitations. 

Enhancements in the "evaluations.md" with expanded content and new visual aids indicate a push towards making complex evaluation tools more accessible through extensive guidance and illustrative support.

The addition of images plays a crucial role in technical documentation where visual references can substantially aid comprehension. By incorporating visuals such as for the evaluation creation and data upload processes, users are more likely to grasp procedural details efficiently, reducing the cognitive load of understanding purely textual instructions. 

The consistent textual updates to headers and sentences, such as shifting "Next steps" to "Related content," further reflect attention to easy navigation and context-appropriate guidance, offering a seamless reading experience. These meticulous adjustments across the documentation underscore an ongoing commitment to enhancing user interaction with technical resources, supporting both novice and seasoned users in effectively leveraging Azure's AI capabilities.

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [default-safety-policies.md](#item-39b6a0) | minor update | Update Date in Default Safety Policies Document | modified | 1 | 1 | 2 | 
| [gpt-4-v-prompt-engineering.md](#item-fd7772) | minor update | Update Date and Adjust Text in GPT-4 Prompt Engineering Document | modified | 2 | 2 | 4 | 
| [gpt-with-vision.md](#item-991388) | breaking change | Revamp of GPT with Vision Document Content and Structure | modified | 12 | 12 | 24 | 
| [prompt-transformation.md](#item-21e047) | minor update | Update Date in Prompt Transformation Document | modified | 1 | 1 | 2 | 
| [dall-e-quickstart.md](#item-fcd528) | minor update | Update Date in DALL-E Quickstart Document | modified | 1 | 1 | 2 | 
| [gpt-v-quickstart.md](#item-2a6183) | minor update | Revise Content and Date in GPT-V Quickstart Document | modified | 3 | 7 | 10 | 
| [evaluations.md](#item-dfaa1c) | minor update | Enhancements and Updates to Evaluations Document | modified | 72 | 36 | 108 | 
| [gpt-with-vision.md](#item-4d8502) | minor update | Updates to GPT with Vision Document | modified | 3 | 3 | 6 | 
| [risks-safety-monitor.md](#item-b2be0b) | minor update | Updates to Risks and Safety Monitor Document | modified | 3 | 3 | 6 | 
| [use-blocklists.md](#item-e99db7) | minor update | Updates to Use Blocklists Document | modified | 5 | 5 | 10 | 
| [create-new-eval.png](#item-d7bc67) | new feature | New Evaluation Creation Image Added | added | 0 | 0 | 0 | 
| [eval-generate-1.png](#item-5be1c8) | new feature | New Image for Evaluation Generation Added | added | 0 | 0 | 0 | 
| [eval-generate-2.png](#item-efeac5) | new feature | New Image for Evaluation Generation Process Added | added | 0 | 0 | 0 | 
| [eval-semantic-similarity-1.png](#item-8729c1) | new feature | New Image for Semantic Similarity Evaluation Added | added | 0 | 0 | 0 | 
| [eval-semantic-similarity-2.png](#item-a316c2) | new feature | New Image for Semantic Similarity Evaluation Process Added | added | 0 | 0 | 0 | 
| [eval-semantic-similarity-3.png](#item-0c0125) | new feature | New Image for Semantic Similarity Evaluation Steps Added | added | 0 | 0 | 0 | 
| [eval-submit-job-2.png](#item-d8110c) | new feature | New Image for Job Submission Process Added | added | 0 | 0 | 0 | 
| [eval-submit-job.png](#item-a9a1f5) | new feature | New Image for Job Submission Process Added | added | 0 | 0 | 0 | 
| [eval-testing-criteria-2.png](#item-220e27) | new feature | New Image for Evaluation Testing Criteria Added | added | 0 | 0 | 0 | 
| [eval-testing-criteria.png](#item-268f4b) | new feature | New Image for Evaluation Testing Criteria Added | added | 0 | 0 | 0 | 
| [upload-data-1.png](#item-cd5af0) | new feature | New Image for Data Upload Process Added | added | 0 | 0 | 0 | 
| [upload-data-2.png](#item-de63da) | new feature | New Image for Data Upload Process Added | added | 0 | 0 | 0 | 


# Modified Contents
## articles/ai-services/openai/concepts/default-safety-policies.md{#item-39b6a0}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update Date in Default Safety Policies Document"
}
```

### Explanation
The code diff presents a minor update to the document titled "default-safety-policies.md" located in the OpenAI concepts section of the Azure AI documentation. The only change made in this modification involves updating the date attribute within the metadata of the document. Specifically, the date has been changed from "02/20/2025" to "05/31/2025". This update does not affect the content of the document itself but reflects a new publication or effective date for the information provided within the document.

## articles/ai-services/openai/concepts/gpt-4-v-prompt-engineering.md{#item-fd7772}

<details>
<summary>Diff</summary>
````diff
@@ -6,13 +6,13 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
 # Image prompt engineering techniques
 
-To unlock the full potential of vision-enabled chat models like GPT-4 Turbo with Vision and GPT-4o, it's essential to tailor the prompts to your specific needs. Here are some guidelines to enhance the accuracy and efficiency of your prompts.
+To unlock the full potential of vision-enabled chat models, it's essential to tailor the prompts to your specific needs. Here are some guidelines to enhance the accuracy and efficiency of your prompts.
 
 ## Fundamentals of writing an image prompt
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update Date and Adjust Text in GPT-4 Prompt Engineering Document"
}
```

### Explanation
The code diff reflects a minor update made to the document "gpt-4-v-prompt-engineering.md" within the Azure AI OpenAI concepts section. The modification includes two primary changes: an update to the metadata date and a slight textual adjustment. 

First, the date was changed from "02/20/2025" to "05/31/2025", signifying a new effective date for the document's content. Second, a sentence was streamlined by removing the phrase "like GPT-4 Turbo with Vision and GPT-4o," thus enhancing clarity while preserving the overall meaning of the guidance regarding prompt engineering techniques for vision-enabled chat models. These modifications serve to update the document's validity and improve readability.

## articles/ai-services/openai/concepts/gpt-with-vision.md{#item-991388}

<details>
<summary>Diff</summary>
````diff
@@ -6,13 +6,13 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
 # Vision-enabled chat model concepts
 
-Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are GPT-4 Turbo with Vision, GPT-4o, and GPT-4o-mini. This guide provides details on their capabilities and limitations.
+Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. This guide provides details on their capabilities and limitations. To see which models support image input, see the [Models page](./models.md).
 
 To try out vision-enabled chat models, see the [quickstart](/azure/ai-services/openai/gpt-v-quickstart).
 
@@ -21,6 +21,16 @@ To try out vision-enabled chat models, see the [quickstart](/azure/ai-services/o
 The vision-enabled models answer general questions about what's present in the images you upload.
 
 
+## Input limitations
+
+This section describes the limitations of vision-enabled chat models.
+
+### Image support
+
+- **Maximum input image size**: The maximum size for input images is restricted to 20 MB.
+- **Low resolution accuracy**: When images are analyzed using the "low resolution" setting, it allows for faster responses and uses fewer input tokens for certain use cases. However, this could impact the accuracy of object and text recognition within the image.
+- **Image chat restriction**: When you upload images in [Azure AI Foundry portal](https://ai.azure.com/) or the API, there is a limit of 10 images per chat call.
+
 ## Special pricing information
 
 > [!IMPORTANT]
@@ -70,16 +80,6 @@ For a typical use case, take a 3-minute video with a 100-token prompt input. The
 Additionally, there's a one-time indexing cost of $0.15 to generate the Video Retrieval index for this 3-minute video. This index can be reused across any number of Video Retrieval and GPT-4 Turbo with Vision API calls.
 -->
 
-## Input limitations
-
-This section describes the limitations of vision-enabled chat models.
-
-### Image support
-
-- **Maximum input image size**: The maximum size for input images is restricted to 20 MB.
-- **Low resolution accuracy**: When images are analyzed using the "low resolution" setting, it allows for faster responses and uses fewer input tokens for certain use cases. However, this could impact the accuracy of object and text recognition within the image.
-- **Image chat restriction**: When you upload images in [Azure AI Foundry portal](https://ai.azure.com/) or the API, there is a limit of 10 images per chat call.
-
 <!--
 ### Video support
 
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "Revamp of GPT with Vision Document Content and Structure"
}
```

### Explanation
The code diff indicates a significant change to the document "gpt-with-vision.md" related to vision-enabled chat models developed by OpenAI. The update includes the modification of the date in the metadata, now set to "05/31/2025," and substantial alterations to the content structure.

Key changes in the document encompass the following:
1. **Content Enhancement**: The description of vision-enabled chat models has been slightly refined for clarity. The previous mention of specific models has been removed, with a reference link to a "Models page" added for users to navigate and see which models support image input.
   
2. **Input Limitations Section**: A new section detailing the input limitations for vision-enabled chat models has been added. This includes specific guidelines about image size restrictions, accuracy considerations for low-resolution settings, and limits on the number of images that can be uploaded in a single chat call. This addition introduces critical information that was previously omitted, thereby improving user understanding of the system's capabilities and constraints.

3. **Removed Redundant Content**: The prior version contained a separate section on input limitations that has been entirely replaced with the new structured format, ensuring that the document presents updated and cohesive information seamlessly.

Overall, these changes greatly enhance the usability and clarity of the document, while also addressing limitations that users need to be aware of when utilizing vision-enabled chat models.

## articles/ai-services/openai/concepts/prompt-transformation.md{#item-21e047}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update Date in Prompt Transformation Document"
}
```

### Explanation
The code diff reflects a minor update made to the document titled "prompt-transformation.md" within the Azure AI OpenAI concepts section. The sole modification is an update to the metadata date, changing it from "02/20/2025" to "05/31/2025." 

This date change indicates a new effective date for the content, ensuring that users refer to the most current version of the document. No additional changes to the body content or structure of the document have been made in this modification. The update serves to keep the document relevant and aligned with its intended publication timeline.

## articles/ai-services/openai/dall-e-quickstart.md{#item-fcd528}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom: devx-track-python, devx-track-dotnet, devx-track-extended-java, devx-
 ms.topic: quickstart
 author: PatrickFarley
 ms.author: pafarley
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 zone_pivot_groups: openai-quickstart-dall-e
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update Date in DALL-E Quickstart Document"
}
```

### Explanation
The code diff shows a minor update to the "dall-e-quickstart.md" document, which is part of the Azure AI OpenAI resources. The only change made is the update of the metadata date from "02/20/2025" to "05/31/2025."

This modification signifies a new effective date for when the information within this document is applicable. It ensures that users accessing the quickstart guide are referring to the most current version, reflecting any scheduled updates or alignment with related documentation timelines. No changes to the substantive content or structure of the document were made, making this update purely administrative.

## articles/ai-services/openai/gpt-v-quickstart.md{#item-2a6183}

<details>
<summary>Diff</summary>
````diff
@@ -9,18 +9,14 @@ ms.custom: devx-track-python, devx-track-js, devx-track-ts
 ms.topic: quickstart
 author: PatrickFarley
 ms.author: pafarley
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 zone_pivot_groups: openai-quickstart-gpt-v
 ---
 
 # Quickstart: Use images in your AI chats
 
-Get started using GPT-4 Turbo with images with the Azure OpenAI in Azure AI Foundry Models.
+Get started using images in your chats with Azure OpenAI in Azure AI Foundry Models.
 
-> [!NOTE]
-> **Model choice**
->
-> The latest vision-capable models are `gpt-4o` and `gpt-4o mini`. These models are in public preview. The latest available GA model is `gpt-4` version `turbo-2024-04-09`.
 
 > [!IMPORTANT]
 > Extra usage fees might apply when using chat completion models with vision functionality.
@@ -62,7 +58,7 @@ Get started using GPT-4 Turbo with images with the Azure OpenAI in Azure AI Foun
 
 ::: zone-end
 
-## Next steps
+## Related content
 
 * [Get started with multimodal vision chat apps using Azure OpenAI AI App template](/azure/developer/ai/get-started-app-chat-vision?tabs=github-codespaces)
 * Learn more about these APIs in the [Vision-enabled models how-to guide](./gpt-v-quickstart.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise Content and Date in GPT-V Quickstart Document"
}
```

### Explanation
The code diff indicates a minor update to the "gpt-v-quickstart.md" document within the Azure AI OpenAI resources. This update includes both content changes and a modification to the metadata date, which has been updated from "02/20/2025" to "05/31/2025."

Key changes in the content include a rephrasing of the introductory sentence to clarify the purpose of the document. The phrase "using GPT-4 Turbo with images" was modified to "using images in your chats," which simplifies the language and makes it more concise. Additionally, a section header was changed from "Next steps" to "Related content," potentially indicating a shift in focus for the linked resources.

Overall, while the update does not introduce any new features or substantial alterations to the content, it aims to enhance clarity and ensure that users have the most up-to-date information regarding usage dates and related resources.

## articles/ai-services/openai/how-to/evaluations.md{#item-dfaa1c}

<details>
<summary>Diff</summary>
````diff
@@ -18,14 +18,39 @@ The evaluation of large language models is a critical step in measuring their pe
 
 Azure OpenAI evaluation enables developers to create evaluation runs to test against expected input/output pairs, assessing the modelâ€™s performance across key metrics such as accuracy, reliability, and overall performance.
 
-## Evaluations support
+## Evaluation support
 
 ### Regional availability
 
-- East US2
+- Australia East
+- Brazil South
+- Canada Central
+- Central US
+- East US 2
+- France Central
+- Germany West Central
+- Italy North
+- Japan East
+- Japan West
+- Korea Central
 - North Central US
+- Norway East
+- Poland Central
+- South Africa North
+- Southeast Asia
+- Spain Central
 - Sweden Central
+- Switzerland North
 - Switzerland West
+- UAE North
+- UK South
+- UK West
+- West Europe
+- West US
+- West US 2
+- West US 3
+
+If your preferred region is missing, refer to [Azure OpenAI regions](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#global-standard-model-availability) and check if it is one of the Azure OpenAI regional availability zones.
 
 ### Supported deployment types
 
@@ -36,6 +61,10 @@ Azure OpenAI evaluation enables developers to create evaluation runs to test aga
 - Global provisioned-managed
 - Data zone provisioned-managed
 
+## Evaluation API (preview)
+
+Evaluation API lets you test model outputs directly through API calls, and programmatically assess model quality and performance. To use Evaluation API, check out the [REST API documentation](https://learn.microsoft.com/azure/ai-services/openai/authoring-reference-preview#evaluation---get-list). 
+
 ## Evaluation pipeline
 
 ### Test data
@@ -90,30 +119,40 @@ Outputs generated during the evaluation will be referenced in subsequent steps u
 
 ### Model deployment
 
-As part of creating evaluations you'll pick which models to use when generating responses (optional) as well as which models to use when grading models with specific testing criteria.  
-
-In Azure OpenAI you'll be assigning specific model deployments to use as part of your evaluations. You can compare multiple model deployments in single evaluation run.
+In Azure OpenAI, you need to create a model deployment to use for your evaluation. You can pick and deploy a single model, or multiple models, depending on your needs. These model deployments will be used when grading your base model or your fine-tuned model with the test criteria of your choice. You can also use the deployed models to auto-generate responses for your provided prompt. 
 
-You can evaluate either base or fine-tuned model deployments. The deployments available in your list depend on those you created within your Azure OpenAI resource. If you can't find the desired deployment, you can create a new one from the Azure OpenAI Evaluation page.
+The deployments available in your list depend on those you created within your Azure OpenAI resource. If you can't find the desired deployment, you can create a new one from the Azure OpenAI Evaluation page.
 
 ### Testing criteria
 
 Testing criteria is used to assess the effectiveness of each output generated by the target model. These tests compare the input data with the output data to ensure consistency. You have the flexibility to configure different criteria to test and measure the quality and relevance of the output at different levels.
 
-:::image type="content" source="../media/how-to/evaluations/testing-criteria.png" alt-text="Screenshot that shows the evaluations testing criteria options." lightbox="../media/how-to/evaluations/testing-criteria.png":::
+:::image type="content" source="../media/how-to/evaluations/eval-testing-criteria.png" alt-text="Screenshot that shows the evaluations testing criteria options." lightbox="../media/how-to/evaluations/eval-testing-criteria.png":::
+
+When you click into each testing criteria, you will see different types of graders as well as preset schemas that you can modify per your own evaluation dataset and criteria. 
+
+:::image type="content" source="../media/how-to/evaluations/eval-testing-criteria-2.png" alt-text="Screenshot that shows the evaluations testing criteria options." lightbox="../media/how-to/evaluations/eval-testing-criteria-2.png":::
 
 ## Getting started
 
 1. Select the **Azure OpenAI Evaluation (PREVIEW)** within [Azure AI Foundry portal](https://ai.azure.com/). To see this view as an option may need to first select an existing Azure OpenAI resource in a supported region.
-2. Select **New evaluation**
+2. Select **+ New evaluation**
 
     :::image type="content" source="../media/how-to/evaluations/new-evaluation.png" alt-text="Screenshot of the Azure OpenAI evaluation UX with new evaluation selected." lightbox="../media/how-to/evaluations/new-evaluation.png":::
 
-3. Enter a name of your evaluation. By default a random name is automatically generated unless you edit and replace it. Select **Upload new dataset**.
+3. Choose how you would like to provide test data for evaluation. You can import stored Chat Completions, create data using provided default templates, or upload your own data. Let's walk through uploading your own data. 
+
+    :::image type="content" source="../media/how-to/evaluations/create-new-eval.png" alt-text="Screenshot of the Azure OpenAI create new evaluation." lightbox="../media/how-to/evaluations/create-new-eval.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/upload.png" alt-text="Screenshot of the Azure OpenAI upload UX." lightbox="../media/how-to/evaluations/upload.png":::
+4. Select your evaluation data which will be in `.jsonl` format. If you already have an existing data, you can select one, or upload a new data.
 
-4. Select your evaluation which will be in `.jsonl` format. If you need a sample test file you can save these 10 lines to a file called `eval-test.jsonl`:
+    :::image type="content" source="../media/how-to/evaluations/upload-data-1.png" alt-text="Screenshot of data upload." lightbox="../media/how-to/evaluations/upload-data-1.png":::
+
+   When you upload new data, you'll see the first three lines of the file as a preview on the right side:
+
+    :::image type="content" source="../media/how-to/evaluations/upload-data-2.png" alt-text="Screenshot of data upload." lightbox="../media/how-to/evaluations/upload-data-2.png":::
+
+   If you need a sample test file, you can use this sample `.jsonl` text. This sample contains sentences of various technical content, and we are going to be assessing semantic similarity across these sentences.
 
     ```jsonl
     {"input": [{"role": "system", "content": "Provide a clear and concise summary of the technical content, highlighting key concepts and their relationships. Focus on the main ideas and practical implications."}, {"role": "user", "content": "Tokenization is a key step in preprocessing for natural language processing, involving the division of text into smaller components called tokens. These can be words, subwords, or characters, depending on the method chosen. Word tokenization divides text at word boundaries, while subword techniques like Byte Pair Encoding (BPE) or WordPiece can manage unknown words by breaking them into subunits. Character tokenization splits text into individual characters, useful for multiple languages and misspellings. The tokenization method chosen greatly affects model performance and its capacity to handle various languages and vocabularies."}], "output": "Tokenization divides text into smaller units (tokens) for NLP applications, using word, subword (e.g., BPE), or character methods. Each has unique benefits, impacting model performance and language processing capabilities."}      
@@ -127,48 +166,45 @@ Testing criteria is used to assess the effectiveness of each output generated by
     {"input": [{"role": "system", "content": "Generate a clear and focused summary that captures the essential technical details while maintaining their relationships and significance."}, {"role": "user", "content": "Transfer learning is a machine learning method where a model developed for one task is reused as the starting point for a model on a second task. This approach is powerful in deep learning, where pre-trained models on large datasets (like ImageNet for computer vision or BERT for NLP) are fine-tuned on specific downstream tasks. Transfer learning reduces the need for large amounts of task-specific training data and computational resources, as the model has already learned useful features from the source domain. Common strategies include feature extraction (freezing pre-trained layers) and fine-tuning (updating all or some pre-trained weights)."}], "output": "Transfer learning reuses models trained on one task for different tasks, particularly effective in deep learning. It leverages pre-trained models through feature extraction or fine-tuning, reducing data and computational needs for new tasks."}      
     {"input": [{"role": "system", "content": "Provide a precise and informative summary that distills the key technical concepts while maintaining their relationships and practical importance."}, {"role": "user", "content": "Ensemble methods combine multiple machine learning models to create a more robust and accurate predictor. Common techniques include bagging (training models on random data subsets), boosting (sequentially training models to correct earlier errors), and stacking (using a meta-model to combine base model predictions). Random Forests, a popular bagging method, create multiple decision trees using random feature subsets. Gradient Boosting builds trees sequentially, with each tree correcting the errors of previous ones. These methods often outperform single models by reducing overfitting and variance while capturing different data aspects."}], "output": "Ensemble methods enhance prediction accuracy by combining multiple models through techniques like bagging, boosting, and stacking. Popular implementations include Random Forests (using multiple trees with random features) and Gradient Boosting (sequential error correction), offering better performance than single models."}
     ```
+    
+5. If you would like to create new responses using inputs from your test data, you can select 'Generate new responses'. This will inject the input fields from our evaluation file into individual prompts for a model of your choice to generate output.
 
-    You'll see the first three lines of the file as a preview:
-
-    :::image type="content" source="../media/how-to/evaluations/preview.png" alt-text="Screenshot that shows a preview of an uploaded evaluation file." lightbox="../media/how-to/evaluations/preview.png":::
-
-5. Under **Responses** select the **Create** button. Select `{{item.input}}` from the **Create with a template** dropdown. This will inject the input fields from our evaluation file into individual prompts for a new model run that we want to able to compare against our evaluation dataset. The model will take that input and generate its own unique outputs which in this case will be stored in a variable called `{{sample.output_text}}`. We'll then use that sample output text later as part of our testing criteria. Alternatively you could provide your own custom system message and individual message examples manually.
-
-6. Select which model you want to generate responses based on your evaluation. If you don't have a model you can create one. For the purpose of this example we're using a standard deployment of `gpt-4o-mini`.
+:::image type="content" source="../media/how-to/evaluations/eval-generate-1.png" alt-text="Screenshot of the UX for generating model responses." lightbox="../media/how-to/evaluations/eval-generate-1.png":::
+   
+You will select the model of your choice. If you do not have a model, you can create a new model deployment. The selected model will take the input data and generate its own unique outputs, which in this case will be stored in a variable called `{{sample.output_text}}`. We'll then use that output later as part of our testing criteria. Alternatively, you could provide your own custom system message and individual message examples manually.
 
-    :::image type="content" source="../media/how-to/evaluations/item-input.png" alt-text="Screenshot of the UX for generating model responses with a model selected." lightbox="../media/how-to/evaluations/item-input.png":::
+:::image type="content" source="../media/how-to/evaluations/eval-generate-2.png" alt-text="Screenshot of the UX for generating model responses." lightbox="../media/how-to/evaluations/eval-generate-2.png":::
 
-    The settings/sprocket symbol controls the basic parameters that are passed to the model. Only the following parameters are supported at this time:
+6. For creating a test criteria, select **Add**. For the example file we provided, we are going to be assessing semantic similarity. Select **Model Scorer**, which contains test criteria presets for Semantic Similarity.
 
-- **Temperature**
-- **Maximum length**
-- **Top P**
+    :::image type="content" source="../media/how-to/evaluations/eval-semantic-similarity-1.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/eval-semantic-similarity-1.png":::
 
- Maximum length is currently capped at 2048 regardless of what model you select.
+   Select **Semantic Similarity** at the top. Scroll to the bottom, and in `User` section, specify `{{item.output}}` as `Ground truth`, and specify `{{sample.output_text}}` as `Output`. This will take the original reference output from your evaluation `.jsonl` file (the sample file provided) and compare it against the output that is generated by the model you chose in the previous step.
 
-7. Select **Add testing criteria** select **Add**.
+    :::image type="content" source="../media/how-to/evaluations/eval-semantic-similarity-2.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/eval-semantic-similarity-2.png":::
 
-8. Select **Semantic Similarity** >  Under **Compare** add `{{item.output}}` under **With** add ``{{sample.output_text}}``. This will take the original reference output from your evaluation `.jsonl` file and compare it against the output that will be generated by giving the model prompts based on your ``{{item.input}}``.
+:::image type="content" source="../media/how-to/evaluations/eval-semantic-similarity-3.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/eval-semantic-similarity-3.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/semantic-similarity-config.png" alt-text="Screenshot of the semantic similarity UX config." lightbox="../media/how-to/evaluations/semantic-similarity-config.png":::
+7. Select **Add** to add this testing criteria. If you would like to add additional testing criteria, you can add them at this step.
 
-9. Select **Add** > at this point you can either add additional testing criteria or you select Create to initiate the evaluation job run.
+8. You are ready to create your Evaluation. Provide your Evaluation name, review everything looks correct, and **Submit** to create the Evaluation job. You'll be taken to a status page for your evaluation job, which will show the status as "Waiting".
 
-10. Once you select **Create** you'll be taken to a status page for your evaluation job.
+:::image type="content" source="../media/how-to/evaluations/eval-submit-job.png" alt-text="Screenshot of the evaluation job submit UX." lightbox="../media/how-to/evaluations/eval-submit-job.png":::
+:::image type="content" source="../media/how-to/evaluations/eval-submit-job-2.png" alt-text="Screenshot of the evaluation job submit UX." lightbox="../media/how-to/evaluations/eval-submit-job-2.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/status.png" alt-text="Screenshot of the evaluation status UX." lightbox="../media/how-to/evaluations/status.png":::
+9. Once your evaluation job has created, you can select the job to view the full details of the job:
 
-11. Once your evaluation job has created you can select the job to view the full details of the job:
+:::image type="content" source="../media/how-to/evaluations/test-complete.png" alt-text="Screenshot of a completed semantic similarity test with mix of pass and failures." lightbox="../media/how-to/evaluations/test-complete.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/test-complete.png" alt-text="Screenshot of a completed semantic similarity test with mix of pass and failures." lightbox="../media/how-to/evaluations/test-complete.png":::
+10. For semantic similarity **View output details** contains a JSON representation that you can copy/paste of your passing tests.
 
-12. For semantic similarity **View output details** contains a JSON representation that you can copy/paste of your passing tests.
+:::image type="content" source="../media/how-to/evaluations/output-details.png" alt-text="Screenshot of the evaluation status UX with output details." lightbox="../media/how-to/evaluations/output-details.png":::
 
-    :::image type="content" source="../media/how-to/evaluations/output-details.png" alt-text="Screenshot of the evaluation status UX with output details." lightbox="../media/how-to/evaluations/output-details.png":::
+11. You can also add more Eval runs by selecting **+ Add Run** button at the top left corner of your evaluation job page.
 
-## Testing criteria details
+## Types of Testing Criteria
 
-Azure OpenAI Evaluation offers multiple testing criteria options. The section below provides additional details on each option.
+Azure OpenAI Evaluation offers various evaluation testing criteria on top of Semantic Similarity we saw in the provided example. This section provides information about each testing criteria at much more detail.
 
 ### Factuality
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements and Updates to Evaluations Document"
}
```

### Explanation
The code diff reflects a significant update to the "evaluations.md" document within the Azure AI OpenAI resources, featuring a total of 108 changes, including 72 additions and 36 deletions. This update enhances the clarity and comprehensiveness of the document by introducing new content and restructuring existing sections.

Key changes include:

1. **Introduction of New Content**: The document has undergone substantial rewrites to clarify the purpose and functionality of the Azure OpenAI evaluation tools. New sections discussing "Evaluation API (preview)" have been added, detailing how users can test model outputs directly through API calls.

2. **Expanded Regional Availability**: The list of regions where evaluation services are available has been expanded significantly, enhancing usability by informing users about service accessibility in various locations.

3. **Updates to Supported Deployment Types**: Clarifications have been made regarding the deployment types available for evaluations, providing a clearer understanding of options.

4. **Refined Evaluation Instructions**: The step-by-step guide for getting started with the evaluation has been enhanced with clearer language, additional details, and corresponding visual aids for better user comprehension. For instance, users are now guided on how to provide test data, generate new responses, and create detailed semantic similarity tests with improved instructions on configuration.

5. **New Visual Aids**: Additional images have been incorporated to visually guide users through different processes within the evaluation framework, fostering a better understanding of the interface and functionalities.

Overall, the modifications greatly improve the document's usability and provide users with the necessary tools and information to effectively utilize the Azure OpenAI evaluation features.

## articles/ai-services/openai/how-to/gpt-with-vision.md{#item-4d8502}

<details>
<summary>Diff</summary>
````diff
@@ -7,14 +7,14 @@ ms.author: pafarley #delegenz
 #customer intent: As a developer, I want to learn how to use vision-enabled chat models so that I can integrate image processing capabilities into my applications.
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
 # Use vision-enabled chat models
 
 
-Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are [o1](./reasoning.md), GPT-4o, GPT-4o-mini, and GPT-4 Turbo with Vision.
+Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are the [o-series reasoning models](./reasoning.md), GPT-4.1 series models, GPT-4.5, GPT-4o series, and GPT-4 Turbo with Vision.
 
 The vision-enabled models can answer general questions about what's present in the images you upload.
 
@@ -383,7 +383,7 @@ Every response includes a `"finish_reason"` field. It has the following possible
 [!INCLUDE [GPT-4 Turbo](../includes/gpt-4-turbo.md)]
 
 
-## Next steps
+## Related content
 
 * [Learn more about Azure OpenAI](../overview.md).
 * [Vision-enabled chats quickstart](../gpt-v-quickstart.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to GPT with Vision Document"
}
```

### Explanation
The code diff reveals a minor update to the "gpt-with-vision.md" document in the Azure AI OpenAI resources. The changes consist of 6 revisions, with 3 lines added and 3 lines deleted, primarily focusing on updating model names and modifying section headers.

Noteworthy modifications include:

1. **Date Update**: The metadata date has been changed from "02/20/2025" to "05/31/2025," reflecting an updated timeline for the document's content.

2. **Clarification on Model Naming**: The description of the current vision-enabled models has been expanded for clarity. Specific terms have been adjusted to better reflect the versions and types of models available. The phrase "o1" has been updated to "the o-series reasoning models," and the mention of "GPT-4o" has been revised to "GPT-4.1 series models" and "GPT-4.5" to accurately represent the model lineup.

3. **Section Header Update**: The section titled "Next steps" has been renamed to "Related content," which potentially aligns better with the context of the links that follow, enhancing the logical structure of the document.

Overall, these updates enhance the clarity and accuracy of the document regarding the available GPT models with vision capabilities, ensuring that readers have access to the latest information for integrating image processing into their applications.

## articles/ai-services/openai/how-to/risks-safety-monitor.md{#item-b2be0b}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley 
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 manager: nitinme
 ---
 
@@ -43,14 +43,14 @@ Adjust your content filter configuration to further align with business needs an
 
 ## Potentially abusive user detection   
 
-The **Potentially abusive user detection** pane leverages user-level abuse reporting to show information about users whose behavior has resulted in blocked content. The goal is to help you get a view of the sources of harmful content so you can take responsive actions to ensure the model is being used in a responsible way. 
+The **Potentially abusive user detection** pane shows information about users whose behavior has resulted in blocked content. The goal is to help you get a view of the sources of harmful content so you can take responsive actions to ensure the model is being used in a responsible way. 
 
 
 To use Potentially abusive user detection, you need:
 - A content filter configuration applied to your deployment.
 - You must be sending user ID information in your Chat Completion requests (see the _user_ parameter of the [Completions API](/azure/ai-services/openai/reference#completions), for example).
     > [!CAUTION]
-    > Use GUID strings to identify individual users. Do not include sensitive personal information in the _user_ field.
+    > Use GUID strings to identify individual users. Don't include sensitive personal information in the _user_ field.
 - An Azure Data Explorer database set up to store the user analysis results (instructions below).
 
 ### Set up your Azure Data Explorer database
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Risks and Safety Monitor Document"
}
```

### Explanation
The code diff indicates a minor update made to the "risks-safety-monitor.md" document in the Azure AI OpenAI resources. The changes consist of 6 modifications, including 3 additions and 3 deletions, predominantly aimed at updating the document's metadata and refining content clarity.

Key changes include:

1. **Date Change**: The document's metadata date has been revised from "02/20/2025" to "05/31/2025," reflecting an updated timeline for the content's relevance.

2. **Content Clarification**: The description of the **Potentially abusive user detection** pane has been streamlined. The sentence structure has been simplified by removing the phrase "leverages user-level abuse reporting," which clarifies the sentence flow without altering the meaning.

3. **Improved Language**: The cautionary note regarding user identification methods has been slightly altered to enhance readability. The phrase "Do not include" has been changed to "Don't include," making it more concise while retaining the original intent.

4. **Content Consistency**: Minor adjustments have been made to maintain consistency throughout the document while also ensuring that the technical details regarding content filters and user detection remain intact.

Overall, these updates enhance the readability and accuracy of the document, providing users with clear instructions and information about using the safety and monitoring features within Azure AI OpenAI.

## articles/ai-services/openai/how-to/use-blocklists.md{#item-e99db7}

<details>
<summary>Diff</summary>
````diff
@@ -5,7 +5,7 @@ description: Learn how to use blocklists with Azure OpenAI
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 05/31/2025
 author: PatrickFarley
 ms.author: pafarley
 ---
@@ -17,7 +17,7 @@ The [configurable content filters](/azure/ai-services/openai/how-to/content-filt
 ## Prerequisites
 
 - An Azure subscription. <a href="https://azure.microsoft.com/free/ai-services" target="_blank">Create one for free</a>.
-- Once you have your Azure subscription, create an Azure OpenAI resource in the Azure portal to get your token, key and endpoint. Enter a unique name for your resource, select the subscription you entered on the application form, select a resource group, supported region, and supported pricing tier. Then select **Create**.
+- Once you have your Azure subscription, create an Azure OpenAI resource in the Azure portal to get your token, key, and endpoint. Enter a unique name for your resource, select the subscription you entered on the application form, select a resource group, supported region, and supported pricing tier. Then select **Create**.
     - The resource takes a few minutes to deploy. After it finishes, select **go to resource**. In the left pane, under **Resource Management**, select **Subscription Key and Endpoint**. The endpoint and either of the keys are used to call APIs.
 - [Azure CLI](/cli/azure/install-azure-cli) installed
 - [cURL](https://curl.haxx.se/) installed
@@ -30,7 +30,7 @@ You can create blocklists with the Azure OpenAI API. The following steps help yo
 
 ### Get your token
 
-First, you need to get a token for accessing the APIs for creating, editing and deleting blocklists. You can get this token using the following Azure CLI command: 
+First, you need to get a token for accessing the APIs for creating, editing, and deleting blocklists. You can get this token using the following Azure CLI command: 
 
 ```bash
 az account get-access-token 
@@ -102,7 +102,7 @@ Copy the cURL command below to a text editor and make the following changes:
 1. Replace {raiBlocklistName} (in the URL) with a custom name for your list. Allowed characters: `0-9, A-Z, a-z, - . _ ~`. 
 1. Replace {raiBlocklistItemName} with a custom name for your list item. 
 1. Replace {token} with the token you got from the "Get your token" step above. 
-1. Replace the value of the `"blocking pattern"` field with the item you'd like to add to your blocklist. The maximum length of a blockItem is 1000 characters. Also specify whether the pattern is regex or exact match. 
+1. Replace the value of the `"blocking pattern"` field with the item you'd like to add to your blocklist. The maximum length of a blockItem is 1,000 characters. Also specify whether the pattern is regex or exact match. 
 
 ```bash
 curl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}/raiBlocklistItems/{raiBlocklistItemName}?api-version=2024-04-01-preview' \ 
@@ -117,7 +117,7 @@ curl --location --request PUT 'https://management.azure.com/subscriptions/{subsc
 ```
 
 > [!NOTE]
-> It can take around 5 minutes for a new term to be added to the blocklist. Please test after 5 minutes. 
+> It can take around 5 minutes for a new term to be added to the blocklist. Test the blocklist after 5 minutes. 
 
 The response code should be `200`. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Use Blocklists Document"
}
```

### Explanation
The code diff outlines a minor update to the "use-blocklists.md" document within the Azure AI OpenAI resources. The changes involve a total of 10 modifications, including 5 additions and 5 deletions, mainly focused on improving the document's clarity and ensuring the correct presentation of information.

Key updates include:

1. **Date Revision**: The document's metadata date has been updated from "02/20/2025" to "05/31/2025," reflecting a new relevance timeline for the information provided.

2. **Punctuation and Clarity Improvements**: Several phrases throughout the document have been revised for better readability. For example, a comma has been added in the phrase "token, key, and endpoint" to clarify the list of required components.

3. **Consistent Formatting**: The note about the maximum length of a block item has been rephrased from "1000 characters" to "1,000 characters," standardizing numerical formatting for improved clarity.

4. **Textual Adjustments for Flow**: A minor adjustment in phrasing for a caution note has been made; changing "Please test after 5 minutes" to "Test the blocklist after 5 minutes" creates a more direct instruction for the user.

5. **Instructions and Command Clarity**: The steps outlining how to acquire a token and create blocklists have been maintained but presented slightly more clearly, enhancing the reader's ability to follow along.

Overall, these enhancements increase the document's clarity and usability, ensuring that users have a more straightforward understanding of how to effectively use blocklists within Azure OpenAI services.

## articles/ai-services/openai/media/how-to/evaluations/create-new-eval.png{#item-d7bc67}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Evaluation Creation Image Added"
}
```

### Explanation
The code diff indicates the addition of a new image file named "create-new-eval.png" to the Azure AI OpenAI documentation. This image is likely intended to enhance the instructional content related to creating new evaluations, providing visual support for users.

Key points regarding this modification include:

1. **New Visual Aid**: The inclusion of "create-new-eval.png" will serve as a graphical reference for users, which can help clarify the steps involved in creating new evaluations within the Azure OpenAI framework.

2. **No Content Changes**: Since this file is a new addition rather than a modification of existing text or images, there are no deletions, changes, or alterations to the current content of the documentation.

3. **Link Accessibility**: Users can access the new image through the provided blob URL, where it is hosted, ensuring that they can conveniently view or download the image for their reference.

Overall, the addition of this image represents an effort to enhance user experience by providing visual guidance, which is an essential part of technical documentation, especially in complex subjects like AI services.

## articles/ai-services/openai/media/how-to/evaluations/eval-generate-1.png{#item-5be1c8}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Evaluation Generation Added"
}
```

### Explanation
The code diff reveals the addition of a new image file named "eval-generate-1.png" to the Azure AI OpenAI documentation. This image is likely intended to provide visual context for users regarding the evaluation generation process within the Azure platform.

Key points regarding this modification include:

1. **Introduction of Visual Content**: The addition of "eval-generate-1.png" enhances the documentation by providing a visual representation that can better illustrate the steps or concepts related to evaluation generation.

2. **No Changes to Existing Content**: As this modification involves only the addition of an image, there are no deletions or changes to existing content, which suggests a focus on expanding the instructional resources available to users.

3. **Accessibility**: The new image can be accessed via the provided blob URL, allowing users to easily view or download it as needed, thereby improving their overall understanding of the evaluation generation process.

In summary, the new image serves as a valuable resource within the documentation, aiming to improve the clarity and effectiveness of the instructional content for users engaged in evaluation tasks with Azure OpenAI services.

## articles/ai-services/openai/media/how-to/evaluations/eval-generate-2.png{#item-efeac5}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Evaluation Generation Process Added"
}
```

### Explanation
The code diff highlights the addition of a new image file named "eval-generate-2.png" to the Azure AI OpenAI documentation. This image is intended to provide additional visual support for users as they learn about the evaluation generation process.

Key points regarding this modification include:

1. **Enhancing Visual Documentation**: The introduction of "eval-generate-2.png" contributes to a more comprehensive understanding of the evaluation generation workflow, helping users better grasp the concepts through visual means.

2. **No Changes to Existing Files**: Since this update only includes the addition of a new image, there are no deletions or modifications to existing content, indicating that the documentation is being expanded rather than altered.

3. **Easy Access**: The new image can be accessed through the provided blob URL, allowing users to view or download it effortlessly, thereby facilitating their engagement with the evaluation generation topic.

Overall, this additional image enhances the instructional materials available for users, potentially improving their comprehension and experience when working with evaluation generation in Azure's OpenAI services.

## articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-1.png{#item-8729c1}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Semantic Similarity Evaluation Added"
}
```

### Explanation
The code diff indicates the addition of a new image file titled "eval-semantic-similarity-1.png" to the Azure AI OpenAI documentation. This image is designed to support users in understanding the concept of semantic similarity evaluation, enhancing the existing resources.

Key points regarding this modification include:

1. **Visual Aid Enhancement**: The addition of "eval-semantic-similarity-1.png" enriches the documentation, providing visual representation to better explain various aspects of semantic similarity evaluations within Azure AI.

2. **No Impact on Existing Content**: This modification solely includes the addition of an image, indicating that no existing files were removed or altered. This approach enhances the documentation without disrupting current resources.

3. **Accessible Resource**: Users can readily access the new image through the provided blob URL, facilitating easy viewing and understanding of the evaluation concepts associated with semantic similarity.

In summary, this addition aims to improve the instructional quality of the documentation concerning semantic similarity evaluations, providing users with valuable visual resources to aid their learning process.

## articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-2.png{#item-a316c2}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Semantic Similarity Evaluation Process Added"
}
```

### Explanation
The code diff displays the addition of a new image file named "eval-semantic-similarity-2.png" to the Azure AI OpenAI documentation. This image is added to further support and clarify the process of evaluating semantic similarity.

Key points regarding this modification include:

1. **Supplementary Visual Content**: The introduction of "eval-semantic-similarity-2.png" serves as an additional visual aid, enriching the understanding of semantic similarity evaluations. This can help users visualize concepts and processes more effectively.

2. **No Alterations to Existing Files**: This update does not change or delete any existing files. Instead, it simply expands the current documentation by adding a supportive resource focused on semantic similarity evaluation.

3. **User Accessibility**: The new image can be accessed conveniently through the provided blob URL, making it easy for users to reference and utilize the image in their understanding of the associated content.

Overall, this addition reinforces the documentation's instructional materials, enhancing the overall learning experience for users interested in semantic similarity evaluations within Azure AI services.

## articles/ai-services/openai/media/how-to/evaluations/eval-semantic-similarity-3.png{#item-0c0125}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Semantic Similarity Evaluation Steps Added"
}
```

### Explanation
The code diff shows the addition of a new image file titled "eval-semantic-similarity-3.png" to the Azure AI OpenAI documentation. This image serves to enhance the explanation of the steps involved in evaluating semantic similarity.

Key points regarding this modification include:

1. **Enhanced Visual Learning Tools**: The addition of "eval-semantic-similarity-3.png" provides another visual resource that aids users in comprehending the processes of semantic similarity evaluation, making the documentation more comprehensive.

2. **No Existing Content Changes**: This update introduces a new resource without removing or altering any pre-existing files, ensuring that existing documentation remains intact and accessible.

3. **Easy Access for Users**: The new image can be accessed via the provided blob URL, allowing users quick and easy access to visualize key concepts relevant to semantic similarity evaluation.

In summary, this addition contributes to the overall instructional effectiveness of the documentation by providing another layer of visual support, which is crucial for users engaging with the content related to semantic similarity evaluations in Azure AI services.

## articles/ai-services/openai/media/how-to/evaluations/eval-submit-job-2.png{#item-d8110c}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Job Submission Process Added"
}
```

### Explanation
The code diff indicates the addition of a new image file called "eval-submit-job-2.png" to the Azure AI OpenAI documentation. This image provides a visual representation associated with the process of submitting a job for evaluation.

Key points regarding this modification include:

1. **Visual Aid for Job Submission**: The inclusion of "eval-submit-job-2.png" serves as an instructional tool that enhances the understanding of the job submission workflow for evaluations, helping users effectively navigate this aspect of the service.

2. **No Modifications to Existing Content**: This update does not involve any deletions or alterations to current files. It simply adds supplementary material to the existing documentation, preserving the integrity of the information available.

3. **User-Friendly Access**: The new image can be accessed through the provided blob URL, allowing users direct access to this visual content, thereby improving their learning experience related to job submissions in Azure's evaluation processes.

Overall, this modification enriches the documentation by providing an additional resource that supports understanding and usability, particularly for users engaged in evaluating services within Azure AI.

## articles/ai-services/openai/media/how-to/evaluations/eval-submit-job.png{#item-a9a1f5}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Job Submission Process Added"
}
```

### Explanation
The code diff shows the addition of a new image file named "eval-submit-job.png" to the Azure AI OpenAI documentation, specifically designed to illustrate the job submission process for evaluations.

Key points regarding this modification include:

1. **Enhanced Visual Instruction**: The new image "eval-submit-job.png" provides a clear visual aid that helps users understand the steps involved in submitting a job for evaluation. This can make the documentation more user-friendly and facilitate better comprehension of the workflow.

2. **No Impact on Existing Files**: This addition does not affect any existing files, meaning that there are no deletions or changes to the current content. It solely adds a new resource to support users.

3. **Accessibility**: Users can easily access the new image through the provided blob URL, which offers direct integration into the relevant documentation context, enhancing the overall instructional resources available for Azure AI services.

In summary, this modification contributes positively to the documentation by adding another visual reference to aid users, making it easier to follow the job submission process for evaluations within Azure AI services.

## articles/ai-services/openai/media/how-to/evaluations/eval-testing-criteria-2.png{#item-220e27}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Evaluation Testing Criteria Added"
}
```

### Explanation
The code diff reveals the addition of a new image file titled "eval-testing-criteria-2.png" to the Azure AI OpenAI documentation. This image serves to visually explain the criteria involved in evaluating jobs within the service.

Key highlights of this modification include:

1. **Supplementary Visual Content**: The addition of "eval-testing-criteria-2.png" enhances the documentation by providing users with a visual representation of the testing criteria used during the evaluation process. This aids in clarifying the standards and expectations for job submissions.

2. **No Existing Content Affected**: There are no deletions or changes to any existing files, ensuring that current documentation remains intact while the new image is simply an addition.

3. **Direct Access for Users**: The image is available through the provided blob URL, making it easily accessible to readers who are looking for visual guidance on evaluation criteria.

In conclusion, this update enriches the documentation on evaluation processes by including an additional visual resource that enhances user understanding, especially regarding the criteria for evaluating tasks within Azure AI services.

## articles/ai-services/openai/media/how-to/evaluations/eval-testing-criteria.png{#item-268f4b}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Evaluation Testing Criteria Added"
}
```

### Explanation
The code diff indicates the addition of a new image file named "eval-testing-criteria.png" to the Azure AI OpenAI documentation. This image is intended to provide visual clarification regarding the testing criteria applied in the evaluation of job submissions.

Key points regarding this modification include:

1. **Visual Aids for Understanding**: The introduction of "eval-testing-criteria.png" enhances the documentation by offering a visual reference that helps users better grasp the specific criteria involved in job evaluations. This approach is beneficial for clarifying complex information in an easily digestible format.

2. **No Impact on Existing Content**: This addition does not modify or remove any existing files, ensuring the current documentation structure remains unchanged while adding valuable new content.

3. **Ease of Access**: Users can directly access the image using the provided blob URL, which facilitates quick retrieval of the visual aid for enhanced understanding during the evaluation process.

In summary, this modification enriches the documentation for Azure AI services by incorporating an additional visual resource, thereby improving user comprehension of the evaluation testing criteria and supporting effective usage of the services.

## articles/ai-services/openai/media/how-to/evaluations/upload-data-1.png{#item-cd5af0}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Data Upload Process Added"
}
```

### Explanation
The code diff shows the addition of a new image file named "upload-data-1.png" to the Azure AI OpenAI documentation. This image serves to illustrate the process of uploading data as part of the evaluation workflow.

Key aspects of this modification include:

1. **Enhanced Documentation with Visual Guidance**: The integration of "upload-data-1.png" provides users with a visual aid that clarifies the steps and considerations involved in uploading data for evaluation purposes. Visual content helps simplify complex processes, making them more approachable for users.

2. **No Existing Content Modified**: This is a straightforward addition that does not affect any existing documentation. There are no deletions or changes to current files, ensuring that the integrity of existing content is preserved while introducing new informative material.

3. **Accessibility of Visual Content**: Users can easily access the newly added image through the provided blob URL, facilitating immediate reference to the visual instructions related to the data upload process.

In summary, this update enhances the Azure AI documentation by incorporating a new visual resource that better supports users in understanding the data upload process during evaluations, ultimately leading to improved user experience.

## articles/ai-services/openai/media/how-to/evaluations/upload-data-2.png{#item-de63da}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "New Image for Data Upload Process Added"
}
```

### Explanation
The code diff outlines the addition of a new image file called "upload-data-2.png" to the Azure AI OpenAI documentation. This image is part of a series designed to visually assist users in understanding the data upload process for evaluations.

Here are the important points regarding this modification:

1. **Visual Learning Tool**: The addition of "upload-data-2.png" complements previous images and provides another visual representation that helps clarify the steps involved in the data upload workflow. This supports a more effective learning experience for users by demonstrating the process in a visual manner.

2. **Static Addition**: This modification solely adds new content without altering or deleting any existing files. This ensures that the current documentation remains intact while enhancing it with additional visual aids.

3. **Direct Access for Users**: The new image can be accessed via the provided blob URL, allowing users to view the visual guide easily as they navigate the process of uploading data for evaluations.

In conclusion, this update strengthens the Azure AI documentation by adding an additional visual resource that aids in clarifying the data upload process, thereby improving user understanding and efficiency during evaluations.


