---
date: '2025-04-01'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:7dbd6d3...MicrosoftDocs:11d129c
summary: "The recent modifications to Azure OpenAI Services documentation include\
  \ both minor and major updates aimed at improving clarity and usability. Key changes\
  \ involve updating dates across documents, removing redundant information, and enhancing\
  \ the content quality. A significant alteration is the removal of the assistants-v2-note.md,\
  \ which is a breaking change for users as it removes specific information about\
  \ Assistants v2 enhancements. Additionally, there has been a major revision made\
  \ to the provisioned-throughput-onboarding.md to better explain the costs associated\
  \ with Provisioned Throughput Units (PTUs). \n\nA new feature includes the addition\
  \ of an image, deploy-model-button.png, to aid users visually. The updates reflect\
  \ a commitment to present accurate and concise information, promoting better understanding\
  \ and engagement with the documentation. Overall, these modifications aim to ensure\
  \ that Azure OpenAI Services documentation remains current, clear, and user-friendly."
title: '[en_US] Diff Insight Report - openai'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:7dbd6d3...MicrosoftDocs:11d129c){target="_blank"}

# Highlights

The recent modifications include both minor and major updates, targeting documentation related to Azure OpenAI Services. Overall, these updates primarily focus on updating dates, simplifying content by removing redundant notes, and enhancing clarity and usability. A significant change was the removal of the `assistants-v2-note.md`, which represents a breaking change. Additionally, there is a major revision to the `provisioned-throughput-onboarding.md`, with an emphasis on understanding costs associated with Provisioned Throughput Units (PTUs).

## New features
- Addition of an image, `deploy-model-button.png`, to enhance visual guidance.

## Breaking changes
- Removal of `assistants-v2-note.md`, eliminating direct reference material that outlined enhancements in Assistants v2.

## Other updates
- Date changes across numerous documents ensuring they reflect current information.
- Streamlining content by removing redundant or outdated information, specifically references to Assistants v2 notes.
- A major overhaul of `provisioned-throughput-onboarding.md` to provide a more comprehensive understanding of PTU costs.
- Updates to the table of contents to reflect changes in focus and organization of topics.

# Insights

The recent updates to the Azure OpenAI Services documentation reflect a concerted effort to present clear, concise, and current information to users. The focus on modifying document dates and removing redundant note references indicates a drive to maintain precision in the documentation timeline. By revising content and eliminating unnecessary elements, the documentation supports better user comprehension and engagement.

The complete removal of the `assistants-v2-note.md` marks a significant update, demanding users to adapt to the lack of direct information on Assistants v2 enhancements. Users may need to rely on other updated documentation sections to understand the features previously detailed in the removed note.

The major revision of `provisioned-throughput-onboarding.md` reflects an intent to provide users with a deeper understanding of PTU costs. This update caters to user needs for effective financial planning and resource management when engaging with Azure OpenAI services. The detailed tables and newly added illustrations improve accessibility by providing visual aids to complex topics.

The inclusion of the `deploy-model-button.png` image illuminates a direction towards more visually accessible documentation, potentially reducing the learning curve for users by offering graphic cues alongside textual instructions.

Overall, these updates are part of the broader maintenance and optimization efforts to keep the Azure OpenAI Services documentation robust, relevant, and user-centric.

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [assistants-reference-messages.md](#item-1c8daa) | minor update | Update date and content in assistants-reference-messages.md | modified | 1 | 3 | 4 | 
| [assistants-reference-runs.md](#item-ac752c) | minor update | Update date and content in assistants-reference-runs.md | modified | 1 | 3 | 4 | 
| [assistants-reference-threads.md](#item-d19db7) | minor update | Update date and content in assistants-reference-threads.md | modified | 1 | 3 | 4 | 
| [assistants-reference.md](#item-52344f) | minor update | Remove content from assistants-reference.md | modified | 0 | 3 | 3 | 
| [provisioned-throughput.md](#item-022e0c) | minor update | Revise content in provisioned-throughput.md | modified | 24 | 81 | 105 | 
| [assistant-functions.md](#item-a47205) | minor update | Update assistant-functions.md for date and content | modified | 1 | 3 | 4 | 
| [assistant.md](#item-b12c67) | minor update | Revise content in assistant.md | modified | 0 | 2 | 2 | 
| [code-interpreter.md](#item-95efbd) | minor update | Update content in code-interpreter.md | modified | 0 | 2 | 2 | 
| [file-search.md](#item-f9d6d7) | minor update | Update date and content in file-search.md | modified | 1 | 4 | 5 | 
| [fine-tuning-deploy.md](#item-286d57) | minor update | Update date and content in fine-tuning-deploy.md | modified | 3 | 3 | 6 | 
| [on-your-data-configuration.md](#item-4875d3) | minor update | Update date and content in on-your-data-configuration.md | modified | 2 | 2 | 4 | 
| [provisioned-get-started.md](#item-c8df1c) | minor update | Update author, date, and content in provisioned-get-started.md | modified | 6 | 6 | 12 | 
| [provisioned-throughput-onboarding.md](#item-3eb72b) | major update | Revise and expand content in provisioned-throughput-onboarding.md | modified | 107 | 55 | 162 | 
| [working-with-models.md](#item-7ec098) | minor update | Update date and documentation references in working-with-models.md | modified | 2 | 2 | 4 | 
| [assistants-python.md](#item-82d745) | minor update | Remove section reference in assistants-python.md | modified | 0 | 2 | 2 | 
| [assistants-v2-note.md](#item-64ae04) | breaking change | Removal of assistants-v2-note.md | removed | 0 | 13 | 13 | 
| [use-your-data-studio.md](#item-705daf) | minor update | Update date in use-your-data-studio.md | modified | 1 | 1 | 2 | 
| [deploy-model-button.png](#item-62cd49) | new feature | Addition of deploy-model-button.png | added | 0 | 0 | 0 | 
| [deployment-ptu-capacity-calculator.png](#item-aca8ed) | minor update | Renaming of deployment-ptu-capacity-calculator.png | renamed | 0 | 0 | 0 | 
| [toc.yml](#item-c945af) | minor update | Updates to the Table of Contents for OpenAI | modified | 3 | 3 | 6 | 


# Modified Contents
## articles/ai-services/openai/assistants-reference-messages.md{#item-1c8daa}

<details>
<summary>Diff</summary>
````diff
@@ -5,7 +5,7 @@ description: Learn how to use Azure OpenAI's Python & REST API messages with Ass
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: reference
-ms.date: 02/27/2025
+ms.date: 03/31/2025
 author: aahill
 ms.author: aahi
 recommendations: false
@@ -14,8 +14,6 @@ ms.custom: devx-track-python
 
 # Assistants API (Preview) messages reference
 
-[!INCLUDE [Assistants v2 note](includes/assistants-v2-note.md)]
-
 This article provides reference documentation for Python and REST for the new Assistants API (Preview). More in-depth step-by-step guidance is provided in the [getting started guide](./how-to/assistant.md).
 
 ## Create message
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and content in assistants-reference-messages.md"
}
```

### Explanation
The recent modification to the file `assistants-reference-messages.md` involves a minor update that includes changing the date from February 27, 2025, to March 31, 2025. Additionally, the update removed an inclusion directive for the "Assistants v2 note" and adjusted the content structure slightly by removing two lines. These changes aim to ensure the documentation reflects the correct date and streamlines the content for better clarity. Overall, this enhances the accuracy and user experience of the documentation related to Azure OpenAI's Assistants API.

## articles/ai-services/openai/assistants-reference-runs.md{#item-ac752c}

<details>
<summary>Diff</summary>
````diff
@@ -5,7 +5,7 @@ description: Learn how to use Azure OpenAI's Python & REST API runs with Assista
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: reference
-ms.date: 09/17/2024
+ms.date: 03/31/2025
 author: aahill
 ms.author: aahi
 recommendations: false
@@ -14,8 +14,6 @@ ms.custom: devx-track-python
 
 # Assistants API (Preview) runs reference
 
-[!INCLUDE [Assistants v2 note](includes/assistants-v2-note.md)]
-
 This article provides reference documentation for Python and REST for the new Assistants API (Preview). More in-depth step-by-step guidance is provided in the [getting started guide](./how-to/assistant.md).
 
 ## Create run
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and content in assistants-reference-runs.md"
}
```

### Explanation
The modification made to the file `assistants-reference-runs.md` is classified as a minor update. This change involved updating the date from September 17, 2024, to March 31, 2025, ensuring the documentation reflects the most current date. Additionally, the update removed an inclusion directive for the "Assistants v2 note" and slightly adjusted the surrounding content structure by deleting two lines. These changes serve to improve the accuracy of the documentation and simplify the text, ultimately enhancing the clarity and usability for users interacting with the Azure OpenAI's Assistants API.

## articles/ai-services/openai/assistants-reference-threads.md{#item-d19db7}

<details>
<summary>Diff</summary>
````diff
@@ -5,7 +5,7 @@ description: Learn how to use Azure OpenAI's Python & REST API threads with Assi
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: reference
-ms.date: 02/27/2025
+ms.date: 03/31/2025
 author: aahill
 ms.author: aahi
 recommendations: false
@@ -14,8 +14,6 @@ ms.custom: devx-track-python
 
 # Assistants API (Preview) threads reference
 
-[!INCLUDE [Assistants v2 note](includes/assistants-v2-note.md)]
-
 This article provides reference documentation for Python and REST for the new Assistants API (Preview). More in-depth step-by-step guidance is provided in the [getting started guide](./how-to/assistant.md).
 
 ## Create a thread
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and content in assistants-reference-threads.md"
}
```

### Explanation
The update made to the file `assistants-reference-threads.md` is categorized as a minor update. This change primarily involved updating the documentation date from February 27, 2025, to March 31, 2025, ensuring the content reflects a more current timeframe. Additionally, the update removed an inclusion directive for the "Assistants v2 note" and streamlined the surrounding content by deleting two lines. These modifications are intended to enhance the accuracy and readability of the documentation regarding Azure OpenAI's Assistants API, making it more user-friendly for developers and users seeking reference information.

## articles/ai-services/openai/assistants-reference.md{#item-52344f}

<details>
<summary>Diff</summary>
````diff
@@ -14,9 +14,6 @@ ms.custom: devx-track-python
 
 # Assistants API (Preview) reference
 
-
-[!INCLUDE [Assistants v2 note](includes/assistants-v2-note.md)]
-
 This article provides reference documentation for Python and REST for the new Assistants API (Preview). More in-depth step-by-step guidance is provided in the [getting started guide](./how-to/assistant.md).
 
 ## Create an assistant
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Remove content from assistants-reference.md"
}
```

### Explanation
The modification to the file `assistants-reference.md` is considered a minor update. This change involved the deletion of three lines from the document that primarily included an inclusion directive for the "Assistants v2 note" and a blank line. By removing this content, the update aims to simplify the document while retaining core information. The overall structure and purpose of the reference documentation regarding the Azure OpenAI's Assistants API remain intact, providing users with essential reference material without unnecessary elements. This streamlining enhances clarity and focus within the documentation.

## articles/ai-services/openai/concepts/provisioned-throughput.md{#item-022e0c}

<details>
<summary>Diff</summary>
````diff
@@ -3,67 +3,43 @@ title: Azure OpenAI Service provisioned throughput
 description: Learn about provisioned throughput and Azure OpenAI.
 ms.service: azure-ai-openai
 ms.topic: conceptual
-ms.date: 03/26/2025
+ms.date: 03/31/2025
 manager: nitinme
-author: mrbullwinkle #ChrisHMSFT
-ms.author: mbullwin #chrhoder
+author: aahill #ChrisHMSFT
+ms.author: aahi #chrhoder
 recommendations: false
 ---
 
 # What is provisioned throughput?
 
 > [!NOTE]
-> The Azure OpenAI Provisioned offering received significant updates on August 12, 2024, including aligning the purchase model with Azure standards and moving to model-independent quota. It is highly recommended that customers onboarded before this date read the Azure [OpenAI provisioned August update](./provisioned-migration.md) to learn more about these changes.
+> If you're looking for what's recently changed with the provisioned throughput offering, see the [update article](./provisioned-migration.md) for more information.
 
-
-The provisioned throughput capability allows you to specify the amount of throughput you require in a deployment. The service then allocates the necessary model processing capacity and ensures it's ready for you. Throughput is defined in terms of provisioned throughput units (PTU) which is a normalized way of representing the throughput for your deployment. Each model-version pair requires different amounts of PTU to deploy and provide different amounts of throughput per PTU. 
-
-## What do the provisioned deployment types provide?
+The provisioned throughput offering is a model deployment type that allows you to specify the amount of throughput you require in a model deployment. The Azure OpenAI service then allocates the necessary model processing capacity and ensures it's ready for you. Provisioned throughput provides:
 
 - **Predictable performance:** stable max latency and throughput for uniform workloads.
 - **Allocated processing capacity:** A deployment configures the amount of throughput. Once deployed, the throughput is available whether used or not.
 - **Cost savings:** High throughput workloads might provide cost savings vs token-based consumption.
 
-> [!NOTE]
-> Customers can take advantage of additional cost savings on provisioned deployments when they buy [Microsoft Azure OpenAI Service reservations](/azure/cost-management-billing/reservations/azure-openai#buy-a-microsoft-azure-openai-service-reservation). 
-
-
-An Azure OpenAI Deployment is a unit of management for a specific OpenAI Model. A deployment provides customer access to a model for inference and integrates more features like Content Moderation ([See content moderation documentation](content-filter.md)). Global provisioned deployments are available in the same Azure OpenAI resources as all other deployment types but allow you to leverage Azure's global infrastructure to dynamically route traffic to the data center with the best availability for each request. Similarly, data zone provisioned deployments are also available in the same resources as all other deployment types but allow you to leverage Azure's global infrastructure to dynamically route traffic to the data center within the Microsoft specified data zone with the best availability for each request. 
-
-## What do you get?
-
-| Topic | Provisioned|
-|---|---|
-| What is it? |Provides guaranteed throughput at smaller increments than the existing provisioned offer. Deployments have a consistent max latency for a given model-version. |
-| Who is it for? | Customers who want guaranteed throughput with minimal latency variance. |
-| Quota |Provisioned Managed Throughput Unit, Global Provisioned Managed Throughput Unit, or Data Zone Provisioned Managed Throughput Unit assigned per region. Quota can be used across any available Azure OpenAI model.|
-| Latency | Max latency constrained from the model. Overall latency is a factor of call shape.  |
-| Utilization | Provisioned-managed Utilization V2 measure provided in Azure Monitor. |
-|Estimating size |Provided sizing calculator in Azure AI Foundry.|
-|Prompt caching | For supported models, we discount up to 100% of cached input tokens. |
-
+> [!TIP]
+> * You can take advantage of additional cost savings when you buy [Microsoft Azure OpenAI Service reservations](/azure/cost-management-billing/reservations/azure-openai#buy-a-microsoft-azure-openai-service-reservation).
+> * Provisioned throughput is available as the following deployment types: [global provisioned](../how-to/deployment-types.md#global-provisioned), [data zone provisioned](../how-to/deployment-types.md#data-zone-provisioned) and [standard provisioned](../how-to/deployment-types.md#provisioned).
 
-## How much throughput per PTU you get for each model
-The amount of throughput (tokens per minute or TPM) a deployment gets per PTU is a function of the input and output tokens in the minute. Generating output tokens requires more processing than input tokens. For the models specified in the table below, 1 output token counts as 3 input tokens towards your TPM per PTU limit. The service dynamically balances the input & output costs, so users do not have to set specific input and output limits. This approach means your deployment is resilient to fluctuations in the workload shape.
+<!--
+Throughput is defined in terms of provisioned throughput units (PTU) which is a normalized way of representing the throughput for your deployment. Each model-version pair requires different amounts of PTU to deploy, and provides different amounts of throughput per PTU. 
 
-To help with simplifying the sizing effort, the following table outlines the TPM per PTU for the specified models. To understand the impact of output tokens on the TPM per PTU limit, use the 3 input token to 1 output token ratio. For a detailed understanding of how different ratios of input and output tokens impact the throughput your workload needs, see the [Azure OpenAI capacity calculator](https://ai.azure.com/resource/calculator). The table also shows Service Level Agreement (SLA) Latency Target Values per model.  For more information about the SLA for Azure OpenAI Service, see the [Service Level Agreements (SLA) for Online Services page](https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services?lang=1)
+An Azure OpenAI deployment is a unit of management for a specific OpenAI Model. A deployment provides customer access to a model for inference and using features, such as [content moderation](content-filter.md). 
+-->
 
-|Topic| **gpt-4o**   | **gpt-4o-mini**  | **o1**|
-| --- | --- | --- | --- |
-|Global & data zone provisioned minimum deployment|15|15|15|
-|Global & data zone provisioned scale increment|5|5|5|
-|Regional provisioned minimum deployment|50|25|50|
-|Regional provisioned scale increment|50|25|50|
-|Input TPM per PTU |2,500|37,000|230|
-|Latency Target Value |25 Tokens Per Second|33 Tokens Per Second|25 Tokens Per Second|
+## When to use provisioned throughput
 
-For a full list see the [Azure OpenAI Service in Azure AI Foundry portal calculator](https://ai.azure.com/resource/calculator).
+You should consider switching from standard deployments to provisioned managed deployments when you have well-defined, predictable throughput and latency requirements. Typically, this occurs when the application is ready for production or has already been deployed in production and there's an understanding of the expected traffic. This allows users to accurately forecast the required capacity and avoid unexpected billing. Provisioned managed deployments are also useful for applications that have real-time/latency sensitive requirements.
 
+## Key concepts
 
-> [!NOTE]
-> Global provisioned and data zone provisioned deployments are only supported for gpt-4o and gpt-4o-mini models at this time. For more information on model availability, review the [models documentation](./models.md).
+### Provisioned Throughput Units (PTU)
 
-## Key concepts
+Provisioned throughput units (PTUs) are generic units of model processing capacity that you can use to size provisioned deployments to achieve the required throughput for processing prompts and generating completions. Provisioned throughput units are granted to a subscription as quota, and used to define costs. Each quota is specific to a region and defines the maximum number of PTUs that can be assigned to deployments in that subscription and region. For information about the costs associated with the provision managed offering and PTUs, see [Understanding costs associated with PTU](../how-to/provisioned-throughput-onboarding.md).
 
 ### Deployment types
 
@@ -83,32 +59,7 @@ az cognitiveservices account deployment create \
 --sku-name GlobalProvisionedManaged
 ```
 
-### Quota
-
-#### Provisioned throughput units
-
-Provisioned throughput units (PTU) are generic units of model processing capacity that you can use to size provisioned deployments to achieve the required throughput for processing prompts and generating completions.   Provisioned throughput units are granted to a subscription as quota. Each quota is specific to a region and defines  the maximum number of PTUs that can be assigned to deployments in that subscription and region.
-
-
-#### Model independent quota
-
-Unlike the Tokens Per Minute (TPM) quota used by other Azure OpenAI offerings, PTUs are model-independent. The PTUs might be used to deploy any supported model/version in the region.
-
-:::image type="content" source="../media/provisioned/model-independent-quota.png" alt-text="Diagram of model independent quota with one pool of PTUs available to multiple Azure OpenAI models." lightbox="../media/provisioned/model-independent-quota.png":::
-
-For provisioned deployments, the new quota shows up in Azure AI Foundry as a quota item named **Provisioned Managed Throughput Unit**. For global provisioned deployments, the new quota shows up in the Azure AI Foundry as a quota item named **Global Provisioned Managed Throughput Unit**.  For data zone provisioned deployments, the new quota shows up in Azure AI Foundry as a quota item named **Data Zone Provisioned Managed Throughput Unit.** In the Foundry Quota pane, expanding the quota item shows the deployments contributing to usage of each quota.
-
-:::image type="content" source="../media/provisioned/ptu-quota-page.png" alt-text="Screenshot of quota UI for Azure OpenAI provisioned." lightbox="../media/provisioned/ptu-quota-page.png":::
-
-#### Obtaining PTU Quota
-
-PTU quota is available by default in many regions. If more quota is required, customers can request quota via the Request Quota link. This link can be found to the right of the designated provisioned deployment type quota tabs in Azure AI Foundry The form allows the customer to request an increase in the specified PTU quota for a given region. The customer receives an email at the included address once the request is approved, typically within two business days. 
-
-#### Per-Model PTU Minimums
-
-The minimum PTU deployment, increments, and processing capacity associated with each unit varies by model type & version.
-
-## Capacity transparency
+### Capacity transparency
 
 Azure OpenAI is a highly sought-after service where customer demand might exceed service GPU capacity. Microsoft strives to provide capacity for all in-demand regions and models, but selling out a region is always a possibility. This constraint can limit some customers' ability to create a deployment of their desired model, version, or number of PTUs in a desired region - even if they have quota available in that region. Generally speaking:
 
@@ -121,36 +72,28 @@ Azure OpenAI is a highly sought-after service where customer demand might exceed
 
 To find the capacity needed for their deployments, use the capacity API or the Azure AI Foundry deployment experience to provide real-time information on capacity availability.
 
-In Azure AI Foundry, the deployment experience identifies when a region lacks the capacity needed to deploy the model. This looks at the desired model, version and number of PTUs. If capacity is unavailable, the experience directs  users to a select an alternative region.
+In Azure AI Foundry, the deployment experience identifies when a region lacks the capacity needed to deploy the model. This looks at the desired model, version and number of PTUs. If capacity is unavailable, the experience directs users to a select an alternative region.
 
-Details on the new deployment experience can be found in the Azure OpenAI [Provisioned get started guide](../how-to/provisioned-get-started.md).
+Details on the deployment experience can be found in the Azure OpenAI [Provisioned get started guide](../how-to/provisioned-get-started.md).
 
-The new [model capacities API](/rest/api/aiservices/accountmanagement/model-capacities/list?view=rest-aiservices-accountmanagement-2024-04-01-preview&tabs=HTTP&preserve-view=true) can  be used to programmatically identify the maximum sized deployment of a specified model.  The API considers both your quota and service capacity in the region.
+The [model capacities API](/rest/api/aiservices/accountmanagement/model-capacities/list?view=rest-aiservices-accountmanagement-2024-04-01-preview&tabs=HTTP&preserve-view=true) can be used to programmatically identify the maximum sized deployment of a specified model.  The API considers both your quota and service capacity in the region.
 
 If an acceptable region isn't available to support the desire model, version and/or PTUs, customers can also try the following steps:
 
 - Attempt the deployment with a smaller number of PTUs.
 - Attempt the deployment at a different time. Capacity availability changes dynamically based on customer demand and more capacity might become available later.
 - Ensure that quota is available in all acceptable regions. The [model capacities API](/rest/api/aiservices/accountmanagement/model-capacities/list?view=rest-aiservices-accountmanagement-2024-04-01-preview&tabs=HTTP&preserve-view=true) and Azure AI Foundry experience consider quota availability in returning alternative regions for creating a deployment.
 
-### Determining the number of PTUs needed for a workload
-
-PTUs represent an amount of model processing capacity. Similar to your computer or databases, different workloads or requests to the model will consume different amounts of underlying processing capacity. The conversion from throughput needs to PTUs can be approximated using historical token usage data or call shape estimations (input tokens, output tokens, and requests per minute) as outlined in our [performance and latency](../how-to/latency.md) documentation. To simplify this process, you can use the [Azure OpenAI Capacity calculator](https://ai.azure.com/resource/calculator) to size specific workload shapes.
+### How can I monitor capacity?
 
-A few high-level considerations:
-- Generations require more capacity than prompts
-- For GPT-4o and later models, the TPM per PTU is set for input and output tokens separately. For older models, larger calls are progressively more expensive to compute. For example, 100 calls of with a 1000 token prompt size requires less capacity than one call with 100,000 tokens in the prompt. This tiering means that the distribution of these call shapes is important in overall throughput. Traffic patterns with a wide distribution that includes some large calls might experience lower throughput per PTU than a narrower distribution with the same average prompt & completion token sizes.
+The [Provisioned-Managed Utilization V2 metric](../how-to/monitoring.md#azure-openai-metrics) in Azure Monitor measures a given deployments utilization on 1-minute increments. All provisioned deployment types are optimized to ensure that accepted calls are processed with a consistent model processing time (actual end-to-end latency is dependent on a call's characteristics).  
 
 ### How utilization performance works
 
 Provisioned deployments provide you with an allocated amount of model processing capacity to run a given model.
 
 In all provisioned deployment types, when capacity is exceeded, the API will return a 429 HTTP Status Error. This fast response enables the user to make decisions on how to manage their traffic. Users can redirect requests to a separate deployment, to a standard pay-as-you-go instance, or use a retry strategy to manage a given request. The service continues to return the 429 HTTP status code until the utilization drops below 100%.
 
-### How can I monitor capacity?
-
-The [Provisioned-Managed Utilization V2 metric](../how-to/monitoring.md#azure-openai-metrics) in Azure Monitor measures a given deployments utilization on 1-minute increments. All provisioned deployment types are optimized to ensure that accepted calls are processed with a consistent model processing time (actual end-to-end latency is dependent on a call's characteristics).  
-
 #### What should  I do when I receive a 429 response?
 The 429 response isn't an error, but instead part of the design for telling users that a given deployment is fully utilized at a point in time. By providing a fast-fail response, you have control over how to handle these situations in a way that best fits your application requirements.
 
@@ -169,7 +112,7 @@ For provisioned deployments, we use a variation of the leaky bucket algorithm to
 
     a.    When the current utilization is above 100%, the service returns a 429 code with the `retry-after-ms` header set to the time until utilization is below 100%
    
-    b.    Otherwise, the service estimates the incremental change to utilization required to serve the request by combining the prompt tokens, less any cached tokens, and the specified `max_tokens` in the call. A customer can receive up to a 100% discount on their prompt tokens depending on the size of their cached tokens. If the `max_tokens` parameter is not specified, the service estimates a value. This estimation can lead to lower concurrency than expected when the number of actual generated tokens is small.  For highest concurrency, ensure that the `max_tokens` value is as close as possible to the true generation size.
+    b.    Otherwise, the service estimates the incremental change to utilization required to serve the request by combining the prompt tokens, less any cached tokens, and the specified `max_tokens` in the call. A customer can receive up to a 100% discount on their prompt tokens depending on the size of their cached tokens. If the `max_tokens` parameter is not specified, the service estimates a value. This estimation can lead to lower concurrency than expected when the number of actual generated tokens is small. For highest concurrency, ensure that the `max_tokens` value is as close as possible to the true generation size.
    
 1. When a request finishes, we now know the actual compute cost for the call. To ensure an accurate accounting, we correct the utilization using the following logic:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise content in provisioned-throughput.md"
}
```

### Explanation
The file `provisioned-throughput.md` underwent significant modifications, amounting to a total of 105 changes, including 24 additions and 81 deletions. The focus of this update was to improve the clarity and relevance of the documentation regarding the Azure OpenAI Service's provisioned throughput. 

Key updates include a revised description of the provisioned throughput offering, which now emphasizes the ability to specify required throughput in model deployments while ensuring allocated processing capacity. This update also enhances the documentation's structure and comprehensibility, removing outdated notes and simplifying explanations.

Additionally, new content has been added to provide users with a clearer understanding of the benefits of the provisioned throughput offerings and to streamline how the information is presented. The revisions aim to make the documentation more user-friendly and to provide essential updates without excessive detail, aligning with best practices for technical documentation. Overall, these changes enhance the user's ability to understand and utilize the Azure OpenAI Service effectively.

## articles/ai-services/openai/how-to/assistant-functions.md{#item-a47205}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ services: cognitive-services
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 09/04/2024
+ms.date: 03/31/2025
 author: aahill
 ms.author: aahi
 recommendations: false
@@ -17,8 +17,6 @@ recommendations: false
 
 The Assistants API supports function calling, which allows you to describe the structure of functions to an Assistant and then return the functions that need to be called along with their arguments.
 
-[!INCLUDE [Assistants v2 note](../includes/assistants-v2-note.md)]
-
 ## Function calling support
 
 ### Supported models
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update assistant-functions.md for date and content"
}
```

### Explanation
The file `assistant-functions.md` has been updated with a minor modification involving a date change and content adjustment. The change includes the update of the `ms.date` from September 4, 2024, to March 31, 2025, indicating a fresh timestamp for the document. Additionally, three lines of content have been removed, specifically the inclusion of a note for the Assistants v2, simplifying the structure of the document.

The remaining content focuses on the functionality of the Assistants API, specifically its support for function calling. This update aims to enhance the clarity and currentness of the documentation while ensuring users receive the most relevant information about utilizing the Assistants API effectively. Overall, the modifications contribute toward maintaining an updated and concise technical resource.

## articles/ai-services/openai/how-to/assistant.md{#item-b12c67}

<details>
<summary>Diff</summary>
````diff
@@ -18,8 +18,6 @@ recommendations: false
 
 Azure OpenAI Assistants (Preview) allows you to create AI assistants tailored to your needs through custom instructions and augmented by advanced tools like code interpreter, and custom functions. In this article, we provide an in-depth walkthrough of getting started with the Assistants API.
 
-[!INCLUDE [Assistants v2 note](../includes/assistants-v2-note.md)]
-
 ## Assistants support
 
 ### Region, model, and API support
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revise content in assistant.md"
}
```

### Explanation
The modifications made to the file `assistant.md` are minor, involving the removal of two lines of content. Specifically, the update eliminates the inclusion of a note regarding the Assistants v2, streamlining the documentation. 

The remaining content continues to emphasize the capabilities of Azure OpenAI Assistants (Preview) in allowing users to create tailored AI assistants through custom instructions and additional tools. The article aims to provide a comprehensive overview of how to get started with the Assistants API, ensuring clarity and focus for users interested in utilizing these features. Overall, the changes contribute to a more concise document while retaining essential information for users.

## articles/ai-services/openai/how-to/code-interpreter.md{#item-95efbd}

<details>
<summary>Diff</summary>
````diff
@@ -20,8 +20,6 @@ Code Interpreter allows the Assistants API to write and run Python code in a san
 > [!IMPORTANT]
 > Code Interpreter has [additional charges](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) beyond the token based fees for Azure OpenAI usage. If your Assistant calls Code Interpreter simultaneously in two different threads, two code interpreter sessions are created. Each session is active by default for one hour.
 
-[!INCLUDE [Assistants v2 note](../includes/assistants-v2-note.md)]
-
 ## Code interpreter support
 
 ### Supported models
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update content in code-interpreter.md"
}
```

### Explanation
The file `code-interpreter.md` has undergone a minor update, which includes the removal of two lines of content. Specifically, the documentation has eliminated the note regarding the Assistants v2, which previously provided additional context about this topic.

The remaining content continues to describe the functionality of the Code Interpreter within the Assistants API, which enables the execution of Python code in a secure environment. Additionally, important information about potential charges associated with the Code Interpreter usage is retained, emphasizing the implications of running sessions and concurrency. Overall, the revisions serve to streamline the document while maintaining critical information for users interested in utilizing the Code Interpreter feature effectively.

## articles/ai-services/openai/how-to/file-search.md{#item-f9d6d7}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ services: cognitive-services
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: how-to
-ms.date: 01/28/2025
+ms.date: 03/31/2025
 author: aahill
 ms.author: aahi
 recommendations: false
@@ -19,9 +19,6 @@ File Search augments the Assistant with knowledge from outside its model, such a
 > [!IMPORTANT]
 > * File search has [additional charges](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) beyond the token based fees for Azure OpenAI usage.
 
-
-[!INCLUDE [Assistants v2 note](../includes/assistants-v2-note.md)]
-
 ## File search support
 
 ### Supported regions
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and content in file-search.md"
}
```

### Explanation
The `file-search.md` document has undergone a minor update, which includes a change in the date from January 28, 2025, to March 31, 2025, along with the removal of four lines of content. Notably, the update has eliminated the note regarding the Assistants v2, streamlining the documentation further.

The content still effectively conveys the functionality of the File Search feature, which enhances the Assistant's capability by incorporating external knowledge sources. Additionally, the essential warning about potential extra charges for using the File Search functionality remains intact, emphasizing the importance of understanding the costs associated with Azure OpenAI services. Overall, the modifications aim to ensure that the document is up-to-date while retaining critical information for users looking to utilize the File Search feature.

## articles/ai-services/openai/how-to/fine-tuning-deploy.md{#item-286d57}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ manager: nitinme
 ms.service: azure-ai-openai
 ms.custom: build-2023, build-2023-dataai, devx-track-python, references_regions
 ms.topic: how-to
-ms.date: 02/24/2025
+ms.date: 03/31/2025
 author: mrbullwinkle
 ms.author: mbullwin
 ---
@@ -389,7 +389,7 @@ Global Standard fine-tuned deployments currently support structured outputs only
 - `gpt-4o-mini-2024-07-18`
 - `gpt-4o-2024-08-06`
 
-[Provisioned managed](./deployment-types.md#provisioned) fine-tuned deployments offer [predictable performance](../concepts/provisioned-throughput.md#what-do-the-provisioned-deployment-types-provide) for fine-tuned deployments. As part of public preview, provisioned managed deployments may be created regionally via the data-plane [REST API](../reference.md#data-plane-inference) version `2024-10-01` or newer. See below for examples.
+[Provisioned managed](./deployment-types.md#provisioned) fine-tuned deployments offer [predictable performance](../concepts/provisioned-throughput.md) for fine-tuned deployments. As part of public preview, provisioned managed deployments may be created regionally via the data-plane [REST API](../reference.md#data-plane-inference) version `2024-10-01` or newer. See below for examples.
 
 Provisioned Managed fine-tuned deployments currently support structured outputs only on GPT-4o.
 
@@ -423,7 +423,7 @@ curl -X PUT "https://management.azure.com/subscriptions/<SUBSCRIPTION>/resourceG
 
 #### Scaling a fine-tuned model on Provisioned Managed
 
-To scale a fine-tuned provision managed deployment to increase or decrease PTU capacity, perform the same `PUT` REST API call as you did when [creating the deployment](#creating-a-provisioned-managed-deployment) and provide an updated `capacity` value for the `sku`. Keep in mind, provisioned deployments must scale in [minimum increments](../concepts/provisioned-throughput.md#how-much-throughput-per-ptu-you-get-for-each-model).
+To scale a fine-tuned provision managed deployment to increase or decrease PTU capacity, perform the same `PUT` REST API call as you did when [creating the deployment](#creating-a-provisioned-managed-deployment) and provide an updated `capacity` value for the `sku`. Keep in mind, provisioned deployments must scale in [minimum increments](../how-to/provisioned-throughput-onboarding.md#how-much-throughput-per-ptu-you-get-for-each-model).
 
 For example, to scale the model deployed in the previous section from 25 to 40 PTU, make another `PUT` call and increase the capacity:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and content in fine-tuning-deploy.md"
}
```

### Explanation
The `fine-tuning-deploy.md` document has been updated with a few minor changes, which include an updated date from February 24, 2025, to March 31, 2025, alongside several content modifications that involve replacing or refining specific phrases or links.

Specifically, the documentation clarifies information regarding provisioned managed fine-tuned deployments, ensuring that users have access to accurate and timely details. The links to related documents have been verified and updated where necessary, enhancing the reliability of references. Overall, the changes strive to keep the document current while improving the clarity and navigation for users looking to understand and utilize fine-tuning and deployment features effectively.

## articles/ai-services/openai/how-to/on-your-data-configuration.md{#item-4875d3}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ ms.service: azure-ai-openai
 ms.topic: how-to
 author: aahill
 ms.author: aahi
-ms.date: 12/03/2024
+ms.date: 03/31/2025
 recommendations: false
 ---
 
@@ -16,7 +16,7 @@ recommendations: false
 > [!NOTE]
 > As of June 2024, the application form for the Microsoft managed private endpoint to Azure AI Search is no longer needed.
 >
-> The managed private endpoint will be deleted from the Microsoft managed virtual network at July 2025. If you have already provisioned a managed private endpoint through the application process before June 2024, enable [Azure AI Search trusted service](#enable-trusted-service-1) as early as possible to avoid service disruption. 
+> The managed private endpoint will be deleted from the Microsoft managed virtual network in July 2025. If you have already provisioned a managed private endpoint through the application process before June 2024, enable [Azure AI Search trusted service](#enable-trusted-service-1) as early as possible to avoid service disruption. 
 
 Use this article to learn how to configure networking and access when using Azure OpenAI On Your Data with Microsoft Entra ID role-based access control, virtual networks, and private endpoints.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and content in on-your-data-configuration.md"
}
```

### Explanation
The `on-your-data-configuration.md` file has received a minor update that includes a change in the document date from December 3, 2024, to March 31, 2025. In addition to the date adjustment, there was also a modification to the phrasing regarding the deletion of the managed private endpoint from the Microsoft managed virtual network.

The note clarifying the timeline for the removal of the managed private endpoint has been made slightly clearer by rephrasing the text for better readability. The document continues to provide essential guidance on configuring networking and access for Azure OpenAI services, focusing on Microsoft Entra ID role-based access control, virtual networks, and private endpoints. These updates ensure that users have access to accurate and up-to-date information regarding their configurations and the associated implications for service continuity.

## articles/ai-services/openai/how-to/provisioned-get-started.md{#item-c8df1c}

<details>
<summary>Diff</summary>
````diff
@@ -6,13 +6,13 @@ manager: nitinme
 ms.service: azure-ai-openai
 ms.custom: openai
 ms.topic: how-to
-author: mrbullwinkle
-ms.author: mbullwin
-ms.date: 03/26/2025
+author: aahill
+ms.author: aahi
+ms.date: 03/31/2025
 recommendations: false
 ---
 
-# Get started using Provisioned Deployments on the Azure OpenAI Service
+# Get started using provisioned deployments on the Azure OpenAI Service
 
 The following guide walks you through key steps in creating a provisioned deployment with your Azure OpenAI Service resource. For more details on the concepts discussed here, see:
 * [Azure OpenAI Provisioned Onboarding Guide](./provisioned-throughput-onboarding.md)
@@ -51,7 +51,7 @@ Provisioned deployments are created via Azure OpenAI resource objects within Azu
 
 ## Create your provisioned deployment - capacity is available
 
-once you have verified your quota, you can create a deployment. To create a provisioned deployment, you can follow these steps; the choices described reflect the entries shown in the screenshot. 
+Once you have verified your quota, you can create a deployment. To create a provisioned deployment, you can follow these steps; the choices described reflect the entries shown in the screenshot. 
 
 :::image type="content" source="../media/provisioned/deployment-screen.png" alt-text="Screenshot of the Azure AI Foundry portal deployment page for a provisioned deployment." lightbox="../media/provisioned/deployment-screen.png":::
 
@@ -80,7 +80,7 @@ Once you have entered the deployment settings, click **Confirm Pricing** to cont
 If you are unsure of the costs, cancel the deployment and proceed once you understand the payment model and underlying costs for provisioned deployment. This step may prevent unexpected, high charges on your payment invoice. Resources to educate yourself include: 
 
 * [Azure Pricing Portal](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) 
-* [Understanding the provisioned throughput purchase model](provisioned-throughput-onboarding.md#understanding-the-provisioned-throughput-purchase-model) 
+* [Understanding the provisioned throughput costs](provisioned-throughput-onboarding.md) 
 
 The image below shows the pricing confirmation you will see. The price shown is an example only. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update author, date, and content in provisioned-get-started.md"
}
```

### Explanation
The `provisioned-get-started.md` document has undergone a minor update which includes changes to the author attribution, an updated date from March 26, 2025, to March 31, 2025, and modifications to the content for clarity and consistency.

Key alterations include the adjustment of the document title to use lowercase for "provisioned deployments," which aligns with standard formatting practices. Additionally, numerous minor edits have been made throughout the text, improving the overall phrasing and ensuring that the instructions are clear and easy to follow. Specifically, there are adjustments in the language used to describe costs associated with the provisioned throughput purchase model, now referred to as "Understanding the provisioned throughput costs" instead of "Understanding the provisioned throughput purchase model," which enhances clarity for users.

These updates aim to enhance the usability of the document for users seeking guidance on getting started with provisioned deployments on the Azure OpenAI Service.

## articles/ai-services/openai/how-to/provisioned-throughput-onboarding.md{#item-3eb72b}

<details>
<summary>Diff</summary>
````diff
@@ -1,46 +1,126 @@
 ---
-title: Azure OpenAI Service Provisioned Throughput Units (PTU) onboarding
-description: Learn about provisioned throughput units onboarding and Azure OpenAI. 
+title:  Understanding costs associated with provisioned throughput units (PTU)
+description: Learn about provisioned throughput costs and billing in Azure OpenAI. 
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 03/27/2025
+ms.date: 03/31/2025
 manager: nitinme
-author: mrbullwinkle 
-ms.author: mbullwin 
+author: aahill 
+ms.author: aahi 
 recommendations: false
 ---
 
-# Provisioned throughput units onboarding
+# Understanding costs associated with provisioned throughput units (PTU)
 
-This article walks you through the process of onboarding to [Provisioned Throughput Units (PTU)](../concepts/provisioned-throughput.md). Once you complete the initial onboarding, we recommend referring to the PTU [getting started guide](./provisioned-get-started.md).
+Use this article to learn about calculating and understanding costs associated with PTU. For an overview of the provisioned throughput offering, see [What is provisioned throughput?](../concepts/provisioned-throughput.md). When you're ready to sign up for the provisioned throughput offering, see the [getting started guide](./provisioned-get-started.md).
 
-## When to use provisioned throughput units (PTU)
+> [!NOTE]
+> In function calling and agent use cases, token usage can be variable. You should understand your expected Tokens Per Minute (TPM) usage in detail before migrating workloads to PTU.
+
+## Provisioned throughput units
 
-You should consider switching from standard deployments to provisioned deployments when you have well-defined, predictable throughput and latency requirements. Typically, this occurs when the application is ready for production or has already been deployed in production and there's an understanding of the expected traffic. This allows users to accurately forecast the required capacity and avoid unexpected billing. 
+Provisioned throughput units (PTUs) are generic units of model processing capacity that you can use to size provisioned deployments to achieve the required throughput for processing prompts and generating completions.  Provisioned throughput units are granted to a subscription as quota. Each quota is specific to a region and defines  the maximum number of PTUs that can be assigned to deployments in that subscription and region.
 
-### Typical PTU scenarios
+## Understanding provisioned throughput billing
 
-- An application that is ready for production or in production. 
-- An application that has predictable capacity/usage expectations. 
-- An application has real-time/latency sensitive requirements. 
+Azure OpenAI [Provisioned](../how-to/deployment-types.md#provisioned), [Data Zone Provisioned](../how-to/deployment-types.md#data-zone-provisioned) (also known as regional), and [Global Provisioned](../how-to/deployment-types.md#global-provisioned) are purchased on-demand at an hourly basis based on the number of deployed PTUs, with substantial term discount available via the purchase of [Azure Reservations](#azure-reservations-for-azure-openai-provisioned-deployments).  
+
+The hourly model is useful for short-term deployment needs, such as validating new models or acquiring capacity for a hackathon. However, the discounts provided by the Azure Reservation for Azure OpenAI Provisioned, Data Zone Provisioned, and Global Provisioned are considerable and most customers with consistent long-term usage will find a reserved model to bea better value proposition. 
 
 > [!NOTE]
-> In function calling and agent use cases, token usage can be variable. You should understand your expected Tokens Per Minute (TPM) usage in detail prior to migrating workloads to PTU.
+> Azure OpenAI Provisioned customers onboarded prior to the August self-service update use a purchase model called the Commitment model. These customers can continue to use this older purchase model alongside the Hourly/reservation purchase model. The Commitment model is not available for new customers or [certain new models](../concepts/provisioned-migration.md#supported-models-on-commitment-payment-model) introduced after August 2024. For details on the Commitment purchase model and options for coexistence and migration, see the [Azure OpenAI Provisioned August Update](../concepts/provisioned-migration.md).
 
-## Sizing and estimation: provisioned deployments
 
-Determining the right amount of provisioned throughput, or PTUs, you require for your workload is an essential step to optimizing performance and cost. If you aren't familiar with the different approaches available to estimate system level throughput, review the system level throughput estimation recommendations in our [performance and latency documentation](./latency.md). This section describes how to use Azure OpenAI capacity calculators to estimate the number of PTUs required to support a given workload.
+## Model independent quota
 
-### Estimate provisioned throughput units and cost
+Unlike the Tokens Per Minute (TPM) quota used by other Azure OpenAI offerings, PTUs are model-independent. The PTUs might be used to deploy any supported model/version in the region.
 
-To get a quick estimate for your workload using input and output TPM, leverage the built-in capacity planner in the deployment details section of the deployment dialogue screen. The built-in capacity planner is part of the deployment workflow to help streamline the sizing and allocation of quota to a PTU deployment for a given workload. For more information on how to identify and estimate TPM data, review the recommendations in our [performance and latency documentation](./latency.md). 
+:::image type="content" source="../media/provisioned/model-independent-quota.png" alt-text="Diagram of model independent quota with one pool of PTUs available to multiple Azure OpenAI models." lightbox="../media/provisioned/model-independent-quota.png":::
+
+Quota for provisioned deployments shows up in Azure AI Foundry as the following deployment types: [global provisioned](../how-to/deployment-types.md#global-provisioned), [data zone provisioned](../how-to/deployment-types.md#data-zone-provisioned) and [standard provisioned](../how-to/deployment-types.md#provisioned).
+
+|deployment type  |Quota name  |
+|---------|---------|
+|[provisioned](../how-to/deployment-types.md#provisioned)     |  Provisioned Managed Throughput Unit       |
+|[global provisioned](../how-to/deployment-types.md#global-provisioned)     | Global Provisioned Managed Throughput Unit        |
+|[data zone provisioned](../how-to/deployment-types.md#data-zone-provisioned)    | Data Zone Provisioned Managed Throughput Unit        |
+
+:::image type="content" source="../media/provisioned/ptu-quota-page.png" alt-text="Screenshot of quota UI for Azure OpenAI provisioned." lightbox="../media/provisioned/ptu-quota-page.png":::
+
+
+> [!NOTE]
+> Global provisioned and data zone provisioned deployments are only supported for gpt-4o and gpt-4o-mini models at this time. For more information on model availability, review the [models documentation](../concepts/models.md).
+
+## Hourly usage
+
+Provisioned, Data Zone Provisioned, and Global Provisioned deployments are charged an hourly rate ($/PTU/hr) on the number of PTUs that have been deployed. For example, a 300 PTU deployment will be charged the hourly rate times 300. All Azure OpenAI pricing is available in the Azure Pricing Calculator. 
+
+If a deployment exists for a partial hour, it will receive a prorated charge based on the number of minutes it was deployed during the hour. For example, a deployment that exists for 15 minutes during an hour will receive 1/4th the hourly charge. 
+
+If the deployment size is changed, the costs of the deployment will adjust to match the new number of PTUs.  
+
+:::image type="content" source="../media/provisioned/hourly-billing.png" alt-text="A diagram showing hourly billing." lightbox="../media/provisioned/hourly-billing.png":::
+
+Paying for provisioned, data zoned provisioned, and global provisioned deployments on an hourly basis is ideal for short-term deployment scenarios. For example: Quality and performance benchmarking of new models, or temporarily increasing PTU capacity to cover an event such as a hackathon. 
+
+Customers that require long-term usage of provisioned, data zoned provisioned, and global provisioned deployments, however, might pay significantly less per month by purchasing a term discount via [Azure Reservations](#azure-reservations-for-azure-openai-provisioned-deployments) as discussed later in the article. 
+
+> [!IMPORTANT]
+> It's not recommended to scale production deployments according to incoming traffic and pay for them purely on an hourly basis. There are two reasons for this:
+> * The cost savings achieved by purchasing Azure Reservations for Azure OpenAI Provisioned, Data Zone Provisioned, and Global Provisioned are significant, and it will be less expensive in many cases to maintain a deployment sized for full production volume paid for via a reservation than it would be to scale the deployment with incoming traffic.
+> * Having unused provisioned quota (PTUs) doesn't guarantee that capacity will be available to support an increase in the size of the deployment when required. Quota limits the maximum number of PTUs that can be deployed, but it isn't a capacity guarantee. Provisioned capacity for each region and model dynamically changes throughout the day and might not be available when required. As a result, it's recommended to maintain a permanent deployment to cover your traffic needs (paid for via a reservation).
+> Charges for deployments on a deleted resource will continue until the resource is purged. To prevent this, delete a resources deployment before deleting the resource. For more information, see [Recover or purge deleted Azure AI services resources](../../recover-purge-resources.md). 
+
+## How much throughput per PTU you get for each model
+The amount of throughput (measured in tokens per minute or TPM) a deployment gets per PTU is a function of the input and output tokens in a given minute. 
+
+Generating output tokens requires more processing than input tokens. For the models specified in the table below, 1 output token counts as 3 input tokens towards your TPM-per-PTU limit. The service dynamically balances the input & output costs, so users do not have to set specific input and output limits. This approach means your deployment is resilient to fluctuations in the workload.
+
+To help with simplifying the sizing effort, the following table outlines the TPM-per-PTU for the specified models. To understand the impact of output tokens on the TPM-per-PTU limit, use the 3 input token to 1 output token ratio. 
+
+For a detailed understanding of how different ratios of input and output tokens impact the throughput your workload needs, see the [Azure OpenAI capacity calculator](https://ai.azure.com/resource/calculator). The table also shows Service Level Agreement (SLA) Latency Target Values per model. For more information about the SLA for Azure OpenAI Service, see the [Service Level Agreements (SLA) for Online Services page](https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services?lang=1)
+
+
+|Topic| **gpt-4o**   | **gpt-4o-mini**  | **o1**|
+| --- | --- | --- | --- |
+|Global & data zone provisioned minimum deployment|15|15|15|
+|Global & data zone provisioned scale increment|5|5|5|
+|Regional provisioned minimum deployment|50|25|50|
+|Regional provisioned scale increment|50|25|50|
+|Input TPM per PTU |2,500|37,000|230|
+|Latency Target Value |25 Tokens Per Second|33 Tokens Per Second|25 Tokens Per Second|
 
-After filling out the input and output TPM data in the built-in capacity calculator, select the **Calculate** button to view your PTU allocation recommendation. 
+For a full list, see the [Azure OpenAI Service in Azure AI Foundry portal calculator](https://ai.azure.com/resource/calculator).
 
-![Screenshot of deployment workflow PTU capacity calculator.](media/provisioned-throughput-onboarding/deployment-ptu-capacity-calculator.png)
+## Determining the number of PTUs needed for a workload
 
+Determining the right amount of provisioned throughput, or PTUs, you require for your workload is an essential step to optimizing performance and cost. 
 
+PTUs represent an amount of model processing capacity. Similar to your computer or databases, different workloads or requests to the model will consume different amounts of underlying processing capacity. The conversion from throughput needs to PTUs can be approximated using historical token usage data or call shape estimations (input tokens, output tokens, and requests per minute) as outlined in our [performance and latency](../how-to/latency.md) documentation. To simplify this process, you can use the [Azure OpenAI Capacity calculator](https://ai.azure.com/resource/calculator) to size specific workload shapes.
 
+A few high-level considerations:
+- Generations require more capacity than prompts
+- For GPT-4o and later models, the TPM per PTU is set for input and output tokens separately. For older models, larger calls are progressively more expensive to compute. For example, 100 calls of with a 1000 token prompt size requires less capacity than one call with 100,000 tokens in the prompt. This tiering means that the distribution of these call shapes is important in overall throughput. Traffic patterns with a wide distribution that includes some large calls might experience lower throughput per PTU than a narrower distribution with the same average prompt & completion token sizes.
+
+### Obtaining PTU quota
+
+PTU quota is available by default in many regions. If more quota is required, customers can request quota via the Request Quota link. This link can be found to the right of the designated provisioned deployment type quota tabs in Azure AI Foundry The form allows the customer to request an increase in the specified PTU quota for a given region. The customer receives an email at the included address once the request is approved, typically within two business days. 
+
+### Per-Model PTU minimums
+
+The minimum PTU deployment, increments, and processing capacity associated with each unit varies by model type & version. See the above [table](#how-much-throughput-per-ptu-you-get-for-each-model) for more information.
+
+## Estimate provisioned throughput units and cost
+
+To get a quick estimate for your workload using input and output TPM, leverage the built-in capacity planner in the deployment details section of the deployment dialogue screen. The built-in capacity planner is part of the deployment workflow to help streamline the sizing and allocation of quota to a PTU deployment for a given workload. For more information on how to identify and estimate TPM data, review the recommendations in our [performance and latency documentation](./latency.md). 
+
+To use the capacity planner, go to the Azure AI Foundry Portal and select the **Deployments** button. Then select **Deploy model**.
+
+:::image type="content" source="../media/provisioned/deploy-model-button.png" alt-text="A screenshot of the model deployment screen." lightbox="../media/provisioned/deploy-model-button.png":::
+
+Choose a model, and click **Confirm**. Select a provision-managed deployment type. After filling out the input and output TPM data in the built-in capacity calculator, select the **Calculate** button to view your PTU allocation recommendation. 
+
+:::image type="content" source="../media/provisioned/deployment-ptu-capacity-calculator.png" alt-text="A screenshot of deployment workflow PTU capacity calculator." lightbox="../media/provisioned/deployment-ptu-capacity-calculator.png":::
 
 To estimate provisioned capacity using request level data, open the capacity planner in the [Azure AI Foundry](https://ai.azure.com). The capacity calculator is under **Shared resources** > **Model Quota** > **Azure OpenAI Provisioned**.
 
@@ -63,34 +143,6 @@ The values in the output column are the estimated value of PTU units required fo
 > [!NOTE]
 > The capacity calculators provide an estimate based on simple input criteria. The most accurate way to determine your capacity is to benchmark a deployment with a representational workload for your use case.
 
-## Understanding the provisioned throughput purchase model
-
-Azure OpenAI Provisioned, Data Zone Provisioned, and Global Provisioned are purchased on-demand at an hourly basis based on the number of deployed PTUs, with substantial term discount available via the purchase of Azure Reservations.   
-
-The hourly model is useful for short-term deployment needs, such as validating new models or acquiring capacity for a hackathon. However, the discounts provided by the Azure Reservation for Azure OpenAI Provisioned, Data Zone Provisioned, and Global Provisioned are considerable and most customers with consistent long-term usage will find a reserved model to bea better value proposition. 
-
-> [!NOTE]
-> Azure OpenAI Provisioned customers onboarded prior to the August self-service update use a purchase model called the Commitment model.  These customers can continue to use this older purchase model alongside the Hourly/reservation purchase model.  The Commitment model is not available for new customers or new models introduced after August 2024.  For details on the Commitment purchase model and options for coexistence and migration, please see the [Azure OpenAI Provisioned August Update](../concepts/provisioned-migration.md).
-## Hourly usage
-
-Provisioned, Data Zone Provisioned, and Global Provisioned deployments are charged an hourly rate ($/PTU/hr) on the number of PTUs that have been deployed. For example, a 300 PTU deployment will be charged the hourly rate times 300. All Azure OpenAI pricing is available in the Azure Pricing Calculator. 
-
-If a deployment exists for a partial hour, it will receive a prorated charge based on the number of minutes it was deployed during the hour. For example, a deployment that exists for 15 minutes during an hour will receive 1/4th the hourly charge. 
-
-If the deployment size is changed, the costs of the deployment will adjust to match the new number of PTUs.   
-
-:::image type="content" source="../media/provisioned/hourly-billing.png" alt-text="A diagram showing hourly billing." lightbox="../media/provisioned/hourly-billing.png":::
-
-Paying for provisioned, data zoned provisioned, and global provisioned deployments on an hourly basis is ideal for short-term deployment scenarios. For example: Quality and performance benchmarking of new models, or temporarily increasing PTU capacity to cover an event such as a hackathon. 
-
-Customers that require long-term usage of provisioned, data zoned provisioned, and global provisioned deployments, however, might pay significantly less per month by purchasing a term discount via Azure Reservations as discussed in the next section. 
-
-> [!NOTE]
-> It is not recommended to scale production deployments according to incoming traffic and pay for them purely on an hourly basis. There are two reasons for this:
-> * The cost savings achieved by purchasing Azure Reservations for Azure OpenAI Provisioned, Data Zone Provisioned, and Global Provisioned are significant, and it will be less expensive in many cases to maintain a deployment sized for full production volume paid for via a reservation than it would be to scale the deployment with incoming traffic.
-> * Having unused provisioned quota (PTUs) does not guarantee that capacity will be available to support an increase in the size of the deployment when required. Quota limits the maximum number of PTUs that can be deployed, but it is not a capacity guarantee. Provisioned capacity for each region and model dynamically changes throughout the day and might not be available when required. As a result, it is recommended to maintain a permanent deployment to cover your traffic needs (paid for via a reservation).
-> * Charges for deployments on a deleted resource will continue until the resource is purged.  To prevent this, delete a resources deployment before deleting the resource.  For more information, see [Recover or purge deleted Azure AI services resources](../../recover-purge-resources.md). 
-
 ## Azure Reservations for Azure OpenAI provisioned deployments
 
 Discounts on top of the hourly usage price can be obtained by purchasing an Azure Reservation for Azure OpenAI Provisioned, Data Zone Provisioned, and Global Provisioned. An Azure Reservation is a term-discounting mechanism shared by many Azure products. For example, Compute and Cosmos DB. For Azure OpenAI Provisioned, Data Zone Provisioned, and Global Provisioned, the reservation provides a discount in exchange for committing to payment for fixed number of PTUs for a one-month or one-year period. 
@@ -105,26 +157,26 @@ Discounts on top of the hourly usage price can be obtained by purchasing an Azur
 
     * All subscriptions in a billing account 
 
-* New reservations can be purchased to cover the same scope as existing reservations, to allow for discounting of new provisioned deployments.  The scope of existing reservations can also be updated at any time without penalty, for example to cover a new subscription. 
+* New reservations can be purchased to cover the same scope as existing reservations, to allow for discounting of new provisioned deployments. The scope of existing reservations can also be updated at any time without penalty, for example to cover a new subscription. 
 
 * Reservations for Global, Data Zone, and Regional deployments aren't interchangeable. You need to purchase a separate reservation for each deployment type. 
 
-* Reservations can be canceled after purchase, but credits are limited.   
+* Reservations can be canceled after purchase, but credits are limited.  
 
 * If the size of provisioned deployments within the scope of a reservation exceeds the amount of the reservation, the excess is charged at the hourly rate. For example, if deployments amounting to 250 PTUs exist within the scope of a 200 PTU reservation, 50 PTUs will be charged on an hourly basis until the deployment sizes are reduced to 200 PTUs, or a new reservation is created to cover the remaining 50. 
 
 * Reservations guarantee a discounted price for the selected term. They don't reserve capacity on the service or guarantee that it will be available when a deployment is created. It's highly recommended that customers create deployments prior to purchasing a reservation to prevent from over-purchasing a reservation. 
 
 > [!IMPORTANT] 
-> * Capacity availability for model deployments is dynamic and changes frequently across regions and models.  To prevent you from purchasing a reservation for more PTUs than you can use, create deployments first, and then purchase the Azure Reservation to cover the PTUs you have deployed.  This best practice will ensure that you can take full advantage of the reservation discount and prevent you from purchasing a term commitment that you cannot use. 
+> * Capacity availability for model deployments is dynamic and changes frequently across regions and models. To prevent you from purchasing a reservation for more PTUs than you can use, create deployments first, and then purchase the Azure Reservation to cover the PTUs you have deployed. This best practice will ensure that you can take full advantage of the reservation discount and prevent you from purchasing a term commitment that you cannot use. 
 >
-> * The Azure role and tenant policy requirements to purchase a reservation are different than those required to create a deployment or Azure OpenAI resource.  Verify authorization to purchase reservations in advance of needing to do so. See Azure OpenAI [Provisioned reservation documentation](https://aka.ms/oai/docs/ptum-reservations) for more details.
+> * The Azure role and tenant policy requirements to purchase a reservation are different than those required to create a deployment or Azure OpenAI resource. Verify authorization to purchase reservations in advance of needing to do so. See Azure OpenAI [Provisioned reservation documentation](https://aka.ms/oai/docs/ptum-reservations) for more details.
 
 ## Important: sizing Azure OpenAI provisioned reservations
 
-The PTU amounts in reservation purchases are independent of PTUs allocated in quota or used in deployments. It's possible to purchase a reservation for more PTUs than you have in quota, or can deploy for the desired region, model, or version.   Credits for over-purchasing a reservation are limited, and customers must take steps to ensure they maintain their reservation sizes in line with their deployed PTUs.  
+The PTU amounts in reservation purchases are independent of PTUs allocated in quota or used in deployments. It's possible to purchase a reservation for more PTUs than you have in quota, or can deploy for the desired region, model, or version. Credits for over-purchasing a reservation are limited, and customers must take steps to ensure they maintain their reservation sizes in line with their deployed PTUs. 
  
-The best practice is to always purchase a reservation after deployments have been created.  This prevents purchasing a reservation and then finding out that the required capacity isn't available for the desired region or model. 
+The best practice is to always purchase a reservation after deployments have been created. This prevents purchasing a reservation and then finding out that the required capacity isn't available for the desired region or model. 
  
 
 Reservations for Global, Data Zone, and Regional deployments aren't interchangeable. You need to purchase a separate reservation for each deployment type.
@@ -135,7 +187,7 @@ To assist customers with purchasing the correct reservation amounts. The total n
 
 Managing Azure Reservations 
 
-After a reservation is created, it is a best practice monitor it to ensure it is receiving the usage you're expecting.  This can be done via the Azure Reservation Portal or Azure Monitor.  Details on these articles and others can be found here: 
+After a reservation is created, it is a best practice monitor it to ensure it is receiving the usage you're expecting. This can be done via the Azure Reservation Portal or Azure Monitor. Details on these articles and others can be found here: 
 
 * [View Azure reservation utilization](/azure/cost-management-billing/reservations/reservation-utilization) 
 * [View Azure Reservation purchase and refund transactions](/azure/cost-management-billing/reservations/view-purchase-refunds) 
````
</details>

### Summary

```json
{
    "modification_type": "major update",
    "modification_title": "Revise and expand content in provisioned-throughput-onboarding.md"
}
```

### Explanation
The `provisioned-throughput-onboarding.md` file has undergone a major update, significantly revising the content to enhance clarity and comprehensiveness about costs associated with Provisioned Throughput Units (PTUs) for the Azure OpenAI Service. 

The title was changed to "Understanding costs associated with provisioned throughput units (PTU)" to better reflect the revised focus of the document. The introductory section has been restructured to guide users in understanding PTU costs better, with new sections added to explain billing, model-independent quotas, and how to estimate the number of PTUs needed based on various models.

Several new concepts have been introduced:
- A detailed explanation of how to calculate PTU costs based on usage and the hourly billing structure.
- The potential benefits of Azure Reservations for cost savings over standard hourly billing.
- Clear instructions on obtaining PTU quotas and how to properly size reservations based on actual deployments to prevent over-purchasing.

Additionally, there are numerous enhancements, such as detailed tables and illustrations that provide clarity on throughput per PTU per model, and best practices for managing Azure Reservations effectively. This extensive modification aims to provide users with a thorough understanding of cost implications and practical guidance for provisioning their Azure OpenAI deployments effectively.

## articles/ai-services/openai/how-to/working-with-models.md{#item-7ec098}

<details>
<summary>Diff</summary>
````diff
@@ -4,7 +4,7 @@ titleSuffix: Azure OpenAI
 description: Learn about managing model deployment life cycle, updates, & retirement.
 ms.service: azure-ai-openai
 ms.topic: conceptual
-ms.date: 03/26/2025
+ms.date: 03/31/2025
 ms.custom: references_regions, build-2023, build-2023-dataai, devx-track-azurepowershell
 manager: nitinme
 author: mrbullwinkle #ChrisHMSFT
@@ -289,7 +289,7 @@ Provisioned deployments support distinct model management practices. Provisioned
 ### Prerequisites
 - Validate that the target model version or model family is supported for your existing deployment type. Migrations can only occur between provisioned deployments of the same deployment type. For more information on deployment types, review the [deployment type documentation](./deployment-types.md).
 - Validate capacity availability for your target model version or model family prior to attempting a migration. For more information on determining capacity availability, review the [capacity transparency documentation](../concepts/provisioned-throughput.md#capacity-transparency).
-- For multi-deployment migrations, validate that you have sufficient quota to support multiple deployments simultaneously. For more information on how to validate quota for each provisioned deployment type, review the [provisioned quota documentation](../concepts/provisioned-throughput.md#quota).
+- For multi-deployment migrations, validate that you have sufficient quota to support multiple deployments simultaneously. For more information on how to validate quota for each provisioned deployment type, review the [provisioned throughput cost documentation](../how-to/provisioned-throughput-onboarding.md).
 
 ### In-place migrations for provisioned deployments
 In-place migrations allow you to maintain the same provisioned deployment name and size while changing the model version or model family assigned to that deployment. With in-place migrations, Azure OpenAI Service takes care of migrating any existing traffic between model versions or model families throughout the migration over a 20-30 minute window. Throughout the migration window, your provisioned deployment will display an "updating" provisioned state. You can continue to use your provisioned deployment as you normally would. Once the in-place migration is complete, the provisioned state will be updated to "succeeded", indicating that all traffic has been migrated over to the target model version or model family. 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date and documentation references in working-with-models.md"
}
```

### Explanation
The `working-with-models.md` file has received a minor update focusing on two primary changes: the update of the document date and the modification of a reference link for better clarity.

The document date was revised from March 26, 2025, to March 31, 2025. Additionally, the reference for validating quota during multi-deployment migrations was updated. The previous reference directed readers to the "provisioned quota documentation," while the revised link now points to the "provisioned throughput cost documentation." This change aims to ensure that readers are directed to the most relevant and accurate resources regarding cost considerations associated with provisioned throughput.

Overall, these adjustments contribute to maintaining the document's accuracy and usability, particularly in guiding users on managing model deployment lifecycle and associated processes effectively.

## articles/ai-services/openai/includes/assistants-python.md{#item-82d745}

<details>
<summary>Diff</summary>
````diff
@@ -44,8 +44,6 @@ pip install openai
 pip install azure-identity
 ```
 
-[!INCLUDE [Assistants v2 note](./assistants-v2-note.md)]
-
 > [!NOTE]
 > This library is maintained by OpenAI. Refer to the [release history](https://github.com/openai/openai-python/releases) to track the latest updates to the library.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Remove section reference in assistants-python.md"
}
```

### Explanation
The `assistants-python.md` file has undergone a minor update, specifically involving the removal of a note reference that was no longer necessary. The two lines removed included an inclusion for an external note regarding "Assistants v2." This adjustment streamlines the document by eliminating a potentially redundant reference, allowing for a clearer presentation of content.

Additionally, the informative note at the end of the document remains intact, guiding users to track library updates through the release history on GitHub. Overall, this modification aims to simplify the document while ensuring that relevant information about the OpenAI library's maintenance and updates is still clearly communicated.

## articles/ai-services/openai/includes/assistants-v2-note.md{#item-64ae04}

<details>
<summary>Diff</summary>
````diff
@@ -1,13 +0,0 @@
----
-manager: nitinme
-author: aahill
-ms.author: aahi
-ms.service: azure-ai-openai
-ms.topic: include
-ms.date: 04/22/2024
----
-
-> [!NOTE]
-> * [File search](../how-to/file-search.md) can ingest up to 10,000 files per assistant - 500 times more than before. It is fast, supports parallel queries through multi-threaded searches, and features enhanced reranking and query rewriting.
->     * Vector store is a new object in the API. Once a file is added to a vector store, it's automatically parsed, chunked, and embedded, made ready to be searched. Vector stores can be used across assistants and threads, simplifying file management and billing.
-> * We've added support for the `tool_choice` parameter which can be used to force the use of a specific tool (like file search, code interpreter, or a function) in a particular run.
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "Removal of assistants-v2-note.md"
}
```

### Explanation
The `assistants-v2-note.md` file has been completely removed, resulting in the deletion of 13 lines of content. This file previously contained important information regarding the enhancements made in the Assistants v2, such as the increased ingestion capacity for file searches, the introduction of vector stores, and the support for a `tool_choice` parameter for specific tool usage during operations.

The removal of this note represents a significant change as it eliminates direct reference material that users may depend on for understanding the latest features and capabilities of the OpenAI Assistants v2. As a result, users could find themselves without essential information that aids in utilizing the functionalities effectively. This modification may require updates to other related documentation to ensure that users are still provided with comprehensive guidance on the now-removed content.

## articles/ai-services/openai/includes/use-your-data-studio.md{#item-705daf}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.custom:
 ms.topic: quickstart
 author: aahill
 ms.author: aahi
-ms.date: 02/12/2024
+ms.date: 03/31/2025
 recommendations: false
 ---
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update date in use-your-data-studio.md"
}
```

### Explanation
The `use-your-data-studio.md` file has been updated to reflect a minor change involving the update of the date metadata. The previous date noted as "02/12/2024" has been changed to "03/31/2025." This modification indicates a possible extension in the relevance of the content or a shift in the publication timeline for the associated material.

Updating the date is essential for maintaining accurate information in the documentation, as it ensures users can rely on the timeliness of the content provided. Such a change may also prompt a review or reassessment of the content itself to match the new date and ensure that all information is up-to-date and accurately reflects any recent advancements or updates within the context of using the data studio.

## articles/ai-services/openai/media/provisioned/deploy-model-button.png{#item-62cd49}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of deploy-model-button.png"
}
```

### Explanation
A new image file, `deploy-model-button.png`, has been added to the documentation repository under the `articles/ai-services/openai/media/provisioned` directory. This addition doesn't involve any deletions or modifications to existing files; instead, it enhances the visual resources available within the documentation.

The inclusion of this image likely serves to improve the user experience by providing a visual reference for users interacting with the OpenAI services. Such images can clarify instructions or demonstrate features visually, making it easier for users to follow along with setup or usage processes. This represents a step towards enhancing the overall quality and effectiveness of the documentation provided to users.

## articles/ai-services/openai/media/provisioned/deployment-ptu-capacity-calculator.png{#item-aca8ed}

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Renaming of deployment-ptu-capacity-calculator.png"
}
```

### Explanation
The image file `deployment-ptu-capacity-calculator.png` has been renamed, moving from its previous location in the `how-to/media/provisioned-throughput-onboarding` directory to its new location in the `media/provisioned` directory. Despite no changes or deletions to the content of the file itself, the renaming of this file suggests a reorganization of content structure within the repository for better clarity and accessibility.

Such a change can enhance the user experience by making resources easier to find, particularly if the image is now relevant to a broader set of topics or documentation sections. This minor update ensures that the resources are logically organized, which can facilitate navigation for users looking for specific materials related to deploying models and capacity calculation in OpenAI services.

## articles/ai-services/openai/toc.yml{#item-c945af}

<details>
<summary>Diff</summary>
````diff
@@ -237,14 +237,14 @@ items:
         href: ./how-to/dynamic-quota.md
       - name: Monitor Azure OpenAI
         href: ./how-to/monitor-openai.md
-      - name: Onboarding to Provisioned Throughput Units (PTU)
-        href: ./how-to/provisioned-throughput-onboarding.md
-        displayName: PTU, provisioned, provisioned throughput units
       - name: Provisioned throughput units (PTU)
         items:
         - name: What is the Provisioned Managed offering (PTU)?
           href: ./concepts/provisioned-throughput.md
           displayName: PTU, provisioned, provisioned throughput units
+        - name: Understanding and calculating PTU costs 
+          href: ./how-to/provisioned-throughput-onboarding.md
+          displayName: PTU, provisioned, provisioned throughput units
         - name: Get started with Provisioned Deployments
           href: ./how-to/provisioned-get-started.md
           displayName: PTU, provisioned, provisioned throughput units
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to the Table of Contents for OpenAI"
}
```

### Explanation
The `toc.yml` file for OpenAI services has been modified with specific updates that involve the addition and removal of entries in the table of contents. In this update, three entries were added while three corresponding entries were removed, resulting in a total of six changes. 

Notably, the link related to "Onboarding to Provisioned Throughput Units (PTU)" has been replaced with a new entry titled "Understanding and calculating PTU costs." This change may suggest a shift in focus to a potentially more relevant or helpful resource for users as they navigate the documentation regarding Provisioned Throughput Units.

These modifications are part of an ongoing effort to keep the documentation organized and ensure that users can easily access the most pertinent and useful information as they work with OpenAI services. The updates enhance the clarity and relevance of the topics presented, which is essential for effective user guidance.


