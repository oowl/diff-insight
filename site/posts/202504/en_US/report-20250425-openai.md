---
date: '2025-04-25'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:57b3ac1...MicrosoftDocs:869fffd
summary: "The recent updates to the Azure OpenAI Service documentation feature significant\
  \ enhancements, including the introduction of the GPT-image-1 model, which offers\
  \ improved capabilities over previous models such as DALL-E. The documentation has\
  \ been revamped to broaden model references, standardize terminology, and include\
  \ new request parameters specific to the GPT-image-1 model, thus improving user\
  \ control over image outputs. \n\nKey changes also involve a comprehensive update\
  \ to the DALL-E documentation, impacting users who are accustomed to that specific\
  \ terminology. Additionally, there are updates on model retirement dates and revisions\
  \ to safety policies, content filtering, and capabilities descriptions. The consistent\
  \ use of inclusive terms like \"image generation models\" reflects a strategic shift\
  \ to embrace a wider range of AI technologies.\n\nOverall, these documentation changes\
  \ signal Microsoft's commitment to innovation and inclusivity in AI, equipping users\
  \ with essential information to fully leverage the evolving capabilities of Azure\
  \ OpenAI."
title: '[en_US] Diff Insight Report - openai'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:57b3ac1...MicrosoftDocs:869fffd){target="_blank"}

# Highlights
The recent documentation updates reflect a comprehensive enhancement of the Azure OpenAI Service's content, focusing primarily on the broadening of model references, terminology standardization, and the introduction of new models. The changes ensure that the documentation is current, consistent, and inclusive of the latest offerings, notably the introduction of the GPT-image-1 model. Key updates also include enhancements to content credentials, content filtering, model retirements, and instructions on prompt transformation, aligning with the shift toward more inclusive references beyond the DALL-E series.

## New features
- Introduction of the GPT-image-1 model, currently available in limited preview, showcasing advancements over previous models such as DALL-E.
- New URI and request parameters specific to the GPT-image-1 model, enhancing user control over image output.
- Detailed example sections and additional new sections for image edits.

## Breaking changes
- Significant overhaul of the DALL-E documentation to include broader image generation references, potentially impacting existing API usage for users exclusively familiar with DALL-E terminology.

## Other updates
- Updates to model retirement dates for better planning and understanding of model lifecycle.
- Consistent terminology changes from "DALL-E models" to more inclusive "image generation models."
- Clarification of safety policies, content filtering, model capabilities, and default prompt transformation across documents.
- Enhanced API documentation and updated example code to include recent advancements and new models.
- Terminology updates throughout language-specific documentation for a unified reading experience.

# Insights
The revised documentation for Azure OpenAI Service signals a robust alignment with the evolving landscape of AI models, reflecting Microsoft's commitment to innovation and inclusivity in its offerings. The introduction of the GPT-image-1 model marks a pivotal expansion in AI capabilities, providing more refined image generation features, such as enhanced accuracy and rendering capacities. 

This move away from DALL-E-specific references to a more inclusive "image generation models" terminology indicates a strategic intent to highlight the variety and sophistication of image-based AI technologies under Azure's umbrella. By implementing these changes, Microsoft aims to prepare developers and users for the broader scope of AI functionalities enabled by Azure OpenAI.

Additionally, the updates reflect a concerted effort to reinforce clarity and usability. By ensuring that documentation accurately represents the latest models and adjusting regional availability information, Microsoft equips its users with precise, actionable insights necessary to optimize their use of Azure OpenAI's capabilities. 

Overall, these updates are not just minor clarifications or format improvements; they represent a strategic realignment towards a future where Azure OpenAI capabilities are integral to a wide array of applications and industries leveraging cutting-edge AI innovation.

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [content-credentials.md](#item-a23b50) | minor update | Update to Content Credentials Information in Documentation | modified | 8 | 8 | 16 | 
| [content-filter.md](#item-dfc7e7) | minor update | Update to Content Filtering System Description | modified | 1 | 1 | 2 | 
| [default-safety-policies.md](#item-39b6a0) | minor update | Update to Default Safety Policies for Image Generation Models | modified | 1 | 1 | 2 | 
| [model-retirements.md](#item-03fc2e) | minor update | Updates to Model Retirement Dates and Deprecated Models | modified | 8 | 8 | 16 | 
| [models.md](#item-db2c37) | minor update | Changes to Model Capabilities and Naming Conventions | modified | 21 | 5 | 26 | 
| [prompt-transformation.md](#item-21e047) | minor update | Updates on Prompt Transformation for Image Generation Models | modified | 5 | 5 | 10 | 
| [faq.yml](#item-6deb71) | minor update | Update to Azure OpenAI Capabilities Comparison | modified | 1 | 1 | 2 | 
| [content-filters.md](#item-6f0627) | minor update | Update to Content Filtering Description | modified | 1 | 1 | 2 | 
| [dall-e.md](#item-ac9616) | breaking change | Revamped DALL-E Documentation and Model Reference | modified | 197 | 23 | 220 | 
| [evaluations.md](#item-dfaa1c) | minor update | Formatting Enhancement in Evaluations Documentation | modified | 8 | 6 | 14 | 
| [reasoning.md](#item-a54b2f) | minor update | Clarifications and Formatting Adjustments in Reasoning Documentation | modified | 9 | 7 | 16 | 
| [responses.md](#item-b9757d) | minor update | Addition of New Model in Responses Documentation | modified | 1 | 0 | 1 | 
| [latest-inference-preview.md](#item-24bf0f) | new feature | Enhancements to Inference Preview Documentation for New Models and Features | modified | 150 | 9 | 159 | 
| [dall-e-dotnet.md](#item-755f0a) | minor update | Terminology Updates in DALL-E Documentation | modified | 2 | 2 | 4 | 
| [dall-e-go.md](#item-132707) | minor update | Terminology Updates in DALL-E Go Documentation | modified | 2 | 2 | 4 | 
| [dall-e-java.md](#item-373f89) | minor update | Terminology Updates in DALL-E Java Documentation | modified | 2 | 2 | 4 | 
| [dall-e-javascript.md](#item-6cffcf) | minor update | Terminology Updates in DALL-E JavaScript Documentation | modified | 2 | 2 | 4 | 
| [dall-e-powershell.md](#item-97878b) | minor update | Terminology Updates in DALL-E PowerShell Documentation | modified | 2 | 2 | 4 | 
| [dall-e-python.md](#item-c91824) | minor update | Terminology Updates in DALL-E Python Documentation | modified | 2 | 2 | 4 | 
| [dall-e-rest.md](#item-4bac64) | minor update | Terminology Updates in DALL-E REST API Documentation | modified | 2 | 2 | 4 | 
| [dall-e-studio.md](#item-439729) | minor update | Terminology Updates in DALL-E Studio Documentation | modified | 2 | 2 | 4 | 
| [dall-e-typescript.md](#item-57b205) | minor update | Terminology Updates in DALL-E TypeScript Documentation | modified | 2 | 2 | 4 | 
| [dotnet.md](#item-46e881) | minor update | Terminology Change in .NET Documentation | modified | 1 | 1 | 2 | 
| [global-batch.md](#item-929e6a) | minor update | Removal of Region Entries in Global Batch Documentation | modified | 0 | 2 | 2 | 
| [structured-outputs-python.md](#item-2734f0) | minor update | Addition of Import Statements in Python Documentation | modified | 2 | 0 | 2 | 
| [index.yml](#item-0adb87) | minor update | Updates to Azure OpenAI Documentation Summary and Descriptions | modified | 3 | 3 | 6 | 
| [overview.md](#item-97d507) | minor update | Clarification of Model Names and Capabilities in Overview | modified | 2 | 2 | 4 | 
| [quotas-limits.md](#item-06c6f9) | minor update | Updates to Quotas and Limits for Azure OpenAI Service | modified | 2 | 1 | 3 | 
| [toc.yml](#item-c945af) | minor update | Update in Table of Contents for Image Generation Reference | modified | 2 | 2 | 4 | 
| [whats-new.md](#item-53303b) | new feature | Introduction of GPT-image-1 and Updates on Model Releases | modified | 11 | 0 | 11 | 


# Modified Contents
## articles/ai-services/openai/concepts/content-credentials.md{#item-a23b50}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: PatrickFarley
 ms.author: pafarley
 ms.service: azure-ai-openai
 ms.topic: conceptual 
-ms.date: 02/20/2025
+ms.date: 04/23/2025
 manager: nitinme
 ---
 
@@ -16,14 +16,14 @@ With the improved quality of content from generative AI models, there is an incr
 
 ## What are Content Credentials? 
 
-Content Credentials in the Azure OpenAI Service provide customers with information about the origin of an image generated by the DALL-E series models. This information is represented by a manifest attached to the image. The manifest is cryptographically signed by a certificate that traces back to Azure OpenAI Service.
+Content Credentials in the Azure OpenAI Service provide customers with information about the origin of an image generated by the image generation models. This information is represented by a manifest attached to the image. The manifest is cryptographically signed by a certificate that traces back to Azure OpenAI Service.
 
 The manifest contains several key pieces of information: 
 
 | Field name | Field content |
 | ---| ---|
-| `"description"` | This field has a value of `"AI Generated Image"` for all DALL-E model generated images, attesting to the AI-generated nature of the image. |
-| `"softwareAgent"` | This field has a value of `"Azure OpenAI DALL-E"` for all images generated by DALL-E series models in Azure OpenAI Service. |
+| `"description"` | This field has a value of `"AI Generated Image"` for all generated images, attesting to the AI-generated nature of the image. |
+| `"softwareAgent"` | This field has a value of `"Azure OpenAI DALL-E"` or `"Azure OpenAI ImageGen"` for all images generated by DALL-E series models or GPT-image-1 models in Azure OpenAI Service. |
 |`"when"` |The timestamp of when the Content Credentials were created. | 
 
 
@@ -32,12 +32,12 @@ Content Credentials in the Azure OpenAI Service can help people understand when
 ## How do I use Content Credentials in my solution today?
 
 Customers may use Content Credentials by:
-- Ensuring that their AI generated images contain Content Credentials
-    No additional set-up is necessary. Content Credentials are automatically applied to all generated images from DALL·E in the Azure OpenAI Service. 
+- Ensuring that their AI-generated images contain Content Credentials
+    No additional set-up is necessary. Content Credentials are automatically applied to all generated images from DALL·E and GPT-image-1 in the Azure OpenAI Service. 
 - Verifying that an image has Content Credentials
-    There are two recommended ways today to check the credential of an image generated by Azure OpenAI DALL-E models:
+    There are two recommended ways today to check the credential of an image generated by Azure OpenAI models:
 
-    - **Content Credentials Verify webpage (contentcredentials.org/verify)**: This is a tool that allows users to inspect the Content Credentials of a piece of content. If an image was generated by DALL-E in Azure OpenAI, the tool will display that its Content Credentials were issued by Microsoft Corporation alongside the date and time of issuance.
+    - **Content Credentials Verify webpage (contentcredentials.org/verify)**: This is a tool that allows users to inspect the Content Credentials of a piece of content. If an image was generated by an Azure OpenAI image generation model, the tool will display that its Content Credentials were issued by Microsoft Corporation alongside the date and time of issuance.
        :::image type="content" source="../media/encryption/credential-check.png" alt-text="Screenshot of the content credential verification website.":::
     
        This page shows that an image generated by Azure OpenAI DALL-E has Content Credentials issued by Microsoft.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to Content Credentials Information in Documentation"
}
```

### Explanation
This modification involves updates to the documentation for Content Credentials in the Azure OpenAI Service, which reflects new developments in the technology. Key changes include:

1. **Date Update**: The date in the metadata has been changed from "02/20/2025" to "04/23/2025" to reflect a more current timeline.
  
2. **Clarification of Model References**: The description that formerly specified "DALL-E series models" has been expanded to include "image generation models," clarifying that the Content Credentials apply to a broader set of models, specifically referencing "GPT-image-1" models in addition to DALL-E.

3. **Instruction Adjustments**: Minor updates to usage instructions now emphasize that Content Credentials are automatically applied to all images generated by both DALL-E and GPT-image-1 models, rather than only DALL-E. 

4. **Verification Instructions Update**: The guidance for verifying Content Credentials has been simplified to refer to "Azure OpenAI models" instead of limiting the reference to DALL-E, thereby encompassing all generated images.

These enhancements improve the clarity and accuracy of the documentation, ensuring that users have the most up-to-date trends and tools pertaining to AI-generated content credentials.

## articles/ai-services/openai/concepts/content-filter.md{#item-dfc7e7}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ manager: nitinme
 > [!IMPORTANT]
 > The content filtering system isn't applied to prompts and completions processed by the audio models such as Whisper in Azure OpenAI Service. Learn more about the [Audio models in Azure OpenAI](models.md?tabs=standard-audio#standard-deployment-regional-models-by-endpoint).
 
-Azure OpenAI Service includes a content filtering system that works alongside core models, including DALL-E image generation models. This system works by running both the prompt and completion through an ensemble of classification models designed to detect and prevent the output of harmful content. The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions. Variations in API configurations and application design might affect completions and thus filtering behavior.
+Azure OpenAI Service includes a content filtering system that works alongside core models, including image generation models. This system works by running both the prompt and completion through an ensemble of classification models designed to detect and prevent the output of harmful content. The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions. Variations in API configurations and application design might affect completions and thus filtering behavior.
 
 The text content filtering models for the hate, sexual, violence, and self-harm categories have been specifically trained and tested on the following languages: English, German, Japanese, Spanish, French, Italian, Portuguese, and Chinese. However, the service can work in many other languages, but the quality might vary. In all cases, you should do your own testing to ensure that it works for your application.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to Content Filtering System Description"
}
```

### Explanation
This modification involves a minor update to the content filter documentation within the Azure OpenAI Service articles. The key changes include:

1. **Clarification in Model References**: The text has been adjusted to replace "DALL-E image generation models" with a broader term, "image generation models." This change emphasizes that the content filtering system works with a wider range of image generation models beyond just DALL-E.

2. **Preserved Context**: The primary function and structure of the content filtering system remain unchanged. It continues to outline how prompts and completions are processed through multiple classification models to prevent harmful content from being generated.

These adjustments enhance the clarity of the documentation by ensuring that the description is inclusive of various image generation models used within the service. Additionally, users are reminded to conduct their own testing to validate the effectiveness of the content filtering across different languages and applications.

## articles/ai-services/openai/concepts/default-safety-policies.md{#item-39b6a0}

<details>
<summary>Diff</summary>
````diff
@@ -65,4 +65,4 @@ Text models in the Azure OpenAI Service can take in and generate both text and c
 | Profanity                                         | Prompts                | N/A                 |
 
 
-In addition to the above safety configurations, Azure OpenAI DALL-E also comes with [prompt transformation](./prompt-transformation.md) by default. This transformation occurs on all prompts to enhance the safety of your original prompt, specifically in the risk categories of diversity, deceptive generation of political candidates, depictions of public figures, protected material, and others. 
\ No newline at end of file
+In addition to the above safety configurations, the latest image generation models also come with [prompt transformation](./prompt-transformation.md) by default. This transformation occurs on all prompts to enhance the safety of your original prompt, specifically in the risk categories of diversity, deceptive generation of political candidates, depictions of public figures, protected material, and others. 
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to Default Safety Policies for Image Generation Models"
}
```

### Explanation
This modification includes a minor update to the "default safety policies" documentation for the Azure OpenAI Service. The critical change is:

1. **Broader Model Reference**: The text has been adjusted to replace the specific mention of "Azure OpenAI DALL-E" with "the latest image generation models." This change indicates that the prompt transformation safety measure is applicable to a wider range of image generation models, not just DALL-E, thereby providing a more inclusive scope regarding safety configurations.

The context remains the same, emphasizing that prompt transformation is a routine procedure intended to enhance the safety of prompts across various risk categories. This update clarifies the application of safety policies in a more encompassing manner, ensuring users understand that prompt transformation applies to all relevant image generation models within the Azure OpenAI Service.

## articles/ai-services/openai/concepts/model-retirements.md{#item-03fc2e}

<details>
<summary>Diff</summary>
````diff
@@ -4,7 +4,7 @@ titleSuffix: Azure OpenAI
 description: Learn about the model deprecations and retirements in Azure OpenAI.
 ms.service: azure-ai-openai
 ms.topic: conceptual
-ms.date: 04/17/2025
+ms.date: 04/23/2025
 ms.custom: 
 manager: nitinme
 author: mrbullwinkle
@@ -95,25 +95,24 @@ These models are currently available for use in Azure OpenAI Service.
 | ---- | ---- | ---- | --- |
 | `dall-e-3` | 3 | No earlier than June 30, 2025 | |
 | `gpt-35-turbo-16k`| 0613 | April, 30, 2025 | `gpt-35-turbo` (0125) <br><br> `gpt-4o-mini`|
-| `gpt-35-turbo` | 1106 | No earlier than May 31, 2025 <br><br> Deployments set to [**Auto-update to default**](/azure/ai-services/openai/how-to/working-with-models?tabs=powershell#auto-update-to-default) will be automatically upgraded to version: `0125`, starting on January 21, 2025. | `gpt-35-turbo` (0125) <br><br> `gpt-4o-mini` |
-| `gpt-35-turbo` | 0125 | No earlier than May 31, 2025 | `gpt-4o-mini` |
+| `gpt-35-turbo` | 1106 | July 16, 2025 <br><br> Deployments set to [**Auto-update to default**](/azure/ai-services/openai/how-to/working-with-models?tabs=powershell#auto-update-to-default) will be automatically upgraded to version: `0125`, starting on January 21, 2025. | `gpt-35-turbo` (0125) <br><br> `gpt-4o-mini` |
+| `gpt-35-turbo` | 0125 | July 16, 2025 | `gpt-4o-mini` |
 | `gpt-4`<br>`gpt-4-32k` | 0314 | June 6, 2025 | `gpt-4o` |
 | `gpt-4`<br>`gpt-4-32k` | 0613 | June 6, 2025 | `gpt-4o` |
 | `gpt-4` | turbo-2024-04-09 | No earlier than June 6, 2025 | `gpt-4o`|
 | `gpt-4` | 1106-preview | To be upgraded to **`gpt-4o` version: `2024-11-20`**, starting no sooner than April 17, 2025 **<sup>1</sup>** <br>Retirement date: May 1, 2025  | `gpt-4o`|
 | `gpt-4` | 0125-preview |To be upgraded to **`gpt-4o` version: `2024-11-20`**, starting no sooner than April 17, 2025 **<sup>1</sup>** <br>Retirement date: May 1, 2025  | `gpt-4o` |
-| `gpt-4` | vision-preview | To be upgraded to **`gpt-4o` version: `2024-11-20`**, starting no sooner than April 17, 2025  **<sup>1</sup>** <br>Retirement date: May 1, 2025 | `gpt-4o`|
+| `gpt-4` | vision-preview | To be upgraded to **`gpt-4o` version: `2024-11-20`**, starting no sooner than April 17, 2025  **<sup>1</sup>** <br>Retirement date: May 15, 2025 | `gpt-4o`|
 | `gpt-4.5-preview` | 2025-02-27 | July 14, 2025 | `gpt-4.1` |
 | `gpt-4.1` | 2025-04-14 | No earlier than April 11, 2026 | |
 | `gpt-4.1-mini` | 2025-04-14 | No earlier than April 11, 2026 |
 | `gpt-4.1-nano` | 2025-04-14 | No earlier than April 11, 2026 |
 | `gpt-4o` | 2024-05-13 | No earlier than June 30, 2025 <br><br>Deployments set to [**Auto-update to default**](/azure/ai-services/openai/how-to/working-with-models?tabs=powershell#auto-update-to-default) will be automatically upgraded to version: `2024-08-06`, starting on March 17, 2025. | |
 | `gpt-4o` | 2024-08-06 | No earlier than August 6, 2025  | |
-| `gpt-4o` | 2024-11-20 | No earlier than November 20, 2025  | |
-| `gpt-4o-mini` | 2024-07-18 | No earlier than July 18, 2025  | |
-| `gpt-4o-realtime-preview` | 2024-10-01 | **Deprecated:** February 25, 2025<br>**Retirement:** No earlier than March 26, 2025 | `gpt-4o-realtime-preview` (version 2024-12-17) or `gpt-4o-mini-realtime-preview` (version 2024-12-17) |
+| `gpt-4o` | 2024-11-20 | January 30, 2026  | |
+| `gpt-4o-mini` | 2024-07-18 | August 16, 2025  | |
 | `gpt-3.5-turbo-instruct` | 0914 | No earlier than May 31, 2025 |  |
-| `o1-preview` | 2024-09-12 | No earlier than April 2, 2025 | `o1` |
+| `o1-preview` | 2024-09-12 | May 29, 2025 | `o1` |
 | `o1` | 2024-12-17 | No earlier than December 17, 2025 | |
 | `o4-mini` | 2025-04-16 | No earlier than April 11, 2026 | |
 | `o3` | 2025-04-16 | No earlier than April 11, 2026 | |
@@ -149,6 +148,7 @@ If you're an existing customer looking for information about these models, see [
 
 | Model | Deprecation date | Retirement date | Suggested replacement |
 | --------- | --------------------- | ------------------- | -------------------- |
+| `gpt-4o-realtime-preview` - 2024-10-01 | February 25, 2025 | March 26, 2025 | `gpt-4o-realtime-preview` (version 2024-12-17) or `gpt-4o-mini-realtime-preview` (version 2024-12-17) |
 | `gpt-35-turbo` - 0301 | | February 13, 2025   | `gpt-35-turbo` (0125) <br><br> `gpt-4o-mini`  |
 | `gpt-35-turbo` - 0613 | | February 13, 2025 | `gpt-35-turbo` (0125) <br><br> `gpt-4o-mini`  |
 | babbage-002 | | January 27, 2025 |  |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Model Retirement Dates and Deprecated Models"
}
```

### Explanation
This modification involves a minor update to the "model retirements" documentation for the Azure OpenAI Service, encapsulating changes related to model deprecation and retirement dates. Key updates include:

1. **Updated Retirement Dates**: Several model retirement dates have been revised. For instance, the retirement date for `gpt-4` vision-preview was changed from May 1, 2025, to May 15, 2025. Additionally, the `gpt-4o` model now has its retirement date set for January 30, 2026, instead of the previously noted date.

2. **Newly Introduced Models and Updates**: Other models such as `gpt-4o-mini` and `o1-preview` have updated retirement or deprecation dates: the `o1-preview` is now marked for retirement on May 29, 2025.

3. **Clarity in Model Migration Pathways**: The documentation retains clear guidance on which models are recommended as replacements for retired models, allowing users to adjust their integrations accordingly.

These updates ensure that users are informed of the latest timelines regarding model availability and necessary actions for transitioning to new or recommended models as the Azure OpenAI Service evolves.

## articles/ai-services/openai/concepts/models.md{#item-db2c37}

<details>
<summary>Diff</summary>
````diff
@@ -4,7 +4,7 @@ titleSuffix: Azure OpenAI
 description: Learn about the different model capabilities that are available with Azure OpenAI.
 ms.service: azure-ai-openai
 ms.topic: conceptual
-ms.date: 04/16/2025
+ms.date: 04/23/2025
 ms.custom: references_regions, build-2023, build-2023-dataai, refefences_regions
 manager: nitinme
 author: mrbullwinkle #ChrisHMSFT
@@ -26,7 +26,7 @@ Azure OpenAI Service is powered by a diverse set of models with different capabi
 | [GPT-4](#gpt-4) | A set of models that improve on GPT-3.5 and can understand and generate natural language and code. |
 | [GPT-3.5](#gpt-35) | A set of models that improve on GPT-3 and can understand and generate natural language and code. |
 | [Embeddings](#embeddings-models) | A set of models that can convert text into numerical vector form to facilitate text similarity. |
-| [DALL-E](#dall-e-models) | A series of models that can generate original images from natural language. |
+| [Image generation](#image-generation-models) | A series of models that can generate original images from natural language. |
 | [Audio](#audio-models) | A series of models for speech to text, translation, and text to speech. GPT-4o audio models support either low-latency, "speech in, speech out" conversational interactions or audio generation. |
 
 ## GPT 4.1 series
@@ -220,9 +220,24 @@ The third generation embeddings models support reducing the size of the embeddin
 
 OpenAI's MTEB benchmark testing found that even when the third generation model's dimensions are reduced to less than `text-embeddings-ada-002` 1,536 dimensions performance remains slightly better.
 
-## DALL-E
+## Image generation models
 
-The DALL-E models generate images from text prompts that the user provides. DALL-E 3 is generally available for use with the REST APIs. DALL-E 2 and DALL-E 3 with client SDKs are in preview.
+The image generation models generate images from text prompts that the user provides. GPT-image-1 is in limited access public preview. DALL-E 3 is generally available for use with the REST APIs. DALL-E 2 and DALL-E 3 with client SDKs are in preview.
+
+### Availability
+
+**For access to `gpt-image-1` registration is required, and access will be granted based on Microsoft's eligibility criteria**. Customers who have access to other limited access models will still need to request access for this model.
+
+Request access: [`computer-use-preview` limited access model application](https://aka.ms/oai/gptimage1access)
+
+Once access has been granted, you will need to create a deployment for the model.
+
+### Region availability
+
+| Model | Region |
+|---|---|
+|`dall-e-3` | East US<br>Australia East<br>Sweden Central|
+|`gpt-image-1` | West US 2 (Global Standard) <br> UAE North (Global Standard) |
 
 ## Audio models
 
@@ -414,10 +429,11 @@ These models can only be used with Embedding API requests.
 
 [!INCLUDE [Image Generation](../includes/model-matrix/standard-image-generation.md)]
 
-### DALL-E models
+### Image generation models
 
 |  Model ID  | Max Request (characters) |
 |  --- | :---: |
+| gpt-image-1 | 4,000 |
 | dall-e-3  | 4,000 |
 
 # [Audio](#tab/standard-audio)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Changes to Model Capabilities and Naming Conventions"
}
```

### Explanation
This modification includes a minor update to the documentation regarding model capabilities within the Azure OpenAI Service. Key changes consist of:

1. **Date Update**: The document's last updated date has been revised from April 16, 2025, to April 23, 2025.

2. **Renaming of DALL-E Section**: The section previously titled “DALL-E models” has been retitled to “Image generation models.” This change provides a broader context for the types of models included under this category.

3. **Expanded Information on Image Generation**: Additional details have been added to the “Image generation models” section, stating that GPT-image-1 is in limited access public preview, alongside DALL-E 3, which is generally available. DALL-E 2 is also included in the preview category for client SDKs.

4. **Access Requirements**: Clear guidance has been provided regarding access to the `gpt-image-1` model, indicating that registration is required and that access will be granted based on Microsoft’s eligibility criteria. A link has been added for users to request access.

5. **Availability by Region**: A new table presenting the regional availability of the `dall-e-3` and `gpt-image-1` models has been introduced, offering users information about where these models can be deployed.

These enhancements aim to clarify the capabilities and availability of various models, helping users better understand how to interact with and utilize the Azure OpenAI Service's offerings.

## articles/ai-services/openai/concepts/prompt-transformation.md{#item-21e047}

<details>
<summary>Diff</summary>
````diff
@@ -12,12 +12,12 @@ manager: nitinme
 
 # What is prompt transformation?
 
-Prompt transformation is a process included in DALL-E 3 image generation that applies a safety and quality system message to your original prompt. It uses a large language model (LLM) call to add the message before sending your prompt to the model for image generation. This system message enriches your original prompt with the goal of generating more diverse and higher-quality images while maintaining intent. 
+Prompt transformation is a process included in the DALL-E 3 and GPT-image-1 (preview) models that applies a safety and quality system message to your original prompt. It uses a large language model (LLM) call to add the message before sending your prompt to the model for image generation. This system message enriches your original prompt with the goal of generating more diverse and higher-quality images while maintaining intent. 
 
 After prompt transformation is applied to the original prompt, content filtering is applied as a secondary step before image generation; for more information, see [Content filtering](./content-filter.md).
 
 > [!TIP]
-> Learn more about image generation prompting in OpenAI's [DALL·E documentation](https://platform.openai.com/docs/guides/images/language-specific-tips).
+> Learn more about image generation prompting in OpenAI's [Image generation documentation](https://platform.openai.com/docs/guides/images/language-specific-tips).
 
 ## Prompt transformation example
 
@@ -30,11 +30,11 @@ After prompt transformation is applied to the original prompt, content filtering
 
 Prompt transformation is essential for responsible and high-quality generations. Not only does prompt transformation improve the safety of your generated image, but it also enriches your prompt in a more descriptive manner, leading to higher quality and descriptive imagery.
 
-Default prompt transformation in Azure OpenAI DALL-E 3 contains safety enhancements that steer the model away from generating images of Copyright Studio characters and artwork, public figures, and other harmful content such as sexual, hate and unfairness, violence, and self-harm content.
+Default prompt transformation contains safety enhancements that steer the model away from generating images of Copyright Studio characters and artwork, public figures, and other harmful content such as sexual, hate and unfairness, violence, and self-harm content.
 
 ## How do I use prompt transformation?
 
-Prompt transformation is applied by default to all Azure OpenAI DALL-E 3 requests. No extra setup is required to benefit from prompt transformation enhancements.
+Prompt transformation is applied by default to all Azure OpenAI DALL-E 3 and GPT-image-1 requests. No extra setup is required to benefit from prompt transformation enhancements.
 
 Like image generation, prompt transformation is non-deterministic due to the nature of large language models. A single original prompt may lead to many image variants.
 
@@ -69,4 +69,4 @@ Output Content:
 ## Next step
 
 > [!div class="nextstepaction"]
-> [DALL-E quickstart](/azure/ai-services/openai/dall-e-quickstart)
+> [Image generation quickstart](/azure/ai-services/openai/dall-e-quickstart)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates on Prompt Transformation for Image Generation Models"
}
```

### Explanation
This modification involves a minor update to the documentation regarding prompt transformation within the Azure OpenAI Service, particularly for image generation models. Notable changes include:

1. **Model Inclusion**: The description now specifies that prompt transformation applies not only to DALL-E 3 but also to the GPT-image-1 (preview) model. This inclusion reflects the expanded scope of prompt transformation across more image generation capabilities.

2. **Updated References**: The documentation now references "Image generation documentation" in place of "DALL·E documentation," signaling a shift to a broader category that encompasses various image generation models.

3. **Clarification on Safety Enhancements**: The explanation regarding the default prompt transformation clarifies that safety enhancements are included without specifying the DALL-E 3, highlighting that these enhancements are a standard feature across applicable models.

4. **Usage Instructions**: The instruction for using prompt transformation has been updated to mention both DALL-E 3 and GPT-image-1, informing users that no additional setup is necessary for either model to take advantage of prompt transformation.

5. **Next Step Link Update**: The link under "Next step" has been updated from "DALL-E quickstart" to "Image generation quickstart," further emphasizing the expanded focus on different image generation models rather than just DALL-E.

These updates aim to provide users with a clearer understanding of prompt transformation processes, the models to which they apply, and the associated safety measures, thereby enhancing user experience and model interaction within the Azure OpenAI Service.

## articles/ai-services/openai/faq.yml{#item-6deb71}

<details>
<summary>Diff</summary>
````diff
@@ -41,7 +41,7 @@ sections:
       - question: |
           How do the capabilities of Azure OpenAI compare to OpenAI?
         answer: | 
-          Azure OpenAI Service gives customers advanced language AI with OpenAI GPT-3, Codex, and DALL-E models with the security and enterprise promise of Azure. Azure OpenAI codevelops the APIs with OpenAI, ensuring compatibility and a smooth transition from one to the other.
+          Azure OpenAI Service gives customers advanced language AI with the latest OpenAI models with the security and enterprise promise of Azure. Azure OpenAI codevelops the APIs with OpenAI, ensuring compatibility and a smooth transition from one to the other.
           
           With Azure OpenAI, customers get the security capabilities of Microsoft Azure while running the same models as OpenAI. 
       - question: |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to Azure OpenAI Capabilities Comparison"
}
```

### Explanation
This modification consists of a minor update to the FAQ section in the Azure OpenAI documentation. The key change is:

1. **Description Update**: The text describing the capabilities of the Azure OpenAI Service has been altered to refer to "the latest OpenAI models" instead of specifying "OpenAI GPT-3, Codex, and DALL-E models." This adjustment broadens the scope, indicating that the service incorporates a range of the most recent advancements from OpenAI rather than limiting it to specific models.

This minor wording change is intended to enhance clarity and reflect the evolving nature of the models available through the Azure OpenAI Service, emphasizing its commitment to providing customers with access to the latest AI technology along with the security features of the Azure platform.

## articles/ai-services/openai/how-to/content-filters.md{#item-6f0627}

<details>
<summary>Diff</summary>
````diff
@@ -15,7 +15,7 @@ ms.custom: FY25Q1-Linter
 
 # How to configure content filters
 
-The content filtering system integrated into Azure AI Foundry runs alongside the core models, including DALL-E image generation models. It uses an ensemble of multi-class classification models to detect four categories of harmful content (violence, hate, sexual, and self-harm) at four severity levels respectively (safe, low, medium, and high), and optional binary classifiers for detecting jailbreak risk, existing text, and code in public repositories. 
+The content filtering system integrated into Azure AI Foundry runs alongside the core models, including image generation models. It uses an ensemble of multi-class classification models to detect four categories of harmful content (violence, hate, sexual, and self-harm) at four severity levels respectively (safe, low, medium, and high), and optional binary classifiers for detecting jailbreak risk, existing text, and code in public repositories. 
 
 The default content filtering configuration is set to filter at the medium severity threshold for all four content harms categories for both prompts and completions. That means that content that is detected at severity level medium or high is filtered, while content detected at severity level low or safe is not filtered by the content filters. Learn more about content categories, severity levels, and the behavior of the content filtering system [here](../concepts/content-filter.md). 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to Content Filtering Description"
}
```

### Explanation
The modification involves a minor update to the content filtering documentation for Azure AI Foundry. The main change is:

1. **Model Specification Update**: The description of the content filtering system has been altered by removing the specific mention of "DALL-E image generation models" and replacing it with a broader reference to "image generation models." This adjustment reflects an inclusive approach, indicating that the content filtering system supports a variety of image generation models rather than focusing solely on DALL-E.

This update helps clarify that the content filtering functionality is applicable to a wider array of models, thereby enhancing the understanding of its capabilities within the Azure AI Foundry ecosystem. The content filtering system, with its multi-class classification approach, continues to be described as a comprehensive solution for detecting harmful content across predefined categories and severity levels.

## articles/ai-services/openai/how-to/dall-e.md{#item-ac9616}

<details>
<summary>Diff</summary>
````diff
@@ -1,43 +1,84 @@
 ---
-title: How to use DALL-E models 
+title: How to use image generation models 
 titleSuffix: Azure OpenAI Service
-description: Learn how to generate images with the DALL-E models, and learn about the configuration options that are available.
+description: Learn how to generate and edit images with image models, and learn about the configuration options that are available.
 author: PatrickFarley
 ms.author: pafarley 
 ms.service: azure-ai-openai
 ms.custom: 
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 04/23/2025
 manager: nitinme
 keywords: 
 zone_pivot_groups: 
 # Customer intent: as an engineer or hobbyist, I want to know how to use DALL-E image generation models to their full capability.
 ---
 
-# How to use the DALL-E models
+# How to use Azure OpenAI image generation models
 
-OpenAI's DALL-E models generate images based on user-provided text prompts. This guide demonstrates how to use the DALL-E models and configure their options through REST API calls.
+OpenAI's image generation models render images based on user-provided text prompts and optionally provided images. This guide demonstrates how to use the image generation models and configure their options through REST API calls.
 
 
 ## Prerequisites
 
 - An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?icid=ai-services).
 - An Azure OpenAI resource created in a supported region. See [Region availability](/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability).
-- - Deploy a *dall-e-3* model with your Azure OpenAI resource.
+- Deploy a `dall-e-3` or `gpt-image-1` model with your Azure OpenAI resource. For more information on deployments, see [Create a resource and deploy a model with Azure OpenAI](/azure/ai-services/openai/how-to/create-resource).
+    - GPT-image-1 is the newer model and features a number of improvements over DALL-E 3. It's available in limited access: apply for access with [this form](https://aka.ms/oai/gptimage1access).
 
-## Call the Image Generation APIs
+## Call the Image Generation API
 
-The following command shows the most basic way to use DALL-E with code. If this is your first time using these models programmatically, we recommend starting with the [DALL-E quickstart](/azure/ai-services/openai/dall-e-quickstart).
+The following command shows the most basic way to use an image model with code. If this is your first time using these models programmatically, we recommend starting with the [quickstart](/azure/ai-services/openai/dall-e-quickstart).
 
+
+#### [GPT-image-1](#tab/gpt-image-1)
 Send a POST request to:
 
 ```
 https://<your_resource_name>.openai.azure.com/openai/deployments/<your_deployment_name>/images/generations?api-version=<api_version>
 ```
 
-**Replace the following placeholders**:
+
+**URL**:
+
+Replace the following values:
 - `<your_resource_name>` is the name of your Azure OpenAI resource.
-- `<your_deployment_name>` is the name of your DALL-E 3 model deployment.
+- `<your_deployment_name>` is the name of your DALL-E 3 or GPT-image-1 model deployment.
+- `<api_version>` is the version of the API you want to use. For example, `2025-04-01-preview`.
+
+**Required headers**:
+- `Content-Type`: `application/json`
+- `api-key`: `<your_API_key>`
+
+**Body**:
+
+The following is a sample request body. You specify a number of options, defined in later sections.
+
+```json
+{
+    "prompt": "A multi-colored umbrella on the beach, disposable camera",
+    "model": "gpt-image-1",
+    "size": "1024x1024", 
+    "n": 1,
+    "quality": "high"
+}
+```
+
+
+
+#### [DALL-E 3](#tab/dalle-3)
+
+Send a POST request to:
+
+```
+https://<your_resource_name>.openai.azure.com/openai/deployments/<your_deployment_name>/images/generations?api-version=<api_version>
+```
+
+**URL**:
+
+Replace the following values:
+- `<your_resource_name>` is the name of your Azure OpenAI resource.
+- `<your_deployment_name>` is the name of your DALL-E 3 or GPT-image-1 model deployment.
 - `<api_version>` is the version of the API you want to use. For example, `2024-02-01`.
 
 **Required headers**:
@@ -58,9 +99,11 @@ The following is a sample request body. You specify a number of options, defined
 }
 ```
 
-## Output
+---
+
+### Output
 
-The output from a successful image generation API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.
+The response from a successful image generation API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.
 
 ```json
 { 
@@ -104,51 +147,182 @@ It's also possible that the generated image itself is filtered. In this case, th
 }
 ```
 
-## Write image prompts
+### Write text-to-image prompts
 
-Your image prompts should describe the content you want to see in the image, and the visual style of image.
+Your prompts should describe the content you want to see in the image, and the visual style of image.
 
-When writing prompts, consider that the image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md).
+When you write prompts, consider that the Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md).
 
 > [!TIP]
 > For a thorough look at how you can tweak your text prompts to generate different kinds of images, see the [Image prompt engineering guide](/azure/ai-services/openai/concepts/gpt-4-v-prompt-engineering).
 
 
-## Specify API options
+### Specify API options
+
+The following API body parameters are available for image generation models.
+
+
+#### [GPT-image-1](#tab/gpt-image-1)
+
+
+#### Size
+
+Specify the size of the generated images. Must be one of `1024x1024`, `1024x1536`, or `1536x1024` for GPT-image-1 models. Square images are faster to generate.
+
+
+#### Quality
+
+There are three options for image quality: `low`, `medium`, and `high`.Lower quality images can be generated faster.
+
+The default value is `high`.
 
-The following API body parameters are available for DALL-E image generation.
+#### Number
 
-### Size
+You can generate between one and 10 images in a single API call. The default value is `1`.
+
+#### Response format
+
+The format in which the generated images are returned. Must be either `url` (a URL pointing to the image) or `b64_json` (the base 64-byte code in JSON format). The default is `url`.
+
+#### User ID
+
+Use the *user* parameter to specify a unique identifier for the user making the request. This is useful for tracking and monitoring usage patterns. The value can be any string, such as a user ID or email address.
+
+#### Output format
+
+Use the *output_format* parameter to specify the format of the generated image. Supported formats are `PNG` and `JPEG`. The default is `PNG`.
+
+> [!NOTE]
+> WEBP images are not supported in the Azure OpenAI Service.
+
+#### Compression
+
+Use the *output_compression* parameter to specify the compression level for the generated image. Input an integer between `0` and `100`, where `0` is no compression and `100` is maximum compression. The default is `100`.
+
+
+#### [DALL-E 3](#tab/dalle-3)
+
+<!--
+| Parameter Name   | Description       | Values         |
+|------------------|-------------|--------------------------|
+| Size             | Specifies the size of generated images. Square images generate faster.    | `1024x1024` (default), `1792x1024`, `1024x1792`           |
+| Style            | DALL-E 3 offers two style options. The natural style is more similar to the default style of older models, while the vivid style generates more hyper-real and cinematic images. </br></br>The natural style is useful in cases where DALL-E 3 over-exaggerates or confuses a subject that's meant to be more simple, subdued, or realistic.     | `natural`, `vivid` (default)           |
+| Quality          | Controls image quality. `hd` has finer details and better consistency; `standard` is faster.    | `hd`, `standard` (default) |
+| Number (`n`)     | Must be set to 1 for DALL-E 3. To get multiple images, make parallel requests.        | `1`              |
+| Response format  | Format for the returned images. Default is `url`.   | `url`, `b64_json`|
+-->
+
+#### Size
 
 Specify the size of the generated images. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for DALL-E 3 models. Square images are faster to generate.
 
-### Style
+#### Style
 
 DALL-E 3 offers two style options: `natural` and `vivid`. The natural style is more similar to the default style of older models, while the vivid style generates more hyper-real and cinematic images.
 
 The natural style is useful in cases where DALL-E 3 over-exaggerates or confuses a subject that's meant to be more simple, subdued, or realistic.
 
 The default value is `vivid`.
 
-### Quality
+#### Quality
 
 There are two options for image quality: `hd` and `standard`. The hd option creates images with finer details and greater consistency across the image. Standard images can be generated faster.
 
 The default value is `standard`.
 
-### Number
+#### Number
 
 With DALL-E 3, you can't generate more than one image in a single API call: the `n` parameter must be set to *1*. If you need to generate multiple images at once, make parallel requests.
 
-### Response format
+#### Response format
 
 The format in which the generated images are returned. Must be one of `url` (a URL pointing to the image) or `b64_json` (the base 64-byte code in JSON format). The default is `url`.
 
+---
+
+## Call the Image Edit API
+
+The Image Edit API allows you to modify existing images based on text prompts you provide. The API call is similar to the image generation API call, but you also need to provide an image URL or base 64-encoded image data.
+
+
+
+#### [GPT-image-1](#tab/gpt-image-1)
+
+Send a POST request to:
+
+```
+https://<your_resource_name>.openai.azure.com/openai/deployments/<your_deployment_name>/images/edits?api-version=<api_version>
+```
+
+
+**URL**:
+
+Replace the following values:
+- `<your_resource_name>` is the name of your Azure OpenAI resource.
+- `<your_deployment_name>` is the name of your DALL-E 3 or GPT-image-1 model deployment.
+- `<api_version>` is the version of the API you want to use. For example, `2025-04-01-preview`.
+
+**Required headers**:
+- `Content-Type`: `application/json`
+- `api-key`: `<your_API_key>`
+
+**Body**:
+
+The following is a sample request body. You specify a number of options, defined in later sections.
+
+```json
+{
+    "image": "<base64_encoded_image>",
+    "prompt": "Add a beach ball in the center.",
+    "model": "gpt-image-1",
+    "size": "1024x1024", 
+    "n": 1,
+    "quality": "high"
+}
+```
+
+### Output
+
+The response from a successful image editing API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.
+
+```json
+{ 
+    "created": 1698116662, 
+    "data": [ 
+        { 
+            "url": "<URL_to_generated_image>",
+            "revised_prompt": "<prompt_that_was_used>" 
+        }
+    ]
+} 
+```
+
+### Specify API options
+
+The following API body parameters are available for image editing models, in addition to the ones available for image generation models.
+
+### Image
+
+The *image* value indicates the image file you want to edit. It can be either a URL string to an image file, or base 64-encoded image data.
+
+
+#### Mask
+
+The *mask* parameter is the same type as the main *image* input parameter. It defines the area of the image that you want the model to change, using fully transparent pixels (alpha of zero) in those areas. The mask can be a URL or base 64-encoded image data. It must be a PNG file and have the same dimensions as the image.
+
+
+#### [DALL-E 3](#tab/dalle-3)
+
+DALL-E models don't support the Image Edit API.
+
+---
+
 ## Related content
 
 * [What is Azure OpenAI Service?](../overview.md)
 * [Quickstart: Generate images with Azure OpenAI Service](../dall-e-quickstart.md)
-* [Image generation API reference](/azure/ai-services/openai/reference#image-generation)
+* [Image API reference](/azure/ai-services/openai/reference#image-generation)
+* [Image API (preview) reference](/azure/ai-services/openai/reference-preview)
 
 
 <!-- OAI HT guide https://platform.openai.com/docs/guides/images/usage
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "Revamped DALL-E Documentation and Model Reference"
}
```

### Explanation
This modification represents a significant overhaul of the documentation related to the DALL-E models in the Azure OpenAI Service, with a shift towards a more inclusive reference to "image generation models." The main updates are as follows:

1. **Title and Description Change**: The document title has been changed to "How to use Azure OpenAI image generation models," and the description has been expanded to encompass both generating and editing images with various models.

2. **Broader Model Inclusion**: References to DALL-E have been generalized to include the new `GPT-image-1` model. The documentation now notes that users can deploy either the `dall-e-3` or `gpt-image-1` model, with additional information provided about the newer model's availability and improvements over DALL-E 3.

3. **Enhanced API Documentation**: The API usage instructions have been elaborated upon, providing new examples and a detailed breakdown of the API parameters for both image generation and editing models. 

4. **Content Filtering and Prompt Guidelines**: The modifications include sections on writing effective prompts and the implications of content moderation filters, ensuring users are informed about best practices for generating images that comply with platform guidelines.

5. **Updating Example Code**: The code snippets and examples have been updated to reflect the new model and API specifications, ensuring that users can utilize current best practices when integrating with the API.

These changes aim to provide clearer guidance and facilitate the effective use of Azure's image generation capabilities, reflecting the latest advancements in the underlying technology. The new content structure also enhances the user experience for both novice and experienced developers, ensuring comprehensive support for the current suite of image models.

## articles/ai-services/openai/how-to/evaluations.md{#item-dfaa1c}

<details>
<summary>Diff</summary>
````diff
@@ -305,12 +305,14 @@ BLEU (BiLingual Evaluation Understudy) score is commonly used in natural languag
 
 ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic summarization and machine translation. It measures the overlap between generated text and reference summaries. ROUGE focuses on recall-oriented measures to assess how well the generated text covers the reference text.
 The ROUGE score provides various metrics, including:
-•	ROUGE-1: Overlap of unigrams (single words) between generated and reference text.
-•	ROUGE-2: Overlap of bigrams (two consecutive words) between generated and reference text.
-•	ROUGE-3: Overlap of trigrams (three consecutive words) between generated and reference text.
-•	ROUGE-4: Overlap of four-grams (four consecutive words) between generated and reference text.
-•	ROUGE-5: Overlap of five-grams (five consecutive words) between generated and reference text.
-•	ROUGE-L: Overlap of L-grams (L consecutive words) between generated and reference text.
+
+- ROUGE-1: Overlap of unigrams (single words) between generated and reference text.
+- ROUGE-2: Overlap of bigrams (two consecutive words) between generated and reference text.
+- ROUGE-3: Overlap of trigrams (three consecutive words) between generated and reference text.
+- ROUGE-4: Overlap of four-grams (four consecutive words) between generated and reference text.
+- ROUGE-5: Overlap of five-grams (five consecutive words) between generated and reference text.
+- ROUGE-L: Overlap of L-grams (L consecutive words) between generated and reference text.
+
 Text summarization and document comparison are among optimal use cases for ROUGE, particularly in scenarios where text coherence and relevance are critical.
 
 Cosine similarity measures how closely two text embeddings—such as model outputs and reference texts—align in meaning, helping assess the semantic similarity between them. Same as other model-based evaluators, you need to provide a model deployment using for evaluation. 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Formatting Enhancement in Evaluations Documentation"
}
```

### Explanation
The code diff showcases a minor update to the documentation regarding evaluation metrics for natural language processing within the Azure AI services. The key modifications include:

1. **List Formatting Improvement**: The descriptions of various ROUGE metrics (ROUGE-1 through ROUGE-L) have been reformatted for better readability. The bullet points now begin with a hyphen followed by a space, making it clearer and more visually consistent. This change enhances the overall presentation of the metrics, improving the user experience by making the information easier to digest.

2. **Content Consistency**: The revised formatting aligns with common documentation practices, which promotes uniformity across the documentation set.

3. **Clarification of Evaluation Usage**: The text briefly reiterates the common applications of ROUGE, emphasizing its importance in text summarization and document comparison. This context aids users in understanding the practical relevance of the metrics provided.

Overall, while the content itself remains largely unchanged, these formatting adjustments contribute to a clearer and more professional presentation of information, facilitating better user comprehension of evaluation methodologies in natural language processing.

## articles/ai-services/openai/how-to/reasoning.md{#item-a54b2f}

<details>
<summary>Diff</summary>
````diff
@@ -49,16 +49,18 @@ Azure OpenAI `o-series` models are designed to tackle reasoning and problem-solv
 | Responses API | ✅ | ✅  | - | - | - | - |
 | Functions/Tools | ✅ | ✅ | ✅  | ✅  |  - | - |
 | Parallel Tool Calls | - | - | -  | -  |  - | - |
-| `max_completion_tokens`<sup>*</sup> | ✅ | ✅ |✅ |✅ |✅ | ✅ |
-| System Messages<sup>**</sup> | ✅ | ✅ | ✅ | ✅ | - | - |
-| [Reasoning summary](#reasoning-summary) <sup>***</sup> | ✅ | ✅ | -  | -  |  - | - |
-| Streaming | ✅ | ✅ | ✅ | - | - | - |
+| `max_completion_tokens` <sup>1</sup> | ✅ | ✅ |✅ |✅ |✅ | ✅ |
+| System Messages <sup>2</sup> | ✅ | ✅ | ✅ | ✅ | - | - |
+| [Reasoning summary](#reasoning-summary) <sup>3</sup> | ✅ | ✅ | -  | -  |  - | - |
+| Streaming <sup>4</sup>  | ✅ | ✅| ✅ | - | - | - |
 
-<sup>*</sup> Reasoning models will only work with the `max_completion_tokens` parameter. <br><br>
+<sup>1</sup> Reasoning models will only work with the `max_completion_tokens` parameter. <br><br>
 
-<sup>**</sup>The latest o<sup>&#42;</sup> series model support system messages to make migration easier. When you use a system message with `o4-mini`, `o3`, `o3-mini`, and `o1` it will be treated as a developer message. You should not use both a developer message and a system message in the same API request.
+<sup>2</sup> The latest o<sup>&#42;</sup> series model support system messages to make migration easier. When you use a system message with `o4-mini`, `o3`, `o3-mini`, and `o1` it will be treated as a developer message. You should not use both a developer message and a system message in the same API request.
 
-<sup>***</sup> Access to the chain-of-thought reasoning summary is limited access only for `o4-mini`. 
+<sup>3</sup> Access to the chain-of-thought reasoning summary is limited access only for `o4-mini`.
+
+<sup>4</sup> Streaming for `o3` is currently only supported for Enterprise subscriptions.
 
 ### Not Supported
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarifications and Formatting Adjustments in Reasoning Documentation"
}
```

### Explanation
The modifications made to the reasoning documentation for Azure OpenAI services involve minor updates aimed at enhancing clarity and readability. Key changes include:

1. **Superscript Notation Improvement**: The references to specific notes (marked with superscripts) have been standardized in their formatting. The symbols used for annotations have been changed from asterisks (e.g., `*`, `**`, `***`) to numerical superscripts (e.g., `1`, `2`, `3`, `4`). This change provides a clearer connection between the annotations and their corresponding explanations, making it easier for readers to identify which notes refer to which items in the table.

2. **Additional Note**: A new note regarding the streaming capabilities for the `o3` model has been added. This note clarifies that streaming is only supported for Enterprise subscriptions, providing users with important information about the limitations of this feature.

3. **Content Consistency**: Beyond just formatting, this update reflects a broader attempt to maintain consistency within the document, ensuring that readers have a coherent understanding of how the various parameters and features interact, especially in relation to the different model capabilities.

Overall, these minor updates contribute to a clearer and more user-friendly documentation experience, helping users better understand the specific functionalities of the reasoning models and their respective requirements in the Azure OpenAI framework.

## articles/ai-services/openai/how-to/responses.md{#item-b9757d}

<details>
<summary>Diff</summary>
````diff
@@ -46,6 +46,7 @@ The responses API is currently available in the following regions:
 - `gpt-4.1` (Version: `2025-04-14`)
 - `gpt-4.1-nano` (Version: `2025-04-14`)
 - `gpt-4.1-mini` (Version: `2025-04-14`)
+- `gpt-image-1` (Version: `2025-04-15`)
 - `o3` (Version: `2025-04-16`)
 - `o4-mini` (Version: `2025-04-16`)
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Addition of New Model in Responses Documentation"
}
```

### Explanation
The recent update to the responses documentation in the Azure OpenAI services includes a minor enhancement by adding a new entry. This change involves:

1. **New Model Inclusion**: The model `gpt-image-1` with the version date of `2025-04-15` has been added to the list of available responses API models. This inclusion informs users of the new capabilities that are now accessible, expanding the options available for leveraging the OpenAI services.

2. **Maintaining Structure**: The format and structure of the document remain consistent, ensuring that the addition blends seamlessly with the existing content. This consistency makes it easier for users to locate and understand the different models available without confusion.

Overall, this modification serves to enhance the documentation by keeping it up-to-date with the latest offerings in the responses API, thereby improving the user experience for developers and users relying on this information for integrating OpenAI capabilities into their applications.

## articles/ai-services/openai/includes/api-versions/latest-inference-preview.md{#item-24bf0f}

<details>
<summary>Diff</summary>
````diff
@@ -5,7 +5,7 @@ description: Latest preview data plane inference documentation generated from Op
 manager: nitinme
 ms.service: azure-ai-openai
 ms.topic: include
-ms.date: 03/24/2025
+ms.date: 04/23/2025
 ---
 
 ## Completions - Create
@@ -1192,7 +1192,7 @@ Status Code: 200
 POST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2025-03-01-preview
 ```
 
-Generates a batch of images from a text caption on a given DALLE model deployment
+Generates a batch of images from a text caption on a given DALL-E or GPT-image-1 model deployment
 
 ### URI Parameters
 
@@ -1219,11 +1219,13 @@ Generates a batch of images from a text caption on a given DALLE model deploymen
 |------|------|-------------|----------|---------|
 | n | integer | The number of images to generate. | No | 1 |
 | prompt | string | A text description of the desired image(s). The maximum length is 4000 characters. | Yes |  |
-| quality | [imageQuality](#imagequality) | The quality of the image that will be generated. | No | standard |
+| quality | [imageQuality](#imagequality) | The quality of the image that will be generated. | No | standard (for DALL-E)</br>high (for GPT-image-1) |
 | response_format | [imagesResponseFormat](#imagesresponseformat) | The format in which the generated images are returned. | No | url |
 | size | [imageSize](#imagesize) | The size of the generated images. | No | 1024x1024 |
 | style | [imageStyle](#imagestyle) | The style of the generated images. | No | vivid |
 | user | string | A unique identifier representing your end-user, which can help to monitor and detect abuse. | No |  |
+| output_format | string | The format in which the generated images are returned. GPT-image-1 models only. | No | PNG |
+| output_compression | integer | The compression level (on a scale of 0-100) of the generated images. GPT-image-1 | No | 0 |
 
 ### Responses
 
@@ -1243,14 +1245,12 @@ Generates a batch of images from a text caption on a given DALLE model deploymen
 |:---|:---|:---|
 |application/json | [dalleErrorResponse](#dalleerrorresponse) | |
 
-### Examples
-
 ### Example
 
 Creates images given a prompt.
 
 ```HTTP
-POST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2025-03-01-preview
+POST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2025-04-01-preview
 
 {
  "prompt": "In the style of WordArt, Microsoft Clippy wearing a cowboy hat.",
@@ -1264,6 +1264,9 @@ POST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?ap
 **Responses**:
 Status Code: 200
 
+> [!NOTE]
+> The GPT-image-1 model doesn't return content filtering annotations.
+
 ```json
 {
   "body": {
@@ -1322,6 +1325,112 @@ Status Code: 200
 }
 ```
 
+
+## Image generations - Edit
+
+```HTTP
+POST https://{endpoint}/openai/deployments/{deployment-id}/images/edits?api-version=2025-04-01-preview
+```
+
+Generates an image based on an input image and text prompt instructions. Requires a GPT-image-1 model deployment
+
+### URI Parameters
+
+| Name | In | Required | Type | Description |
+|------|------|----------|------|-----------|
+| endpoint | path | Yes | string<br>url | Supported Azure OpenAI endpoints (protocol and hostname, for example: `https://aoairesource.openai.azure.com`. Replace "aoairesource" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com |
+| deployment-id | path | Yes | string |  |
+| api-version | query | Yes | string |  |
+
+### Request Header
+
+**Use either token based authentication or API key. Authenticating with token based authentication is recommended and more secure.**
+
+| Name | Required | Type | Description |
+| --- | --- | --- | --- |
+| Authorization | True | string | **Example:** `Authorization: Bearer {Azure_OpenAI_Auth_Token}`<br><br>**To generate an auth token using Azure CLI: `az account get-access-token --resource https://cognitiveservices.azure.com`**<br><br>Type: oauth2<br>Authorization Url: `https://login.microsoftonline.com/common/oauth2/v2.0/authorize`<br>scope: `https://cognitiveservices.azure.com/.default`|
+| api-key | True | string | Provide Azure OpenAI API key here |
+
+### Request Body
+
+**Content-Type**: application/json
+
+| Name | Type | Description | Required | Default |
+|------|------|-------------|----------|---------|
+| image | file | The input image to edit. Must be a valid image URL or base64-encoded image. tbd | Yes |  |
+| n | integer | The number of images to generate. | No | 1 |
+| prompt | string | A text description of how the input image should be edited. The maximum length is 4000 characters. | Yes |  |
+| mask | file | A mask image to define the area of the input image that the model should edit, using fully transparent pixels (alpha of zero) in those areas. Must be a valid image URL or base64-encoded image. | No |  |
+| quality | string | The quality of the image that will be generated. Values are 'low', 'medium', 'high' | No | high |
+| response_format | [imagesResponseFormat](#imagesresponseformat) | The format in which the generated images are returned. | No | url |
+| size | [imageSize](#imagesize) | The size of the generated images. | No | 1024x1024 |
+| style | [imageStyle](#imagestyle) | The style of the generated images. | No | vivid |
+| user | string | A unique identifier representing your end-user, which can help to monitor and detect abuse. | No |  |
+| output_format | [imageOutputFormat](#imageoutputformat) | The format in which the generated images are returned. | No | PNG |
+| output_compression | integer | The compression level (on a scale of 0-100) of the generated images. GPT-image-1 | No | 0 |
+
+
+### Responses
+
+**Status Code:** 200
+
+**Description**: Ok 
+
+|**Content-Type**|**Type**|**Description**|
+|:---|:---|:---|
+|application/json | [generateImagesResponse](#generateimagesresponse) | |
+
+**Status Code:** default
+
+**Description**: An error occurred. 
+
+|**Content-Type**|**Type**|**Description**|
+|:---|:---|:---|
+|application/json | [dalleErrorResponse](#dalleerrorresponse) | |
+
+### Example
+
+Creates images given an input image and text instructions.
+
+```HTTP
+POST https://{endpoint}/openai/deployments/{deployment-id}/images/edits?api-version=2025-04-01-preview
+
+{
+  "image": "<base64_encoded_image>",
+  "prompt": "Add a beach ball in the center.",
+  "model": "gpt-image-1",
+  "size": "1024x1024", 
+  "n": 1,
+  "quality": "high"
+}
+
+```
+
+**Responses**:
+Status Code: 200
+
+> [!NOTE]
+> The GPT-image-1 model doesn't return content filtering annotations.
+
+```json
+{
+  "body": {
+    "created": 1698342300,
+    "data": [
+      {
+        "b64_json": "<base64_encoded_image>",
+        "revised_prompt": "A vivid, natural representation of Microsoft Clippy wearing a cowboy hat.",
+      }],
+    "usage": 
+      {
+        "input_tokens": 557,
+        "output_tokens": 1000,
+      }
+    
+  }
+}
+```
+
 ## List - Assistants
 
 ```HTTP
@@ -6012,6 +6121,17 @@ Speech request.
 | speed | number | The speed of the synthesized audio. Select a value from `0.25` to `4.0`. `1.0` is the default. | No | 1.0 |
 | voice | enum | The voice to use for speech synthesis.<br>Possible values: `alloy`, `echo`, `fable`, `onyx`, `nova`, `shimmer` | Yes |  |
 
+### imageOutputFormat
+
+The requested output format for the generated image.
+
+| Property | Value |
+|----------|-------|
+| **Description** | The requested output format for the generated image. |
+| **Type** | string |
+| **Default** | PNG |
+| **Values** | `PNG`<br>`JPEG` |
+
 ### imageQuality
 
 The quality of the image that will be generated.
@@ -6020,8 +6140,8 @@ The quality of the image that will be generated.
 |----------|-------|
 | **Description** | The quality of the image that will be generated. |
 | **Type** | string |
-| **Default** | standard |
-| **Values** | `standard`<br>`hd` |
+| **Default** | standard (for DALL-E)<br>high (for GPT-image-1) |
+| **Values** | `standard`, `hd` (for DALL-E)<br>`low`, `medium`, `high` (for GPT-image-1) |
 
 ### imagesResponseFormat
 
@@ -6062,11 +6182,32 @@ The style of the generated images.
 |------|------|-------------|----------|---------|
 | n | integer | The number of images to generate. | No | 1 |
 | prompt | string | A text description of the desired image(s). The maximum length is 4000 characters. | Yes |  |
-| quality | [imageQuality](#imagequality) | The quality of the image that will be generated. | No | standard |
+| quality | [imageQuality](#imagequality) | The quality of the image that will be generated. | No | standard (for DALL-E)</br>high (for GPT-image-1) |
 | response_format | [imagesResponseFormat](#imagesresponseformat) | The format in which the generated images are returned. | No | url |
 | size | [imageSize](#imagesize) | The size of the generated images. | No | 1024x1024 |
 | style | [imageStyle](#imagestyle) | The style of the generated images. | No | vivid |
 | user | string | A unique identifier representing your end-user, which can help to monitor and detect abuse. | No |  |
+| output_format | string | The format in which the generated images are returned. GPT-image-1 models only. | No | PNG |
+| output_compression | integer | The compression level (on a scale of 0-100) of the generated images. GPT-image-1 | No | 0 |
+
+### imageEditsRequest
+
+
+| Name | Type | Description | Required | Default |
+|------|------|-------------|----------|---------|
+| image | file | The input image to edit. Must be a valid image URL or base64-encoded image. tbd | Yes |  |
+| n | integer | The number of images to generate. | No | 1 |
+| prompt | string | A text description of how the input image should be edited. The maximum length is 4000 characters. | Yes |  |
+| mask | file | A mask image to define the area of the input image that the model should edit, using fully transparent pixels (alpha of zero) in those areas. Must be a valid image URL or base64-encoded image. | No |  |
+| quality | string | The quality of the image that will be generated. Values are 'low', 'medium', 'high' | No | high |
+| response_format | [imagesResponseFormat](#imagesresponseformat) | The format in which the generated images are returned. | No | url |
+| size | [imageSize](#imagesize) | The size of the generated images. | No | 1024x1024 |
+| style | [imageStyle](#imagestyle) | The style of the generated images. | No | vivid |
+| user | string | A unique identifier representing your end-user, which can help to monitor and detect abuse. | No |  |
+| output_format | [imageOutputFormat](#imageoutputformat) | The format in which the generated images are returned. | No | PNG |
+| output_compression | integer | The compression level (on a scale of 0-100) of the generated images. GPT-image-1 | No | 0 |
+
+
 
 ### generateImagesResponse
 
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Enhancements to Inference Preview Documentation for New Models and Features"
}
```

### Explanation
The recent modifications to the latest inference preview documentation for Azure OpenAI services represent a significant update that includes the addition of new features and clarifications regarding existing functionalities. Here are the key enhancements:

1. **Updated Version Information**: The date of the document has been updated from `03/24/2025` to `04/23/2025`, reflecting the latest changes and thus ensuring users have access to the most recent information.

2. **Expanded Model Capabilities**: The text now specifies that image generation can be performed using both the DALL-E model and the newly introduced `GPT-image-1` model. This addition broadens the scope of what users can achieve with their requests, emphasizing the flexibility of the API.

3. **New URI and Request Parameters**: Several new parameters specific to the `GPT-image-1` model, such as `output_format` and `output_compression`, have been introduced. These parameters allow users to dictate the format and compression level of the generated images, providing enhanced control over the output.

4. **Detailed Example Section**: The documentation now includes specific examples of API requests, illustrating how to create and edit images using the new models. This practical guidance serves as a useful reference for developers integrating these functionalities into their applications.

5. **Responses and Error Handling**: Additional information regarding responses from the API, including successful and error states, has been documented to enhance understanding of how to manage interactions with the service. Notably, there is clarification on the lack of content filtering annotations returned by the `GPT-image-1` model.

6. **New Section for Image Edits**: A new segment has been added specifically for generating images based on input images and text prompts, enhancing the documentation's comprehensiveness regarding image editing capabilities.

These updates collectively enhance the documentation's detail and usability, ensuring that users are fully equipped to utilize the latest features of the Azure OpenAI APIs effectively.

## articles/ai-services/openai/includes/dall-e-dotnet.md{#item-755f0a}

<details>
<summary>Diff</summary>
````diff
@@ -125,7 +125,7 @@ The URL of the generated image is printed to the console.
 ```
 
 > [!NOTE]
-> The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
+> The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
 
 ## Clean up resources
 
@@ -136,5 +136,5 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E Documentation"
}
```

### Explanation
The latest changes to the DALL-E documentation include minor modifications aimed at improving clarity and consistency in terminology throughout the document. The key updates are as follows:

1. **Terminology Consistency**: The phrase "image generation APIs" has been changed to "Image APIs" in two separate instances. This adjustment aligns the language used in the documentation, making it consistent with other sections and thus enhancing readability for users.

2. **Clarification on Content Moderation**: The note regarding the content moderation filter is now phrased more clearly, but the core message remains the same. It informs users that if the service detects a prompt as harmful, it will not return a generated image. The updated phrasing ensures that users understand the implications of the content moderation mechanism.

3. **Updated Guide Reference**: The reference to the "DALL-E how-to guide" has been updated to "Image API how-to guide." This change reflects a broader categorization, accommodating the inclusion of other image-related APIs, and aids users in navigating related resources more effectively.

These minor updates are part of ongoing efforts to keep the documentation accurate, clear, and user-friendly, ensuring that users can easily find and understand the content regarding the DALL-E capabilities within the Azure OpenAI framework.

## articles/ai-services/openai/includes/dall-e-go.md{#item-132707}

<details>
<summary>Diff</summary>
````diff
@@ -248,7 +248,7 @@ Image generated, HEAD request on URL returned 200
 Image URL: <SAS URL>
 ```
 > [!NOTE]
-> The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
+> The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
 
 ## Clean up resources
 
@@ -259,5 +259,5 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E Go Documentation"
}
```

### Explanation
The recent changes to the DALL-E Go documentation involve minor updates focused on adjusting the terminology for consistency and clarity. Here are the notable modifications:

1. **Consistency in Terminology**: The change from "image generation APIs" to "Image APIs" in the documentation reflects an effort to harmonize terminology. This adjustment improves readability and consistency with other sections of the documentation, ensuring users can easily understand and find related content.

2. **Clarification on Content Moderation**: The note about the content moderation filter retains its informative essence but adopts the updated phrasing. It informs users that if a prompt is deemed harmful, the system will not return a generated image, which helps set appropriate expectations for users regarding the API's functionality.

3. **Updated Guide Reference**: The reference to "DALL-E how-to guide" has been changed to "Image API how-to guide." This modification broadens the context, allowing for the inclusion of other image-related functionalities and serving as a more comprehensive resource for the users.

These updates contribute to the overall clarity and usability of the documentation, ensuring that it remains an effective reference for developers and users engaging with the DALL-E services within the Azure OpenAI framework.

## articles/ai-services/openai/includes/dall-e-java.md{#item-373f89}

<details>
<summary>Diff</summary>
````diff
@@ -263,7 +263,7 @@ Completed getImages.
 ```
 
 > [!NOTE]
-> The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
+> The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
 
 
 ## Clean up resources
@@ -275,5 +275,5 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E Java Documentation"
}
```

### Explanation
The recent modifications to the DALL-E Java documentation consist of minor updates that aim to enhance consistency and clarity in the language used. The key changes are as follows:

1. **Terminology Improvement**: The documentation now consistently refers to "Image APIs" instead of "image generation APIs." This adjustment not only standardizes the terminology throughout the document but also improves the overall clarity for users navigating different sections of the documentation.

2. **Content Moderation Explanation**: The note regarding the content moderation filter has been rephrased but retains its core message. Users are informed that if the service determines a prompt to be harmful, it will not return a generated image. This clarity is vital for setting appropriate user expectations about the API's behavior.

3. **Updated Resource References**: The reference to exploring "DALL-E how-to guide" has been updated to "Image API how-to guide," broadening the scope and ensuring the guide encompasses various functionalities related to image APIs. This change helps users locate relevant resources more easily.

These updates contribute to maintaining a clear and user-friendly documentation experience for developers working with the DALL-E services within the Azure OpenAI ecosystem. Overall, the adjustments enhance the usability and consistency of the content, ensuring it serves as a reliable resource for its audience.

## articles/ai-services/openai/includes/dall-e-javascript.md{#item-6cffcf}

<details>
<summary>Diff</summary>
````diff
@@ -202,7 +202,7 @@ Image generation result URL: <SAS URL>
 ```
 
 > [!NOTE]
-> The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
+> The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
 
 ## Clean up resources
 
@@ -213,5 +213,5 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E JavaScript Documentation"
}
```

### Explanation
The recent updates to the DALL-E JavaScript documentation involve minor but important changes that enhance clarity and consistency in terminology. The main adjustments include:

1. **Terminology Consistency**: The phrase "image generation APIs" has been replaced with "Image APIs." This change ensures uniformity across the documentation and helps avoid confusion, making it easier for users to understand and locate related content.

2. **Content Moderation Clarification**: The note regarding the content moderation filter remains intact but is worded in a way that aligns with the new terminology. It continues to inform users that if a prompt is identified as harmful, the service will not generate an image, thus setting appropriate expectations.

3. **Updated Resource References**: The phrase "DALL-E how-to guide" has been modified to "Image API how-to guide," which not only aligns with the terminology update but also allows users to access a broader range of image-related resources more efficiently.

These modifications contribute to a clearer and more user-friendly experience within the documentation, making it a reliable guide for developers who are working with DALL-E services in the Azure OpenAI platform. Overall, the changes help maintain a consistent narrative throughout the documentation.

## articles/ai-services/openai/includes/dall-e-powershell.md{#item-97878b}

<details>
<summary>Diff</summary>
````diff
@@ -106,7 +106,7 @@ For the recommended keyless authentication with Microsoft Entra ID, you need to:
 
 PowerShell requests the image from Azure OpenAI and stores the output image in the _generated_image.png_ file in your specified directory. For convenience, the full path for the file is returned at the end of the script.
 
-The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md).
+The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md).
 
 ## Clean up resources
 
@@ -117,5 +117,5 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 - Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E PowerShell Documentation"
}
```

### Explanation
The recent updates to the DALL-E PowerShell documentation include minor but meaningful changes aimed at improving consistency and clarity in terminology. The primary modifications are as follows:

1. **Terminology Standardization**: The term "image generation APIs" has been updated to "Image APIs." This adjustment standardizes the terminology used throughout the documentation, making it more coherent and easier for users to follow.

2. **Content Moderation Note**: The explanation regarding the content moderation filter has been revised to reflect the new terminology while maintaining its original meaning. The note informs users that if their prompt is identified as harmful content, the service will not generate any images, thereby ensuring that users understand the operational limitations.

3. **Resource Reference Updates**: The phrase "DALL-E how-to guide" has been changed to "Image API how-to guide," which aligns with the updated terminology and enhances user clarity. This change allows users to more easily find relevant resources related to the functionality of image APIs as a whole.

These adjustments work together to create a clearer, more consistent documentation experience for developers utilizing DALL-E services within the Azure OpenAI framework, thus enriching the overall usability of the documentation.

## articles/ai-services/openai/includes/dall-e-python.md{#item-c91824}

<details>
<summary>Diff</summary>
````diff
@@ -120,7 +120,7 @@ Wait a few moments to get the response.
 
 Azure OpenAI stores the output image in the _generated_image.png_ file in your specified directory. The script also displays the image in your default image viewer.
 
-The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md).
+The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md).
 
 ## Clean up resources
 
@@ -131,6 +131,6 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
 * See the [API reference](../reference.md#image-generation)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E Python Documentation"
}
```

### Explanation
The recent updates to the DALL-E Python documentation encompass minor but significant changes aimed at enhancing clarity and consistency in terminology. The key modifications are outlined below:

1. **Terminology Alignment**: The phrase "image generation APIs" has been revised to "Image APIs." This adjustment standardizes the terminology used throughout the documentation, fostering a unified understanding and reducing potential confusion among users.

2. **Content Moderation Clarification**: The section regarding the content moderation filter has been updated to reflect the new terminology. It continues to specify that if a prompt is identified as harmful content, the service will not generate an image. This ensures that users are informed about the limitations of the service.

3. **Updated Resource References**: The mention of the "DALL-E how-to guide" has been changed to "Image API how-to guide." This change aligns with the updated terminology and encourages users to explore broader resources relevant to image APIs, rather than focusing solely on DALL-E.

These changes collectively improve the cohesiveness and readability of the documentation for developers using DALL-E services in the Azure OpenAI environment, thereby enhancing the overall user experience.

## articles/ai-services/openai/includes/dall-e-rest.md{#item-4bac64}

<details>
<summary>Diff</summary>
````diff
@@ -99,7 +99,7 @@ The output from a successful image generation API call looks like the following
 } 
 ```
 
-The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md). For examples of error responses, see the [DALL-E how-to guide](../how-to/dall-e.md).
+The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter.md). For examples of error responses, see the [DALL-E how-to guide](../how-to/dall-e.md).
 
 The system returns an operation status of `Failed` and the `error.code` value in the message is set to `contentFilter`. Here's an example:
 
@@ -137,6 +137,6 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
 * See the [API reference](../reference.md#image-generation)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E REST API Documentation"
}
```

### Explanation
The recent changes made to the DALL-E REST API documentation involve minor yet impactful updates focused on terminology improvements for better clarity and consistency. The main points of modification are as follows:

1. **Terminology Consistency**: The term "image generation APIs" has been updated to "Image APIs." This modification ensures that the language used throughout the documentation is standardized, promoting a clearer understanding for users navigating the materials.

2. **Clarification of Content Moderation**: The explanation related to the content moderation filter has been revised to utilize the new terminology. It retains the original message that the service will not generate images if a prompt is flagged as harmful content, ensuring that necessary information is still communicated effectively.

3. **Updated References for Guide**: The reference to the "DALL-E how-to guide" has also been changed to "Image API how-to guide." This alteration aligns with the updated terminology and encourages users to explore documentation resources that address a wider scope of image-related functionalities.

These modifications collectively contribute to enhancing the usability and coherence of the DALL-E REST API documentation, providing a more user-friendly experience for developers and users working with Azure OpenAI's image generation capabilities.

## articles/ai-services/openai/includes/dall-e-studio.md{#item-439729}

<details>
<summary>Diff</summary>
````diff
@@ -31,7 +31,7 @@ On the model's page, select **Open in playground**.
 Start exploring Azure OpenAI capabilities with a no-code approach through the **Images playground**. Enter your image prompt into the text box and select **Generate**. When the AI-generated image is ready, it appears on the page.
 
 > [!NOTE]
-> The image generation APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated image. For more information, see [Content filtering](../concepts/content-filter.md).
+> The Image APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated image. For more information, see [Content filtering](../concepts/content-filter.md).
 
 In the **Images playground**, you can also view Python and cURL code samples, which are prefilled according to your settings. Select **View code** near the top of the page. You can use this code to write an application that completes the same task.
 
@@ -44,6 +44,6 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
 * See the [API reference](../reference.md#image-generation)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E Studio Documentation"
}
```

### Explanation
Recent updates to the DALL-E Studio documentation incorporate minor revisions focused on enhancing the clarity and consistency of terminology used in the text. The key changes made are as follows:

1. **Terminology Consistency**: The phrase "image generation APIs" has been changed to "Image APIs." This update is part of a broader effort to ensure consistent use of terminology across the documentation, thereby reducing potential confusion for users.

2. **Content Moderation Note**: The section discussing the content moderation filter for the image generation process now employs the updated terminology. It clearly states that if the prompt is deemed harmful by Azure OpenAI, the service will not provide a generated image. This clarification helps maintain transparency regarding the service's functionality.

3. **Updated Resource Reference**: The mention of the "DALL-E how-to guide" has been replaced with "Image API how-to guide," which aligns with the updated terminology. This encourages users to delve deeper into the resources pertinent to the capabilities of the Image APIs as a whole.

These modifications aim to improve the overall user experience by promoting clearer understanding and easier navigation through the DALL-E Studio documentation, ultimately aiding developers in making the most of Azure OpenAI's image generation features.

## articles/ai-services/openai/includes/dall-e-typescript.md{#item-57b205}

<details>
<summary>Diff</summary>
````diff
@@ -248,7 +248,7 @@ Image generation result URL: <SAS URL>
 ```
 
 > [!NOTE]
-> The image generation APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
+> The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](../concepts/content-filter.md) article.
 
 ## Clean up resources
 
@@ -259,5 +259,5 @@ If you want to clean up and remove an Azure OpenAI resource, you can delete the
 
 ## Next steps
 
-* Explore the image generation APIs in more depth with the [DALL-E how-to guide](../how-to/dall-e.md).
+* Explore the Image APIs in more depth with the [Image API how-to guide](../how-to/dall-e.md).
 * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Updates in DALL-E TypeScript Documentation"
}
```

### Explanation
The latest changes to the DALL-E TypeScript documentation involve minor adjustments aimed at enhancing the clarity and uniformity of the terminology used throughout the document. The notable modifications include:

1. **Consistent Terminology**: The term "image generation APIs" has been updated to "Image APIs." This change is part of an initiative to standardize terms across the documentation, making it easier for users to follow and understand the content.

2. **Clarification of Content Moderation**: The note regarding the content moderation filter has been revised to use the updated terminology. It emphasizes that if the Azure OpenAI service detects harmful content in the user's prompt, it will not provide a generated image. This clarification is essential for ensuring users are informed about the filter's functionality.

3. **Updated Guide Reference**: The reference to the "DALL-E how-to guide" is now described as the "Image API how-to guide." This revision aligns with the updated terminology and encourages users to explore documentation that covers a broader range of image-related functionalities.

These changes collectively contribute to improving the user experience by fostering a clearer understanding and more coherent navigation through the DALL-E TypeScript documentation, ultimately assisting developers in leveraging Azure OpenAI's image generation features effectively.

## articles/ai-services/openai/includes/language-overview/dotnet.md{#item-46e881}

<details>
<summary>Diff</summary>
````diff
@@ -264,7 +264,7 @@ bytes.ToStream().CopyTo(stream);
 
 ```
 
-- [C# DALL-E quickstart guide](/azure/ai-services/openai/dall-e-quickstart?tabs=dalle3%2Ccommand-line%2Ckeyless%2Ctypescript-keyless&pivots=programming-language-csharp)
+- [C# Image generation quickstart guide](/azure/ai-services/openai/dall-e-quickstart?tabs=dalle3%2Ccommand-line%2Ckeyless%2Ctypescript-keyless&pivots=programming-language-csharp)
 
 ## Reasoning models
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Terminology Change in .NET Documentation"
}
```

### Explanation
The recent modification to the .NET documentation for Azure OpenAI features a minor update that focuses on refining terminology for better clarity. The notable change is as follows:

1. **Terminology Update**: The link reference previously labeled as "[C# DALL-E quickstart guide]" has been changed to "[C# Image generation quickstart guide]." This alteration aims to broaden the scope of the guide's subject matter, aligning it with the overarching capabilities of the Azure OpenAI service that encompass various image generation tasks, rather than solely focusing on DALL-E.

This change enhances the document by ensuring that the terminology accurately reflects the content and functionality available to users, thereby improving overall understanding and accessibility of the documentation for developers working with the Azure OpenAI services in .NET.

## articles/ai-services/openai/includes/model-matrix/global-batch.md{#item-929e6a}

<details>
<summary>Diff</summary>
````diff
@@ -19,7 +19,6 @@ ms.date: 04/21/2025
 | eastus2            | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | -                      | ✅                       | ✅                       |
 | francecentral      | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | -                      | ✅                       | ✅                       |
 | germanywestcentral | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | ✅                       | ✅                       | ✅                       |
-| italynorth         | ✅                        | -                      | -                      | -                      | -                           | -               | -                           | -                      | -                      | -                      |
 | japaneast          | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | -                      | ✅                       | ✅                       |
 | koreacentral       | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | ✅                       | ✅                       | ✅                       |
 | northcentralus     | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | -                      | ✅                       | ✅                       |
@@ -31,7 +30,6 @@ ms.date: 04/21/2025
 | spaincentral       | ✅                        | -                      | -                      | -                      | -                           | -               | -                           | -                      | -                      | -                      |
 | swedencentral      | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | -                      | ✅                       | ✅                       |
 | switzerlandnorth   | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | -                      | ✅                       | ✅                       |
-| uaenorth           | ✅                        | -                      | -                      | -                      | -                           | -               | -                           | -                      | -                      | -                      |
 | uksouth            | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | -                      | ✅                       | ✅                       |
 | westeurope         | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | ✅                       | ✅                       | ✅                       |
 | westus             | ✅                        | ✅                       | ✅                       | ✅                       | ✅                            | ✅                | ✅                            | ✅                       | ✅                       | ✅                       |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Removal of Region Entries in Global Batch Documentation"
}
```

### Explanation
The recent modifications made to the Global Batch documentation remove certain entries reflecting the availability of services across various regions. Specifically, two regions—"italynorth" and "uaenorth"—have been deleted from the region table, which outlines the support status for different capabilities in each region.

1. **Deletion of Entries**: The entries for the "italynorth" and "uaenorth" regions have been removed from the documentation. This signifies that these regions may no longer support specific features or models offered by Azure OpenAI, which is critical information for users deploying services based on regional capabilities.

2. **Impact on Documentation**: This change can affect planning and implementation for developers and organizations using Azure OpenAI services, as they need to be aware of changes in service availability in their respective regions.

Overall, these updates ensure that the documentation remains current and accurately conveys the available capabilities across different geographic locations for users relying on the Global Batch service in Azure OpenAI.

## articles/ai-services/openai/includes/structured-outputs-python.md{#item-2734f0}

<details>
<summary>Diff</summary>
````diff
@@ -20,6 +20,7 @@ pip install openai pydantic --upgrade
 If you are new to using Microsoft Entra ID for authentication see [How to configure Azure OpenAI Service with Microsoft Entra ID authentication](../how-to/managed-identity.md).
 
 ```python
+import os
 from pydantic import BaseModel
 from openai import AzureOpenAI
 from azure.identity import DefaultAzureCredential, get_bearer_token_provider
@@ -105,6 +106,7 @@ pip install openai pydantic --upgrade
 ```
 
 ```python
+import os
 from pydantic import BaseModel
 from openai import AzureOpenAI
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Addition of Import Statements in Python Documentation"
}
```

### Explanation
The recent update to the Structured Outputs Python documentation involves a minor enhancement by adding necessary import statements to ensure users have clear instructions for setting up their environment.

1. **Addition of 'os' Import**: The `import os` statement has been added to the top of the Python code samples. This addition implies that the functionality or features being discussed may require the use of operating system-specific operations or file handling, which is facilitated by the `os` module.

2. **Clarity and Completeness**: This change improves the completeness of the provided examples, ensuring that users have all the necessary imports available for the code snippets to function as intended. Such minor updates are essential for user experience as they reduce the chances of confusion or errors when users copy and execute the code.

Overall, this modification serves as a helpful update for developers working with the Azure OpenAI services in Python, making the code examples more robust and functional.

## articles/ai-services/openai/index.yml{#item-0adb87}

<details>
<summary>Diff</summary>
````diff
@@ -1,11 +1,11 @@
 ### YamlMime:Landing
 
 title: Azure OpenAI Service documentation # < 60 chars
-summary: Azure OpenAI Service provides access to OpenAI's models including o-series, GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, DALLE-3 and Embeddings model series with the security and enterprise capabilities of Azure. 
+summary: Azure OpenAI Service provides access to OpenAI's models including o-series, GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, GPT-image-1 and Embeddings model series with the security and enterprise capabilities of Azure. 
   
 metadata:
   title: Azure OpenAI Service documentation - Quickstarts, Tutorials, API Reference - Azure AI services | Microsoft Docs
-  description: Learn how to use Azure OpenAI's powerful models including o-series, GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, DALL-E 3 and Embeddings model series
+  description: Learn how to use Azure OpenAI's powerful models including o-series, GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, GPT-image-1, and Embeddings model series
   ms.service: azure-ai-openai
   ms.custom:
   ms.topic: landing-page
@@ -42,7 +42,7 @@ landingContent:
              url: chatgpt-quickstart.md
            - text: Vision-enabled models
              url: gpt-v-quickstart.md  
-           - text: DALL-E
+           - text: Image generation
              url: dall-e-quickstart.md
            - text: Use your data (preview)
              url: use-your-data-quickstart.md
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Azure OpenAI Documentation Summary and Descriptions"
}
```

### Explanation
The changes made to the `index.yml` file for the Azure OpenAI Service documentation include minor updates to the summary and descriptions, enhancing the clarity and accuracy of the content provided.

1. **Modification in Summary**: The summary has been updated to replace "DALLE-3" with "GPT-image-1," reflecting a more accurate name for one of the models offered under the Azure OpenAI Service. This small change ensures proper identification of the models available to users.

2. **Refinement of Description**: Similarly, the description has undergone a modification to substitute "DALL-E 3" with "GPT-image-1." This change emphasizes an updated reference consistent with the summary.

3. **Terminology Update**: Additionally, the entry for "DALL-E" in the landing content section has been altered to "Image generation," which broadens the description to encompass a wider range of capabilities rather than limiting it to a specific model. 

These updates help streamline the documentation and make it more user-friendly, providing accurate and up-to-date information for users navigating the Azure OpenAI capabilities. Overall, such enhancements contribute to a clearer understanding of available resources and tools within the Azure OpenAI Service.

## articles/ai-services/openai/overview.md{#item-97d507}

<details>
<summary>Diff</summary>
````diff
@@ -52,7 +52,7 @@ Start with the [Create and deploy an Azure OpenAI Service resource](./how-to/cre
 
 ## Comparing Azure OpenAI and OpenAI
 
-Azure OpenAI Service gives customers advanced language AI with OpenAI GPT-4, GPT-3, Codex, DALL-E, speech to text, and text to speech models with the security and enterprise promise of Azure. Azure OpenAI co-develops the APIs with OpenAI, ensuring compatibility and a smooth transition from one to the other.
+Azure OpenAI Service gives customers advanced language AI with OpenAI GPT-4, GPT-3, Codex, GPT-image-1 (preview), DALL-E, speech to text, and text to speech models with the security and enterprise promise of Azure. Azure OpenAI co-develops the APIs with OpenAI, ensuring compatibility and a smooth transition from one to the other.
 
 With Azure OpenAI, customers get the security capabilities of Microsoft Azure while running the same models as OpenAI. Azure OpenAI offers private networking, regional availability, and responsible AI content filtering.  
 
@@ -129,7 +129,7 @@ Prompt construction can be difficult. In practice, the prompt acts to configure
 
 The service provides users access to several different models. Each model provides a different capability and price point.
 
-The DALL-E models (some in preview; see [models](./concepts/models.md#dall-e)) generate images from text prompts that the user provides.
+The image generation models (some in preview; see [models](./concepts/models.md#image-generation-models)) generate and edit images from text prompts that the user provides.
 
 The audio API models can be used to transcribe and translate speech to text. The text to speech models, currently in preview, can be used to synthesize text to speech.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarification of Model Names and Capabilities in Overview"
}
```

### Explanation
The recent modifications made to the `overview.md` file for the Azure OpenAI Service documentation primarily involve updates to model names and descriptions for improved clarity and accuracy.

1. **Model Name Update**: The term "DALL-E" has been replaced with "GPT-image-1 (preview)" in the context of the available models. This change provides a clearer indication of the model being referenced, ensuring that users are aware of the latest naming conventions and offerings.

2. **Refinement of Descriptions**: The mention of "DALL-E models" has also been updated to "image generation models," which broadens the context of the description. This new phrasing helps reduce ambiguity and encompasses the capabilities of multiple models that deal with image generation and editing based on text prompts.

3. **Continuity and Compatibility**: The description continues to highlight the integration of Azure OpenAI with OpenAI, emphasizing the seamless transition between the two services, a crucial point for users familiar with both platforms.

These updates enhance the documentation by ensuring it remains current and user-friendly, providing a more precise understanding of the capabilities available within the Azure OpenAI Service. By refining terminology and clarifying the features, the documentation improves the overall user experience and understanding of the service.

## articles/ai-services/openai/quotas-limits.md{#item-06c6f9}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.custom:
   - ignite-2023
   - references_regions
 ms.topic: conceptual
-ms.date: 4/14/2025
+ms.date: 04/23/2025
 ms.author: mbullwin
 ---
 
@@ -26,6 +26,7 @@ The following sections provide you with a quick guide to the default quotas and
 | Azure OpenAI resources per region per Azure subscription | 30 |
 | Default DALL-E 2 quota limits | 2 concurrent requests |
 | Default DALL-E 3 quota limits| 2 capacity units (6 requests per minute)|
+| Default GPT-image-1 quota limits | 2 capacity units (6 requests per minute) |
 | Default speech to text audio API quota limits | 3 requests per minute |
 | Maximum prompt tokens per request | Varies per model. For more information, see [Azure OpenAI Service models](./concepts/models.md)|
 | Max Standard deployments per resource | 32 |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Quotas and Limits for Azure OpenAI Service"
}
```

### Explanation
The recent changes made to the `quotas-limits.md` file for the Azure OpenAI Service documentation involve updating quota limits to include new model specifics and adjusting a date format for clarity.

1. **Date Update**: The modification includes a change in the documentation date from "4/14/2025" to "04/23/2025," ensuring that the content reflects the most recent update and assists users in understanding the currency of the information provided.

2. **Addition of GPT-image-1 Quota**: A new entry has been introduced to provide specific quota limits for the "GPT-image-1" model, stating that it has a limit of "2 capacity units (6 requests per minute)." This update is essential for users to understand the operational capacity available for this new image generation model.

3. **Clarification of Quota Limits**: The documentation maintains clear delineations of the quota limits for various models, including those for DALL-E 2 and DALL-E 3, thereby ensuring users have comprehensive details about the capacities and limitations that affect their use of Azure OpenAI services.

By incorporating these updates, the documentation enhances usability, making it easier for users to access relevant information about limits and quotas specific to the Azure OpenAI Service models. This ensures that users are better informed about their service usage and operational parameters.

## articles/ai-services/openai/toc.yml{#item-c945af}

<details>
<summary>Diff</summary>
````diff
@@ -36,7 +36,7 @@ items:
       displayName: ChatGPT, chatgpt
     - name: Vision-enabled chats
       href: gpt-v-quickstart.md
-    - name: DALL-E
+    - name: Image generation
       href: dall-e-quickstart.md
     - name: Use your data
       href: use-your-data-quickstart.md
@@ -128,7 +128,7 @@ items:
       displayName: cua, computer using model
     - name: Vision-enabled chats
       href: ./how-to/gpt-with-vision.md
-    - name: DALL-E
+    - name: Image generation
       href: ./how-to/dall-e.md
     - name: Function calling
       href: ./how-to/function-calling.md
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update in Table of Contents for Image Generation Reference"
}
```

### Explanation
The modifications made to the `toc.yml` file for the Azure OpenAI Service documentation involve a change in terminology and organization related to image generation capabilities.

1. **Terminology Update**: The term "DALL-E" has been replaced with "Image generation" in two respective sections of the table of contents. This change broadens the context, allowing for a more inclusive reference to image generation capabilities beyond just the DALL-E models, which may help users understand that there are multiple models or approaches available under this category.

2. **Actionable Links**: The links associated with the renamed topics remain unchanged. They point to relevant quick start guides for both image generation and use cases involving image capabilities. This maintains continuity in user navigation while aligning the names with current offerings.

3. **Consistency in Documentation**: By updating the terminology, the documentation aims for consistency in language used across various resources. This aids users in finding related information more easily and clarifies that image generation encompasses more than just one particular model.

These updates enhance the clarity and usability of the documentation by appropriately framing the content related to image generation services in Azure OpenAI, ensuring that users can access and understand the resources available for working with image generation more efficiently.

## articles/ai-services/openai/whats-new.md{#item-53303b}

<details>
<summary>Diff</summary>
````diff
@@ -21,6 +21,17 @@ This article provides a summary of the latest releases and major documentation u
 
 ## April 2025
 
+### GPT-image-1 released (preview, limited access)
+
+GPT-image-1 (2025-04-15) is the latest image generation model from Azure OpenAI. It features major improvements over DALL-E, including:
+- Better at responding to precise instructions.
+- Reliably renders text.
+- Accepts images as input, which enables the new capabilities of image editing and inpainting.
+
+Request access: [Limited access model application](https://aka.ms/oai/gptimage1access)
+
+Follow the [image generation how-to guide](/en-us/azure/ai-services/openai/how-to/dall-e) to get started with the new model.
+
 ### o4-mini and o3 models released
 
 `o4-mini` and `o3` models are now available. These are the latest reasoning models from Azure OpenAI offering significantly enhanced reasoning, quality, and performance. For more information, see the [getting started with reasoning models page](./how-to/reasoning.md).
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Introduction of GPT-image-1 and Updates on Model Releases"
}
```

### Explanation
The recent updates to the `whats-new.md` file for Azure OpenAI services include the introduction of a new image generation model and updates on the availability of existing models.

1. **Introduction of GPT-image-1**: A new section highlights the release of the GPT-image-1 model, which is currently in preview with limited access. This model is noted for significant advancements over previous iterations, particularly DALL-E, with capabilities such as:
   - Enhanced accuracy in following specific instructions.
   - Reliable rendering of text within images.
   - The ability to accept images as input, facilitating novel functionalities like image editing and inpainting.

2. **Access Information**: Users are provided with a direct link to apply for limited access to the GPT-image-1 model, making it straightforward for interested parties to engage with this new offering.

3. **Guidance on Utilization**: A referral to a how-to guide for image generation is included, ensuring users have a clear path to start using the new capabilities effectively.

4. **Updates on o4-mini and o3 Models**: The document also mentions the release of the `o4-mini` and `o3` models, which are the latest reasoning models from Azure OpenAI. These models promise enhanced reasoning, quality, and performance, thus enriching users' experience with Azure’s AI capabilities.

Overall, these updates significantly enhance the documentation by providing clear, actionable information regarding new features and model enhancements, allowing users to stay informed about the latest offerings from Azure OpenAI services.


