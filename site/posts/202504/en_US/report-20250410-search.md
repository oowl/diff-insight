---
date: '2025-04-10'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:24b31df...MicrosoftDocs:a9279bd
summary: "The recent updates to the Azure AI Search documentation include several\
  \ key modifications aimed at improving user experience and understanding. Noteworthy\
  \ changes consist of new visual aids, updates to API versions, clarified terminology,\
  \ and enhancements to user guides and tutorials. A significant breaking change is\
  \ the comprehensive revision of the \"What's New\" document, changing its structure\
  \ to focus on consolidated features rather than monthly updates. \n\nNew features\
  \ include visual improvements in tutorials, advanced vector search capabilities\
  \ with better compression, enhanced guidance for semantic configurations, and new\
  \ functionalities for indexing structured documents. \n\nAdditionally, there are\
  \ updates regarding API version consistency, improved clarity in tutorials, better\
  \ information on storage options for vector fields, and enhanced guidance for managing\
  \ search services through various command options.\n\nOverall, these updates signify\
  \ a strategic effort to make Azure AI Search documentation more accessible, informative,\
  \ and user-friendly, reflecting ongoing improvements in the platform's features\
  \ and usability."
title: '[en_US] Diff Insight Report - search'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:24b31df...MicrosoftDocs:a9279bd){target="_blank"}

# Highlights
The recent updates introduce a variety of modifications across multiple documents within the Azure AI Search documentation. Notable changes include the addition of new visual aids, updates to API versions, clarifications in terminology, and significant enhancements to user guides and tutorial content. One major breaking change involves a comprehensive revision of the "What's New" document, impacting the presentation and detail of updates. New features like improved vector search compression and capabilities, semantic configurations, and expanded support for customer-managed keys are highlighted.

## New features
- Addition of images across multiple tutorials and guides for visual clarity.
- Support for advanced features in vector search solutions, including improved compression and query handling.
- Enhanced documentation for semantic ranking and configurations with new insights and clearer instructions.
- Introduction of new skillsets and functionalities for indexing structured documents.

## Breaking changes
- A comprehensive rewrite of the "What's New" document introduces a thematic organizational approach, removing specific monthly update sections and focusing on consolidated features.
- Major updates to vector quantization processes with new rescoring techniques and detailed configuration guidance, impacting users' current implementations.

## Other updates
- API version updates and terminology standardization across documents for consistency and relevance.
- Enhanced tutorials with rephrased instructions, improved prerequisites clarity, and additional context for better user guidance.
- Updates to storage options and limitations for vector fields, ensuring users are better informed about their configuration choices.
- Improved guidance for managing search services via CLI, PowerShell, and REST APIs with updated procedures and notes.

# Insights
The updates reflect a continued effort to enhance the Azure AI Search documentation by prioritizing user understanding and engagement. Several new images and visual illustrations have been integrated into guides and tutorials to aid comprehension, a trend consistent with improving accessibility to complex technical concepts. Clarifications in prerequisite and procedure sections across tutorials represent a deliberate step towards minimizing confusion and ensuring that users can seamlessly follow through the documentation.

Breaking changes, notably in the "What's New" document, suggest a strategic shift in how information is presented, aiming to provide users with a concise yet comprehensive view of significant updates. By transitioning from monthly update details to a larger, thematic approach, users can focus on major improvements without being bogged down by less critical details.

Furthermore, the emphasis on new features and revisions in vector search and semantic configuration documents reflects ongoing developments to keep Azure AI Search at the forefront of search technology. These changes include enhanced documentation for vector field storage management, quantization options, and updated API references, aligning with the platform's growth and user needs for scalability, efficiency, and security. Overall, the update signifies substantial progress in making the platform more intuitive and resource-abundant for users.

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [.openpublishing.redirection.search.json](#item-8b66f9) | minor update | Update JSON formatting in search redirection file | modified | 1 | 1 | 2 | 
| [cognitive-search-attach-cognitive-services.md](#item-68eaec) | minor update | Clarification in cognitive services attachment instructions | modified | 1 | 1 | 2 | 
| [cognitive-search-skill-document-intelligence-layout.md](#item-62c06f) | minor update | Updates to Document Intelligence Layout skill documentation | modified | 8 | 7 | 15 | 
| [cognitive-search-tutorial-blob-dotnet.md](#item-ba889d) | minor update | Improvements to C# Skillsets Tutorial Documentation | modified | 13 | 13 | 26 | 
| [cognitive-search-tutorial-blob.md](#item-ff148f) | minor update | Enhancements to REST Skillsets Tutorial Documentation | modified | 10 | 10 | 20 | 
| [cognitive-search-tutorial-debug-sessions.md](#item-7e10e9) | minor update | Updates to Debug Skillsets Tutorial Documentation | modified | 27 | 27 | 54 | 
| [preview-generic.md](#item-51bbcc) | minor update | Minor Edits to Preview Generic Include File | modified | 4 | 4 | 8 | 
| [add-two-each.png](#item-56b26e) | new feature | Addition of Image for Search Capacity Planning | added | 0 | 0 | 0 | 
| [change-pricing-tier.png](#item-f71721) | new feature | Addition of Image for Changing Pricing Tier | added | 0 | 0 | 0 | 
| [initial-values.png](#item-465304) | new feature | Addition of Image for Initial Values | added | 0 | 0 | 0 | 
| [portal-notifications.png](#item-4bd098) | new feature | Addition of Image for Portal Notifications | added | 0 | 0 | 0 | 
| [pricing-tier-list.png](#item-ff6b12) | new feature | Addition of Image for Pricing Tier List | added | 0 | 0 | 0 | 
| [provisioning-status.png](#item-ede201) | new feature | Addition of Image for Provisioning Status | added | 0 | 0 | 0 | 
| [updating-message.png](#item-fc9f1b) | new feature | Addition of Image for Updating Message | added | 0 | 0 | 0 | 
| [portal-add-facetable-field.png](#item-d0b7a4) | new feature | Addition of Image for Adding Facetable Field in Portal | added | 0 | 0 | 0 | 
| [portal-facet-query.png](#item-57be1f) | new feature | Addition of Image for Facet Query in Portal | added | 0 | 0 | 0 | 
| [search-data-source.png](#item-e559ff) | new feature | Addition of Image for Data Source in SQL Database Indexing | added | 0 | 0 | 0 | 
| [service-creation-upgrade-metadata.png](#item-d1251d) | new feature | Addition of Image for Service Creation Upgrade Metadata | added | 0 | 0 | 0 | 
| [upgrade-button.png](#item-894e31) | new feature | Addition of Image for Upgrade Button | added | 0 | 0 | 0 | 
| [upgrade-confirmation.png](#item-880793) | new feature | Addition of Image for Upgrade Confirmation | added | 0 | 0 | 0 | 
| [upgrade-panel.png](#item-0c9673) | new feature | Addition of Image for Upgrade Panel | added | 0 | 0 | 0 | 
| [assign-key-vault.png](#item-e19e19) | new feature | Addition of Image for Key Vault Assignment | added | 0 | 0 | 0 | 
| [resource-training.md](#item-07788d) | minor update | Updates to Resource Training Document | modified | 2 | 3 | 5 | 
| [search-api-migration.md](#item-07297b) | minor update | Updates to Search API Migration Document | modified | 2 | 2 | 4 | 
| [search-api-preview.md](#item-511f5d) | minor update | Updates to Search API Preview Document | modified | 9 | 4 | 13 | 
| [search-capacity-planning.md](#item-0dd6c9) | minor update | Enhancements to Search Capacity Planning Document | modified | 80 | 32 | 112 | 
| [search-create-service-portal.md](#item-f90159) | minor update | Updates to Create Service Portal Documentation | modified | 6 | 6 | 12 | 
| [search-faceted-navigation-examples.md](#item-2b1158) | new feature | Faceted Navigation Examples Documentation Added | added | 721 | 0 | 721 | 
| [search-faceted-navigation.md](#item-f29d1e) | minor update | Updates Made to Faceted Navigation Article | modified | 112 | 167 | 279 | 
| [search-faq-frequently-asked-questions.yml](#item-eab2ba) | minor update | Updates to Azure AI Search FAQ Document | modified | 24 | 7 | 31 | 
| [search-how-to-index-sql-database.md](#item-86d873) | minor update | Updates to SQL Database Indexing Guide | modified | 50 | 39 | 89 | 
| [search-how-to-large-index.md](#item-d34e42) | minor update | Updates to Large Indexing Guide for Azure AI Search | modified | 4 | 4 | 8 | 
| [search-how-to-load-search-index.md](#item-a72573) | minor update | Updates to Data Loading Instructions in Azure AI Search | modified | 2 | 2 | 4 | 
| [search-how-to-semantic-chunking.md](#item-4a1d07) | minor update | Updates to Semantic Chunking Guide in Azure AI Search | modified | 7 | 5 | 12 | 
| [search-how-to-upgrade.md](#item-990225) | new feature | Guide for Upgrading Azure AI Search Services | added | 115 | 0 | 115 | 
| [search-howto-index-encrypted-blobs.md](#item-a7097a) | minor update | Updates to Tutorial on Indexing Encrypted Blobs in Azure AI Search | modified | 43 | 41 | 84 | 
| [search-howto-managed-identities-data-sources.md](#item-edf98d) | minor update | Updates to Managed Identities and Data Sources Configuration Document | modified | 6 | 6 | 12 | 
| [search-howto-reindex.md](#item-46738a) | minor update | Updates to Reindexing Procedure in Azure AI Search | modified | 3 | 3 | 6 | 
| [search-howto-run-reset-indexers.md](#item-fb10c8) | minor update | Enhancements to Run and Reset Indexers Documentation | modified | 26 | 20 | 46 | 
| [search-indexer-how-to-access-private-sql.md](#item-1bd4cc) | minor update | Updated API Version for Shared Private Link Resources | modified | 1 | 1 | 2 | 
| [search-indexer-howto-access-private.md](#item-73d30d) | minor update | Updates to Shared Private Link and Skillset Requirements | modified | 5 | 5 | 10 | 
| [search-indexer-tutorial.md](#item-a3e3ff) | minor update | Enhanced C# Tutorial for Indexing Azure SQL Data | modified | 54 | 58 | 112 | 
| [search-limits-quotas-capacity.md](#item-3b201a) | minor update | Updated Limits and Quotas for Azure AI Search | modified | 10 | 9 | 19 | 
| [search-manage-azure-cli.md](#item-7fdd08) | minor update | Clarifications and Updates to Azure CLI Management Guide | modified | 6 | 8 | 14 | 
| [search-manage-powershell.md](#item-3c3485) | minor update | Enhancements and Clarifications to PowerShell Management Guide | modified | 7 | 9 | 16 | 
| [search-manage-rest.md](#item-405ec7) | minor update | Improvements and Additions to REST Management Guide | modified | 63 | 39 | 102 | 
| [search-markdown-data-tutorial.md](#item-32ea2a) | minor update | Updates to Markdown Data Tutorial | modified | 34 | 30 | 64 | 
| [search-performance-tips.md](#item-218e77) | minor update | Updates to Performance Tips for Azure AI Search | modified | 9 | 9 | 18 | 
| [search-region-support.md](#item-25b0f1) | minor update | Updates to Azure AI Search Region Support Document | modified | 62 | 59 | 121 | 
| [search-reliability.md](#item-3e9b1a) | minor update | Updates to Search Reliability Document | modified | 6 | 6 | 12 | 
| [search-security-manage-encryption-keys.md](#item-db3487) | minor update | Updates to Manage Encryption Keys Document | modified | 6 | 6 | 12 | 
| [search-security-network-security-perimeter.md](#item-49c0d7) | minor update | Clarifications to Network Security Perimeter Document | modified | 3 | 3 | 6 | 
| [search-security-overview.md](#item-6b3f1e) | minor update | Update to CMK Support Information | modified | 1 | 1 | 2 | 
| [search-semi-structured-data.md](#item-d3388d) | minor update | Enhancements to the Semi-Structured Data Indexing Tutorial | modified | 25 | 25 | 50 | 
| [search-sku-manage-costs.md](#item-6e0122) | minor update | Adjustments to Cost Management for Azure AI Search | modified | 4 | 4 | 8 | 
| [search-sku-tier.md](#item-7686b8) | minor update | Revisions to Azure AI Search Service Tier Document | modified | 26 | 17 | 43 | 
| [search-synapseml-cognitive-services.md](#item-dcc36f) | minor update | Update to SynapseML Tutorial for Azure AI Search | modified | 39 | 39 | 78 | 
| [semantic-how-to-configure.md](#item-7a92a6) | minor update | Enhancements to Semantic Configuration Instructions | modified | 57 | 1 | 58 | 
| [semantic-how-to-enable-disable.md](#item-71ac1e) | minor update | Updates to Semantic Ranker Enable/Disable Guide | modified | 3 | 3 | 6 | 
| [semantic-how-to-query-request.md](#item-85530d) | minor update | Clarifications in Query Request for Semantic Ranking | modified | 2 | 2 | 4 | 
| [semantic-how-to-query-rewrite.md](#item-3e168f) | minor update | Enhancements in Query Rewrite Process Documentation | modified | 27 | 30 | 57 | 
| [semantic-search-overview.md](#item-b7497b) | minor update | Updates to Semantic Search Overview Documentation | modified | 2 | 2 | 4 | 
| [toc.yml](#item-c4768f) | minor update | Updates to Table of Contents for Search Documentation | modified | 10 | 6 | 16 | 
| [tutorial-create-custom-analyzer.md](#item-ad5520) | minor update | Revision of Custom Analyzer Tutorial | modified | 62 | 62 | 124 | 
| [tutorial-multiple-data-sources.md](#item-71558f) | minor update | Updates to Multiple Data Sources Tutorial | modified | 8 | 8 | 16 | 
| [tutorial-optimize-indexing-push-api.md](#item-ef0e96) | minor update | Enhancements to Indexing Using the Push API Tutorial | modified | 54 | 56 | 110 | 
| [tutorial-rag-build-solution-pipeline.md](#item-25ce01) | minor update | Updates to RAG Indexing Pipeline Tutorial | modified | 6 | 6 | 12 | 
| [vector-search-how-to-chunk-documents.md](#item-b79133) | minor update | Clarification and Updates in Document Chunking Techniques for Vector Search | modified | 4 | 2 | 6 | 
| [vector-search-how-to-quantization.md](#item-744f48) | breaking change | Major Revisions and Updates to Vector Quantization Process | modified | 186 | 46 | 232 | 
| [vector-search-how-to-storage-options.md](#item-ee1680) | minor update | Revision of Storage Options for Vector Fields in Azure AI Search | modified | 58 | 27 | 85 | 
| [vector-search-index-size.md](#item-bb2846) | minor update | Updates on Vector Index Size Limitations and Guidelines | modified | 5 | 23 | 28 | 
| [vector-search-overview.md](#item-56e5fa) | minor update | Clarification on Vector Index Quotas and Service Upgrades | modified | 1 | 1 | 2 | 
| [vector-store.md](#item-db9b8c) | minor update | Updates on Vector Storage Capacity and Service Management | modified | 4 | 4 | 8 | 
| [whats-new.md](#item-fa71b4) | breaking change | Major Rewrite and Update of What's New in Azure AI Search | modified | 67 | 125 | 192 | 


# Modified Contents
## articles/search/.openpublishing.redirection.search.json{#item-8b66f9}

<details>
<summary>Diff</summary>
````diff
@@ -382,4 +382,4 @@
             "redirect_document_id": false
         }
     ]
-}
\ No newline at end of file
+  }
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update JSON formatting in search redirection file"
}
```

### Explanation
The code diff represents a minor update to the JSON configuration file located at `articles/search/.openpublishing.redirection.search.json`. The modifications include a single addition and deletion, where a line at the end of the file was altered to ensure compliance with JSON formatting requirements. Specifically, an extra space was added before the closing curly brace (`}`) to maintain consistency, while also updating the file to include a newline at the end, which is best practice for JSON files. This change does not affect the functionality of the redirection but improves the file's structure.

## articles/search/cognitive-search-attach-cognitive-services.md{#item-68eaec}

<details>
<summary>Diff</summary>
````diff
@@ -50,7 +50,7 @@ Using the Azure portal or newer preview REST APIs and beta SDK packages, you can
 
 1. On your Azure AI services multi-service resource, [assign the identity](/azure/role-based-access-control/role-assignments-portal) to the **Cognitive Services User** role.
 
-1. Using the Azure portal, or the [Skillset 2024-11-01-preview REST API](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), or an Azure SDK beta package that provides the syntax, configure a skillset to use an identity:
+1. Using the Azure portal, or the [Skillset 2024-11-01-preview REST API](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or later, or an Azure SDK beta package that provides the syntax, configure a skillset to use an identity:
 
    + The managed identity used on the connection belongs to the search service. It can be system managed or user assigned.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarification in cognitive services attachment instructions"
}
```

### Explanation
The code diff indicates a minor update in the Markdown file `cognitive-search-attach-cognitive-services.md`. The modification involves a change in the wording of instructions regarding the configuration of a skillset using Azure services. Specifically, the phrasing was adjusted to clarify that the referenced REST API is valid for the version "2024-11-01-preview" and later, rather than exclusively for that specific version. This change enhances clarity for users by emphasizing the applicability of the instructions to future versions, providing better guidance in setting up skillsets with managed identities in Azure.

## articles/search/cognitive-search-skill-document-intelligence-layout.md{#item-62c06f}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - references_regions
   - ignite-2024
 ms.topic: reference
-ms.date: 02/13/2025
+ms.date: 04/07/2025
 ---
 
 # Document Layout skill
@@ -22,12 +22,13 @@ The **Document Layout** skill analyzes a document to extract regions of interest
 
 This article is the reference documentation for the Document Layout skill. For usage information, see [Structure-aware chunking and vectorization](search-how-to-semantic-chunking.md).
 
-The **Document Layout** skill calls the [Document Intelligence Public preview version 2024-07-31-preview](/rest/api/aiservices/operation-groups?view=rest-aiservices-v4.0%20(2024-07-31-preview)&preserve-view=true). It's currently only available in the following Azure regions:
+The **Document Layout** skill calls the [Document Intelligence Public preview version 2024-07-31-preview](/rest/api/aiservices/operation-groups?view=rest-aiservices-v4.0%20(2024-07-31-preview)&preserve-view=true). 
 
-+ East US
-+ West US2
-+ West Europe
-+ North Central US
+Supported regions varies by modality:
+
++ In code, your skillset can call Document Intelligence through an Azure AI multi-service resource in any region that provides both AI enrichment and Document Intelligence. See [Product availability by region](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/table) to find regions that provide both *AI enrichment* in Azure AI Search and *Document Intelligence* under Azure AI services.
+
++ In the [Import and vectorize data](search-import-data-portal.md) wizard in the Azure portal, you can enable document layout detection in the data source connection step. Document layout detection in the portal is available in the following Azure regions: **East US**, **West Europe**, **North Central US**. Create an Azure AI multi-service resource in one of these three regions to get the portal experience.
 
 Supported file formats include:
 
@@ -59,7 +60,7 @@ Microsoft.Skills.Util.DocumentIntelligenceLayoutSkill
 
 ## Supported languages
 
-Refer to [Azure AI Document Intelligence layout model supported languages](/azure/ai-services/document-intelligence/language-support/ocr?view=doc-intel-3.1.0&tabs=read-print%2Clayout-print%2Cgeneral#layout) for printed text.
+Refer to [Azure AI Document Intelligence layout model supported languages](/azure/ai-services/document-intelligence/language-support/ocr?view=doc-intel-3.1.0&tabs=read-print%2Clayout-print%2Cgeneral#layout&preserve-view=true) for printed text.
 
 ## Limitations
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Document Intelligence Layout skill documentation"
}
```

### Explanation
The code diff reflects a minor update to the Markdown file `cognitive-search-skill-document-intelligence-layout.md`. Several changes were made to enhance the clarity and accuracy of the documentation related to the Document Layout skill. Notable modifications include:

1. **Update of the Effective Date**: The date was changed from "02/13/2025" to "04/07/2025," indicating a more current timeline for the content's relevance.
  
2. **Clarification on Supported Regions**: The original list of supported Azure regions was removed and replaced with more detailed guidance about the regions where Document Intelligence can be used, emphasizing that availability varies by modality. It directs users to refer to the Azure global infrastructure for specifics.

3. **Improved Instructions on Portal Usage**: Additional explanations were provided about enabling document layout detection within the Azure portal, outlining that it is available specifically in the East US, West Europe, and North Central US regions.

4. **Link Corrections**: The link to the supported languages section was updated to ensure it remains functional and preserves context for users.

These revisions aim to provide clearer, more actionable guidance for users implementing the Document Layout skill in their applications within Azure, enhancing the user experience and documentation accuracy.

## articles/search/cognitive-search-tutorial-blob-dotnet.md{#item-ba889d}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Skillsets using C#'
+title: 'Tutorial: Skillsets Using C#'
 titleSuffix: Azure AI Search
 description: Use C# and the Azure SDK for .NET to create skillsets. This skillset applies AI transformations and analyses to create searchable content from images and unstructured text.
 
@@ -9,7 +9,7 @@ manager: nitinme
 
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 01/17/2025
+ms.date: 03/31/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
@@ -18,19 +18,17 @@ ms.custom:
 
 # C# Tutorial: Use skillsets to generate searchable content in Azure AI Search
 
-In this tutorial, learn how to use the [Azure SDK for .NET](https://www.nuget.org/packages/Azure.Search.Documents/) to create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
+Learn how to use the [Azure SDK for .NET](https://www.nuget.org/packages/Azure.Search.Documents/) to create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
 
-Skillsets add AI processing to raw content, making that content more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations: from image analysis, to natural language processing, to customized processing that you provide externally.
+Skillsets add AI processing to raw content, making it more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations, from image analysis to natural language processing to customized processing that you provide externally.
 
-This tutorial helps you learn how to:
+In this tutorial, you:
 
 > [!div class="checklist"]
 > + Define objects in an enrichment pipeline.
 > + Build a skillset. Invoke OCR, language detection, entity recognition, and key phrase extraction.
 > + Execute the pipeline. Create and load a search index.
-> + Check the results using full text search.
-
-If you don't have an Azure subscription, open a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
+> + Check the results using full-text search.
 
 ## Overview
 
@@ -42,16 +40,18 @@ Once content is extracted, the [skillset](cognitive-search-working-with-skillset
 
 ## Prerequisites
 
-+ [Visual Studio](https://visualstudio.microsoft.com/downloads/)
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+
++ [Azure Storage](/azure/storage/common/storage-account-create).
 
-+ [Azure.Search.Documents 11.x NuGet package](https://www.nuget.org/packages/Azure.Search.Documents) 
++ [Azure AI Search](search-create-app-portal.md).
 
-+ [Azure Storage](/azure/storage/common/storage-account-create)
++ [Azure.Search.Documents 11.x NuGet package](https://www.nuget.org/packages/Azure.Search.Documents).
 
-+ [Azure AI Search](search-create-app-portal.md)
++ [Visual Studio](https://visualstudio.microsoft.com/downloads/).
 
 > [!NOTE]
-> You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ### Download files
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Improvements to C# Skillsets Tutorial Documentation"
}
```

### Explanation
The code diff represents a minor update to the Markdown file `cognitive-search-tutorial-blob-dotnet.md`, focusing on refining the tutorial for using skillsets with C# in Azure AI Search. Key changes include:

1. **Title Capitalization**: The title has been updated for consistent capitalization. 

2. **Updated Date**: The modification date has been changed from "01/17/2025" to "03/31/2025," indicating a more accurate timeframe for the tutorial's relevance.

3. **Revised Learning Objectives**: The phrasing of several key sentences has been adjusted for grammatical consistency and clarity. For instance, introductory phrases have been modified to create a more direct invitation to the reader (e.g., "In this tutorial, you:").

4. **Clarified Prerequisites**: The section detailing prerequisites for following the tutorial has been reorganized. It now clearly lists necessary items, such as having an Azure account and initial requirements like Visual Studio and specific NuGet packages in a more structured manner.

5. **Minor Text Adjustments**: Punctuation and terms (like changing "full text search" to "full-text search") have been standardized for consistency throughout the document.

Overall, these changes improve the clarity, readability, and structure of the tutorial, ensuring that users can more easily engage with and implement Azure AI Search skillsets using C#.

## articles/search/cognitive-search-tutorial-blob.md{#item-ff148f}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Skillsets using REST'
+title: 'Tutorial: Skillsets Using REST'
 titleSuffix: Azure AI Search
 description: Use the Search REST APIs to create skillsets. This skillset applies AI transformations and analyses to create searchable content from images and unstructured text.
 
@@ -9,25 +9,23 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 01/17/2025
+ms.date: 03/31/2025
 ---
 
 # REST Tutorial: Use skillsets to generate searchable content in Azure AI Search
 
-In this tutorial, learn how to call REST APIs that create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
+Learn how to call REST APIs that create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
 
-Skillsets add AI processing to raw content, making that content more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations: from image analysis, to natural language processing, to customized processing that you provide externally.
+Skillsets add AI processing to raw content, making it more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations, from image analysis to natural language processing to customized processing that you provide externally.
 
-This tutorial helps you learn how to:
+In this tutorial, you:
 
 > [!div class="checklist"]
 > + Define objects in an enrichment pipeline.
 > + Build a skillset. Invoke OCR, language detection, entity recognition, and key phrase extraction.
 > + Execute the pipeline. Create and load a search index.
 > + Check the results using full text search.
 
-If you don't have an Azure subscription, open a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Overview
 
 This tutorial uses a REST client and the [Azure AI Search REST APIs](/rest/api/searchservice/) to create a data source, index, indexer, and skillset.
@@ -38,20 +36,22 @@ Once content is extracted, the [skillset](cognitive-search-working-with-skillset
 
 ## Prerequisites
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client)
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
 + [Azure Storage](/azure/storage/common/storage-account-create)
 
 + [Azure AI Search](search-create-app-portal.md)
 
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client)
+
 > [!NOTE]
-> You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ### Download files
 
 Download a zip file of the sample data repository and extract the contents. [Learn how](https://docs.github.com/get-started/start-your-journey/downloading-files-from-github).
 
-+ [Sample data files (mixed media)](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/ai-enrichment-mixed-media). 
++ [Sample data files (mixed media)](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/ai-enrichment-mixed-media).
 
 + [Sample REST file](https://github.com/Azure-Samples/azure-search-rest-samples/tree/main/skillset-tutorial)
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to REST Skillsets Tutorial Documentation"
}
```

### Explanation
The code diff shows a minor update to the Markdown file `cognitive-search-tutorial-blob.md`, which focuses on utilizing skillsets using REST APIs in Azure AI Search. Key updates include:

1. **Title Capitalization**: The title has been adjusted for consistent capitalization, enhancing readability.

2. **Updated Date**: The document's modification date has been updated from "01/17/2025" to "03/31/2025," ensuring users have the most current date for reference.

3. **Revised Phrasing**: The tutorial's introductory phrases have been slightly rephrased for clarity and direct engagement. For instance, it now uses "In this tutorial, you:" to clearly outline learning objectives.

4. **Clarified Prerequisites**: The prerequisites section has been reorganized, listing first the need for an Azure account and specific tools before introducing Visual Studio Code and its REST client. This structuring helps users quickly understand what they will need prior to diving into the tutorial.

5. **Content Standardization**: Various phrases have been standardized for punctuation, such as changing "full text search" to "full-text search" for consistency.

6. **Additional Resource Links**: The addition of a link to "Sample REST file" provides users with direct access to further materials, enhancing their learning experience.

These modifications aim to make the tutorial more user-friendly and clearer, helping users effectively learn how to create skillsets with REST APIs in Azure AI Search.

## articles/search/cognitive-search-tutorial-debug-sessions.md{#item-7e10e9}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Debug skillsets'
+title: 'Tutorial: Debug Skillsets'
 titleSuffix: Azure AI Search
 description: Practice creating and completing a debug session on an Azure AI Search skillset. This tutorial provides a buggy sample skillset that you resolve in a debug session.
 
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 12/03/2024
+ms.date: 03/31/2025
 ---
 
 # Tutorial: Fix a skillset using Debug Sessions
@@ -19,15 +19,15 @@ In Azure AI Search, a skillset coordinates the actions of skills that analyze, t
 
 **Debug Sessions** is an Azure portal tool that provides a holistic visualization of a skillset that executes on Azure AI Search. Using this tool, you can drill down to specific steps to easily see where an action might be falling down.
 
-In this article, use **Debug Sessions** to find and fix missing inputs and outputs. The tutorial is all-inclusive. It provides sample data, a REST file that creates objects, and instructions for debugging problems in the skillset.
-
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
+In this tutorial, you use **Debug Sessions** to find and fix missing inputs and outputs. The tutorial is all-inclusive. It provides sample data, a REST file that creates objects, and instructions for debugging problems in the skillset.
 
 ## Prerequisites
 
-+ Azure AI Search. [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription. You can use a free service for this tutorial. The free tier doesn't provide managed identity support for an Azure AI Search service. You must use keys for connections to Azure Storage.
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+
++ Azure AI Search. [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. You can use a free service for this tutorial. The Free tier doesn't provide managed identity support for an Azure AI Search service. You must use keys for connections to Azure Storage.
 
-+ Azure Storage account with [Blob storage](/azure/storage/blobs/), used for hosting sample data, and for persisting cached data created during a debug session. If you're using a free search service, the storage account must have shared access keys enabled and it must allow public network access.
++ Azure Storage account with [Blob storage](/azure/storage/blobs/), used for hosting sample data and for persisting cached data created during a debug session. If you're using a free search service, the storage account must have shared access keys enabled and it must allow public network access.
 
 + [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
@@ -44,29 +44,29 @@ This section creates the sample data set in Azure Blob Storage so that the index
 
 1. [Download sample data (clinical-trials-pdf-19)](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/_ARCHIVE/clinical-trials/clinical-trials-pdf-19), consisting of 19 files.
 
-1. [Create an Azure Storage account](/azure/storage/common/storage-account-create?tabs=azure-portal) or [find an existing account](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Storage%2storageAccounts/). 
+1. [Create an Azure Storage account](/azure/storage/common/storage-account-create?tabs=azure-portal) or [find an existing account](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Storage%2storageAccounts/).
 
    + Choose the same region as Azure AI Search to avoid bandwidth charges.
 
    + Choose the StorageV2 (general purpose V2) account type.
 
-1. Navigate to the Azure Storage services pages in the Azure portal and create a Blob container. Best practice is to specify the access level "private". Name your container `clinicaltrialdataset`.
+1. Go to the Azure Storage services pages in the Azure portal and create a Blob container. Best practice is to specify the access level "private". Name your container `clinicaltrialdataset`.
 
 1. In container, select **Upload** to upload the sample files you downloaded and unzipped in the first step.
 
-1. While in the Azure portal, copy the connection string for Azure Storage. You can get the connection string from **Settings** > **Access Keys** in the Azure portal.
+1. In the Azure portal, select **Settings** > **Access Keys** and copy the connection string for Azure Storage.
 
 ## Copy a key and URL
 
 This tutorial uses API keys for authentication and authorization. You need the search service endpoint and an API key, which you can get from the Azure portal.
 
-1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com), go to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
 1. Under **Settings** > **Keys**, copy an admin key. Admin keys are used to add, modify, and delete objects. There are two interchangeable admin keys. Copy either one.
 
    :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of the URL and API keys in the Azure portal.":::
 
-A valid API key establishes trust, on a per request basis, between the application sending the request and the search service handling it.
+A valid API key establishes trust, on a per-request basis, between the application sending the request and the search service handling it.
 
 ## Create data source, skillset, index, and indexer
 
@@ -82,9 +82,9 @@ In this section, create a "buggy" workflow that you can fix in this tutorial.
 
 ## Check results in the Azure portal
 
-The sample code intentionally creates a buggy index as a consequence of problems that occurred during skillset execution. The problem is that the index is missing data. 
+The sample code intentionally creates a buggy index as a consequence of problems that occurred during skillset execution. The problem is that the index is missing data.
 
-1. In Azure portal, on the search service **Overview** page, select the **Indexes** tab. 
+1. In Azure portal, on the search service **Overview** page, select the **Indexes** tab.
 
 1. Select *clinical-trials*.
 
@@ -98,15 +98,15 @@ The sample code intentionally creates a buggy index as a consequence of problems
 
 1. Run the query. You should see empty values for `organizations` and `locations`.
 
-    These fields should have been populated through the skillset's [Entity Recognition skill](cognitive-search-skill-entity-recognition-v3.md), used to detect organizations and locations anywhere within the blob's content. In the next exercise, you'll debug the skillset to determine what went wrong.
+    These fields should have been populated through the skillset's [Entity Recognition skill](cognitive-search-skill-entity-recognition-v3.md), used to detect organizations and locations anywhere within the blob's content. In the next exercise, you debug the skillset to determine what went wrong.
 
 Another way to investigate errors and warnings is through the Azure portal.
 
-1. Open the **Indexers** tab and select *clinical-trials-idxr*.
+1. On the **Indexers** tab, select *clinical-trials-idxr*.
 
    Notice that while the indexer job succeeded overall, there were warnings.
 
-1. Select **Success** to view the warnings (if there were mostly errors, the detail link would be **Failed**). You'll see a long list of every warning emitted by the indexer.
+1. Select **Success** to view the warnings. If there are mostly errors, the detail link is **Failed**. You should see a long list of every warning emitted by the indexer.
 
    :::image type="content" source="media/cognitive-search-debug/portal-indexer-errors-warnings.png" alt-text="Screenshot of view warnings." :::
 
@@ -116,27 +116,27 @@ Another way to investigate errors and warnings is through the Azure portal.
 
 1. Select **+ Add Debug Session**.
 
-1. Give the session a name. 
+1. Give the session a name.
 
 1. In Indexer template, provide the indexer name. The indexer has references to the data source, the skillset, and index.
 
 1. Select the storage account.
 
-1. **Save** the session. 
+1. **Save** the session.
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-create-session.png" lightbox="media/cognitive-search-debug/debug-tutorial-create-session.png" alt-text="Screenshot of Debug session definition page." :::
   
 1. A debug session opens to the settings page. You can make modifications to the initial configuration and override any defaults. A debug session only works with a single document. The default is to accept the first document in the collection as the basis of your debug sessions. You can [choose a specific document to debug](cognitive-search-how-to-debug-skillset.md#create-a-debug-session) by providing its URI in Azure Storage.
 
-1. When the debug session has finished initializing, you should see a skills workflow with mappings and a search index. The enriched document data structure appears in a details pane on the side. We excluded it from the following screenshot so that you could see more of the workflow.
+1. When the debug session finishes initializing, you should see a skills workflow with mappings and a search index. The enriched document data structure appears in a details pane on the side. We excluded it from the following screenshot so that you could see more of the workflow.
 
    :::image type="content" source="media/cognitive-search-debug/debug-execution-complete1.png" lightbox="media/cognitive-search-debug/debug-execution-complete1.png" alt-text="Screenshot of Debug Session visual editor." :::
 
 ## Find issues with the skillset
 
 Any issues reported by the indexer are indicated as **Errors** and **Warnings**.
 
-Notice that the number of errors and warning is a much smaller list than the one displayed earlier because this list is only detailing the errors for a single document. Like the list displayed by the indexer, you can select on a warning message and see the details of this warning.
+Notice that the number of errors and warnings is a smaller list than the one displayed earlier because this list is only detailing the errors for a single document. Like the list displayed by the indexer, you can select on a warning message and see the details of this warning.
 
 Select **Warnings** to review the notifications. You should see four:
 
@@ -163,7 +163,7 @@ Because all four notifications are about this skill, your next step is to debug
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-skill-detail.png" alt-text="Screenshot of the skill details pane.":::
 
-1. Hover over each input (or select an input) to show the values in the **Expression evaluator**. Notice that the displayed result for this input doesn’t look like a text input. It looks like a series of new line characters `\n \n\n\n\n` instead of text. The lack of text means that no entities can be identified, so either this document fails to meet the prerequisites of the skill, or there's another input that should be used instead.
+1. Hover over each input (or select an input) to show the values in the **Expression evaluator**. Notice that the displayed result for this input doesn’t look like a text input. It looks like a series of new line characters `\n \n\n\n\n` instead of text. The lack of text means that no entities can be identified. Either this document doesn't meet the prerequisites of the skill, or there's another input that should be used instead.
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-skill-input-null.png" alt-text="Screenshot of skill input showing null values.":::
 
@@ -177,7 +177,7 @@ Because all four notifications are about this skill, your next step is to debug
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-edit-skill.png" alt-text="Screenshot of Expression Evaluator for fixed merged_content input." :::
 
-1. Select **Run** in the session's window menu. This kicks off another execution of the skillset using the document. 
+1. Select **Run** in the session's window menu. This kicks off another execution of the skillset using the document.
 
 1. Once the debug session execution completes, notice that the warnings count has reduced by one. Warnings show that the error for text input is gone, but the other warnings remain. The next step is to address the warning about the missing or empty value `/document/languageCode`.
 
@@ -221,11 +221,11 @@ The messages say to check the 'outputFieldMappings' property of your indexer, so
 
 1. Select **Run**.
 
-All of the errors have been resolved.
+All of the errors are resolved.
 
 ## Commit changes to the skillset
 
-When the debug session was initiated, the search service created a copy of the skillset. This was done to protect the original skillset on your search service. Now that you have finished debugging your skillset, the fixes can be committed (overwrite the original skillset). 
+When the debug session was initiated, the search service created a copy of the skillset. This was done to protect the original skillset on your search service. Now that you debugged your skillset, the fixes can be committed (overwrite the original skillset).
 
 Alternatively, if you aren't ready to commit changes, you can save the debug session and reopen it later.
 
@@ -243,7 +243,7 @@ Alternatively, if you aren't ready to commit changes, you can save the debug ses
 
 1. Select **Refresh** to show the status of the reset and run commands.
 
-When the indexer has finished running, there should be a green checkmark and the word Success next to the time stamp for the latest run in the **Execution history** tab. To ensure that the changes have been applied:
+When the indexer finishes running, there should be a green checkmark and the word Success next to the time stamp for the latest run in the **Execution history** tab. To ensure that the changes are applied:
 
 1. In the left pane, open **Indexes**.
 
@@ -263,7 +263,7 @@ The free service is limited to three indexes, indexers, and data sources. You ca
 
 ## Next steps
 
-This tutorial touched on various aspects of skillset definition and processing. To learn more about concepts and workflows, refer to the following articles:
+This tutorial touched on various aspects of skillset definition and processing. To learn more about concepts and workflows, see the following articles:
 
 + [How to map skillset output fields to fields in a search index](cognitive-search-output-field-mapping.md)
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Debug Skillsets Tutorial Documentation"
}
```

### Explanation
The code diff indicates a minor update to the Markdown file `cognitive-search-tutorial-debug-sessions.md`, which serves as a tutorial for debugging skillsets in Azure AI Search. Key modifications include:

1. **Title Capitalization**: The title has been changed for consistent capitalization, improving the professional appearance of the document.

2. **Revised Modification Date**: The date has been updated from "12/03/2024" to "03/31/2025," reflecting the latest changes.

3. **Clarified Tutorial Overview**: Phrasing in the introduction has been revised for clarity. It now directly states that users will utilize **Debug Sessions** to find and fix issues, enhancing the tutorial's instructive tone.

4. **Prerequisites Section Improvement**: The prerequisites have been reorganized, clearly stating that users need an active Azure subscription before outlining the creation of Azure services and storage accounts.

5. **Consistency in Phrasing**: Slight adjustments in wording and punctuation have been made throughout the document for consistency, such as changing "per request basis" to "per-request basis" and ensuring uniform list formatting.

6. **New Links and References**: The update introduces clearer instructions and additional resources like links to create services and accounts, and outlines important steps for managing Azure resources effectively.

7. **Detailed Debugging Steps**: Steps in debugging have been clarified for better readability, with a consistent numbering format that improves the instructional flow, allowing users to follow along effortlessly.

Overall, these changes serve to improve the clarity, consistency, and usability of the tutorial on debugging skillsets, making it more accessible for users working with Azure AI Search.

## articles/search/includes/previews/preview-generic.md{#item-51bbcc}

<details>
<summary>Diff</summary>
````diff
@@ -1,13 +1,13 @@
 ---
-title: include file
-description: include file
+title: Include file
+description: Include file
 author: eric-urban
 ms.author: eur
 ms.service: azure-ai-speech
 ms.topic: include
-ms.date: 9/20/2024
+ms.date: 03/19/2024
 ms.custom: include, ignite-2024
 ---
 
 > [!NOTE]
-> This feature is currently in public preview. This preview is provided without a service-level agreement, and is not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
+> This feature is currently in public preview. This preview is provided without a service-level agreement and isn't recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Minor Edits to Preview Generic Include File"
}
```

### Explanation
The code diff presents a minor update to the Markdown file `preview-generic.md`, which serves as an include file for generic previews in Azure AI documentation. Key modifications include:

1. **Title and Description Capitalization**: Both the title and description have been updated to capitalize the first letter, enhancing the professional appearance of the document.

2. **Updated Modification Date**: The date has been changed from "9/20/2024" to "03/19/2024," reflecting a more accurate timestamp for the document's contents.

3. **Phrasing Adjustments**: The text in the note regarding the public preview of the feature has been slightly restructured. The phrase "and is not recommended" has been changed to "and isn't recommended," making the tone more conversational and direct, while maintaining clarity.

4. **Overall Consistency**: The changes made contribute to a more coherent and visually consistent document, which can aid in reader comprehension and improve the document's usability.

These updates are geared towards refining the presentation and ensuring the document accurately reflects the current status and recommendations regarding the public preview feature in Azure AI services.

## articles/search/media/search-capacity-planning/add-two-each.png{#item-56b26e}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Search Capacity Planning"
}
```

### Explanation
The code diff indicates the addition of a new image file named `add-two-each.png` to the `search-capacity-planning` media folder within the Azure AI documentation. The specific aspects of this modification include:

1. **File Addition**: A new image file has been introduced, enhancing the visual components of the documentation. This could serve to illustrate concepts related to search capacity planning, making it more accessible for users.

2. **No Modifications to Existing Content**: As this is an addition, there are no deletions or changes to the existing documentation content. The image will complement current informational materials rather than alter them.

3. **Potential Usage**: The added image is likely intended to provide visual support for explanations or processes discussed in the relevant articles, aiding users in understanding capacity planning in the context of Azure AI services.

Overall, this update contributes to the enhancement of the documentation by providing graphical content, which can improve user engagement and comprehension.

## articles/search/media/search-capacity-planning/change-pricing-tier.png{#item-f71721}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Changing Pricing Tier"
}
```

### Explanation
The code diff highlights the addition of a new image file named `change-pricing-tier.png` to the `search-capacity-planning` media directory in the Azure AI documentation. Key points regarding this modification include:

1. **New Visual Resource**: This image file has been included to assist in visual explanations related to changing the pricing tier within the context of search capacity planning. The introduction of such media can enhance the understanding of complex concepts for users.

2. **No Impact on Existing Content**: There are no deletions or modifications to current documentation as this is a standalone addition. The existing materials remain unchanged, providing users with additional resources without altering any previous content.

3. **Enhancement of Documentation**: The addition of this image aims to complement the textual explanations, which can make the documentation more user-friendly and accessible, as visual aids typically help clarify instructions and processes.

Overall, this update serves to enrich the documentation with visual content, thereby improving the reader's comprehension of pricing tier adjustments in search capacity planning within the Azure AI framework.

## articles/search/media/search-capacity-planning/initial-values.png{#item-465304}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Initial Values"
}
```

### Explanation
The code diff indicates the addition of a new image file titled `initial-values.png` to the `search-capacity-planning` media folder in the Azure AI documentation. The main points of this modification are as follows:

1. **Introduction of New Visual Content**: This image has been added to provide a visual reference related to initial values in the context of search capacity planning. Such graphical elements support readers in better understanding the associated topics.

2. **No Alteration to Existing Documentation**: There are no deletions or changes to previous content, as this addition does not affect existing elements of the documentation. It simply enhances the overall resources available to users.

3. **Support for Clarification**: By including this image, the documentation aims to clarify concepts tied to initial values, which can be particularly beneficial for those working with Azure AI services. Visual representations can often simplify complex information for learners.

In summary, the addition of `initial-values.png` enriches the Azure AI documentation by providing supplementary visual assistance, thereby potentially increasing user comprehension regarding initial values in search capacity planning.

## articles/search/media/search-capacity-planning/portal-notifications.png{#item-4bd098}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Portal Notifications"
}
```

### Explanation
The code diff details the addition of a new image file named `portal-notifications.png` to the `search-capacity-planning` media directory within the Azure AI documentation. The key aspects of this update include:

1. **New Visual Element**: This image has been incorporated to visually represent portal notifications, which can help users identify and understand notifications related to search capacity planning in the Azure platform.

2. **No Impact on Existing Documentation**: The addition does not include any deletions or changes to current documentation, ensuring that existing content remains intact while enhancing the resources available for user reference.

3. **Enhancement of User Experience**: By providing visual content, the documentation aims to improve the clarity of instructions and notifications that users may encounter. This enhances the overall user experience, especially for those who benefit from visual aids in understanding workflows and features.

In conclusion, the introduction of `portal-notifications.png` contributes to the Azure AI documentation by offering a visual tool that aids user comprehension of portal notifications related to search capacity planning, without altering any existing material.

## articles/search/media/search-capacity-planning/pricing-tier-list.png{#item-ff6b12}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Pricing Tier List"
}
```

### Explanation
The code diff presents the addition of a new image file named `pricing-tier-list.png` in the `search-capacity-planning` media folder of the Azure AI documentation. The important details regarding this modification are:

1. **Introduction of a New Image**: This file serves as a visual aid that outlines the various pricing tiers available for Azure AI services. It assists users in comparing and understanding the different pricing options.

2. **No Modifications to Existing Content**: There are no deletions or changes to existing documentation, allowing the current content to remain unaffected while enhancing it with this additional resource.

3. **Improvement in Clarity**: The visual representation of the pricing tier list aims to clarify the differences between various pricing models, making it easier for users to make informed decisions based on their needs and budgeting.

In summary, the addition of `pricing-tier-list.png` enriches the Azure AI documentation by providing critical visual support for users seeking to understand pricing tiers related to search capacity planning, thereby improving overall usability without altering any prior material.

## articles/search/media/search-capacity-planning/provisioning-status.png{#item-ede201}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Provisioning Status"
}
```

### Explanation
The code diff indicates the addition of a new image file titled `provisioning-status.png` within the `search-capacity-planning` media directory of the Azure AI documentation. The key points regarding this modification are:

1. **New Resource Introduction**: This image is intended to visually depict the status of provisioning within the Azure AI services. It can significantly aid users in understanding the various stages of provisioning related to their resources.

2. **No Alteration of Existing Content**: The addition does not involve any deletions or changes to existing files or documentation, ensuring that current resources remain unchanged while providing supplementary visual content.

3. **Enhancement of User Understanding**: By including a visual representation of the provisioning status, the documentation enhances clarity for users navigating the provisioning process, making it easier to comprehend where their resources stand during setup.

In conclusion, the `provisioning-status.png` image enriches the Azure AI documentation by offering valuable visual context about provisioning status in search capacity planning, improving user understanding without affecting any existing content.

## articles/search/media/search-capacity-planning/updating-message.png{#item-fc9f1b}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Updating Message"
}
```

### Explanation
The code diff showcases the addition of a new image file named `updating-message.png` in the `search-capacity-planning` media directory of the Azure AI documentation. The key aspects of this modification are:

1. **New Visual Aid Introduction**: This image is designed to illustrate the updating message for users as they interact with Azure AI services. It serves to enhance user experience by providing visual feedback during updates.

2. **No Changes to Existing Documentation**: The modification consists solely of this new image, with no deletions or alterations made to existing content, thus ensuring that established documentation remains intact.

3. **Improvement in User Engagement**: By providing a visual reference for the updating message, the documentation aims to improve user engagement and clarity regarding the update process, allowing users to better understand what to expect during updates.

In summary, the `updating-message.png` image enhances the Azure AI documentation by offering a valuable illustration of the updating message feature in search capacity planning, improving user experience without modifying any of the existing materials.

## articles/search/media/search-faceted-navigation/portal-add-facetable-field.png{#item-d0b7a4}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Adding Facetable Field in Portal"
}
```

### Explanation
The code diff indicates the addition of a new image file called `portal-add-facetable-field.png` located in the `search-faceted-navigation` media directory of the Azure AI documentation. The main points regarding this modification are:

1. **Introduction of Visual Content**: This image is intended to illustrate the process of adding a facetable field within the portal, providing users with a visual reference to enhance their understanding of the feature.

2. **No Impact on Existing Documentation**: The addition does not result in any changes or deletions of current files; it simply adds to the existing resources, maintaining the integrity of the documentation.

3. **Enhancement of User Guidance**: By including this visual guide, the documentation aims to facilitate better comprehension for users navigating the faceted navigation features in Azure AI, thereby supporting them in utilizing these tools more effectively.

In conclusion, the `portal-add-facetable-field.png` image enriches the Azure AI documentation by providing a practical visual representation of how to add a facetable field in the portal, thereby improving user clarity and support without altering existing content.

## articles/search/media/search-faceted-navigation/portal-facet-query.png{#item-57be1f}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Facet Query in Portal"
}
```

### Explanation
The code diff reveals the addition of a new image file named `portal-facet-query.png` in the `search-faceted-navigation` media directory of the Azure AI documentation. The primary aspects of this modification include:

1. **Visual Aid Implementation**: This image serves to illustrate how to perform a facet query within the Azure portal, offering users a visual guide to enhance their understanding of the functionality.

2. **Integrity of Existing Documentation**: The addition of this image does not result in any deletions or modifications to the existing files, thereby preserving the established content and structure of the documentation.

3. **Improvement in User Instruction**: By incorporating this image, the documentation aims to facilitate a clearer understanding of performing facet queries, thereby assisting users in effectively utilizing these features when navigating within Azure AI.

In summary, the `portal-facet-query.png` image enhances the Azure AI documentation by providing a visual representation of how to execute a facet query in the portal, ultimately improving user comprehension and guidance while keeping the existing documentation intact.

## articles/search/media/search-how-to-index-sql-database/search-data-source.png{#item-e559ff}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Data Source in SQL Database Indexing"
}
```

### Explanation
The code diff indicates the addition of a new image file entitled `search-data-source.png`, located within the `search-how-to-index-sql-database` media directory of the Azure AI documentation. The key aspects surrounding this modification are:

1. **Visual Representation Addition**: This image is designed to visually instruct users on how to set up a data source for indexing a SQL database within Azure AI, enhancing user understanding of the process.

2. **No Alteration to Current Documentation**: The addition of this image does not involve any changes or deletions of existing files; it complements the current content without affecting its structure.

3. **User Guidance Enhancement**: Incorporating this visual element is intended to improve user guidance, making it easier for users to grasp the steps necessary for configuring a data source in the context of SQL database indexing.

In summary, the `search-data-source.png` image enriches the Azure AI documentation by providing a visual guide for users on how to configure a data source for indexing SQL databases, thereby facilitating a clearer understanding of the process while maintaining the integrity of existing documentation.

## articles/search/media/search-how-to-upgrade/service-creation-upgrade-metadata.png{#item-d1251d}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Service Creation Upgrade Metadata"
}
```

### Explanation
The code diff signifies the addition of a new image file named `service-creation-upgrade-metadata.png` to the `search-how-to-upgrade` media directory within the Azure AI documentation. The main points regarding this modification include:

1. **Illustrative Content Introduction**: This image serves as a visual aid, providing users with guidance on the metadata involved in the service creation upgrade process. It aims to enhance clarity and understanding of this specific topic.

2. **No Impact on Existing Documentation**: The addition is clean and does not involve any deletions or modifications to previous content, ensuring the overall structure and integrity of the documentation remain intact.

3. **Enhanced User Experience**: By incorporating this visual representation, the documentation aims to improve users' ability to follow the upgrade process for service creation effectively, thereby contributing to a more intuitive understanding of the subject matter.

In summary, the `service-creation-upgrade-metadata.png` image enhances the Azure AI documentation by providing a useful visual reference for users navigating the service creation upgrade process, ultimately supporting better user comprehension while preserving the existing content framework.

## articles/search/media/search-how-to-upgrade/upgrade-button.png{#item-894e31}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Upgrade Button"
}
```

### Explanation
The code diff indicates the addition of a new image file called `upgrade-button.png` in the `search-how-to-upgrade` media directory within the Azure AI documentation. Key points about this modification include:

1. **Visual Aid Addition**: This image provides users with a clear visual representation of the upgrade button, which is a crucial element in the upgrade process for Azure AI services. It helps users identify the button they need to interact with during the upgrade procedure.

2. **No Changes to Existing Content**: The addition does not result in any alterations or removals of current documentation content, maintaining the stability of the existing materials while enhancing them with this visual asset.

3. **Improving User Guidance**: By adding this image, the documentation aims to facilitate a better user experience by making instructions more accessible and easier to follow, thereby supporting efficient service upgrades.

In summary, the `upgrade-button.png` image enriches the Azure AI documentation by providing a visual guide to the upgrade button used in the upgrade process, thus assisting users in navigating the procedure more effectively while leaving existing content unchanged.

## articles/search/media/search-how-to-upgrade/upgrade-confirmation.png{#item-880793}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Upgrade Confirmation"
}
```

### Explanation
The code diff shows the addition of a new image file named `upgrade-confirmation.png` to the `search-how-to-upgrade` media directory within the Azure AI documentation. The key points of this modification are as follows:

1. **Visual Reference for Confirmation**: This image visually represents the upgrade confirmation, an important step in the upgrade process. It assists users by clarifying what to expect upon successful completion of the upgrade.

2. **No Impact on Existing Documentation**: The addition of this image does not involve any deletions or modifications to prior content, preserving the integrity of the existing documentation while enhancing it with visual support.

3. **Enhanced User Support**: By including this confirmation image, the documentation improves user guidance, making it easier for users to confirm that the upgrade has been executed correctly and to recognize completion of the process.

In summary, the `upgrade-confirmation.png` image enhances the Azure AI documentation by providing users with a visual guide to the upgrade confirmation step, which aids understanding and assures that users can accurately identify when the upgrade has been successfully completed, all without altering the current documentation structure.

## articles/search/media/search-how-to-upgrade/upgrade-panel.png{#item-0c9673}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Upgrade Panel"
}
```

### Explanation
The code diff reflects the addition of a new image file named `upgrade-panel.png` to the `search-how-to-upgrade` media directory within the Azure AI documentation. The key highlights of this modification are:

1. **Illustration of the Upgrade Panel**: This image provides a visual representation of the upgrade panel, which is integral to the upgrade process. It aids users by helping them identify the upgrade panel interface they will interact with.

2. **No Changes to Existing Content**: The addition does not modify or delete any existing documentation content, ensuring the stability of current resources while supplementing them with this helpful visual element.

3. **Improved User Experience**: By incorporating this visual, the documentation enhances user support, making instructions clearer and more approachable. Users can easily refer to the image to confirm they are in the correct section during the upgrade process.

In summary, the `upgrade-panel.png` image enriches the Azure AI documentation by providing users with a clear visual guide of the upgrade panel, facilitating a better understanding of the upgrade process while maintaining the integrity of existing documentation content.

## articles/search/media/search-security-manage-encryption-keys/assign-key-vault.png{#item-e19e19}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Addition of Image for Key Vault Assignment"
}
```

### Explanation
The code diff highlights the addition of a new image file named `assign-key-vault.png` to the `search-security-manage-encryption-keys` media directory within the Azure AI documentation. The main points regarding this modification include:

1. **Visual Aid for Key Vault Assignment**: The new image visually illustrates the process of assigning a key vault, which is crucial for managing encryption keys within Azure. This aids users in understanding the steps involved in the assignment process.

2. **Maintained Documentation Integrity**: This addition does not affect any existing documentation as there are no deletions or modifications. It enhances the documentation by providing additional context without disrupting current content.

3. **Enhanced Clarity for Users**: By adding this visual element, the documentation becomes more user-friendly, helping users better comprehend the key vault assignment process. This visual reference allows users to align their actions with the corresponding interface shown in the image.

In summary, the `assign-key-vault.png` image enriches the Azure AI documentation by adding a visual representation of the key vault assignment process, improving user understanding and interaction with encryption key management while keeping the existing documentation intact.

## articles/search/resource-training.md{#item-07788d}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ author: haileytap
 ms.author: haileytapia
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 10/28/2024
+ms.date: 04/07/2025
 ---
 
 # Training - Azure AI Search
@@ -27,10 +27,9 @@ Learning paths are a collection of training modules that are organized around sp
 
 | Module | Learning path |
 |--------|---------------|
-[Fundamentals of Knowledge Mining and Azure AI Search](/training/modules/intro-to-azure-search/) | [Microsoft Azure AI Fundamentals:](/training/paths/document-intelligence-knowledge-mining/) |
+[Fundamentals of Knowledge Mining and Azure AI Search](/training/modules/intro-to-azure-search/) | [Microsoft Azure AI Fundamentals](/training/paths/document-intelligence-knowledge-mining/) |
 | [Create an Azure AI Search solution](/training/modules/create-azure-cognitive-search-solution/) | [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
 | [Create a custom skill for Azure AI Search](/training/modules/create-azure-ai-custom-skill/) | [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
-| [Build an Azure Machine Learning custom skill for Azure AI Search](/training/modules/build-azure-machine-learn-custom-skill-for-azure-cognitive-search/) | |
 | [Enrich your data with Azure AI Language](/training/modules/enrich-search-index-using-language-studio/) | |
 | [Create a knowledge store with Azure AI Search](/training/modules/create-knowledge-store-azure-cognitive-search/) | [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
 | [Implement advanced search features in Azure AI Search](/training/modules/implement-advanced-search-features-azure-cognitive-search/)| [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Resource Training Document"
}
```

### Explanation
The code diff indicates a minor update to the `resource-training.md` file in the Azure AI documentation. This update involves both changes to content and metadata. The main aspects of this modification are:

1. **Date Update**: The modification includes an update of the document's date from "10/28/2024" to "04/07/2025." This likely reflects new revisions or an anticipated update timeline for the training content associated with Azure AI Search.

2. **Rephrasing in Learning Path Table**: There are minor adjustments in the learning paths table within the document. The title "Microsoft Azure AI Fundamentals:" has been changed to "Microsoft Azure AI Fundamentals" by removing the colon. While this is a small edit, it may improve consistency and readability.

3. **Content Modifications**: The diff shows that the content has been slightly reorganized, with three lines being deleted and two new lines added. The changes mainly focus on the structure of the training modules and their associated learning paths, which contribute to a clearer and more efficient presentation for users.

Overall, this update enhances the clarity and accuracy of the training document for Azure AI Search, making it more user-friendly while keeping the details current.

## articles/search/search-api-migration.md{#item-07297b}

<details>
<summary>Diff</summary>
````diff
@@ -12,7 +12,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 02/14/2025
+ms.date: 03/10/2025
 ---
 
 # Upgrade to the latest REST API in Azure AI Search
@@ -24,7 +24,7 @@ Use this article to migrate to newer versions of the [**Search Service REST APIs
 | Data plane | [`2024-07-01`](/rest/api/searchservice/search-service-api-versions#2024-07-01) | Stable |
 | Data plane | [`2024-11-01-preview`](/rest/api/searchservice/search-service-api-versions#2024-11-01-preview) | Preview |
 | Control plane | [`2023-11-01`](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2023-11-0&preserve-view=true1) | Stable |
-| Control plane | [`2024-03-01-preview`](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true) | Preview |
+| Control plane | [`2025-02-01-preview`](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true) | Preview |
 
 Upgrade instructions focus on code changes that get you through breaking changes from previous versions so that existing code runs the same as before, but on the newer API version. Once your code is in working order, you can decide whether to adopt newer features. To learn more about new features, see [vector code samples](https://github.com/Azure/azure-search-vector-samples) and [What's New](whats-new.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Search API Migration Document"
}
```

### Explanation
The code diff showcases a minor update to the `search-api-migration.md` file, which provides guidance for migrating to the latest REST API in Azure AI Search. The key points of this modification are:

1. **Date Revision**: The date of the document has been changed from "02/14/2025" to "03/10/2025." This likely updates the timeline for users, reflecting when the relevant content or guidance is expected to be current.

2. **Revision in Control Plane API Versions**: The documentation includes an update of the preview version for the control plane API. The original entry for the `2024-03-01-preview` has been replaced with `2025-02-01-preview`. This change ensures users have access to the most up-to-date information regarding the API versions they might utilize during migration.

3. **Incremental Content Changes**: Overall, there are minor structural changes, with two lines added and two lines removed. These streamline the information presented about API versions and enhance clarity for users migrating to the newer API.

In conclusion, the updates made to the `search-api-migration.md` document improve user access to essential information about the latest version of the Azure AI Search REST APIs while providing a clearer roadmap for migration activities.

## articles/search/search-api-preview.md{#item-511f5d}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 11/05/2024
+ms.date: 03/31/2025
 ---
 
 # Preview features in Azure AI Search
@@ -26,6 +26,9 @@ Preview features are removed from this list if they're retired or transition to
 
 |Feature&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  | Category | Description | Availability  |
 |---------|------------------|-------------|---------------|
+| [**flightingOptIn parameter in a semantic configuration**](semantic-how-to-configure.md#opt-in-for-prerelease-semantic-ranking-models) | Queries| You can opt in to use prerelease semantic ranking models if one is available in a search service region. | [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true). |
+| [**Rescore vector queries over binary embeddings using full precision vectors**](vector-search-how-to-quantization.md#recommended-rescoring-techniques) | Relevance (scoring) | For vector indexes that contain quantized binary embeddings, you can rescore query results using a full precision query vector. The query engine uses the dot product for rescoring, which improves the quality of search results. Set `enableRescoring` and `discardOriginals` to use this feature.| [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true). |
+| [**Facet hierarchies, aggregations, and facet filters**](search-faceted-navigation-examples.md) | Queries| New facet query parameters support nested facets. For numeric facetable fields, you can sum the values of each field. You can also specify filters on a facet to add inclusion or exclusion criteria. | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2025-03-01-preview&preserve-view=true). |
 | [**Query rewrite in the semantic reranker**](semantic-how-to-query-rewrite.md) | Relevance (scoring) | You can set options on a semantic query to rewrite the query input into a revised or expanded query that generates more relevant results from the L2 ranker. | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&preserve-view=true).|
 | [**Document Layout skill**](cognitive-search-skill-document-intelligence-layout.md) | Applied AI (skills) | A new skill used to analyze a document for structure and provide [structure-aware chunking](search-how-to-semantic-chunking.md). | [Create or Update Skillset (preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true). |
 | [**Keyless billing for Azure AI skills processing**](cognitive-search-attach-cognitive-services.md). | Applied AI (skills) | You can now use a managed identity and roles for a keyless connection to Azure AI services for built-in skills processing. This capability removes restrictions for having both search and AI services in the same region.  | [Create or Update Skillset  (preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true).|
@@ -57,7 +60,9 @@ Preview features are removed from this list if they're retired or transition to
 
 |Feature&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  | Category | Description | Availability  |
 |---------|------------------|-------------|---------------|
-| [**Network security perimeter**](search-security-network-security-perimeter.md) | Service | Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. | The Azure portal and the [Network Security Perimeter APIs 2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true). |
+| [**Service upgrade**](search-how-to-upgrade.md) | Feature | Upgrade your search service to higher storage limits in your region. With a one-time upgrade, you no longer need to recreate your service. | The Azure portal and [Upgrade Service (2025-02-01-preview)](/rest/api/searchmanagement/services/upgrade?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true). |
+| [**Pricing tier change**](search-capacity-planning.md#change-your-pricing-tier) | Feature | Change the [pricing tier](search-sku-tier.md) of your search service. This provides flexibility to scale storage, increase request throughput, and decrease latency based on your needs. In this preview, you can only change between Basic and Standard (S1, S2, and S3) tiers. | The Azure portal and [Update Service (2025-02-01-preview)](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#searchupdateservicewithsku). |
+| [**Network security perimeter**](search-security-network-security-perimeter.md) | Service | Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. | The Azure portal and the [Network Security Perimeter APIs 2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) or the latest preview version. |
 | [**Search service under a user-assigned managed identity**](search-howto-managed-identities-data-sources.md) | Service | Configures a search service to use a previously created user-assigned managed identity. | [Services - Update](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true#identity), 2021-04-01-preview, or the latest preview version. We recommend using the latest preview version. |
 
 ## Preview features in Azure SDKs
@@ -91,10 +96,10 @@ For data plane operation on content, [**`2024-05-01-preview`**](/rest/api/search
 GET {endpoint}/indexes('{indexName}')?api-version=2024-05-01-Preview
 ```
 
-For management operations on the search service, [**`2024-06-01-preview`**](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) is the most recent preview version. The following example shows the syntax for Update Service 2024-06-01-preview version.
+For management operations on the search service, [**`2025-05-01-preview`**](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-05-01-preview&preserve-view=true) is the most recent preview version. The following example shows the syntax for Update Service 2025-05-01-preview version.
 
 ```rest
-PATCH https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2024-06-01-preview
+PATCH https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2025-05-01-preview
 
 {
   "tags": {
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Search API Preview Document"
}
```

### Explanation
This code diff reflects a minor update to the `search-api-preview.md` documentation, which details the preview features available in Azure AI Search. The main highlights of this modification are:

1. **Date Change**: The date in the document has been updated from "11/05/2024" to "03/31/2025," indicating the document now reflects a more accurate timeline for the preview features and their expected availability.

2. **Introduction of New Features**: Several new preview features have been added to the documentation. These include:
   - **FlightingOptIn Parameter** for semantic ranking models, allowing users to opt into prerelease features.
   - **Rescoring for Vector Queries**, enabling better search result quality through full precision vectors.
   - **Support for Facet Hierarchies and Filters**, enhancing queries with nested facets and aggregation capabilities.

3. **Replacements of Outdated Features**: The diff shows that the mention of the `Network Security Perimeter` feature has been moved within the document. This also indicates changes to the descriptions and annotations surrounding this control feature.

4. **Updates to Example API Versions**: The examples in the document have been updated to refer to the newest preview API versions, specifically transitioning from version `2024-06-01-preview` to `2025-05-01-preview` for management operations. This ensures users have the most up-to-date information for their integration efforts.

Overall, the modifications enhance the clarity and utility of the `search-api-preview.md` documentation by incorporating new features, refining existing content, and ensuring that the information aligns with the latest developments in Azure AI Search.

## articles/search/search-capacity-planning.md{#item-0dd6c9}

<details>
<summary>Diff</summary>
````diff
@@ -11,14 +11,14 @@ ms.custom:
   - ignite-2023
   - ignite-2024
 ms.topic: conceptual
-ms.date: 03/11/2025
+ms.date: 03/31/2025
 ---
 
 # Estimate and manage capacity of a search service
 
 In Azure AI Search, capacity is based on *replicas* and *partitions* that can be scaled to your workload. Replicas are copies of the search engine. Partitions are units of storage. Each new search service starts with one each, but you can add or remove replicas and partitions independently to accommodate fluctuating workloads. Adding capacity increases the [cost of running a search service](search-sku-manage-costs.md#billable-events).
 
-The physical characteristics of replicas and partitions, such as processing speed and disk IO, vary by [service tier](search-sku-tier.md). On a standard search service, the replicas and partitions are faster and larger than those of a basic service.
+The physical characteristics of replicas and partitions, such as processing speed and disk IO, vary by [pricing tier](search-sku-tier.md). On a standard search service, the replicas and partitions are faster and larger than those of a basic service.
 
 Changing capacity isn't instantaneous. It can take up to an hour to commission or decommission partitions, especially on services with large amounts of data.
 
@@ -30,73 +30,121 @@ When scaling a search service, you can choose from the following tools and appro
 + [Management REST API](/rest/api/searchmanagement/services/create-or-update)
 
 > [!NOTE]
-> Higher capacity partitions are available at the same billing rate on newer services created after April and May 2024. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits) for partition size upgrades.
+> If your service was created before April or May 2024, a one-time upgrade to higher storage limits might be available at no extra cost. For more information, see [Upgrade your search service](search-how-to-upgrade.md).
 
 ## Concepts: search units, replicas, partitions
 
 Capacity is expressed in *search units* that can be allocated in combinations of *partitions* and *replicas*.  
 
 | Concept  | Definition|
 |----------|-----------|
-|*Search unit* | A single increment of total available capacity (36 units). A minimum of one unit is required to run the service. The first replica and partition pair is the first search unit. However, each extra instance of a replica *or* a partition consumes an extra search unit. For example, you start with one replica and partition (one search unit), add a second replica, you are now consuming two search units. A search unit is also the billing unit for an Azure AI Search service. |
+|*Search unit* | A single increment of total available capacity (36 units). A minimum of one unit is required to run the service. The first replica and partition pair is the first search unit. However, each extra instance of a replica *or* a partition consumes an extra search unit. For example, you start with one replica and partition (one search unit), add a second replica, you're now consuming two search units. A search unit is also the billing unit for an Azure AI Search service. |
 |*Replica* | Instances of the search service, used primarily to load balance query operations. Each replica hosts one copy of an index. If you allocate three replicas, you have three copies of an index available for servicing query requests.|
 |*Partition* | Physical storage and I/O for read/write operations (for example, when rebuilding or refreshing an index). Each partition has a slice of the total index. If you allocate three partitions, your index is divided into thirds. |
 
-Review the [partitions and replicas table](#partition-and-replica-combinations) for possible combinations that stay under the 36 unit limit. 
+Review the [partitions and replicas table](#partition-and-replica-combinations) for possible combinations that stay under the 36 unit limit.
 
 ## When to add capacity
 
-Initially, a service is allocated a minimal level of resources consisting of one partition and one replica. The [tier you choose](search-sku-tier.md) determines partition size and speed, and each tier is optimized around a set of characteristics that fit various scenarios. If you choose a higher-end tier, you might [need fewer partitions](search-performance-tips.md#service-capacity) than if you go with S1. One of the questions you'll need to answer through self-directed testing is whether a larger and more expensive partition yields better performance than two cheaper partitions on a service provisioned at a lower tier.
+Initially, a service is allocated a minimal level of resources consisting of one partition and one replica. The [tier you choose](search-sku-tier.md) determines partition size and speed, and each tier is optimized around a set of characteristics that fit various scenarios. If you choose a higher-end tier, you might [need fewer partitions](search-performance-tips.md#service-capacity) than if you go with S1. One of the questions you need to answer through self-directed testing is whether a larger and more expensive partition yields better performance than two cheaper partitions on a service provisioned at a lower tier.
 
-A single service must have sufficient resources to handle all workloads (indexing and queries). Neither workload runs in the background. You can schedule indexing for times when query requests are naturally less frequent, but the service won't otherwise prioritize one task over another. Additionally, a certain amount of redundancy smooths out query performance when services or nodes are updated internally.
+A single service must have sufficient resources to handle all workloads (indexing and queries). Neither workload runs in the background. You can schedule indexing for times when query requests are naturally less frequent, but the service doesn't otherwise prioritize one task over another. Additionally, a certain amount of redundancy smooths out query performance when services or nodes are updated internally.
 
-Some guidelines for determining whether to add capacity include:
+Guidelines for determining whether to add capacity include:
 
-+ Meeting the high availability criteria for service level agreement
-+ The frequency of HTTP 503 errors is increasing
-+ Large query volumes are expected
++ Meeting the high availability criteria for service-level agreement.
++ The frequency of HTTP 503 errors is increasing.
++ Large query volumes are expected.
++ A [one-time upgrade](#how-to-upgrade-capacity) to newer infrastructure and larger partitions isn’t sufficient.
++ The current number of partitions isn’t adequate for indexing workloads.
 
-As a general rule, search applications tend to need more replicas than partitions, particularly when the service operations are biased toward query workloads. Each replica is a copy of your index, allowing the service to load balance requests against multiple copies. All load balancing and replication of an index is managed by Azure AI Search and you can alter the number of replicas allocated for your service at any time. You can allocate up to 12 replicas in a Standard search service and 3 replicas in a Basic search service. Replica allocation can be made either from the [Azure portal](search-create-service-portal.md) or one of the programmatic options.
+As a general rule, search applications tend to need more replicas than partitions, particularly when the service operations are biased toward query workloads. Each replica is a copy of your index, allowing the service to load balance requests against multiple copies. Azure AI Search manages all load balancing and replication of an index, and you can alter the number of replicas allocated for your service at any time. You can allocate up to 12 replicas in a Standard search service and 3 replicas in a Basic search service. Replica allocation can be made either from the [Azure portal](search-create-service-portal.md) or one of the programmatic options.
 
 Extra partitions are helpful for intensive indexing workloads. Extra partitions spread read/write operations across a larger number of compute resources.
 
-Finally, larger indexes take longer to query. As such, you might find that every incremental increase in partitions requires a smaller but proportional increase in replicas. The complexity of your queries and query volume will factor into how quickly query execution is turned around.
+Finally, larger indexes take longer to query. As such, you might find that every incremental increase in partitions requires a smaller but proportional increase in replicas. The complexity of your queries and query volume factors into how quickly query execution is turned around.
 
 > [!NOTE]
-> Adding more replicas or partitions increases the cost of running the service, and can introduce slight variations in how results are ordered. Be sure to check the [pricing calculator](https://azure.microsoft.com/pricing/calculator/) to understand the billing implications of adding more nodes. The [chart below](#chart) can help you cross-reference the number of search units required for a specific configuration. For more information on how additional replicas impact query processing, see [Ordering results](search-pagination-page-layout.md#ordering-results).
+> Adding more replicas or partitions increases the cost of running the service, and can introduce slight variations in how results are ordered. Be sure to check the [pricing calculator](https://azure.microsoft.com/pricing/calculator/) to understand the billing implications of adding more nodes. The [chart below](#chart) can help you cross-reference the number of search units required for a specific configuration. For more information on how extra replicas affect query processing, see [Ordering results](search-pagination-page-layout.md#ordering-results).
 
 <a name="adjust-capacity"></a>
 
+## How to upgrade capacity
+
+Some Azure AI Search capabilities are only available to new services. One such capability is higher storage capacity, which applies to [services created after April 2024](search-limits-quotas-capacity.md#service-limits). However, if you created your service before April 2024, you can get higher capacity without recreating your service by performing a one-time upgrade. For more information, see [Upgrade your search service](search-how-to-upgrade.md).
+
 ## How to change capacity
 
-To increase or decrease the capacity of your search service, add or remove partitions and replicas.
+To increase or decrease the capacity of your service, you have two options:
+
++ [Add or remove partitions and replicas](#add-or-remove-partitions-and-replicas)
++ [Change your pricing tier](#change-your-pricing-tier)
 
-1. Sign in to the [Azure portal](https://portal.azure.com/) and select the search service.
+### Add or remove partitions and replicas
 
-1. Under **Settings**, open the **Scale** page to modify replicas and partitions. 
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.
+
+1. From the left pane, select **Settings** > **Scale**.
 
    The following screenshot shows a Standard service provisioned with one replica and partition. The formula at the bottom indicates how many search units are being used (1). If the unit price was $100 (not a real price), the monthly cost of running this service would be $100 on average.
 
-   :::image type="content" source="media/search-capacity-planning/1-initial-values.png" alt-text="Scale page showing current values" border="true":::
+   :::image type="content" source="media/search-capacity-planning/initial-values.png" alt-text="Screenshot of the Scale page showing the current replica and partition values." border="true":::
 
 1. Use the slider to increase or decrease the number of partitions. Select **Save**.
 
    This example adds a second replica and partition. Notice the search unit count; it's now four because the billing formula is replicas multiplied by partitions (2 x 2). Doubling capacity more than doubles the cost of running the service. If the search unit cost was $100, the new monthly bill would now be $400.
 
-   For the current per unit costs of each tier, visit the [Pricing page](https://azure.microsoft.com/pricing/details/search/).
+   For the current per unit costs of each tier, visit the [pricing page](https://azure.microsoft.com/pricing/details/search/).
+
+   :::image type="content" source="media/search-capacity-planning/add-two-each.png" alt-text="Screenshot of the Scale page with added replicas and partitions." border="true":::
 
-   :::image type="content" source="media/search-capacity-planning/2-add-2-each.png" alt-text="Add replicas and partitions" border="true":::
+1. Check your notifications to confirm that the operation started.
 
-1. After saving, you can check notifications to confirm the action succeeded.
+   :::image type="content" source="media/search-capacity-planning/portal-notifications.png" alt-text="Screenshot of the notification of the scaling operation in the Azure portal." border="true":::
 
-   :::image type="content" source="media/search-capacity-planning/3-save-confirm.png" alt-text="Save changes" border="true":::
+   This operation can take several hours to complete. You can’t cancel the process after it starts, and there’s no real-time monitoring of replica and partition adjustments. However, the following message displays while changes are underway.
 
-   Changes in capacity can take anywhere from 15 minutes up to several hours to complete. You can't cancel once the process has started and there's no real-time monitoring for replica and partition adjustments. However, the following message remains visible while changes are underway.
+   :::image type="content" source="media/search-capacity-planning/updating-message.png" alt-text="Screenshot of the Updating message in the Azure portal." border="true":::
 
-   :::image type="content" source="media/search-capacity-planning/4-updating.png" alt-text="Status message in the Azure portal" border="true":::
+### Change your pricing tier
 
 > [!NOTE]
-> After a service is provisioned, it cannot be upgraded to a higher tier. You must create a search service at the new tier and reload your indexes. See [Create an Azure AI Search service in the Azure portal](search-create-service-portal.md) for help with service provisioning.
+> The 2025-02-01-preview supports changes between Basic and Standard (S1, S2, and S3) tiers. Currently, you can only switch from a lower tier to a higher tier, such as going from Basic to S1. Your region also can't have [capacity constraints on the higher tier](search-region-support.md).
+
+Your [pricing tier](search-sku-tier.md) determines the maximum storage of your search service. If you need more <!-- or less capacity -->capacity, you can switch to a different pricing tier that accommodates your storage needs.
+
+In addition to capacity, changing your pricing tier affects the workload and maximum limits of your service. Before you proceed, compare the [service limits](search-limits-quotas-capacity.md) of your current tier and your desired tier. These include limits on:
+
++ Partition storage
++ Indexes
++ Vectors
++ Indexers
++ Shared private link resources
++ Synonyms
++ Index aliases
++ Semantic ranker throttling
+
+Generally, switching to a higher tier increases your [storage limit](search-limits-quotas-capacity.md#service-limits) and [vector limit](search-limits-quotas-capacity.md#vector-index-size-limits), increases request throughput, and decreases latency<!-- , while switching to a lower tier decreases your storage limit and vector limit, decreases request throughput, and increases latency -->.
+
+To change your pricing tier:
+
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.
+
+1. From the left pane, select **Settings** > **Scale**.
+
+1. Under your current tier, select **Change Pricing Tier**.
+
+   :::image type="content" source="media/search-capacity-planning/change-pricing-tier.png" alt-text="Screenshot of the Change Pricing Tier button in the Azure portal." border="true":::
+
+1. On the **Select Pricing Tier** page, choose a higher tier from the list. Currently, you can only move up between Basic, S1, S2, and S3. Other pricing tiers are unavailable and appear dimmed.
+
+1. To switch to the higher tier, select **Select**.
+
+   :::image type="content" source="media/search-capacity-planning/pricing-tier-list.png" alt-text="Screenshot of the Select Pricing Tier page and the list of higher tiers in the Azure portal." border="true":::
+
+   This operation can take several hours to complete. You can’t cancel the process after it starts, and there’s no real-time monitoring of tier changes. However, on the **Overview** page, a **Provisioning** status indicates the operation is underway for your service.
+
+   :::image type="content" source="media/search-capacity-planning/provisioning-status.png" alt-text="Screenshot of the service Overview page with a Provisioning status." border="true":::
 
 ## How scale requests are handled
 
@@ -143,25 +191,25 @@ The following chart applies to Standard tier and higher. It shows all possible c
 
 Basic search services have lower search unit counts.
 
-+ On search services created before April 3, 2024, a basic search service can have exactly one partition and up to three replicas, for a maximum limit of three SUs. The only adjustable resource is replicas. 
++ On search services created before April 3, 2024, Basic services can have exactly one partition and up to three replicas for a maximum limit of three SUs. The only adjustable resource is replicas. However, you might be able to increase your partition count by [upgrading your service](search-how-to-upgrade.md).
 
-+ On search services created after April 3, 2024 in [supported regions](search-limits-quotas-capacity.md#service-limits), basic services can have up to three partitions and three replicas. The maximum SU limit is nine to support a full complement of partitions and replicas.
++ On search services created after April 3, 2024 in [supported regions](search-limits-quotas-capacity.md#service-limits), Basic services can have up to three partitions and three replicas. The maximum SU limit is nine to support a full complement of partitions and replicas.
 
 For search services on any billable tier, regardless of creation date, you need a minimum of two replicas for high availability on queries.
 
 For billing rates per tier and currency, see the [Azure AI Search pricing page](https://azure.microsoft.com/pricing/details/search/).
 
 ## Estimate capacity using a billable tier
 
-Storage needs are determined by the size of the indexes you expect to build. There are no solid heuristics or generalities that help with estimates. The only way to determine the size of an index is [build one](search-what-is-an-index.md). Its size is based on tokenization and embeddings, and whether you enable suggesters, filtering, and sorting, or can take advantage of [vector compression](vector-search-how-to-quantization.md).
+The size of the indexes you expect to build determines storage needs. There are no solid heuristics or generalities that help with estimates. The only way to determine the size of an index is [build one](search-what-is-an-index.md). Its size is based on tokenization and embeddings, and whether you enable suggesters, filtering, and sorting, or can take advantage of [vector compression](vector-search-how-to-quantization.md).
 
 We recommend estimating on a billable tier, Basic or above. The Free tier runs on physical resources shared by multiple customers and is subject to factors beyond your control. Only the dedicated resources of a billable search service can accommodate larger sampling and processing times for more realistic estimates of index quantity, size, and query volumes during development. 
 
 1. [Review service limits at each tier](search-limits-quotas-capacity.md#service-limits) to determine whether lower tiers can support the number of indexes you need. Consider whether you need multiple copies of an index for active development, testing, and production. 
 
    A search service is subject to object limits (maximum number of indexes, indexers, skillsets, etc.) and storage limits. Whichever limit is reached first is the effective limit. 
 
-1. [Create a service at a billable tier](search-create-service-portal.md). Tiers are optimized for certain workloads. For example, Storage Optimized tier has a limit of 10 indexes because it's designed to support a low number of very large indexes.
+1. [Create a service at a billable tier](search-create-service-portal.md). Tiers are optimized for certain workloads. For example, the Storage Optimized tier has a limit of 10 indexes because it's designed to support a low number of large indexes.
 
     + Start low, at Basic or S1, if you're not sure about the projected load.
 
@@ -187,11 +235,11 @@ Storage requirements can be inflated if you include data that will never be sear
 
 ## Service-level agreement considerations
 
-The Free tier and preview features aren't covered by [service-level agreements (SLAs)](https://azure.microsoft.com/support/legal/sla/search/v1_0/). For all billable tiers, SLAs take effect when you provision sufficient redundancy for your service. 
+The Free tier and preview features aren't covered by [service-level agreements (SLAs)](https://azure.microsoft.com/support/legal/sla/search/v1_0/). For all billable tiers, SLAs take effect when you provision sufficient redundancy for your service.
 
-+ Two or more replicas satisfy query (read) SLAs. 
++ Two or more replicas satisfy query (read) SLAs.
 
-+ Three or more replicas satisfy query and indexing (read-write) SLAs. 
++ Three or more replicas satisfy query and indexing (read-write) SLAs.
 
 The number of partitions doesn't affect SLAs.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to Search Capacity Planning Document"
}
```

### Explanation
The code diff presents a minor update to the `search-capacity-planning.md` file, which provides guidance on estimating and managing the capacity of a search service in Azure AI. The notable aspects of these modifications are:

1. **Date Update**: The document date has been changed from "03/11/2025" to "03/31/2025," reflecting a more accurate timeline for the content's relevance.

2. **Terminology Changes**: The term "service tier" has been replaced with "pricing tier" throughout the document, aligning the language with current Azure documentation standards. This change helps clarify how pricing affects service characteristics.

3. **New Content Addition**: The document now includes additional sections, particularly on upgrading capacity. A new section highlights that services created prior to April or May 2024 may undergo a one-time upgrade to allow higher storage limits without recreating the service.

4. **Enhanced Clarity**: The explanations around search units, replicas, and partitions have been expanded for better understanding. More detailed guidelines are provided for determining when to add capacity, which now includes specific criteria such as meeting high availability criteria and addressing increasing error rates.

5. **Updated Instructions**: The instructions on how to change capacity or pricing tiers have been revised for clarity. These instructions include step-by-step processes and associated documentation links, guiding users in managing their search service effectively.

6. **Visuals and Examples**: The document includes new screenshots reflecting updates to the Azure portal, helping users visualize the changes and processes discussed.

Overall, these updates serve to enhance the usability and clarity of the `search-capacity-planning.md` article, providing users with more detailed guidance on managing capacity for Azure AI Search services in a cost-effective and efficient manner.

## articles/search/search-create-service-portal.md{#item-f90159}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - references_regions
   - build-2024
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 03/21/2025
 ---
 
 # Create an Azure AI Search service in the Azure portal
@@ -33,13 +33,13 @@ You can also use:
 
 ## Before you start
 
-Some properties are fixed for the lifetime of the search service. Before creating your service, decide on the following properties:
+Some properties are fixed for the lifetime of the search service. Before you create your service, decide on the following properties:
 
 | Property | Description |
 |--|--|
 | [Name](#name-your-service) | Becomes part of the URL endpoint. The name must be unique and follow naming rules. |
 | [Region](search-region-support.md) | Determines data residency and availability of certain features. For example, semantic ranker and Azure AI integration have region requirements. Choose a region that supports the features you need. |
-| [Tier](search-sku-tier.md) | Determines infrastructure, service limits, and billing. Some features aren't available on lower or specialized tiers. |
+| [Tier](search-sku-tier.md) | Determines infrastructure, service limits, and billing. Some features aren't available on lower or specialized tiers. In the 2025-02-01-preview, you can also [switch from a lower tier to a higher tier](search-capacity-planning.md#change-your-pricing-tier). |
 
 ## Subscribe to Azure
 
@@ -131,7 +131,7 @@ Currently, the following regions offer cross-regional availability for Azure AI
 + Americas: West US, East US
 + Europe: Switzerland North, Sweden Central
 
-This list isn't definitive, and depending on your tier, you might have more choices. Region status can also change quickly, so confirm your region choice before creating your search service.
+This list isn't definitive, and depending on your tier, you might have more choices. Region status can also change quickly, so confirm your region choice before you create your search service.
 
 ## Choose a tier
 
@@ -149,8 +149,8 @@ The Basic and Standard tiers are the most common for production workloads, but m
 :::image type="content" source="media/search-create-service-portal/select-pricing-tier.png" lightbox="media/search-create-service-portal/select-pricing-tier.png" alt-text="Screenshot of the Select Pricing Tier page in the Azure portal." border="true":::
 
 > [!NOTE]
-> + You can't change the tier after creating your search service, so choose carefully.
-> + Search services created after April 3, 2024 have larger partitions and higher vector quotas at every billable tier.
+> + After you create your service, you can move up between Basic and Standard (S1, S2, and S3) tiers. Switching to a lower tier isn't currently supported. For more information, see [Change your pricing tier](search-capacity-planning.md#change-your-pricing-tier).
+> + Services created after April 3, 2024 have larger partitions and higher vector quotas at every billable tier.
 
 ## Create your service
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Create Service Portal Documentation"
}
```

### Explanation
The code diff showcases a minor update to the `search-create-service-portal.md` file, which provides guidance on creating an Azure AI Search service through the Azure portal. The key changes made in this version are:

1. **Date Change**: The date in the document has been updated from "02/20/2025" to "03/21/2025," reflecting a new revision date.

2. **Clarity Improvements**: The phrasing in several sections has been clarified. For instance, the language has been adjusted to enhance readability, such as changing "Before creating your service" to "Before you create your service."

3. **New Tier Information**: Additional context has been provided regarding the pricing tiers. The document now mentions that users can switch from a lower tier to a higher tier after creating the service, which updates users about the flexibility available in managing service tiers.

4. **Reiteration of Region Choices**: The note regarding region choices emphasizes the importance of confirming the region prior to service creation. The wording has been adjusted to maintain consistency in instructions.

5. **Removal of Restrictions Misconception**: The modifications clarify that while users cannot switch to a lower tier post-creation, they can still adjust between the Basic and Standard tiers after the service has been established.

Overall, these changes enhance the clarity and usability of the `search-create-service-portal.md`, ensuring that users receive accurate and accessible information on creating and managing their Azure AI Search services.

## articles/search/search-faceted-navigation-examples.md{#item-2b1158}

<details>
<summary>Diff</summary>
````diff
@@ -0,0 +1,721 @@
+---
+
+title: Faceted navigation examples
+titleSuffix: Azure AI Search
+description: Examples that demonstrate query syntax for facet hierarchies, distinct counts, facet aggregations, and facet filters.
+
+manager: nitinme
+author: HeidiSteen
+ms.author: heidist
+ms.service: azure-ai-search
+ms.topic: how-to
+ms.date: 04/04/2025
+---
+
+# Faceted navigation examples
+
+This section extends [faceted navigation configuration](search-faceted-navigation.md) with examples that demonstrate basic usage and other scenarios.
+
+Facetable fields are defined in an index, but facet parameters and expressions are defined in query requests. If you have an index with facetable fields, you can try new preview features like [facet hierarchies](#facet-hierarchy-example), [facet aggregations](#facet-aggregation-example), and [facet filters](#facet-filtering-example) on existing indexes.
+
+## Facet parameters and syntax
+
+Depending on the API, a facet query is usually an array of facet expressions that are applied to search results. Each facet expression contains a facetable field name, optionally followed by a comma-separated list of name-value pairs.
+
++ *facet query* is a query request that includes a facet property.
++ *facetable field* is a field definition in the search index attributed with the `facetable` property.
++ *count* is the number of matches for each facet found in the search results.
+
+The following table describes facet parameters used in the examples.
+
+| Facet parameter | Description | Usage | Example |
+|-----------------|-------------|-------|---------|
+| `count` | Maximum number of facet terms per structure.| Integer. Default is 10. There's no upper limit, but higher values degrade performance, especially if the faceted field contains a large number of unique terms. This is due to the way facet queries are distributed across shards. You can set `count` to zero or to a value that's greater than or equal to the number of unique values in the facetable field to get an accurate count across all shards. The tradeoff is increased latency. | `Tags,count:5` limits the faceted navigation response to 5 facet buckets that containing the most facet counts, but they can be in any order. |
+| `sort` | Determines order of facet buckets. | Valid values are `count`, `-count`, `value`, `-value`. Use `count` to list facets from greatest to smallest. Use `-count` to sort in ascending order (smallest to greatest). Use `value` to sort alphanumerically by facet value in ascending order. Use `-value` to sort descending by value. | `"facet=Category,count:3,sort:count"` gets the top three facet buckets in search results, listed in descending order by the number of matches in each Category. If the top three categories are Budget, Extended-Stay, and Luxury, and Budget has 5 hits, Extended-Stay has 6, and Luxury has 4, then the facet buckets are ordered as Extended-Stay, Budget, Luxury. Another example is`"facet=Rating,sort:-value"`. It produces facets for all possible ratings, in descending order by value. If ratings are from 1 to 5, the facets are ordered 5, 4, 3, 2, 1, irrespective of how many documents match each rating. |
+| `values` | Provides values for facet labels. | Set to pipe-delimited numeric or `Edm.DateTimeOffset` values specifying a dynamic set of facet entry values. The values must be listed in sequential, ascending order to get the expected results. | `"facet=baseRate,values:10 | 20"` produces three facet buckets: one for base rate 0 up to but not including 10, one for 10 up to but not including 20, and one for 20 and higher. A string `"facet=lastRenovationDate,values:2024-02-01T00:00:00Z"` produces two facet buckets: one for hotels renovated before February 2024, and one for hotels renovated February 1, 2024 or later. |
+| `interval` | Provides an interval sequence for facets that can be grouped into intervals. | An integer interval greater than zero for numbers, or minute, hour, day, week, month, quarter, year for date time values. | `"facet=baseRate,interval:100"` produces facet buckets based on base rate ranges of size 100. If base rates are all between $60 and $600, there are facet buckets for 0-100, 100-200, 200-300, 300-400, 400-500, and 500-600. The string `"facet=lastRenovationDate,interval:year"` produces one facet bucket for each year a hotel was renovated. |
+| `timeoffset` | Specifies the UTC time offset to account for in setting time boundaries. | Set to (`[+-]hh:mm, [+-]hhmm, or [+-]hh`). If used, the `timeoffset` parameter must be combined with the interval option, and only when applied to a field of type `Edm.DateTimeOffset`. | `"facet=lastRenovationDate,interval:day,timeoffset:-01:00"` uses the day boundary that starts at 01:00:00 UTC (midnight in the target time zone). |
+
+`count` and `sort` can be combined in the same facet specification, but they can't be combined with `interval` or `values`.
+
+`interval` and `values` can't be combined together.
+
+Interval facets on date time are computed based on the UTC time if `timeoffset` isn't specified. For example, for `"facet=lastRenovationDate,interval:day"`, the day boundary starts at 00:00:00 UTC.
+
+## Basic facet example
+
+The following facet queries work against the [hotels sample index](search-get-started-portal.md). You can use **JSON view** in Search Explorer to paste in the JSON query. For help with getting started, see [Add faceted navigation to search results](search-faceted-navigation.md).
+
+This first query retrieves facets for Categories, Ratings, Tags, and rooms with baseRate values in specific ranges. Notice the last facet is on a subfield of the Rooms collection. Facets count the parent document (Hotels) and not intermediate subdocuments (Rooms), so the response determines the number of *hotels* that have any rooms in each pricing category.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "ocean view",  
+  "facets": [ "Category", "Rating", "Tags", "Rooms/BaseRate,values:80|150|220" ],
+  "count": true 
+}  
+```
+
+This second example uses a filter to narrow down the previous faceted query result after the user selects Rating 3 and category "Motel".
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "water view",  
+  "facets": [ "Tags", "Rooms/BaseRate,values:80|150|220" ],
+  "filter": "Rating eq 3 and Category eq 'Motel'",
+  "count": true  
+} 
+```
+
+The third example sets an upper limit on unique terms returned in a query. The default is 10, but you can increase or decrease this value using the count parameter on the facet attribute. This example returns facets for city, limited to 5.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "view",  
+  "facets": [ "Address/City,count:5" ],
+  "count": true
+} 
+```
+
+This example shows three facets for "Category", "Tags", and "Rating", with a count override on "Tags" and a range override for "Rating", which is otherwise stored as a double in the index.
+
+```http
+POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
+{
+    "search": "*",
+    "facets": [ 
+        "Category", 
+        "Tags,count:5", 
+        "Rating,values:1|2|3|4|5"
+    ],
+    "count": true
+}
+```
+
+For each faceted navigation tree, there's a default limit of the top 10 facet instances found by the query. This default makes sense for navigation structures because it keeps the values list to a manageable size. You can override the default by assigning a value to "count". For example, `"Tags,count:5"` reduces the number of tags under the Tags section to the top five.
+
+For Numeric and DateTime values only, you can explicitly set values on the facet field (for example, `facet=Rating,values:1|2|3|4|5`) to separate results into contiguous ranges (either ranges based on numeric values or time periods). Alternatively, you can add "interval", as in `facet=Rating,interval:1`. 
+
+Each range is built using 0 as a starting point, a value from the list as an endpoint, and then trimmed of the previous range to create discrete intervals.
+
+## Distinct values example
+
+You can formulate a query that returns a distinct value count for each facetable field. This example formulates an empty or unqualified query (`"search": "*"`) that matches on all documents, but by setting `top` to zero, you get just the counts, with no results.
+
+For brevity, this query includes just two fields marked as `facetable` in the hotels sample index.
+
+```http
+POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
+{
+    "search": "*",
+    "count": true,
+    "top": 0,
+    "facets": [ 
+        "Category", "Address/StateProvince""
+    ]
+}
+```
+
+Results from this query are as follows:
+
+```json
+{
+  "@odata.count": 50,
+  "@search.facets": {
+    "Address/StateProvince": [
+      {
+        "count": 9,
+        "value": "WA"
+      },
+      {
+        "count": 6,
+        "value": "CA "
+      },
+      {
+        "count": 4,
+        "value": "FL"
+      },
+      {
+        "count": 3,
+        "value": "NY"
+      },
+      {
+        "count": 3,
+        "value": "OR"
+      },
+      {
+        "count": 3,
+        "value": "TX"
+      },
+      {
+        "count": 2,
+        "value": "GA"
+      },
+      {
+        "count": 2,
+        "value": "MA"
+      },
+      {
+        "count": 2,
+        "value": "TN"
+      },
+      {
+        "count": 1,
+        "value": "AZ"
+      }
+    ],
+    "Category": [
+      {
+        "count": 13,
+        "value": "Budget"
+      },
+      {
+        "count": 12,
+        "value": "Suite"
+      },
+      {
+        "count": 7,
+        "value": "Boutique"
+      },
+      {
+        "count": 7,
+        "value": "Resort and Spa"
+      },
+      {
+        "count": 6,
+        "value": "Extended-Stay"
+      },
+      {
+        "count": 5,
+        "value": "Luxury"
+      }
+    ]
+  },
+  "value": []
+}
+```
+
+## Facet hierarchy example
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and available in the Azure portal, you can configure a facet hierarchy using the `>` and `;` operators.
+
+The nesting (hierarchical) operator `>` denotes a parent–child relationship, and the semicolon operator `;` denotes multiple fields at the same nesting level, which are all children of the same parent. The parent must contain only one field. Both the parent and child fields must be `facetable`. 
+
+The order of operations in a facet expression that includes facet hierarchies are:
+
+* options operator (comma `,`) that separates facet parameters for the facet field, such as the comma in `Rooms/BaseRate,values`
+* parentheses, such as the ones enclosing `(Rooms/BaseRate,values:50 ; Rooms/Type)`.
+* nesting operator (angled bracket `>`)
+* append operator (semicolon `;`), demonstrated in a second example `"Tags>(Rooms/BaseRate,values:50 ; Rooms/Type)"` in this section, where two child facets are peers under the Tags parent.
+
+There are several examples for facet hierarchies. The first example is a query that returns just a few documents, which is helpful for viewing a full response. Facets count the parent document (Hotels) and not intermediate subdocuments (Rooms), so the response determines the number of *hotels* that have any rooms in each facet bucket.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{
+  "search": "ocean",  
+  "facets": ["Address/StateProvince>Address/City", "Tags>Rooms/BaseRate,values:50"],
+  "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+  "count": true 
+}
+```
+
+Results from this query are as follows. Both hotels have pools. For other tags, only one hotel provides the amenity.
+
+```json
+{
+  "@odata.count": 2,
+  "@search.facets": {
+    "Tags": [
+      {
+        "value": "pool",
+        "count": 2,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 2
+            }
+          ]
+        }
+      },
+      {
+        "value": "air conditioning",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "bar",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "restaurant",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "view",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      }
+    ],
+    "Address/StateProvince": [
+      {
+        "value": "FL",
+        "count": 1,
+        "@search.facets": {
+          "Address/City": [
+            {
+              "value": "Tampa",
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "HI",
+        "count": 1,
+        "@search.facets": {
+          "Address/City": [
+            {
+              "value": "Honolulu",
+              "count": 1
+            }
+          ]
+        }
+      }
+    ]
+  },
+  "value": [
+    {
+      "@search.score": 1.6076145,
+      "HotelName": "Ocean Water Resort & Spa",
+      "Description": "New Luxury Hotel for the vacation of a lifetime. Bay views from every room, location near the pier, rooftop pool, waterfront dining & more.",
+      "Tags": [
+        "view",
+        "pool",
+        "restaurant"
+      ],
+      "Address": {
+        "City": "Tampa",
+        "StateProvince": "FL"
+      }
+    },
+    {
+      "@search.score": 1.0594962,
+      "HotelName": "Windy Ocean Motel",
+      "Description": "Oceanfront hotel overlooking the beach features rooms with a private balcony and 2 indoor and outdoor pools. Inspired by the natural beauty of the island, each room includes an original painting of local scenes by the owner. Rooms include a mini fridge, Keurig coffee maker, and flatscreen TV. Various shops and art entertainment are on the boardwalk, just steps away.",
+      "Tags": [
+        "pool",
+        "air conditioning",
+        "bar"
+      ],
+      "Address": {
+        "City": "Honolulu",
+        "StateProvince": "HI"
+      }
+    }
+  ]
+}
+```
+
+This second example extends the previous one, demonstrating multiple top-level facets with multiple children. Notice the semicolon (`;`) operator separates each child.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "+ocean",  
+  "facets": ["Address/StateProvince > Address/City", "Tags > (Rooms/BaseRate,values:50 ; Rooms/Type)"],
+  "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+  "count": true 
+}  
+```
+
+A partial response, trimmed for brevity, shows Tags with child facets for the rooms base rate and type. In the hotels sample index, both hotels that match to `+ocean` have rooms in each type and a pool.
+
+```json
+{
+  "@odata.count": 2,
+  "@search.facets": {
+    "Tags": [
+      {
+        "value": "pool",
+        "count": 2,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 2
+            }
+          ],
+          "Rooms/Type": [
+            {
+              "value": "Budget Room",
+              "count": 2
+            },
+            {
+              "value": "Deluxe Room",
+              "count": 2
+            },
+            {
+              "value": "Standard Room",
+              "count": 2
+            },
+            {
+              "value": "Suite",
+              "count": 2
+            }
+          ]
+        }}]},
+  ...
+}
+```
+
+This last example shows precedence rules for parentheses that affects nesting levels. Suppose you want to return a facet hierarchy in this order.
+
+```
+Address/StateProvince
+  Address/City
+    Category
+    Rating
+```
+
+To return this hierarchy, create a query where Category and Rating are siblings under Address/City.
+
+```json
+  { 
+    "search": "beach",  
+    "facets": [
+        "Address/StateProvince > (Address/City > (Category ; Rating))"
+        ],
+    "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+    "count": true 
+  }
+```
+
+If you remove the innermost parentheses, Category and Rating are no longer siblings because the precedence rules mean that the `>` operator is evaluated before `;`.
+
+```json
+  { 
+    "search": "beach",  
+    "facets": [
+        "Address/StateProvince > (Address/City > Category ; Rating)"
+        ],
+    "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+    "count": true 
+  }
+```
+
+The top-level parent is still Address/StateProvince, but now Address/City and Rating are on same level.
+
+```
+Address/StateProvince
+  Rating
+  Address/City
+    Category
+```
+
+## Facet filtering example
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and available in the Azure portal, you can configure facet filters.
+
+Facet filtering enables you to constrain the facet values returned to those matching a specified regular expression. Two new parameters accept a regular expression that is applied to the facet field:
+
+* `includeTermFilter` filters the facet values to those that match the regular expression
+* `excludeTermFilter` filters the facet values to those that don't match the regular expression 
+
+If a facet string satisfies both conditions, the `excludeTermFilter` takes precedence because the set of bucket strings is first evaluated with `includeTermFilter` and then excluded with `excludeTermFilter`.
+
+Only those facet values that match the regular expression are returned. You can combine these parameters with other facet options (for example, `count`, `sort`, and [hierarchical faceting](#facet-hierarchy-example)) on string fields.
+
+Because the regular expression is nested within a JSON string value, you must escape both the double quote (`"`) and the backslash (`\`) characters. The regular expression itself is delimited by the forward slash (`/`). For more information about escape patterns, see [Regular expression search](query-lucene-syntax.md#bkmk_regex).
+
+The following example shows how to escape special characters in your regular expression such as backslash, double quotes, or regular expression syntax characters. 
+
+```json
+{
+    "search": "*", 
+    "facets": ["name,includeTermFilter:/EscapeBackslash\\\OrDoubleQuote\\"OrRegexCharacter\\(/"] 
+}
+```
+
+Here's an example of a facet filter that matches on Budget and Extended-Stay hotels, with Rating as a child of each hotel category.
+
+```http
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{ 
+    "search": "*", 
+    "facets": ["(Category,includeTermFilter:/(Budget|Extended-Stay)/)>Rating,values:1|2|3|4|5"],
+    "select": "HotelName, Category, Rating",
+    "count": true 
+} 
+```
+
+The following example is an abbreviated response (hotel documents are omitted for brevity).
+
+```json
+{
+  "@odata.count": 50,
+  "@search.facets": {
+    "Category": [
+      {
+        "value": "Budget",
+        "count": 13,
+        "@search.facets": {
+          "Rating": [
+            {
+              "to": 1,
+              "count": 0
+            },
+            {
+              "from": 1,
+              "to": 2,
+              "count": 0
+            },
+            {
+              "from": 2,
+              "to": 3,
+              "count": 4
+            },
+            {
+              "from": 3,
+              "to": 4,
+              "count": 5
+            },
+            {
+              "from": 4,
+              "to": 5,
+              "count": 4
+            },
+            {
+              "from": 5,
+              "count": 0
+            }
+          ]
+        }
+      },
+      {
+        "value": "Extended-Stay",
+        "count": 6,
+        "@search.facets": {
+          "Rating": [
+            {
+              "to": 1,
+              "count": 0
+            },
+            {
+              "from": 1,
+              "to": 2,
+              "count": 0
+            },
+            {
+              "from": 2,
+              "to": 3,
+              "count": 4
+            },
+            {
+              "from": 3,
+              "to": 4,
+              "count": 1
+            },
+            {
+              "from": 4,
+              "to": 5,
+              "count": 1
+            },
+            {
+              "from": 5,
+              "count": 0
+            }
+          ]
+        }
+      }
+    ]
+  }, 
+  "value": [  ALL 50 HOTELS APPEAR HERE ]
+}
+```
+
+## Facet aggregation example
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and available in the Azure portal, you can aggregate facets.
+
+Facet aggregations allow you to compute metrics from facet values. The aggregation capability works alongside the existing faceting options. The only supported metric is `sum`. Adding `metric: sum` to a numeric facet aggregates all the values of each bucket. 
+
+You can add a default value to use if a document contains a null for that field: `"facets": [ "Rooms/SleepsCount, metric: sum, default:2"]`. If a room has a null value for the Rooms/SleepsCount field, the default substitutes for the missing value.
+
+You can sum any facetable field of a numeric data type (except vectors and geographic coordinates). 
+
+Here's an example using the hotels-sample-index. The Rooms/SleepsCount field is facetable and numeric, so we choose this field to demonstrate sum. If we sum that field, we get the sleep count for the entire hotel. Recall that facets count the parent document (Hotels) and not intermediate subdocuments (Rooms), so the response sums the SleepsCount of all rooms for the entire hotel. In this query, we add a filter to sum the SleepsCount for just one hotel.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+
+{ 
+      "search": "*",
+      "filter": "HotelId eq '41'",
+      "facets": [ "Rooms/SleepsCount, metric: sum"],
+      "select": "HotelId, HotelName, Rooms/Type, Rooms/SleepsCount",
+      "count": true
+}
+```
+
+A response for the query might look like the following example. Windy Ocean Model can accommodate a total of 40 guests.
+
+```json
+{
+  "@odata.count": 1,
+  "@search.facets": {
+    "Rooms/SleepsCount": [
+      {
+        "sum": 40.0
+      }
+    ]
+  },
+  "value": [
+    {
+      "@search.score": 1.0,
+      "HotelId": "41",
+      "HotelName": "Windy Ocean Motel",
+      "Rooms": [
+        {
+          "Type": "Suite",
+          "SleepsCount": 4
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Budget Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Budget Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 4
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 4
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        }
+      ]
+    }
+  ]
+}
+```
+
+## Next steps
+
+Revisit [facet navigation configuration](search-faceted-navigation.md) for tools and APIs, and review [best practices](search-faceted-navigation.md#best-practices-for-working-with-facets) for working with facets in code.
+
+We recommend the [C#: Add search to web apps](tutorial-csharp-overview.md) for an example of faceted navigation that includes code for the presentation layer. The sample also includes filters, suggestions, and autocomplete. It uses JavaScript and React for the presentation layer.
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Faceted Navigation Examples Documentation Added"
}
```

### Explanation
The code diff indicates the introduction of a new document titled `search-faceted-navigation-examples.md`, which provides extensive examples showcasing how to implement and utilize faceted navigation within Azure AI Search. This newly added documentation consists of 721 lines of content and covers the following key areas:

1. **Introduction to Faceted Navigation**: The document expands on existing faceted navigation configurations, providing users with practical examples for implementing features such as facet hierarchies, aggregations, and filters.

2. **Facet Parameters and Syntax**: It details the parameters used in facet queries, explaining terms such as `facet query`, `facetable field`, and `count`. Each parameter is broken down in a table format, highlighting its purpose, usage, and providing practical examples.

3. **Facet Examples**: Multiple query examples illustrate how to retrieve facets from the hotels sample index. These include basic queries, use of filters to refine results, and setting limits on the number of unique facet terms returned. Each example is provided in the context of a REST API call format.

4. **Distinct Values Calculation**: The documentation describes how to formulate a query that returns distinct value counts for specified facetable fields, including example queries that demonstrate this functionality.

5. **Advanced Facet Hierarchies**: Users are introduced to creating hierarchical facets with examples showing parent-child relationships among facets. This section explains how to format queries to produce desired hierarchical structures in the results.

6. **Facet Filtering and Aggregation**: New features like facet filtering with regular expressions and facet aggregation for computing metrics are introduced. Examples are provided to demonstrate how to implement these features, including handling enriched response details.

7. **Next Steps Guidance**: The document encourages users to revisit the main faceted navigation configuration resource, check best practices for working with facets, and provides links to relevant tutorials showcasing faceted navigation in application development.

Overall, this comprehensively structured document serves as a robust reference for users looking to implement and utilize faceted navigation effectively in Azure AI Search applications, making it a valuable addition to the documentation library.

## articles/search/search-faceted-navigation.md{#item-f29d1e}

<details>
<summary>Diff</summary>
````diff
@@ -1,37 +1,45 @@
 ---
-title: Add a faceted navigation category hierarchy
+title: Add facets to a query
 titleSuffix: Azure AI Search
-description: Add faceted navigation for self-directed filtering in applications that integrate with Azure AI Search.
+description: Add faceted navigation for self-directed navigation in applications that integrate with Azure AI Search.
 
 manager: nitinme
 author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
-ms.topic: concept-article
-ms.date: 02/26/2025
+ms.topic: how-to
+ms.date: 04/04/2025
 ---
 
-# Add faceted navigation to a search app
+# Add faceted navigation to search results
 
-Faceted navigation is used for self-directed drill-down filtering on query results in a search app, where your application offers form controls for scoping search to groups of documents (for example, categories or brands), and Azure AI Search provides the data structures and filters to back the experience.
+Faceted navigation is used for self-directed filtering on query results in a search app, where your application offers form controls for scoping search to groups of documents (for example, categories or brands), and Azure AI Search provides the data structures and filters to back the experience.
 
-In this article, learn how to return a faceted navigation structure in Azure AI Search.
+In this article, learn the steps for returning a faceted navigation structure in Azure AI Search. Once you're familiar with basic concepts and clients, continue to [Facet examples](search-faceted-navigation-examples.md) for syntax about various use cases, including basic faceting and distinct counts. 
+
+More facet capabilities are available through preview APIs:
+
++ hierarchical facet structures
++ facet filtering
++ facet aggregations
+
+[Facet navigation examples](search-faceted-navigation-examples.md) provide the syntax and usage for the preview features.
 
 ## Faceted navigation in a search page
 
-Facets are dynamic and returned on a query. A search response brings with it all of the facet categories used to navigate the documents in the result. The query executes first, and then facets are pulled from the current results and assembled into a faceted navigation structure.
+Facets are dynamic because they're based on each specific query result set. A search response brings with it all of the facet buckets used to navigate the documents in the result. The query executes first, and then facets are pulled from the current results and assembled into a faceted navigation structure.
 
-In Azure AI Search, facets are one layer deep and can't be hierarchical. If you aren't familiar with faceted navigation structures, the following example shows one on the left. Counts indicate the number of matches for each facet. The same document can be represented in multiple facets.
+In Azure AI Search, facets are one layer deep and can't be hierarchical unless you use the preview API. If you aren't familiar with faceted navigation structures, the following example shows one on the left. Counts indicate the number of matches for each facet. The same document can be represented in multiple facets.
 
 :::image source="media/search-faceted-navigation/azure-search-facet-nav.png" alt-text="Screenshot of faceted search results.":::
 
 Facets can help you find what you're looking for, while ensuring that you don't get zero results. As a developer, facets let you expose the most useful search criteria for navigating your search index.
 
 ## Faceted navigation in code
 
-Facets are enabled on supported fields in an index, and then specified on a query. At query time, a faceted navigation structure is returned at the top of the response.
+Facets are enabled on supported fields in an index, and then specified on a query. The faceted navigation structure is returned at the beginning of the response, followed by the results.
 
-The following REST example is an unqualified query (`"search": "*"`) that is scoped to the entire index (see the [built-in hotels sample](search-get-started-portal.md)). It returns a faceted navigation structure for the "Category" field.
+The following REST example is an empty query (`"search": "*"`) that is scoped to the entire index (see the [built-in hotels sample](search-get-started-portal.md)). The `facets` parameter specifies the "Category" field.
 
 ```http
 POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
@@ -47,7 +55,7 @@ POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-
 }
 ```
 
-The response for the example includes the faceted navigation structure at the top. The structure consists of "Category" values and a count of the hotels for each one. It's followed by the rest of the search results, trimmed here to just one document for brevity. This example works well for several reasons. The number of facets for this field fall under the limit (default is 10) so all of them appear, and every hotel in the index of 50 hotels is represented in exactly one of these categories.
+The response for the example starts with the faceted navigation structure. The structure consists of "Category" values and a count of the hotels for each one. It's followed by the rest of the search results, trimmed here to just one document for brevity. This example works well for several reasons. The number of facets for this field fall under the limit (default is 10) so all of them appear, and every hotel in the index of 50 hotels is represented in exactly one of these categories.
 
 ```json
 {
@@ -94,214 +102,142 @@ The response for the example includes the faceted navigation structure at the to
                 "concierge"
             ],
             "ParkingIncluded": false,
-        }
+        },
+        . . . 
     ]
 }
 ```
 
 ## Enable facets on fields
 
-During [index creation or update](search-how-to-create-search-index.md), facets are enabled when you set `"facetable": true` on new fields that you add to an index. Although it's not strictly required, it's a best practice to also set the "filterable" attribute so that you can build the necessary filters that back the faceted navigation experience in your search application.
+You can add facets to new fields that contain plain text or numeric content. Supported data types include strings, dates, boolean fields, and numeric fields (but not vectors).
 
-Here's a JSON example of the hotels sample index, showing "facetable" and "filterable" on low cardinality fields that contain single values or short phrases: "Category", "Tags", "Rating".
+You can use the Azure portal, REST APIs, Azure SDKs or any method that supports the creation or update of index schemas in Azure AI Search. As a first step, identify which fields to use for faceting.
 
-```json
-{
-  "name": "hotels",  
-  "fields": [
-    { "name": "hotelId", "type": "Edm.String", "key": true, "searchable": false, "sortable": false, "facetable": false },
-    { "name": "Description", "type": "Edm.String", "filterable": false, "sortable": false, "facetable": false },
-    { "name": "HotelName", "type": "Edm.String", "facetable": false },
-    { "name": "Category", "type": "Edm.String", "filterable": true, "facetable": true },
-    { "name": "Tags", "type": "Collection(Edm.String)", "filterable": true, "facetable": true },
-    { "name": "Rating", "type": "Edm.Int32", "filterable": true, "facetable": true },
-    { "name": "Location", "type": "Edm.GeographyPoint" }
-  ]
-}
-```
+### Choose which fields to attribute
 
-### Prerequisites
+Facets can be calculated over single-value fields and collections. Fields that work best in faceted navigation have these characteristics:
 
-Add faceting to new fields that contain plain text or numeric content. Supported data types include strings, dates, boolean fields, and numeric fields (but not vectors).
+* Human readable (nonvector) content.
+* Low cardinality (a few distinct values that repeat throughout documents in your search corpus).
+* Short descriptive values (one or two words) that render nicely in a navigation tree.
+
+The values within a field, and not the field name itself, produce the facets in a faceted navigation structure. If the facet is a string field named *Color*, facets are blue, green, and any other value for that field. Review field values to ensure there are no typos, nulls, or casing differences. Consider [assigning a normalizer](search-normalizers.md) to a filterable and facetable field to smooth out minor variations in the text. For example, "Canada", "CANADA", and "canada" would all be normalized to one bucket.
+
+### Avoid unsupported fields
 
 You can't set facets on existing fields, on vector fields, or fields of type `Edm.GeographyPoint` or `Collection(Edm.GeographyPoint)`.
 
-On complex fields, "facetable" must be null.
+On complex field collections, "facetable" must be null. 
 
 ### Start with new field definitions
 
-Because attribution determines how a field is indexed, many attributes can only be set when fields are created. This restriction applies to facets and filters. If your index already exists and you add a new field definition, existing documents in the index get a null value for the new field. This null value is replaced the next time you [refresh the index](search-howto-reindex.md).
+Attributes that affect how a field is indexed can only be set when fields are created. This restriction applies to facets and filters. 
 
-### Choosing which fields to attribute
+If your index already exists, you can add a new field definition that provides facets. Existing documents in the index get a null value for the new field. This null value is replaced the next time you [refresh the index](search-howto-reindex.md).
 
-Facets can be calculated over single-value fields and collections. Fields that work best in faceted navigation have these characteristics:
+#### [**Azure portal**](#tab/portal-facet)
 
-* Human readable (nonvector) content.
+1. In the search services page of the [Azure portal](https://portal.azure.com), go to the **Fields** tab of the index and select **Add field**.
 
-* Low cardinality (a few distinct values that repeat throughout documents in your search corpus).
+1. Provide a name, data type, and attributes. We recommend adding filterable because it's common to set filters based on a facet bucket in the response. We recommend sortable because filters produce unordered results, and you might want to sort them in your application.
 
-* Short descriptive values (one or two words) that render nicely in a navigation tree.
+   You can also set searchable if you also want to support full text search on the field, and retrievable if you want to include the field in the search response.
 
-The values within a field, and not the field name itself, produce the facets in a faceted navigation structure. If the facet is a string field named *Color*, facets are blue, green, and any other value for that field.
+   :::image type="content" source="media/search-faceted-navigation/portal-add-facetable-field.png" alt-text="Screenshot of the Add fields page in the Azure portal." border="true" lightbox="media/search-faceted-navigation/portal-add-facetable-field.png":::
 
-### Defaults in REST and Azure SDKs
+1. Save the field definition.
 
-If you're using one of the Azure SDKs, your code must explicitly set the "facetable" attribute on a field.
+#### [**REST**](#tab/rest-facet)
 
-The REST API has defaults for field attributes based on the [data type](/rest/api/searchservice/supported-data-types). The following data types are "filterable" and "facetable" by default:
+When you define an index schema, facets are enabled when you set `"facetable": true` on new fields that you add to an index. Although it's not strictly required, it's a best practice to also set the "filterable" attribute so that you can build the necessary filters that back the faceted navigation experience in your search application.
+
+Start with [Create or Update Index](search-how-to-create-search-index.md) request and specify the fields collection.
+
+  Here's a JSON example of the hotels sample index, showing "facetable" and "filterable" on low cardinality fields that contain single values or short phrases: "Category", "Tags", "Rating".
+
+  ```json
+  {
+    "name": "hotels",  
+    "fields": [
+      { "name": "hotelId", "type": "Edm.String", "key": true, "searchable": false, "sortable": false, "facetable": false },
+      { "name": "Description", "type": "Edm.String", "filterable": false, "sortable": false, "facetable": false },
+      { "name": "HotelName", "type": "Edm.String", "facetable": false },
+      { "name": "Category", "type": "Edm.String", "filterable": true, "facetable": true },
+      { "name": "Tags", "type": "Collection(Edm.String)", "filterable": true, "facetable": true },
+      { "name": "Rating", "type": "Edm.Int32", "filterable": true, "facetable": true },
+      { "name": "Location", "type": "Edm.GeographyPoint" }
+    ]
+  }
+  ```
+
+#### Defaults in REST
+
+Both the Azure portal and the REST API have defaults for field attributes based on the [data type](/rest/api/searchservice/supported-data-types). The following data types are "filterable" and "facetable" by default:
 
 * `Edm.String` and `Collection(Edm.String)`
 * `Edm.DateTimeOffset` and `Collection(Edm.DateTimeOffset)`
 * `Edm.Boolean` and`Collection(Edm.Boolean)`
 * `Edm.Int32`, `Edm.Int64`, `Edm.Double`, and their collection equivalents
 
-## Return facets in a query
+#### [**Azure SDKs**](#tab/sdk-facet)
 
-Recall that facets are calculated from results in a query response. You only get facets for documents found by the current query. 
+If you're using one of the Azure SDKs, your code must explicitly set facetable on a field.
 
-1. Facets are configured at query-time. Use the [Search POST](/rest/api/searchservice/documents/search-post) or [Search GET](/rest/api/searchservice/documents/search-get) request, or an equivalent Azure SDK API, to specify facets. 
+Assign the facet property to fields using APIs that create or update an index.
 
-1. Set facet query parameters in the request. In Search POST, `facets` are an array of facet expressions to apply to the search query. Each facet expression contains a field name, optionally followed by a comma-separated list of name-value pairs. Valid facet parameters are `count`, `sort`, `values`, `interval`, and `timeoffset`.
+* [Azure SDK for .NET: SearchIndex.Fields Property](/dotnet/api/azure.search.documents.indexes.models.searchindex.fields)
+* [Azure SDK for Python: SearchField Class](/python/api/azure-search-documents/azure.search.documents.indexes.models.searchfield)
+* [Azure SDK for Java: SearchField Class](/java/api/com.azure.search.documents.indexes.models.searchfield)
+* [Azure SDK for JavaScript: Simple Field interface](/javascript/api/@azure/search-documents/simplefield)
 
-    | Facet parameter | Description and usage |
-    |-----------------|-----------------------|
-    | `count` | Maximum number of facet terms; default is 10. An example is `Tags,count:5`. There's no upper limit on the number of terms, but higher values degrade performance, especially if the faceted field contains a large number of unique terms. This is due to the way faceting queries are distributed across shards. You can set count to zero or to a value that's greater than or equal to the number of unique values in the "facetable" field to get an accurate count across all shards. The tradeoff is increased latency.
-    | `sort` | Set to "count", "-count", "value", "-value". Use `count` to sort descending by count. Use `-count` to sort ascending by count. Use `value` to sort ascending by value. Use `-value` to sort descending by value (for example, `"facet=category,count:3,sort:count"` gets the top three categories in facet results in descending order by the number of documents with each city name). If the top three categories are Budget, Motel, and Luxury, and Budget has five hits, Motel has 6, and Luxury has 4, then the buckets are in the order Motel, Budget, Luxury. For `-value`, `"facet=rating,sort:-value"` produces buckets for all possible ratings, in descending order by value (for example, if the ratings are from 1 to 5, the buckets are ordered 5, 4, 3, 2, 1, irrespective of how many documents match each rating). |
-    | `values` | Set to pipe-delimited numeric or `Edm.DateTimeOffset` values specifying a dynamic set of facet entry values. For example, `"facet=baseRate,values:10 | 20"` produces three buckets: one for base rate 0 up to but not including 10, one for 10 up to but not including 20, and one for 20 and higher. A string `"facet=lastRenovationDate,values:2010-02-01T00:00:00Z"` produces two buckets: one for hotels renovated before February 2010, and one for hotels renovated February 1, 2010 or later. The values must be listed in sequential, ascending order to get the expected results. |
-    | `interval` | An integer interval greater than zero for numbers, or minute, hour, day, week, month, quarter, year for date time values. For example, `"facet=baseRate,interval:100"` produces buckets based on base rate ranges of size 100. If base rates are all between $60 and $600, there are buckets for 0-100, 100-200, 200-300, 300-400, 400-500, and 500-600. The string `"facet=lastRenovationDate,interval:year"` produces one bucket for each year when hotels were renovated. |
-    | `timeoffset` | Can be set to (`[+-]hh:mm, [+-]hhmm, or [+-]hh`). If used, the timeoffset parameter must be combined with the interval option, and only when applied to a field of type `Edm.DateTimeOffset`. The value specifies the UTC time offset to account for in setting time boundaries. For example: `"facet=lastRenovationDate,interval:day,timeoffset:-01:00"` uses the day boundary that starts at 01:00:00 UTC (midnight in the target time zone). |
+---
 
-`count` and `sort` can be combined in the same facet specification, but they can't be combined with interval or values, and interval and values can't be combined together.
+## Return facets in a query
 
-Interval facets on date time are computed based on the UTC time if `timeoffset` isn't specified. For example, for `"facet=lastRenovationDate,interval:day"`, the day boundary starts at 00:00:00 UTC.
+Recall that facets are dynamically calculated from results in a query response. You only get facets for documents found by the current query.
 
-### Basic facet example
+#### [**Azure portal**](#tab/portal-facet-response)
 
-The following example works against the [hotels sample index](search-get-started-portal.md). You can use **JSON view** in Search Explorer to paste in the JSON query. This example shows three facets for "Category", "Tags", and "Rating", with a count override on "Tags" and a range override for "Rating", which is otherwise stored as a double in the index.
+Use JSON view in Search Explorer to set facet parameters in the [Azure portal](https://portal.azure.com).
 
-```http
-POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
-{
-    "search": "*",
-    "facets": [ 
-        "Category", 
-        "Tags,count:5", 
-        "Rating,values:1|2|3|4|5"
-    ],
-    "count": true
-}
-```
+1. Select an index and open Search Explorer in JSON View.
+1. Provide a query in JSON. You can type it out, copy the JSON from a REST example, or use intellisense to help with syntax. Refer to the REST example in the next tab for reference on facet expressions.
+1. Select **Search** to return faceted results, articulated in JSON.
 
-For each faceted navigation tree, there's a default limit of the top 10 facet instances found by the query. This default makes sense for navigation structures because it keeps the values list to a manageable size. You can override the default by assigning a value to "count". For example, `"Tags,count:5"` reduces the number of tags under the Tags section to the top five.
+Here's a screenshot of the [basic facet query example](search-faceted-navigation-examples.md#basic-facet-example) on the [hotels sample index](search-get-started-portal.md). You can paste in other examples in this article to return the results in Search Explorer.
 
-For Numeric and DateTime values only, you can explicitly set values on the facet field (for example, `facet=Rating,values:1|2|3|4|5`) to separate results into contiguous ranges (either ranges based on numeric values or time periods). Alternatively, you can add "interval", as in `facet=Rating,interval:1`. 
+:::image type="content" source="media/search-faceted-navigation/portal-facet-query.png" alt-text="Screenshot of the Search Explorer page in the Azure portal." border="true" lightbox="media/search-faceted-navigation/portal-facet-query.png":::
 
-Each range is built using 0 as a starting point, a value from the list as an endpoint, and then trimmed of the previous range to create discrete intervals.
+#### [**REST**](#tab/rest-facet-response)
 
-### Distinct values example
+1. Facets are configured at query-time. Use the [Search POST](/rest/api/searchservice/documents/search-post) or [Search GET](/rest/api/searchservice/documents/search-get) request, or an equivalent Azure SDK API, to specify facets. 
 
-You can formulate a query that returns a distinct value count for each "facetable" field. This example formulates an empty or unqualified query (`"search": "*"`) that matches on all documents, but by setting `top` to zero, you get just the counts, with no results.
+1. Set facet query parameters in the request. In Search POST, `facets` are an array of facet expressions to apply to the search query. Each facet expression contains a field name, optionally followed by a comma-separated list of name-value pairs. Valid facet parameters are `count`, `sort`, `values`, `interval`, and `timeoffset`.
 
-For brevity, this query includes just two fields marked as "facetable" in the hotels sample index.
+    | Facet parameter | Description and usage |
+    |-----------------|-----------------------|
+    | `count` | Maximum number of facet terms per structure; default is 10. An example is `Tags,count:5`. There's no upper limit on the number of terms, but higher values degrade performance, especially if the faceted field contains a large number of unique terms. This is due to the way faceting queries are distributed across shards. You can set count to zero or to a value that's greater than or equal to the number of unique values in the "facetable" field to get an accurate count across all shards. The tradeoff is increased latency.
+    | `sort` | Set to `count`, `-count`, `value`, `-value`. Use `count` to sort descending by count. Use `-count` to sort ascending by count. Use `value` to sort ascending by value. Use `-value` to sort descending by value (for example, `"facet=category,count:3,sort:count"` gets the top three categories in facet results in descending order by the number of documents with each Category name). If the top three categories are Budget, Motel, and Luxury, and Budget has five hits, Motel has 6, and Luxury has 4, then the buckets are in the order Motel, Budget, Luxury. For `-value`, `"facet=rating,sort:-value"` produces buckets for all possible ratings, in descending order by value (for example, if the ratings are from 1 to 5, the buckets are ordered 5, 4, 3, 2, 1, irrespective of how many documents match each rating). |
+    | `values` | Set to pipe-delimited numeric or `Edm.DateTimeOffset` values specifying a dynamic set of facet entry values. For example, `"facet=baseRate,values:10 | 20"` produces three buckets: one for base rate 0 up to but not including 10, one for 10 up to but not including 20, and one for 20 and higher. A string `"facet=lastRenovationDate,values:2010-02-01T00:00:00Z"` produces two buckets: one for hotels renovated before February 2010, and one for hotels renovated February 1, 2010 or later. The values must be listed in sequential, ascending order to get the expected results. |
+    | `interval` | An integer interval greater than zero for numbers, or minute, hour, day, week, month, quarter, year for date time values. For example, `"facet=baseRate,interval:100"` produces buckets based on base rate ranges of size 100. If base rates are all between $60 and $600, there are buckets for 0-100, 100-200, 200-300, 300-400, 400-500, and 500-600. The string `"facet=lastRenovationDate,interval:year"` produces one bucket for each year when hotels were renovated. |
+    | `timeoffset` | Can be set to (`[+-]hh:mm, [+-]hhmm, or [+-]hh`). If used, the `timeoffset` parameter must be combined with the interval option, and only when applied to a field of type `Edm.DateTimeOffset`. The value specifies the UTC time offset to account for in setting time boundaries. For example: `"facet=lastRenovationDate,interval:day,timeoffset:-01:00"` uses the day boundary that starts at 01:00:00 UTC (midnight in the target time zone). |
 
-```http
-POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
-{
-    "search": "*",
-    "count": true,
-    "top": 0,
-    "facets": [ 
-        "Category", "Address/StateProvince""
-    ]
-}
-```
+`count` and `sort` can be combined in the same facet specification, but they can't be combined with `interval` or `values`, and `interval` and `values` can't be combined together.
 
-Results from this query are as follows:
+Interval facets on date time are computed based on the UTC time if `timeoffset` isn't specified. For example, for `"facet=lastRenovationDate,interval:day"`, the day boundary starts at 00:00:00 UTC.
 
-```json
-{
-  "@odata.count": 50,
-  "@search.facets": {
-    "Address/StateProvince": [
-      {
-        "count": 9,
-        "value": "WA"
-      },
-      {
-        "count": 6,
-        "value": "CA "
-      },
-      {
-        "count": 4,
-        "value": "FL"
-      },
-      {
-        "count": 3,
-        "value": "NY"
-      },
-      {
-        "count": 3,
-        "value": "OR"
-      },
-      {
-        "count": 3,
-        "value": "TX"
-      },
-      {
-        "count": 2,
-        "value": "GA"
-      },
-      {
-        "count": 2,
-        "value": "MA"
-      },
-      {
-        "count": 2,
-        "value": "TN"
-      },
-      {
-        "count": 1,
-        "value": "AZ"
-      }
-    ],
-    "Category": [
-      {
-        "count": 13,
-        "value": "Budget"
-      },
-      {
-        "count": 12,
-        "value": "Suite"
-      },
-      {
-        "count": 7,
-        "value": "Boutique"
-      },
-      {
-        "count": 7,
-        "value": "Resort and Spa"
-      },
-      {
-        "count": 6,
-        "value": "Extended-Stay"
-      },
-      {
-        "count": 5,
-        "value": "Luxury"
-      }
-    ]
-  },
-  "value": []
-}
-```
+---
 
 ## Best practices for working with facets
 
 This section is a collection of tips and workarounds that are helpful for application development.
 
-### Initialize a faceted navigation structure
+We recommend the [C#: Add search to web apps](tutorial-csharp-overview.md) for an example of faceted navigation that includes code for the presentation layer. The sample also includes filters, suggestions, and autocomplete. It uses JavaScript and React for the presentation layer.
+
+### Initialize a faceted navigation structure with an unqualified or empty search string
 
-It's useful to initialize a search page with an open query (`"search": "*"`) to completely fill in the faceted navigation structure. As soon as you pass query terms in the request, the faceted navigation structure is scoped to just the matches in the results, rather than the entire index.
+It's useful to initialize a search page with an open query (`"search": "*"`) to completely fill in the faceted navigation structure. As soon as you pass query terms in the request, the faceted navigation structure is scoped to just the matches in the results, rather than the entire index. This practice is helpful for verifying facet and filter behaviors during testing. If you include match criteria in the query, the response excludes documents that don't match, which has the potential downstream effect of excluding facets.
 
 ### Clear facets
 
@@ -315,7 +251,15 @@ Remember that you can't use `Edm.GeographyPoint` or `Collection(Edm.GeographyPoi
 
 ### Check for bad data
 
-As you prepare data for indexing, check fields for null values, misspellings or case discrepancies, and single and plural versions of the same word. By default, filters and facets don't undergo lexical analysis or [spell check](speller-how-to-add.md), which means that all values of a "facetable" field are potential facets, even if the words differ by one character. Optionally, you can [assign a normalizer](search-normalizers.md) to a "filterable" and "facetable" field to smooth out variations in casing and characters.
+As you prepare data for indexing, check fields for null values, misspellings or case discrepancies, and single and plural versions of the same word. By default, filters and facets don't undergo lexical analysis or [spell check](speller-how-to-add.md), which means that all values of a "facetable" field are potential facets, even if the words differ by one character. 
+
+[Normalizers](search-normalizers.md) can mitigate data discrepancies, correcting for casing and character differences. Otherwise, to inspect your data, you can check fields at their source, or run queries that return values from the index.
+
+An index isn't the best place to fix nulls or invalid values. You should fix data problems in your source, assuming it's a database or persistent storage, or in a data cleansing step that you perform prior to indexing. 
+
+### Ordering facet buckets
+
+Although you can sort within a bucket, there's no parameters for controlling the order of facet buckets in the navigation structure as a whole. If you want facet buckets in a specific order, you must provide it in application code.
 
 ### Discrepancies in facet counts
 
@@ -346,4 +290,5 @@ Content type
 
 ## Next steps
 
-We recommend the [C#: Add search to web apps](tutorial-csharp-overview.md) for an example of faceted navigation that includes code for the presentation layer. The sample also includes filters, suggestions, and autocomplete. It uses JavaScript and React for the presentation layer.
+> [!div class="nextstepaction"]
+> [Facet navigation examples](search-faceted-navigation-examples.md)
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates Made to Faceted Navigation Article"
}
```

### Explanation
The recent modifications to the `search-faceted-navigation.md` file involve a revision of the content to enhance clarity and accuracy regarding the implementation and use of faceted navigation in Azure AI Search. Key changes include:

1. **Title and Description Updates**: The title has been changed from "Add a faceted navigation category hierarchy" to "Add facets to a query," and the description has been slightly refined to emphasize self-directed navigation.

2. **Content Reorganization**: The article has been restructured to provide clearer steps for implementing faceted navigation and to highlight the availability of additional capabilities through preview APIs, including hierarchical facets and aggregations.

3. **Clarification of Important Concepts**: Terms like "self-directed filtering" have been emphasized, and the language has been revised to clearly distinguish between query results and the underlying data structures that support faceted navigation.

4. **Adding Examples and Features**: The document mentions the availability of example syntax for new features in the linked "Facet examples" section, guiding users towards more practical implementations.

5. **Introduction of Best Practices**: New sections provide practical guidance for developing an effective faceted navigation experience, including tips for handling data quality, initializing the navigation structure, and considerations for field attributes.

6. **Technical Adjustments**: Some technical details regarding field attributes, facets, and facet query parameters have been clarified, ensuring that users can easily follow the instructions related to how to enable and use facets in queries effectively.

7. **Removal of Obsolete Content**: A significant number of lines were removed from the document, indicating a streamlining of the content, potentially removing redundant or outdated information to improve focus on essential instructions.

In summary, these updates aim to enhance the usability and effectiveness of the `search-faceted-navigation.md` document, making it more helpful for developers looking to implement faceted navigation in their Azure AI Search applications.

## articles/search/search-faq-frequently-asked-questions.yml{#item-eab2ba}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ metadata:
   ms.author: haileytapia
   ms.service: azure-ai-search
   ms.topic: faq
-  ms.date: 01/16/2025
+  ms.date: 03/21/2025
 title: Azure AI Search Frequently Asked Questions
 summary:  Find answers to commonly asked questions about Azure AI Search.
 
@@ -36,7 +36,7 @@ sections:
         answer: |
           For vectors, the embedding models you use determines the linguistic experience. 
           
-          For nonvector strings and numbers, the default analyzer used for tokenization is standard Lucene and it's language agnostic. Otherwise, language support is expressed through [language analyzers](index-add-language-analyzers.md#supported-language-analyzers) that apply linguistic rules to inbound (indexing) and outbound (queries) content. Some features, such as [speller](speller-how-to-add.md#supported-languages) and [query rewrite](semantic-how-to-query-rewrite.md), are limited to a subset of languages.
+          For nonvector strings and numbers, the default analyzer used for tokenization is standard Lucene, which is language agnostic. Otherwise, language support is expressed through [language analyzers](index-add-language-analyzers.md#supported-language-analyzers) that apply linguistic rules to inbound (indexing) and outbound (queries) content. Some features, such as [speller](speller-how-to-add.md#supported-languages) and [query rewrite](semantic-how-to-query-rewrite.md), are limited to a subset of languages.
 
       - question: |
           How do I integrate search into my solution?
@@ -56,15 +56,27 @@ sections:
           You can't pause a search service. In Azure AI Search, computing resources are allocated when the service is created. It's not possible to release and reclaim those resources on-demand.
 
       - question: |
-          Can I upgrade, downgrade, rename or move the service?
+          Can I upgrade or downgrade the service?
         answer: |
-          Service tier, name, and region are fixed for the lifetime of the service.
+          Services created before April 2024 in select regions can be [upgraded to higher capacity clusters](search-how-to-upgrade.md). Downgrading your service isn't supported. 
+          
+          To get more capacity, you can also [switch to a higher pricing tier](search-capacity-planning.md#change-your-pricing-tier). Your region can't have [capacity constraints on the higher tier](search-region-support.md), and you can only move up between Basic and Standard (S1, S2, and S3) tiers, such as going from Basic to S1. Currently, you can't switch to a lower tier.
+          
+      - question: |
+          Can I rename or move the service?
+        answer: |
+          Service name and region are fixed for the lifetime of the service.
           
       - question: |
           If I migrate my search service to another subscription or resource group, should I expect any downtime?
         answer: |
           As long as you follow the [checklist before moving resources](/azure/azure-resource-manager/management/move-resource-group-and-subscription) and make sure each step is completed, there shouldn't be any downtime.
 
+      - question: |
+          Why do I see different storage limits for same-tier search services?
+        answer: |
+          Storage limits can vary by service creation date. In most supported regions, [newer services have higher storage limits than older services](search-limits-quotas-capacity.md#partition-storage-gb), even if they're on the same tier. However, you might be able to [upgrade your old service](search-how-to-upgrade.md) to access the new limits.
+
   - name: Indexing 
     questions:
       - question: |
@@ -130,7 +142,12 @@ sections:
       - question: |
           Why do I see different vector index size limits between my new search services and existing search services?
         answer: |
-          Azure AI Search rolled out improved vector index size limits worldwide for new search services, but [some regions experience capacity constraints](search-region-support.md), and some regions don't have the required infrastructure. New search services created in supported regions should see increased vector index size limits. Unfortunately, we can't migrate existing services to the new limits. Also, only vector indexes that use the Hierarchical Navigable Small World (HNSW) algorithm report on vector index size in the Azure portal. If your index uses exhaustive KNN, vector index size is reported as zero, even though the index contains vectors. 
+          Azure AI Search rolled out improved vector index size limits worldwide for new search services, but [some regions experience capacity constraints](search-region-support.md), and some regions don't have the required infrastructure. New search services created after May 2024 in supported regions should see increased vector index size limits. Alternatively, if you have an existing service in a supported region, you can [upgrade your service](search-how-to-upgrade.md) to access the new limits.
+          
+      - question: |
+          Why does my vector index show zero storage?
+        answer: |    
+          Only vector indexes that use the Hierarchical Navigable Small World (HNSW) algorithm report on vector index size in the Azure portal. If your index uses exhaustive KNN, vector index size is reported as zero, even though the index contains vectors. 
 
       - question: |
           How do I enable vector search on a search index?
@@ -141,7 +158,7 @@ sections:
           
           * Add a "vectorSearch" section to the index schema specifying the configuration used by vector search fields, including the parameters of the Approximate Nearest Neighbor algorithm used, like HNSW.
           
-          * Use the latest stable version[**2024-07-01**](/rest/api/searchservice), or an Azure SDK to create or update the index, load documents, and issue queries. For more information, see [Create a vector index](vector-search-how-to-create-index.md).
+          * Use the latest stable version, [**2024-07-01**](/rest/api/searchservice), or an Azure SDK to create or update the index, load documents, and issue queries. For more information, see [Create a vector index](vector-search-how-to-create-index.md).
 
   - name: Queries
     questions:
@@ -189,7 +206,7 @@ sections:
       - question: |
           Does Azure AI Search process customer data in other regions?
         answer: |
-          Processing (vectorization or applied AI transformations) is performed in the Geo that hosts the Azure AI services used by skills, or the Azure apps or functions hosting custom skills, or the Azure OpenAI or Azure AI Foundry region that hosts your deployed models. These resources are specified by you, so you can choose whether to provision them in the same Geo as your search service or not
+          Processing (vectorization or applied AI transformations) is performed in the Geo that hosts the Azure AI services used by skills, or the Azure apps or functions hosting custom skills, or the Azure OpenAI or Azure AI Foundry region that hosts your deployed models. These resources are specified by you, so you can choose whether to deploy them in the same Geo as your search service or not.
           
           If you send data to external (non-Azure) models or services, the processing location is determined by the external service. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Azure AI Search FAQ Document"
}
```

### Explanation
The recent changes in the `search-faq-frequently-asked-questions.yml` file include several updates aimed at improving the content and clarify frequently asked questions about Azure AI Search. Key modifications consist of:

1. **Date Update**: The metadata field has been updated to reflect a new last modified date, changing from January 16, 2025, to March 21, 2025.

2. **Clarifications in Answers**: Several answers have been rephrased for clarity and coherence. For instance, explanations regarding the default analyzers and linguistic support have been refined to ensure clarity without altering the original meaning.

3. **Addition of Questions**: New questions have been added to the FAQ section, providing users with additional valuable information:
   - A new question regarding the ability to rename or move services has been introduced, emphasizing fixed service names and regions.
   - An inquiry regarding differing storage limits for same-tier services has been added, noting that new services tend to have higher limits and giving information on how older services can be upgraded.

4. **Enhanced Descriptions**: Some existing answers have been expanded with more detailed explanations, particularly concerning restrictions on service modifications and discrepancies in storage limits across services.

5. **Formatting Improvements**: Minor formatting and wording changes have been made throughout to improve readability and consistency, such as clarifying the mention of algorithms related to vector index sizes.

6. **Removal of Obsolete Content**: A few lines of outdated or less relevant information have been removed, streamlining the content to focus on the most pertinent details that users may seek.

These updates collectively serve to enhance the usability of the FAQ document, ensuring that users can easily find clear and concise answers to their questions about Azure AI Search functionalities.

## articles/search/search-how-to-index-sql-database.md{#item-86d873}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 11/20/2024
+ms.date: 03/18/2025
 ---
 
 # Index data from Azure SQL Database
@@ -30,13 +30,17 @@ This article also provides:
 
 ## Prerequisites
 
-+ An [Azure SQL database](/azure/azure-sql/database/sql-database-paas-overview) with data in a single table or view, or a [SQL Managed Instance with a public endpoint](search-how-to-index-sql-managed-instance.md).
++ An [Azure SQL database](/azure/azure-sql/database/sql-database-paas-overview) or a [SQL Managed Instance with a public endpoint](search-how-to-index-sql-managed-instance.md).
 
-  Use a table if your data is large or if you need [incremental indexing](#CaptureChangedRows) using SQL's native change detection capabilities.
++ A single table or view.
 
-  Use a view if you need to consolidate data from multiple tables. Large views aren't ideal for SQL indexer. A workaround is to create a new table just for ingestion into your Azure AI Search index. You can use SQL integrated change tracking to track new and changed rows, which is easier to implement than High Water Mark.
+  Use a table if your data is large or if you need incremental indexing using SQL's native change detection capabilities ([SQL integrated change tracking](#indexing-new-changed-and-deleted-rows)) to reflect new, changed, and deleted rows in the search index.
 
-+ Read permissions. Azure AI Search supports SQL Server authentication, where the user name and password are provided on the connection string. Alternatively, you can [set up a managed identity and use Azure roles](search-howto-managed-identities-sql.md).
+  Use a view if you need to consolidate data from multiple tables. Large views aren't ideal for SQL indexer. A workaround is to create a new table just for ingestion into your Azure AI Search index. If you choose to go with a view, you can use [High Water Mark](#indexing-new-changed-and-deleted-rows) for change detection, but must use a workaround for deletion detection.
+
++ Primary key must be single-valued. On a table, it must also be non-clustered for full SQL integrated change tracking.
+
++ Read permissions. Azure AI Search supports SQL Server authentication, where the user name and password are provided on the connection string. Alternatively, you can [set up a managed identity and use Azure roles](search-howto-managed-identities-sql.md) with membership in **SQL Server Contributor** or **SQL DB Contributor** roles.
 
 To work through the examples in this article, you need the Azure portal or a [REST client](search-get-started-rest.md). If you're using Azure portal, make sure that access to all public networks is enabled in the Azure SQL firewall and that the client has access via an inbound rule. For a REST client that runs locally, configure the SQL Server firewall to allow inbound access from your device IP address. Other approaches for creating an Azure SQL indexer include Azure SDKs.
 
@@ -52,12 +56,12 @@ Use these instructions to create and load a table in Azure SQL Database for test
 
 1. On your Azure SQL database, select **Query editor (preview)** and then select **New Query**.
 
-1. Paste in and then run the T-SQL script that creates the hotels table.
+1. Paste in and then run the T-SQL script that creates the hotels table. A non-clustered primary key is a requirement for SQL integrated change tracking.
 
    ```tsql
    CREATE TABLE tbl_hotels
     (
-        Id TINYINT PRIMARY KEY,
+        Id TINYINT PRIMARY KEY NONCLUSTERED,
         Modified DateTime NULL DEFAULT '0000-00-00 00:00:00',
         IsDeleted TINYINT,
         HotelName VARCHAR(40),
@@ -93,9 +97,9 @@ Use these instructions to create and load a table in Azure SQL Database for test
    SELECT Description FROM tbl_hotels;
     ```
 
-You should see results similar to the following screenshot.
+   You should see results similar to the following screenshot.
 
-:::image type="content" source="media/search-how-to-index-sql-database/tsql-query-results.png" alt-text="Screenshot of query results showing the description field.":::
+   :::image type="content" source="media/search-how-to-index-sql-database/tsql-query-results.png" alt-text="Screenshot of query results showing the description field.":::
 
 The Description field provides the most verbose content. You should target this field for full text search and optional vectorization.
 
@@ -104,39 +108,42 @@ Now that you have a database table, you can use the Azure portal, REST client, o
 > [!TIP]
 > Another resource that provides sample content and code can be found on [Azure-Samples/SQL-AI-samples](https://github.com/Azure-Samples/SQL-AI-samples/tree/main/AzureSQLACSSamples/src).
 
-## Use the Azure portal
-
-You can use either the **Import data** wizard or **Import and vectorize data** wizard to automate indexing from an SQL database table or view. The data source configuration is similar for both wizards.
-
-1. [Start the wizard](search-import-data-portal.md#starting-the-wizards).
-
-1. On **Connect to your data**, select or verify that the data source type is either *Azure SQL Database* or *SQL database*.
+## Set up the indexer pipeline
 
-   The data source name refers to the data source connection object in Azure AI Search. If you use the vector wizard, your data source name is autogenerated using a custom prefix specified at the end of the wizard workflow.
+In this step, specify the data source, index, and indexer.
 
-1. Specify the server name, database name, and table or view name.
+### [**Azure portal**](#tab/portal-sql)
 
-   the Azure portal validates the connection. If the database is paused due to inactivity, navigate to the database server page and make sure database status is *online*. You can run a query on any table to activate the database.
+1. Make sure your SQL database is active and not paused due to inactivity. In the Azure portal, navigate to the database server page and verify the database status is *online*. You can run a query on any table to activate the database.
 
    :::image type="content" source="media/search-how-to-index-sql-database/database-online.png" alt-text="Screenshot of the database status page in the Azure portal.":::
 
-1. Specify an authentication method, either a SQL Server login defined during server setup, or a managed identity.
+1. Make sure you have a table or view that meets the requirements for indexers and change detection.
 
-   If you [configure Azure AI Search to use a managed identity](search-howto-managed-identities-data-sources.md), and you create a role assignment on the database server that grants **SQL Server Contributor** or **SQL DB Contributor** permissions to the identity, your indexer can connect to Azure SQL using Microsoft Entra ID and roles.
+   First, you can only pull from a single table or view. We recommend tables because they support SQL integrated change tracking policy, which detects new, updated, and deleted rows. A high water mark policy doesn't support row deletion and is harder to implement.
 
-1. For the **Import and vectorize data** wizard, you can specify options for change and deletion tracking.
+   Second, the primary key must be a single value (compound keys aren't supported) and non-clustered.
 
-   + Deletion tracking is based on [soft delete using custom metadata](#soft-delete-column-deletion-detection-policy).
+1. Switch to your search service and create a data source. Under **Search management** > **Data sources**, select **Add data source**:
 
-   + Change tracking is based on [SQL Server integrated change tracking](#sql-integrated-change-tracking-policy) or [high water mark change tracking](#high-water-mark-change-detection-policy).
+   1. For data source type, choose *Azure SQL Database*.
+   1. Provide a name for the data source object on Azure AI Search.
+   1. Use the dropdowns to select the subscription, account type, server, database, table or view, schema, and table name.
+   1. For change tracking we recommend **SQL Integrated Change Tracking Policy**.
+   1. For authentication, we recommend connecting with a [managed identity](search-howto-managed-identities-data-sources.md). Your search service must have **SQL Server Contributor** or **SQL DB Contributor** role membership on the database.
+   1. Select **Create** to create the data source.
 
-1. Continue with the remaining steps to complete the wizard:
+   :::image type="content" source="media/search-how-to-index-sql-database/search-data-source.png" alt-text="Screenshot of the data source creation page in the Azure portal.":::
 
-   + [Quickstart: Import data wizard](search-get-started-portal.md)
+1. Start the **Import data** wizard to create the index and indexer.
 
-   + [Quickstart: Import and vectorize data wizard](search-get-started-portal-import-vectors.md)
+   1. On the Overview page, select **Import data**.
+   1. Select the data source you just created, and select **Next**.
+   1. Skip the **Add cognitive skills (Optional)** page.
+   1. On **Customize target index**, name the index, set the key to your primary key in the table, and then group select *Retrievable* and *Searchable* for all fields, and optionally add *Filterable* and *Sortable* for short strings or numeric values.
+   1. On **Create an indexer**, name the indexer and select **Submit**.
 
-## Use the REST APIs
+### [**REST**](#tab/test-sql)
 
 This section demonstrates the REST API calls that create a data source, index, and indexer.
 
@@ -178,6 +185,7 @@ The data source definition specifies the data to index, credentials, and policie
    + Alternatively, you can specify a managed identity connection string that doesn't include database secrets with the following format: `Initial Catalog|Database=<your database name>;ResourceId=/subscriptions/<your subscription ID>/resourceGroups/<your resource group name>/providers/Microsoft.Sql/servers/<your SQL Server name>/;Connection Timeout=connection timeout length;`.
 
     For more information, see [Connect to Azure SQL Database indexer using a managed identity](search-howto-managed-identities-sql.md).
+
 > [!NOTE]
 > For the container name property, the value is restricted to only allow letters, numbers, underscores (_), dots (.), single dashes (-), and square brackets ([])
 
@@ -280,6 +288,8 @@ Once the index and data source have been created, you're ready to create the ind
 
 An indexer runs automatically when it's created. You can prevent this by setting "disabled" to true. To control indexer execution, [run an indexer on demand](search-howto-run-reset-indexers.md) or [put it on a schedule](search-howto-schedule-indexers.md).
 
+---
+
 ## Check indexer status
 
 To monitor the indexer status and execution history, check the indexer execution history in the Azure portal, or send a [Get Indexer Status](/rest/api/searchservice/indexers/get-status) REST API request
@@ -350,22 +360,23 @@ For Azure SQL indexers, there are two change detection policies:
 
 + "SqlIntegratedChangeTrackingPolicy" (applies to tables only)
 
-+ "HighWaterMarkChangeDetectionPolicy" (works for tables and views)
++ "HighWaterMarkChangeDetectionPolicy" (works for views)
 
 ### SQL Integrated Change Tracking Policy
 
 We recommend using "SqlIntegratedChangeTrackingPolicy" for its efficiency and its ability to identify deleted rows.
 
 Database requirements:
 
-+ SQL Server 2012 SP3 and later, if you're using SQL Server on Azure VMs
-+ Azure SQL Database or SQL Managed Instance
-+ Tables only (no views)
-+ On the database, [enable change tracking](/sql/relational-databases/track-changes/enable-and-disable-change-tracking-sql-server) for the table
-+ No composite primary key (a primary key containing more than one column) on the table
-+ No clustered indexes on the table. As a workaround, any clustered index would have to be dropped and re-created as nonclustered index, however, performance might be affected in the source compared to having a clustered index
++ Azure SQL Database or SQL Managed Instance. SQL Server 2016 or later if you're using an Azure VM.
++ Database must have [change tracking enabled](/sql/relational-databases/track-changes/enable-and-disable-change-tracking-sql-server)
++ Tables only (no views).
++ Tables can't be clustered. To meet this requirement, drop the clustered index and recreate it as non-clustered index. This workaround often degrades performance. Duplicating content in a second table that's dedicated to indexer processing can be a helpful mitigation. 
++ Tables can't be empty. If you use TRUNCATE TABLE to clear rows, a reset and rerun of the indexer won't remove the corresponding search documents. To remove orphaned search documents, you must [index them with a delete action](search-howto-reindex.md#delete-orphan-documents).
++ Primary key can't be a compound key (containing more than one column).
++ Primary key must be non-clustered if you want deletion detection.
 
-Change detection policies are added to data source definitions. To use this policy, create or update your data source like this:
+Change detection policies are added to data source definitions. To use this policy, edit the data source definition in the Azure portal, or use REST to update your data source like this:
 
 ```http
 POST https://myservice.search.windows.net/datasources?api-version=2024-07-01
@@ -382,10 +393,10 @@ api-key: admin-key
     }
 ```
 
-When using SQL integrated change tracking policy, don't specify a separate data deletion detection policy. The SQL integrated change tracking policy has built-in support for identifying deleted rows. However, for the deleted rows to be detected automatically, the document key in your search index must be the same as the primary key in the SQL table. 
+When using SQL integrated change tracking policy, don't specify a separate data deletion detection policy. The SQL integrated change tracking policy has built-in support for identifying deleted rows. However, for the deleted rows to be detected automatically, the document key in your search index must be the same as the primary key in the SQL table, and the primary key must be non-clustered.
 
-> [!NOTE]  
-> When using [TRUNCATE TABLE](/sql/t-sql/statements/truncate-table-transact-sql) to remove a large number of rows from a SQL table, the indexer needs to be [reset](/rest/api/searchservice/indexers/reset) to reset the change tracking state to pick up row deletions.
+<!-- > [!NOTE]  
+> When using [TRUNCATE TABLE](/sql/t-sql/statements/truncate-table-transact-sql) to remove a large number of rows from a SQL table, the indexer needs to be [reset](/rest/api/searchservice/indexers/reset) to reset the change tracking state to pick up row deletions. -->
 
 <a name="HighWaterMarkPolicy"></a>
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to SQL Database Indexing Guide"
}
```

### Explanation
The recent modifications to the `search-how-to-index-sql-database.md` file introduce several updates aimed at clarifying the process of indexing data from an Azure SQL Database. Key changes include:

1. **Date Update**: The document metadata has been revised to reflect a new last updated date, now set to March 18, 2025.

2. **Prerequisites Clarification**: The prerequisites section has been improved for better clarity. It emphasizes that users can utilize either an Azure SQL database or a SQL Managed Instance with a public endpoint. The section has been reorganized to clearly outline the need for a primary key to be single-valued and should be non-clustered for full SQL integrated change tracking functionality.

3. **Expanded Requirements**: Additional explicit requirements are included, like the necessity for the primary key to be non-clustered and the recommendation to use SQL integrated change tracking for change detection, which enhances users' understanding of setup requirements.

4. **Streamlined Instructions**: Instructions on creating a table and data source have been streamlined and clarified. The document emphasizes the importance of following specific criteria when setting up the indexer pipeline, such as ensuring the SQL database is active and correctly configuring authentication methods.

5. **New Section Introduced**: A new section titled "Set up the indexer pipeline" has been added, which replaces the previous "Use the Azure portal" section. This new section organizes the information more logically and aligns steps that users need to perform to create and load an index from an SQL database.

6. **Improved Notes and Warnings**: Warnings about common issues, such as needing to reset the indexer following a TRUNCATE TABLE operation to handle deletions properly, have been emphasized for better visibility.

7. **Technical Adjustments**: The document has undergone minor technical refinements to ensure that it communicates key actions succinctly, particularly in how to handle change detection policies.

These updates collectively enhance the guidance provided in the document, ensuring that users have a clear, step-by-step understanding of how to properly index data from an Azure SQL Database into Azure AI Search, along with the necessary considerations and requirements for effective implementation.

## articles/search/search-how-to-large-index.md{#item-d34e42}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: conceptual
-ms.date: 02/25/2025
+ms.date: 03/21/2025
 ---
 
 # Index large data sets in Azure AI Search
@@ -21,7 +21,7 @@ These strategies assume familiarity with the [two basic approaches for importing
 
 This article complements [Tips for better performance](search-performance-tips.md), which offers best practices on index and query design. A well-designed index that includes only the fields and attributes you need is an important prerequisite for large-scale indexing.
 
-We recommend using a newer search service, created after April 3, 2024, for [higher storage per partition](search-limits-quotas-capacity.md#service-limits).
+We recommend using a search service created after April 3, 2024 for [higher storage per partition](search-limits-quotas-capacity.md#service-limits). Older services can also be [upgraded to benefit from higher partition storage](search-how-to-upgrade.md).
 
 > [!NOTE]
 > The strategies described in this article assume a single large data source. If your solution requires indexing from multiple data sources, see [Index multiple data sources in Azure AI Search](/samples/azure-samples/azure-search-dotnet-scale/multiple-data-sources/) for a recommended approach.
@@ -46,7 +46,7 @@ Because the optimal batch size depends on your index and your data, the best app
 
 ### Manage threads and a retry strategy
 
-Indexers have built-in thread management, but when you're using the push APIs, your application code needs to manage threads. Make sure there are sufficient threads to make full use of the available capacity, especially if you've recently increased partitions or upgraded to a higher tier search service. 
+Indexers have built-in thread management, but when you're using the push APIs, your application code needs to manage threads. Make sure there are sufficient threads to make full use of the available capacity, especially if you recently [upgraded your service](search-how-to-upgrade.md), [switched to a higher tier](search-capacity-planning.md#change-your-pricing-tier), or [increased partitions](search-capacity-planning.md#add-or-remove-partitions-and-replicas).
 
 1. [Increase the number of concurrent threads](tutorial-optimize-indexing-push-api.md#use-multiple-threadsworkers) in your client code.
 
@@ -97,7 +97,7 @@ When there are no longer any new or updated documents in the data source, indexe
 For more information about setting schedules, see [Create Indexer REST API](/rest/api/searchservice/indexers/create) or see [Schedule indexers for Azure AI Search](search-howto-schedule-indexers.md).
 
 > [!NOTE]
-> Some indexers that run on an older runtime architecture have a 24-hour rather than 2-hour maximum processing window. The two-hour limit is for newer content processors that run in an [internally managed multitenant environment](search-howto-run-reset-indexers.md#indexer-execution-environment). Whenever possible, Azure AI Search tries to offload indexer and skillset processing to the multi-tenant environment. If the indexer can't be migrated, it runs in the private environment and it can run for as long as 24 hours. If you're scheduling an indexer that exhibits these characteristics, assume a 24-hour processing window.
+> Some indexers that run on an older runtime architecture have a 24-hour rather than 2-hour maximum processing window. The two-hour limit is for newer content processors that run in an [internally managed multitenant environment](search-howto-run-reset-indexers.md#indexer-execution-environment). Whenever possible, Azure AI Search tries to offload indexer and skillset processing to the multitenant environment. If the indexer can't be migrated, it runs in the private environment and it can run for as long as 24 hours. If you're scheduling an indexer that exhibits these characteristics, assume a 24-hour processing window.
 
 <a name="parallel-indexing"></a>
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Large Indexing Guide for Azure AI Search"
}
```

### Explanation
The modifications to the `search-how-to-large-index.md` document primarily focus on updating content to improve clarity and provide users with accurate information regarding large data set indexing in Azure AI Search. Key changes include:

1. **Date Update**: The metadata has been updated to reflect a new last modified date, now set to March 21, 2025.

2. **Clarifications about Search Services**: The recommendation for using a newer search service created after April 3, 2024, has been rephrased. The update now highlights that older services can also be upgraded to benefit from higher partition storage, providing users with more options for maximizing storage capabilities.

3. **Enhanced Thread Management Guidance**: The section detailing thread management during indexing has been enhanced. It now advises users to consider recent changes—such as upgrading the service, switching to a higher tier, or increasing partitions—to ensure adequate thread management for optimal performance when using push APIs.

4. **Content Consistency**: Overall consistency improvements were made throughout the document, particularly in terms of clarifying the processing limits for indexers. The distinction between indexers running on older architectures and newer content processors is reiterated, ensuring users have a clear understanding of the expected performance windows.

These updates work together to refine the guidance offered in the document, ensuring clarity and comprehensiveness for users who need to index large datasets with Azure AI Search, while also informing them of the best practices and options available for optimization.

## articles/search/search-how-to-load-search-index.md{#item-a72573}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.author: heidist
 
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 10/31/2024
+ms.date: 03/21/2025
 ---
 
 # Load data into a search index in Azure AI Search
@@ -28,7 +28,7 @@ You can prepare these documents yourself, but if content resides in a [supported
 
 Once data is indexed, the physical data structures of the index are locked in. For guidance on what can and can't be changed, see [Update and rebuild an index](search-howto-reindex.md).
 
-Indexing isn't a background process. A search service will balance indexing and query workloads, but if [query latency is too high](search-performance-analysis.md#impact-of-indexing-on-queries), you can either [add capacity](search-capacity-planning.md#adjust-capacity) or identify periods of low query activity for loading an index.
+Indexing isn't a background process. A search service will balance indexing and query workloads, but if [query latency is too high](search-performance-analysis.md#impact-of-indexing-on-queries), you can either [add capacity](search-capacity-planning.md#add-or-remove-partitions-and-replicas) or identify periods of low query activity for loading an index.
 
 For more information, see [Data import strategies](search-what-is-data-import.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Data Loading Instructions in Azure AI Search"
}
```

### Explanation
The recent modifications to the `search-how-to-load-search-index.md` file include several updates aimed at improving the clarity and accuracy of instructions for loading data into a search index in Azure AI Search. Key changes include:

1. **Date Update**: The document has been updated to reflect a new last modified date, now set to March 21, 2025.

2. **Content Clarity**: The instruction regarding handling query latency during indexing has been refined. The previous mention of "adjust capacity" has been changed to link specifically to the section on adding or removing partitions and replicas. This helps users better understand the specific actions they can take to alleviate query performance issues due to indexing.

3. **Overall Structural Integrity**: The overall structure of the document has been preserved, with minor edits that enhance readability and ensure that the flow of information remains clear.

These updates serve to make the guidance provided in the document more precise and actionable, ensuring that users have a coherent understanding of how to efficiently load data into their Azure AI Search indexes while managing performance implications.

## articles/search/search-how-to-semantic-chunking.md{#item-4a1d07}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: rawan
 ms.author: rawan
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 11/22/2024
+ms.date: 04/07/2025
 ms.custom:
   - references_regions
   - ignite-2024
@@ -36,13 +36,15 @@ For illustration purposes, this article uses the [sample health plan PDFs](https
 
 + [An indexer-based indexing pipeline](search-indexer-overview.md) with an index that accepts the output. The index must have fields for receiving headings and content.
 
++ [An index projection](search-how-to-define-index-projections.md) for one-to-many indexing.
+
 + [A supported data source](search-indexer-overview.md#supported-data-sources) having text content that you want to chunk.
 
-+ [A skillset with Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) that splits documents based on paragraph boundaries.
++ A skillset with these two skills:
 
-+ [An Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) that generates vector embeddings.
+  + [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) that splits documents based on paragraph boundaries. This skill has region requirements. An Azure AI multi-service resource must be in the same region as Azure AI Search with AI Enrichment.
 
-+ [An index projection](search-how-to-define-index-projections.md) for one-to-many indexing.
+  + [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) that generates vector embeddings. This skill also has region requirements. The model must be in the same region as Azure AI Search.
 
 ## Prepare data files
 
@@ -52,7 +54,7 @@ The raw inputs must be in a [supported data source](search-indexer-overview.md#s
 
 + Supported indexers can be any indexer that can handle the supported file formats. These indexers include [Blob indexers](search-howto-indexing-azure-blob-storage.md), [OneLake indexers](search-how-to-index-onelake-files.md), [File indexers](search-file-storage-integration.md).
 
-+ Supported regions for this feature include: East US, West US2, West Europe, North Central US. Be sure to [check this list](search-region-support.md#azure-public-regions) for updates on regional availability.
++ Supported regions for the portal experience of this feature include: East US, West Europe, North Central US. If your setting up your skillset programmatically, you can use any Document Intelligence region that also provides the AI enrichment feature of Azure AI Search. For more information, see [Product availability by region](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/table).
 
 You can use the Azure portal, REST APIs, or an Azure SDK package to [create a data source](search-howto-indexing-azure-blob-storage.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Semantic Chunking Guide in Azure AI Search"
}
```

### Explanation
The modifications to the `search-how-to-semantic-chunking.md` document include several updates that enhance the clarity and relevance of the information regarding semantic chunking in Azure AI Search. Key changes include:

1. **Date Update**: The last modified date of the document has been updated to April 7, 2025.

2. **Enhanced Skillset Descriptions**: The list of required skills and components for setting up a semantic chunking process has been revised for clarity. It now specifies that users need a skillset containing both the Document Layout skill and the Azure OpenAI Embedding skill, along with highlighting that both skills have region requirements for deployment within the same geographical area as Azure AI Search.

3. **Clarified Indexer Requirements**: The document now explicitly mentions that the relevant indexers support various file formats, enhancing user understanding of the requirements they need to meet for effective data handling.

4. **Supported Regions Clarification**: There has been a refinement in the description of supported regions for the feature, noting that specific regions are supported for the portal experience, while programmatic setups may utilize any Document Intelligence region that offers the AI enrichment feature of Azure AI Search. This clarification guides users regarding regional availability and offers links for further exploration on the subject.

5. **Structural Improvements**: Overall structure and formatting enhancements have been applied throughout the document to improve readability and ensure a smooth flow of information.

These updates collectively serve to provide users with clearer, more actionable guidance on implementing semantic chunking in their Azure AI Search applications, ensuring they are well-informed about necessary skills, components, and regional considerations.

## articles/search/search-how-to-upgrade.md{#item-990225}

<details>
<summary>Diff</summary>
````diff
@@ -0,0 +1,115 @@
+---
+title: Service Upgrade in the Azure Portal
+titleSuffix: Azure AI Search
+description: Learn how to upgrade your existing Azure AI Search service to high-capacity storage and processors in your region.
+manager: nitinme
+author: haileytap
+ms.author: haileytapia
+ms.service: azure-ai-search
+ms.topic: how-to
+ms.custom: references_regions
+ms.date: 04/04/2025
+---
+
+# Upgrade your Azure AI Search service in the Azure portal
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+An upgrade brings older search services to the capabilities of new services created in the same region. Specifically, it upgrades the computing power of the underlying service. This one-time operation doesn't introduce breaking changes to your application, and you shouldn't need to change any code.
+
+For [eligible services](#upgrade-eligibility), an upgrade increases the [partition storage](#higher-storage-limits) and [vector index size](#higher-vector-limits) on the same tier at no extra cost.
+
+> [!TIP]
+> Looking to [change your pricing tier](search-capacity-planning.md#change-your-pricing-tier)? You can now move up between Basic and Standard (S1, S2, and S3) tiers.
+
+This article describes how to upgrade your service in the [Azure portal](https://portal.azure.com/). Alternatively, you can use the [Search Management REST APIs](/rest/api/searchmanagement/) to upgrade your service programmatically. For more information, see [Manage your search service using REST](search-manage-rest.md#upgrade-a-service).
+
+## About service upgrades
+
+In April 2024, Azure AI Search increased the [storage capacity](search-limits-quotas-capacity.md#service-limits) of newly created search services. Services created before April 2024 saw no capacity changes, so if you wanted larger and faster partitions, you had to create a new service. However, some older services can now be upgraded to benefit from the higher capacity partitions.
+
+In this preview, an upgrade only increases the [storage limit](#higher-storage-limits) and [vector index size](#higher-vector-limits) of [eligible services](#upgrade-eligibility).
+
+### Upgrade eligibility
+
+To qualify for an upgrade, your service:
+
+> [!div class="checklist"]
+> + Must have been created before April 2024. Services created after April 2024 should already have higher capacity. To see when you created your service, [check your service creation date](#check-your-service-creation-or-upgrade-date).
+> + Must be in a region where higher capacity is enabled.
+> + Must be in one of the following regions:
+>   + East US
+>   + North Central US
+>   + West Central US
+>   + UK South
+
+<!-- Check the footnotes in the [list of supported regions](search-region-support.md). -->
+
+### Higher storage limits
+
+For [eligible services](#upgrade-eligibility), the following table compares the storage limit (per partition) before and after an upgrade.
+
+| | Basic <sup>1</sup> | S1 | S2 | S3/HD | L1 | L2 |
+|-|-|-|-|-|-|-|
+| **Limit before upgrade** | 2 GB | 25 GB | 100 GB | 200 GB | 1 TB | 2 TB |
+| **Limit after upgrade** | 15 GB | 160 GB | 512 GB | 1 TB | 2 TB | 4 TB |
+
+<sup>1</sup> Basic services created before April 3, 2024 were originally limited to one partition, which increases to three partitions after an upgrade. [Partition counts for all other pricing tiers](search-limits-quotas-capacity.md#service-limits) stay the same.
+
+### Higher vector limits
+
+For [eligible services](#upgrade-eligibility), the following table compares the vector index size (per partition) before and after an upgrade.
+
+| | Basic | S1 | S2 | S3/HD | L1 | L2 |
+|-|-|-|-|-|-|-|
+| **Limit before upgrade** | 0.5 GB <sup>1</sup> or 1 GB <sup>2</sup> | 1 GB <sup>1</sup> or 3 GB <sup>2</sup> | 6 GB <sup>1</sup> or 12 GB <sup>2</sup> | 12 GB <sup>1</sup> or 36 GB <sup>2</sup> | 12 GB | 36 GB |
+| **Limit after upgrade** | 5 GB | 35 GB | 150 GB | 300 GB | 150 GB | 300 GB |
+
+<sup>1</sup> Applies to services created before July 1, 2023.
+
+<sup>2</sup> Applies to services created between July 1, 2023 and April 3, 2024 in all regions except Germany West Central, Qatar Central, and West India, to which the <sup>1</sup> limits apply.
+
+## Check your service creation or upgrade date
+
+On the **Overview** page, you can view various metadata about your search service, including the **Create date (UTC)** and **Upgrade date (UTC)**.
+
+:::image type="content" source="media/search-how-to-upgrade/service-creation-upgrade-metadata.png" alt-text="Screenshot of the service creation and service upgrade dates in the Azure portal." border="true":::
+
+The date you created your service partially determines its [upgrade eligibility](#upgrade-eligibility). If your service has never been upgraded, the **Upgrade date (UTC)** doesn't appear.
+
+## Upgrade your service
+
+You can’t undo a service upgrade. Before you proceed, make sure that you want to permanently increase the [storage limit](#higher-storage-limits) and [vector index size](#higher-vector-limits) of your search service. We recommend that you test this operation in a nonproduction environment.
+
+To upgrade your service:
+
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.
+
+1. On the **Overview** page, select **Upgrade** from the command bar.
+
+   :::image type="content" source="media/search-how-to-upgrade/upgrade-button.png" alt-text="Screenshot of the Upgrade button on the command bar in the Azure portal." border="true":::
+
+   If this button appears dimmed, an upgrade isn’t available for your service. Your service either has the [latest upgrade](#check-your-service-creation-or-upgrade-date) or is in an [unsupported region](#upgrade-eligibility).
+
+1. Review the upgrade details for your service, and then select **Upgrade**.
+
+   :::image type="content" source="media/search-how-to-upgrade/upgrade-panel.png" alt-text="Screenshot of your service upgrade details in the Azure portal." border="true":::
+
+   A confirmation appears reminding you that the upgrade can't be undone.
+
+1. To permanently upgrade your service, select **Upgrade**.
+
+   :::image type="content" source="media/search-how-to-upgrade/upgrade-confirmation.png" alt-text="Screenshot of the upgrade confirmation in the Azure portal." border="true":::
+
+1. Check your notifications to confirm that the operation started.
+
+   The upgrade is an asynchronous operation, so you can continue using your service. Depending on the size of your service, the upgrade can take several hours to complete.
+
+   If the upgrade fails, your service returns to its original state.
+
+## Next step
+
+After you upgrade your search service, you might want to reconsider your scale configuration:
+
+> [!div class="nextstepaction"]
+> [Estimate and manage capacity of an Azure AI Search service](search-capacity-planning.md)
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "Guide for Upgrading Azure AI Search Services"
}
```

### Explanation
A new document has been added titled "Service Upgrade in the Azure Portal," which serves as a comprehensive guide for users looking to upgrade their existing Azure AI Search services to access higher-capacity storage and processors. This document, created by the author Hailey Tap, outlines the following key points:

1. **Purpose and Benefits**: The upgrade process is described as a one-time operation that enhances the computing power of older search services without implementing breaking changes. It brings legacy services up to par with newer offerings in the same region.

2. **Upgrade Eligibility**: The document details eligibility criteria for upgrades, including the service creation date (must be before April 2024) and specific regions where upgrades are available. 

3. **Storage and Vector Index Size Improvements**: Users are informed that upgrades will increase both storage limits and vector index sizes for eligible services at no additional cost, stimulating performance enhancements.

4. **Detailed Upgrade Processes**: The article provides step-by-step instructions for upgrading services via the Azure portal or programmatically through the Search Management REST APIs, including information about how to check the service creation and upgrade dates directly from the Azure portal.

5. **Visual Aids**: The guide enriches the text with helpful visuals and screenshots that illustrate the upgrade process, including buttons and confirmation dialogs users may encounter.

6. **Next Steps**: After an upgrade, users are encouraged to reassess their scale configurations to effectively manage their enhanced service capacity.

This addition is aimed at improving user experience by simplifying the upgrade process, ensuring users are well-informed about the benefits and steps required to leverage enhanced capabilities of the Azure AI Search service.

## articles/search/search-howto-index-encrypted-blobs.md{#item-a7097a}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index encrypted blobs'
+title: 'Tutorial: Index Encrypted Blobs'
 titleSuffix: Azure AI Search
 description: Learn how to index and extract text from encrypted documents in Azure Blob Storage with Azure AI Search.
 
@@ -11,110 +11,112 @@ ms.custom:
   - ignite-2023
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 02/24/2025
+ms.date: 03/28/2025
 ---
 
 # Tutorial: Index and enrich encrypted blobs for full-text search in Azure AI Search
 
-This tutorial shows you how to use [Azure AI Search](search-what-is-azure-search.md) to index documents that have been previously encrypted with a customer-managed key in [Azure Blob Storage](/azure/storage/blobs/storage-blobs-introduction). 
+Learn how to use [Azure AI Search](search-what-is-azure-search.md) to index documents that were encrypted with a customer-managed key in [Azure Blob Storage](/azure/storage/blobs/storage-blobs-introduction).
 
-Normally, an indexer can't extract content from blobs that have been encrypted using the [client-side encryption](/azure/storage/blobs/client-side-encryption) of the Azure Blob Storage client library because the indexer doesn't have access to the customer-managed encryption key in [Azure Key Vault](/azure/key-vault/general/overview). However, by leveraging the [DecryptBlobFile custom skill](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile), followed by the [Document Extraction skill](cognitive-search-skill-document-extraction.md), you can provide controlled access to the key to decrypt the files and then extract content from them. This unlocks the ability to index and enrich these documents without compromising the encryption status of your stored documents.
+Normally, an indexer can't extract content from blobs that were encrypted using [client-side encryption](/azure/storage/blobs/client-side-encryption) in the Azure Blob Storage client library. This is because the indexer doesn't have access to the customer-managed encryption key in [Azure Key Vault](/azure/key-vault/general/overview). However, using the [DecryptBlobFile custom skill](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile) and the [Document Extraction skill](cognitive-search-skill-document-extraction.md), you can provide controlled access to the key to decrypt the files and then extract content from them. This unlocks the ability to index and enrich these documents without compromising the encryption status of your stored documents.
 
-Starting with previously encrypted whole documents (unstructured text) such as PDF, HTML, DOCX, and PPTX in Azure Blob Storage, this tutorial uses a REST client and the Search REST APIs to perform the following tasks:
+Starting with previously encrypted whole documents (unstructured text) such as PDF, HTML, DOCX, and PPTX in Azure Blob Storage, this tutorial uses a REST client and the Search REST APIs to:
 
 > [!div class="checklist"]
-> + Define a pipeline that decrypts the documents and extracts text from them.
-> + Define an index to store the output.
-> + Execute the pipeline to create and load the index.
-> + Explore results using full text search and a rich query syntax.
-
-If you don't have an Azure subscription, open a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
+> + Define a pipeline that decrypts the documents and extracts text from them
+> + Define an index to store the output
+> + Execute the pipeline to create and load the index
+> + Explore results using full-text search and a rich query syntax
 
 ## Prerequisites
 
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+
 + [Azure AI Search](search-create-service-portal.md) on any tier or region.
 
 + [Azure Storage](https://azure.microsoft.com/services/storage/), Standard performance (general-purpose v2).
 
-+ Blobs encrypted with a customer-managed key. See [Tutorial: Encrypt and decrypt blobs using Azure Key Vault](/azure/storage/blobs/storage-encrypt-decrypt-blobs-key-vault) if you need to create sample data.
++ Blobs encrypted with a customer-managed key. To create sample data, see [Tutorial: Encrypt and decrypt blobs using Azure Key Vault](/azure/storage/blobs/storage-encrypt-decrypt-blobs-key-vault).
 
 + [Azure Key Vault](https://azure.microsoft.com/services/key-vault/) in the same subscription as Azure AI Search. The key vault must have **soft-delete** and **purge protection** enabled.
 
-Custom skill deployment creates an Azure Function app and an Azure Storage account. Since these resources are created for you, they aren't listed as a prerequisite. When you're finished with this tutorial, remember to clean up the resources so that you aren't billed for services you're not using.
+Custom skill deployment creates an Azure Function app and an Azure Storage account. These resources are created for you, so they aren't listed as a prerequisite. When you finish this tutorial, remember to clean up the resources so that you aren't billed for services you're not using.
 
 > [!NOTE]
-> Skillsets often require [attaching an Azure AI services multi-service resource](cognitive-search-attach-cognitive-services.md). As written, this skillset has no dependency on Azure AI services and thus no key is required. If you later add enrichments that invoke built-in skills, remember to update your skillset accordingly.
+> Skillsets often require [attaching an Azure AI services multi-service resource](cognitive-search-attach-cognitive-services.md). As written, this skillset has no dependency on Azure AI services, so no key is required. If you later add enrichments that invoke built-in skills, remember to update your skillset accordingly.
 
 ## Deploy the custom skill
 
-This example uses the sample [DecryptBlobFile](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile) project from the [Azure Search Power Skills](https://github.com/Azure-Samples/azure-search-power-skills) GitHub repository. In this section, you deploy the skill to an Azure Function so that it can be used in a skillset. A built-in deployment script creates an Azure Function resource with a **psdbf-function-app-** prefix and loads the skill. You are prompted to provide a subscription and resource group. Be sure to choose the same subscription that your Azure Key Vault instance lives in.
+This tutorial uses the sample [DecryptBlobFile](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile) project from the [Azure Search Power Skills](https://github.com/Azure-Samples/azure-search-power-skills) GitHub repository. In this section, you deploy the skill to an Azure Function so that it can be used in a skillset. A built-in deployment script creates an Azure Function resource with a **psdbf-function-app-** prefix and loads the skill. You're prompted to provide a subscription and resource group. Be sure to choose the subscription that contains your Azure Key Vault instance.
 
-Operationally, the DecryptBlobFile skill takes the URL and SAS token for each blob as inputs, and it outputs the downloaded, decrypted file using the file reference contract that Azure AI Search expects. Recall that DecryptBlobFile needs the encryption key to perform the decryption. As part of setup, you also create an access policy that grants DecryptBlobFile function access to the encryption key in Azure Key Vault.
+Operationally, the DecryptBlobFile skill takes the URL and SAS token for each blob as inputs. It outputs the downloaded, decrypted file using the file reference contract that Azure AI Search expects. Recall that DecryptBlobFile needs the encryption key to perform the decryption. As part of setup, you also create an access policy that grants DecryptBlobFile function access to the encryption key in Azure Key Vault.
 
-1. Click the **Deploy to Azure** button found on the [DecryptBlobFile landing page](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile#deployment), which will open the provided Resource Manager template within the Azure portal.
+1. On the [DecryptBlobFile landing page](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile#deployment), select **Deploy to Azure** to open the Resource Manager template in the Azure portal.
 
-1. Choose the same subscription where your Azure Key Vault instance exists (this tutorial won't work if you select a different subscription).
+1. Choose the subscription where your Azure Key Vault instance exists. This tutorial doesn't work if you choose a different subscription.
 
 1. Select an existing resource group or create a new one. A dedicated resource group makes cleanup easier later.
 
-1. Select **Review + create**, make sure you agree to the terms, and then select **Create** to deploy the Azure Function.
+1. Select **Review + create**, agree to the terms, and then select **Create** to deploy the Azure Function.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/arm-template.png" alt-text="Screenshot of the ARM template page in Azure portal." border="true":::
 
 1. Wait for the deployment to finish.
 
-You should have an Azure Function app that contains the decryption logic and an Azure Storage resource that will store application data. In the next several steps, you'll give the app permissions to access the key vault and collect information that you'll need for the REST calls.
+You should have an Azure Function app that contains the decryption logic and an Azure Storage resource that will store application data. In the next steps, you give the app permissions to access the key vault and collect information that you'll need for the REST calls.
 
 ## Grant permissions in Azure Key Vault
 
-1. Navigate to your Azure Key Vault service in the Azure portal. [Create an access policy](/azure/key-vault/general/assign-access-policy-portal) in the Azure Key Vault that grants key access to the custom skill.
+1. Go to your Azure Key Vault service in the Azure portal. [Create an access policy](/azure/key-vault/general/assign-access-policy-portal) in the Azure Key Vault that grants key access to the custom skill.
 
-1. On the left pane, select **Access policies**, and then select **+ Create** to start the **Create an access policy** wizard.
+1. From the left pane, select **Access policies**, and then select **+ Create** to start the **Create an access policy** wizard.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/keyvault-access-policies.png" alt-text="Screenshot of the Access Policy command in the left pane." border="true":::
 
-1. On the **Permissions** page under **Configure from template**, select **Azure Data Lake Storage or Azure Storage**.
+1. On the **Permissions** page, under **Configure from template**, select **Azure Data Lake Storage or Azure Storage**.
 
 1. Select **Next**.
 
-1. On the **Principal** page, select the Azure Function instance that you deployed. You can search for it using the resource prefix that was used to create it in step 2, which has a default prefix value of **psdbf-function-app**.
+1. On the **Principal** page, select the Azure Function instance you deployed. You can search for it using its resource prefix, which has a default value of **psdbf-function-app**.
 
 1. Select **Next**.
 
 1. On **Review + create**, select **Create**.
 
 ## Collect app information
 
-1. Navigate to the **psdbf-function-app** function in the Azure portal, and make a note of the following properties you'll need for the REST calls:
+1. Go to the **psdbf-function-app** function in the Azure portal. Make a note of the following properties you'll need for the REST calls.
 
 1. Get the function URL, which can be found under **Essentials** on the main page for the function.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/function-uri.png" alt-text="Screenshot of the overview page and Essentials section of the Azure Function app." border="true":::
 
-1. Get the host key code, which can be found by navigating to **App keys**, clicking to show the **default** key, and copying the value.
+1. Get the host key code, which can be found by going to **App keys** and showing the **default** key, and copying the value.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/function-host-key.png" alt-text="Screenshot of the App Keys page of the Azure Function app." border="true":::
 
-## Get an admin api-key and URL for Azure AI Search
+## Get an admin key and URL for Azure AI Search
+
+1. Sign in to the [Azure portal](https://portal.azure.com).
 
-1. Sign in to the [Azure portal](https://portal.azure.com), and in your search service **Overview** page, get the name of your search service. You can confirm your service name by reviewing the endpoint URL. If your endpoint URL were `https://mydemo.search.windows.net`, your service name would be `mydemo`.
+1. On your search service **Overview** page, get the name of your search service. You can confirm your service name by reviewing the endpoint URL. For example, if your endpoint URL is `https://mydemo.search.windows.net`, your service name is `mydemo`.
 
-2. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
+1. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
 
-All requests require an api-key in the header of every request sent to your service. A valid key establishes trust, on a per request basis, between the application sending the request and the service that handles it.
+An API key is required in the header of every request sent to your service. A valid key establishes trust, on a per-request basis, between the application sending the request and the service that handles it.
 
 ## Set up a REST client
 
-Create variables for endpoints and keys:
+Create the following variables for endpoints and keys.
 
 | Variable    | Where to get it |
 |-------------|-----------------|
 | `admin-key` | On the **Keys** page of the Azure AI Search service.  |
 | `search-service-name` | The name of the Azure AI Search service. The URL is `https://{{search-service-name}}.search.windows.net`. |
 | `storage-connection-string` | In the storage account, on the **Access Keys** tab, select **key1** > **Connection string**. |
 | `storage-container-name` | The name of the blob container that has the encrypted files to be indexed. |
-| `function-uri` |  In the Azure Function under **Essentials** on the main page. |
-| `function-code` | In the Azure Function, by navigating to **App keys**, clicking to show the **default** key, and copying the value. |
+| `function-uri` |  In the Azure Function, under **Essentials** on the main page. |
+| `function-code` | In the Azure Function, by going to **App keys**, showing the **default** key, and copying the value. |
 | `api-version` | Leave as **2020-06-30**. |
 | `datasource-name` | Leave as **encrypted-blobs-ds**. |
 | `index-name` | Leave as **encrypted-blobs-idx**. |
@@ -123,25 +125,25 @@ Create variables for endpoints and keys:
 
 ## Review and run each request
 
-Use HTTP requests to create the objects of an enrichment pipeline:
+Use the following HTTP requests to create the objects of an enrichment pipeline.
 
 + **PUT request to create the index**: This search index holds the data that Azure AI Search uses and returns.
 
-+ **POST request to create the data source**: This data source specifies the connection to your storage account containing the encrypted blob files. 
++ **POST request to create the data source**: This data source specifies the connection to your storage account containing the encrypted blob files.
 
-+ **PUT request to create the skillset**: The skillset specifies the custom skill definition for the Azure Function that will decrypt the blob file data, and a [DocumentExtractionSkill](cognitive-search-skill-document-extraction.md) to extract the text from each document after it has been decrypted.
++ **PUT request to create the skillset**: The skillset specifies the custom skill definition for the Azure Function that will decrypt the blob file data. It also specifies a [DocumentExtractionSkill](cognitive-search-skill-document-extraction.md) to extract the text from each document after it's decrypted.
 
 + **PUT request to create the indexer**: Running the indexer retrieves the blobs, applies the skillset, and indexes and stores the results. You must run this request last. The custom skill in the skillset invokes the decryption logic.
 
 ## Monitor indexing
 
-Indexing and enrichment commence as soon as you submit the Create Indexer request. Depending on how many documents are in your storage account, indexing can take a while. To find out whether the indexer is still running, send a **Get Indexer Status** request and review the response to learn whether the indexer is running, or to view error and warning information.  
+Indexing and enrichment commence as soon as you submit the Create Indexer request. Depending on how many documents are in your storage account, indexing can take a while. To find out whether the indexer is still running, send a **Get Indexer Status** request and review the response to learn whether the indexer is running or view error and warning information.  
 
-If you're using the Free tier, the following message is expected: `"Could not extract content or metadata from your document. Truncated extracted text to '32768' characters"`. This message appears because blob indexing on the Free tier has a [32K limit on character extraction](search-limits-quotas-capacity.md#indexer-limits). You won't see this message for this data set on higher tiers. 
+If you're using the Free tier, expect the following message: `"Could not extract content or metadata from your document. Truncated extracted text to '32768' characters"`. This message appears because blob indexing on the Free tier has a [32,000 limit on character extraction](search-limits-quotas-capacity.md#indexer-limits). You don't see this message for this data set on higher tiers.
 
 ## Search your content
 
-After indexer execution is finished, you can run some queries to verify that the data has been successfully decrypted and indexed. Navigate to your Azure AI Search service in the Azure portal, and use the [Search Explorer](search-explorer.md) to run queries over the indexed data.
+After indexer execution is finished, you can run queries to verify that the data is successfully decrypted and indexed. Go to your Azure AI Search service in the Azure portal and use the [Search Explorer](search-explorer.md) to run queries over the indexed data.
 
 ## Clean up resources
 
@@ -151,6 +153,6 @@ You can find and manage resources in the Azure portal, using the All resources o
 
 ## Next steps
 
-Now that you have successfully indexed encrypted files, you can [iterate on this pipeline by adding more skills](cognitive-search-defining-skillset.md). This will allow you to enrich and gain additional insights to your data.
+Now that you've indexed encrypted files, you can [iterate on this pipeline by adding more skills](cognitive-search-defining-skillset.md) to enrich and gain more insights into your data.
 
-If you're working with doubly encrypted data, you might want to investigate the index encryption features available in Azure AI Search. Although the indexer needs decrypted data for indexing purposes, once the index exists, it can be encrypted in a search index using a customer-managed key. This will ensure that your data is always encrypted when at rest. For more information, see [Configure customer-managed keys for data encryption in Azure AI Search](search-security-manage-encryption-keys.md).
+If you're working with doubly encrypted data, you might want to investigate the index encryption features available in Azure AI Search. Although the indexer needs decrypted data for indexing purposes, once the index exists, it can be encrypted in a search index using a customer-managed key. This ensures that your data is always encrypted when at rest. For more information, see [Configure customer-managed keys for data encryption in Azure AI Search](search-security-manage-encryption-keys.md).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Tutorial on Indexing Encrypted Blobs in Azure AI Search"
}
```

### Explanation
The document titled "Tutorial: Index Encrypted Blobs" has been modified with several enhancements that improve clarity and usability for users indexing encrypted documents in Azure Blob Storage using Azure AI Search. Key updates include:

1. **Title Change**: The title format has been standardized to "Tutorial: Index Encrypted Blobs," capitalizing the terms for consistency.

2. **Updated Dates**: The last modified date has been changed to March 28, 2025.

3. **Clarification of Content**: The introduction was rephrased for clearer communication regarding the indexing of documents encrypted with customer-managed keys in Azure Blob Storage. The explanation of limitations with client-side encryption has also been refined for better understanding.

4. **Bullet Points Reformatting**: Checklist items for tasks within the tutorial were reformatted into bullet points for enhanced readability and easier comprehension.

5. **Detailed Prerequisites**: The prerequisites section was made clearer by adjusting wording and enhancing itemization, specifically emphasizing the need for an active Azure account.

6. **Technical Instructions Enhancement**: Several operational steps were rephrased for consistency and readability, including instructions on deploying the custom skill and managing access policies in Azure Key Vault.

7. **API Key Information**: Adjustments were made to the explanations of API key utilization, making it more succinct and standard in terminology.

8. **Monitoring and Cleanup Instructions**: The instructions on monitoring the indexing process and cleaning up resources have been streamlined, making them more concise.

These updates collectively aim to create a smoother onboarding and implementation experience for users as they navigate the process of indexing encrypted blobs, ensuring a clear and actionable tutorial.

## articles/search/search-howto-managed-identities-data-sources.md{#item-edf98d}

<details>
<summary>Diff</summary>
````diff
@@ -17,9 +17,9 @@ ms.date: 11/22/2024
 # Configure a search service to connect using a managed identity in Azure AI Search
 
 > [!IMPORTANT]
-> User-assigned managed identity assignment is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true#identity) provides user-assigned managed identity assignment for Azure AI Search. Support for a *system-assigned* managed identity is generally available.
+> User-assigned managed identity assignment is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#identity) provides user-assigned managed identity assignment for Azure AI Search. Support for a *system-assigned* managed identity is generally available.
 
-You can use Microsoft Entra ID and role assignments for outbound connections from Azure AI Search to resources providing data, applied AI, or vectorization during indexing or queries. 
+You can use Microsoft Entra ID and role assignments for outbound connections from Azure AI Search to resources providing data, applied AI, or vectorization during indexing or queries.
 
 To use roles on an outbound connection, first configure your search service to use either a [system-assigned or user-assigned managed identity](/azure/active-directory/managed-identities-azure-resources/overview) as the security principal for your search service in a Microsoft Entra tenant. Once you have a managed identity, you can assign roles for authorized access. Managed identities and role assignments eliminate the need for passing secrets and credentials in a connection string or code.
 
@@ -131,7 +131,7 @@ For more information, see [Create or Update Service (Management REST API)](/rest
 ## Create a user-assigned managed identity
 
 > [!IMPORTANT]
-> Part of this scenario is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true#identity) provides user-assigned managed identity configuration for Azure AI Search.
+> Part of this scenario is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#identity) provides user-assigned managed identity configuration for Azure AI Search.
 
 A user-assigned managed identity is a resource on Azure. You can create multiple user-assigned managed identities if you want more granularity in role assignments. For example, you might want separate identities for different applications and scenarios.
 
@@ -170,12 +170,12 @@ Associating a user-assigned managed identity is supported in the Azure portal, i
 
 ### [**REST API**](#tab/rest-user)
 
-You can use a preview Management REST API instead of the Azure portal to assign a user-assigned managed identity. Use API versions `2021-04-01-preview` or later. This example uses `2024-06-01-preview`.
+You can use a preview Management REST API instead of the Azure portal to assign a user-assigned managed identity. Use API versions `2021-04-01-preview` or later. This example uses `2025-05-01-preview`.
 
-1. Formulate a request to [UPDATE](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true#identity) a search service.
+1. Formulate a request to [UPDATE](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-05-01-preview&preserve-view=true#identity) a search service.
 
     ```http
-    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2024-06-01-preview
+    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2025-05-01-preview
     {
       "location": "[region]",
       "sku": {
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Managed Identities and Data Sources Configuration Document"
}
```

### Explanation
The document "Configure a search service to connect using a managed identity in Azure AI Search" has undergone minor updates, primarily focusing on API version changes and clarifications regarding managed identities. Key changes include:

1. **API Version Updates**: The examples and references to the Management REST API have been updated from their previous versions to reflect the most current versions available. Instances of API versions have been revised from `2024-03-01-preview` and `2024-06-01-preview` to `2025-02-01-preview` and `2025-05-01-preview`, ensuring users have access to the latest functionalities and improvements.

2. **Maintaining Important Notes**: Notable important notes regarding user-assigned managed identities and their public preview status continue to be highlighted, maintaining user awareness of the feature's current development stage and associated terms of use.

3. **Clarification of Managed Identity Usages**: The document maintains clarity on using Microsoft Entra ID and role assignments, emphasizing the simplification provided by managed identities in terms of security for outbound connections and role assignments for authorized access.

4. **Consistency in Terminology**: Minor grammatical consistency improvements were made in the descriptions, enhancing the overall readability and professional tone of the document.

These adjustments ensure that the documentation remains accurate, user-friendly, and aligned with the most up-to-date Azure features, assisting users in effectively utilizing managed identities for secure data source connections within Azure AI Search.

## articles/search/search-howto-reindex.md{#item-46738a}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 12/09/2024
+ms.date: 03/21/2025
 ---
 
 # Update or rebuild an index in Azure AI Search
@@ -134,7 +134,7 @@ The following table explains the various per-document status codes that can be r
 
 If your client code frequently encounters a 207 response, one possible reason is that the system is under load. You can confirm this by checking the statusCode property for 503. If the statusCode is 503, we recommend throttling indexing requests. Otherwise, if indexing traffic doesn't subside, the system could start rejecting all requests with 503 errors.
 
-Status code 429 indicates that you have exceeded your quota on the number of documents per index. You must either create a new index or upgrade for higher capacity limits.
+Status code 429 indicates that you've exceeded your quota on the number of documents per index. You must either [upgrade for higher capacity limits](search-how-to-upgrade.md) or create a new index.
 
 > [!NOTE]
 > When you upload `DateTimeOffset` values with time zone information to your index, Azure AI Search normalizes these values to UTC. For example, 2024-01-13T14:03:00-08:00 is stored as 2024-01-13T22:03:00Z. If you need to store time zone information, add an extra column to your index for this data point.
@@ -228,7 +228,7 @@ Some modifications require an index drop and rebuild, replacing a current index
 | Assign an analyzer to a field | [Analyzers](search-analyzers.md) are defined in an index, assigned to fields, and then invoked during indexing to inform how tokens are created. You can add a new analyzer definition to an index at any time, but you can only *assign* an analyzer when the field is created. This is true for both the **analyzer** and **indexAnalyzer** properties. The **searchAnalyzer** property is an exception (you can assign this property to an existing field). |
 | Update or delete an analyzer definition in an index | You can't delete or change an existing analyzer configuration (analyzer, tokenizer, token filter, or char filter) in the index unless you rebuild the entire index. |
 | Add a field to a suggester | If a field already exists and you want to add it to a [Suggesters](index-add-suggesters.md) construct, rebuild the index. |
-| Switch tiers | In-place upgrades aren't supported. If you require more capacity, create a new service and rebuild your indexes from scratch. To help automate this process, you can use a code sample that backs up your index to a series of JSON files. You can then recreate the index in a search service you specify.|
+| Upgrade your service or tier | If you need more capacity, check if you can [upgrade your service](search-how-to-upgrade.md) or [switch to a higher service tier](search-capacity-planning.md#change-your-pricing-tier). If not, you must create a new service and rebuild your indexes from scratch. To help automate this process, you can use a code sample that backs up your index to a series of JSON files. You can then recreate the index in a search service you specify. |
 
 The order of operations is:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Reindexing Procedure in Azure AI Search"
}
```

### Explanation
The document titled "Update or rebuild an index in Azure AI Search" has been modified to enhance clarity and provide updated information. Key changes include:

1. **Date Update**: The last modified date has been updated from December 9, 2024, to March 21, 2025, reflecting the most recent edits.

2. **Clarity in Error Messaging**: The description of the status code 429, which indicates quota limits, has been rephrased to enhance clarity. The phrase now encourages users to consider upgrading for higher capacity limits before creating a new index, while also providing a direct link to the relevant upgrade documentation.

3. **Improved Indexing Instructions**: The instruction regarding upgrading the service or switching tiers has been rephrased for clarity. It now explicitly outlines the options for increasing capacity, including links to relevant documents for upgrading the service and switching tiers, as well as the necessity of creating a new service if these options are not available.

4. **Consistency in Terminology**: Minor adjustments in wording throughout the document promote consistency and professionalism in language used, ensuring users have a clear understanding of procedures and options regarding index management.

These updates collectively aim to enhance the user experience by providing clearer instructions and ensuring that users have access to the latest information for managing indexing and upgrading in Azure AI Search.

## articles/search/search-howto-run-reset-indexers.md{#item-fb10c8}

<details>
<summary>Diff</summary>
````diff
@@ -9,26 +9,28 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/19/2024
+ms.date: 03/19/2025
 ---
 
 # Run or reset indexers, skills, or documents
 
 In Azure AI Search, there are several ways to run an indexer:
 
-+ [Run immediately upon indexer creation](search-howto-create-indexers.md), assuming it's not created in "disabled" mode.
++ [Run immediately upon indexer creation](search-howto-create-indexers.md). This is the default unless you create the indexer in a "disabled" state.
 + [Run on a schedule](search-howto-schedule-indexers.md) to invoke execution at regular intervals.
 + Run on demand, with or without a "reset".
 
 This article explains how to run indexers on demand, with and without a reset. It also describes indexer execution, duration, and concurrency.
 
 ## How indexers connect to Azure resources
 
-Indexers are one of the few subsystems that make overt outbound calls to other Azure resources. In terms of Azure roles, indexers don't have separate identities: a connection from the search engine to another Azure resource is made using the [system or user-assigned managed identity](search-howto-managed-identities-data-sources.md) of a search service. If the indexer connects to an Azure resource on a virtual network, you should create a [shared private link](search-indexer-howto-access-private.md) for that connection. For more information about secure connections, see the [Security in Azure AI Search](search-security-overview.md).
+Indexers are one of the few subsystems that make overt outbound calls to other Azure resources. You can use keys or roles to authenticate the connection.
+
+In terms of Azure roles, indexers don't have separate identities: a connection from the search engine to another Azure resource is made using the [system or user-assigned managed identity](search-howto-managed-identities-data-sources.md) of a search service, plus a role assignment on the target Azure resource. If the indexer connects to an Azure resource on a virtual network, you should create a [shared private link](search-indexer-howto-access-private.md) for that connection.
 
 ## Indexer execution
 
-A search service runs one indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions). Every search service starts with one search unit, but each new partition or replica increases the search units of your service. You can check the search unit count in the Azure portal's Essential section of the **Overview** page. If you need concurrent processing, make sure your search units include sufficient replicas. Indexers don't run in the background, so you might detect more query throttling than usual if the service is under pressure.
+A search service runs one indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions). Every search service starts with one search unit, but each new partition or replica increases the search units of your service. You can check the search unit count in the Azure portal's Essential section of the **Overview** page. If you need concurrent processing, make sure your search units include sufficient replicas. Indexers don't run in the background, so you might experience more query throttling than usual if the service is under pressure.
 
 The following screenshot shows the number of search units, which determines how many indexers can run at once.
 
@@ -42,45 +44,46 @@ You can run multiple indexers at one time assuming sufficient capacity, but each
 
 An indexer job runs in a managed execution environment. Currently, there are two environments:
 
-+ A private execution environment runs on search clusters that are specific to your search service. If your search service is Standard2 or higher, you can [set the `executionEnvironment` parameter](search-how-to-create-indexers.md?tabs=indexer-rest#create-an-indexer) in the indexer definition to always run an indexer in the private execution environment. 
++ A private execution environment runs on search clusters that are specific to your search service.
 
 + A multitenant environment has content processors that are managed and secured by Microsoft at no extra cost. This environment is used to offload computationally intensive processing, leaving service-specific resources available for routine operations. Whenever possible, most skillsets execute in the multitenant environment. This is the default.
 
-  *Computationally intensive processing* refers to skillsets running on content processors and indexer jobs that process a high volume of documents, or documents of a large size. Non-skillset processing on the multitenant content processors is determined by hueristics and system information and isn't under customer control. S2 services and higher support pinning an indexer and skillset processing exclusively to your search clusters through the `executionEnvironment` parameter.
+  *Computationally intensive processing* refers to skillsets running on content processors and indexer jobs that process a high volume of documents, or documents of a large size. Non-skillset processing on the multitenant content processors is determined by heuristics and system information and isn't under customer control. 
+
+You can prevent usage of the multitenant environment on Standard2 or higher services by pinning an indexer and skillset processing exclusively to your search clusters. [Set the `executionEnvironment` parameter](search-how-to-create-indexers.md?tabs=indexer-rest#create-an-indexer) in the indexer definition to always run an indexer in the private execution environment.
 
-  > [!NOTE]
-  > [IP firewalls](search-indexer-securing-resources.md#network-access-and-indexer-execution-environments) block the multitenant environment, so if you have a firewall, create a rule that allows multitenant processing.
+[IP firewalls](search-indexer-securing-resources.md#setting-up-ip-ranges-for-indexer-execution) block the multitenant environment, so if you have a firewall, [create a rule](search-indexer-howto-access-ip-restricted.md#configure-ip-firewall-rules-to-allow-indexer-connections-from-azure-ai-search) that allows multitenant processor connections.
 
 Indexer limits vary for each environment:
 
 | Workload | Maximum duration | Maximum jobs | Execution environment |
 |----------|------------------|---------------------|-----------------------------|
-| Private execution | 24 hours | One indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions) <sup>1</sup>.  | Indexing doesn't run in the background. Instead, the search service will balance all indexing jobs against ongoing queries and object management actions (such as creating or updating indexes). When running indexers, you should expect to see [some query latency](search-performance-analysis.md#impact-of-indexing-on-queries) if indexing volumes are large. |
+| Private execution | 24 hours | One indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions) <sup>1</sup>.  | Indexing doesn't run in the background. Instead, the search service balances all indexing jobs against ongoing queries and object management actions (such as creating or updating indexes). When running indexers, you should expect to see [some query latency](search-performance-analysis.md#impact-of-indexing-on-queries) if indexing volumes are large. |
 | Multitenant| 2 hours <sup>2</sup> | Indeterminate <sup>3</sup> | Because the content processing cluster is multitenant, content processors are added to meet demand. If you experience a delay in on-demand or scheduled execution, it's probably because the system is either adding processors or waiting for one to become available.|
 
 <sup>1</sup> Search units can be [flexible combinations](search-capacity-planning.md#partition-and-replica-combinations) of partitions and replicas, but indexer jobs aren't tied to one or the other. In other words, if you have 12 units, you can have 12 indexer jobs running concurrently in private execution, no matter how the search units are deployed.
 
-<sup>2</sup> If more than two hours are needed to process all of the data, [enable change detection](search-howto-create-indexers.md#change-detection-and-internal-state) and [schedule the indexer](search-howto-schedule-indexers.md) to run at 5 minute intervals to resume indexing quickly if it stops due to a timeout. See [Indexing a large data set](search-howto-large-index.md) for more strategies.
+<sup>2</sup> If more than two hours are needed to process all of the data, [enable change detection](search-howto-create-indexers.md#change-detection-and-internal-state) and [schedule the indexer](search-howto-schedule-indexers.md) to run at 5-minute intervals to resume indexing quickly if it stops due to a time out. See [Indexing a large data set](search-howto-large-index.md) for more strategies.
 
 <sup>3</sup> "Indeterminate" means that the limit isn't quantified by the number of jobs. Some workloads, such as skillset processing, can run in parallel, which could result in many jobs even though only one indexer is involved. Although the environment doesn't impose constraints, [indexer limits](search-limits-quotas-capacity.md#indexer-limits) for your search service still apply.
 
 ## Run without reset
 
-A [Run Indexer](/rest/api/searchservice/indexers/run) operation will detect and process only what it necessary to synchronize the search index with changes in the underlying data source. Incremental indexing starts by locating an internal high-water mark to find the last updated search document, which becomes the starting point for indexer execution over new and updated documents in the data source.
+A [Run Indexer](/rest/api/searchservice/indexers/run) operation detects and processes only what it necessary to synchronize the search index with changes in the underlying data source. Incremental indexing starts by locating an internal high-water mark to find the last updated search document, which becomes the starting point for indexer execution over new and updated documents in the data source.
 
-[Change detection](search-howto-create-indexers.md#change-detection-and-internal-state) is essential for determining what's new or updated in the data source. Indexers use the change detection capabilities of the underlying data source to determine what's new or updated in the data source. 
+[Change detection](search-howto-create-indexers.md#change-detection-and-internal-state) is essential for determining what's new or updated in the data source. Indexers use the change detection capabilities of the underlying data source to determine what's new or updated in the data source.
 
 + Azure Storage has built-in change detection through its LastModified property.
 
 + Other data sources, such as Azure SQL or Azure Cosmos DB, have to be configured for change detection before the indexer can read new and updated rows. 
 
-If the underlying content is unchanged, a run operation has no effect. In this case, indexer execution history will indicate `0\0` documents processed.
+If the underlying content is unchanged, a run operation has no effect. In this case, indexer execution history indicates `0\0` documents processed.
 
-You'll need to reset the indexer, as explained in the next section, to reprocess in full.
+You need to reset the indexer, as explained in the next section, to reprocess in full.
 
 ## Resetting indexers
 
-After the initial run, an indexer keeps track of which search documents have been indexed through an internal *high-water mark*. The marker is never exposed, but internally the indexer knows where it last stopped.
+After the initial run, an indexer keeps track of which search documents are indexed through an internal *high-water mark*. The marker is never exposed, but internally the indexer knows where it last stopped.
 
 If you need to rebuild all or part of an index, you can clear the indexer's high-water mark through a reset. Reset APIs are available at decreasing levels in the object hierarchy:
 
@@ -90,23 +93,26 @@ If you need to rebuild all or part of an index, you can clear the indexer's high
 
 After reset, follow with a Run command to reprocess new and existing documents. Orphaned search documents having no counterpart in the data source can't be removed through reset/run. If you need to delete documents, see [Documents - Index](/rest/api/searchservice/documents) instead.
 
+> [!NOTE]
+> Tables can't be empty. If you use TRUNCATE TABLE to clear rows, a reset and rerun of the indexer won't remove the corresponding search documents. To remove orphaned search documents, you must [index them with a delete action](search-howto-reindex.md#delete-orphan-documents).
+
 <a name="reset-indexers"></a>
 
 ## How to reset and run indexers
 
-Reset clears the high-water mark. All documents in the search index will be flagged for full overwrite, without inline updates or merging into existing content. For indexers with a skillset and [enrichment caching](cognitive-search-incremental-indexing-conceptual.md), resetting the index will also implicitly reset the skillset. 
+Reset clears the high-water mark. All documents in the search index are flagged for full overwrite, without inline updates or merging into existing content. For indexers with a skillset and [enrichment caching](cognitive-search-incremental-indexing-conceptual.md), resetting the index also implicitly resets the skillset. 
 
 The actual work occurs when you follow a reset with a Run command:
 
 + All new documents found the underlying source are added to the search index. 
-+ All documents that exist in both the data source and search index will be overwritten in the search index. 
-+ Any enriched content created from skillsets will be rebuilt. The enrichment cache, if one is enabled, is refreshed.
++ All documents that exist in both the data source and search index are overwritten in the search index. 
++ Any enriched content created from skillsets are rebuilt. The enrichment cache, if one is enabled, is refreshed.
 
-As previously noted, reset is a passive operation: you must follow up a Run request to rebuild the index. 
+As previously noted, reset is a passive operation: you must follow with a Run request to rebuild the index. 
 
 Reset/run operations apply to a search index or a knowledge store, to specific documents or projections, and to cached enrichments if a reset explicitly or implicitly includes skills.
 
-Reset also applies to create and update operations. It will not trigger deletion or clean up of orphaned documents in the search index. For more information about deleting documents, see [Documents - Index](/rest/api/searchservice/documents/).
+Reset also applies to create and update operations. It won't trigger deletion or clean up of orphaned documents in the search index. For more information about deleting documents, see [Documents - Index](/rest/api/searchservice/documents/).
 
 Once you reset an indexer, you can't undo the action.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to Run and Reset Indexers Documentation"
}
```

### Explanation
The document titled "Run or reset indexers, skills, or documents" has been updated to improve clarity, accuracy, and user guidance regarding indexer operations in Azure AI Search. Key changes include:

1. **Date Update**: The last modified date has been changed from December 19, 2024, to March 19, 2025, reflecting the latest revisions.

2. **Clarified Indexer Running Options**: The section on how indexers can be run has been refined for better clarity, stating that running immediately upon creation is the default behavior unless the indexer is created in a "disabled" state.

3. **Authentication Details**: Information regarding how indexers authenticate connections has been expanded. It now specifies that keys or roles may be used for authentication, in addition to managed identities.

4. **Execution Environment Improvements**: The description of the execution environments has been made clearer, differentiating between private execution and multitenant environments and outlining the benefits of each. The updated text clarifies that multitenant environments are utilized for offloading processing tasks.

5. **Note Regarding Firewalls**: The note about IP firewalls has been expanded to provide clearer instructions on allowing multitenant processing connections.

6. **Expanded Change Detection Information**: The document now includes specific references to how change detection works with various data sources. Azure Storage's built-in detection through its LastModified property is highlighted, along with the need for configuration in other database types like Azure SQL and Azure Cosmos DB.

7. **Clarified Steps for Indexer Reset and Run**: The instructions regarding resetting and running indexers have been modified slightly for grammar and clarity, emphasizing that resetting clears the high-water mark and overwrites documents in the search index.

8. **Orphan Document Handling**: The documentation now includes a note that tables cannot be empty when using TRUNCATE TABLE and emphasizes the need to index deletions separately.

Overall, these updates enhance the documentation's user-friendliness, ensuring that readers have clear guidance on how to effectively manage indexers, including how to run them and the implications of resetting.

## articles/search/search-indexer-how-to-access-private-sql.md{#item-1bd4cc}

<details>
<summary>Diff</summary>
````diff
@@ -87,7 +87,7 @@ For more information about connection properties, see [Create an Azure SQL Manag
    Because shared private link support for SQL managed instances is still in preview, you need a preview version of the management REST API. Use `2021-04-01-preview` or a later preview API version for this step. We recommend using the latest preview API version.
 
    ```azurecli
-   az rest --method put --uri https://management.azure.com/subscriptions/{{search-service-subscription-ID}}/resourceGroups/{{search service-resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}/sharedPrivateLinkResources/{{shared-private-link-name}}?api-version=2024-06-01-preview --body @create-pe.json
+   az rest --method put --uri https://management.azure.com/subscriptions/{{search-service-subscription-ID}}/resourceGroups/{{search service-resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}/sharedPrivateLinkResources/{{shared-private-link-name}}?api-version=2025-05-01-preview --body @create-pe.json
    ```
 
    Provide the subscription ID, resource group name, and service name of your Azure AI Search resource.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updated API Version for Shared Private Link Resources"
}
```

### Explanation
The document titled "How to Access Private SQL" has been modified to update the API version used for creating shared private link resources in Azure AI Search. The key changes include:

1. **API Version Update**: The previously stated API version (`2024-06-01-preview`) has been updated to a more recent version (`2025-05-01-preview`). This change reflects the need for users to utilize the latest available preview of the management REST API to ensure compatibility and access to the most recent features.

2. **Minor Text Adjustments**: The surrounding documentation text has been adjusted slightly to match the new API version, ensuring that the command example is consistent with the latest updates.

These updates enhance the document's relevance and accuracy, guiding users to employ the correct API version when working with shared private link resources in Azure AI Search.

## articles/search/search-indexer-howto-access-private.md{#item-73d30d}

<details>
<summary>Diff</summary>
````diff
@@ -63,7 +63,7 @@ While both scenarios have a dependency on Azure Private Link, they're independen
 
 When evaluating shared private links for your scenario, remember these constraints.
 
-+ Several of the resource types used in a shared private link are in preview. If you're connecting to a preview resource (Azure Database for MySQL or Azure SQL Managed Instance), use a preview version of the Management REST API to create the shared private link. These versions include `2020-08-01-preview`, `2021-04-01-preview`, `2024-03-01-preview`, and `2024-06-01-preview`. We recommend the latest preview API.
++ Several of the resource types used in a shared private link are in preview. If you're connecting to a preview resource (Azure Database for MySQL or Azure SQL Managed Instance), use a preview version of the Management REST API to create the shared private link. These versions include `2020-08-01-preview`, `2021-04-01-preview`, `2024-03-01-preview`, `2024-06-01-preview`, and `2025-02-01-preview`. We recommend the latest preview API.
 
 + Indexer execution must use the [private execution environment](search-howto-run-reset-indexers.md#indexer-execution-environment) that's specific to your search service. Private endpoint connections aren't supported from the multitenant content processing environment. The configuration setting for this requirement is covered in this article.
 
@@ -78,8 +78,8 @@ When evaluating shared private links for your scenario, remember these constrain
   | Workload | Tier requirements | Region requirements | Service creation requirements |
   |----------|-------------------|---------------------|---------------------|
   | Indexers without skillsets | Basic and higher | None | None |
-  | Skillsets with embedding skills ([integrated vectorization](vector-search-integrated-vectorization.md)) | Basic and higher | [High capacity regions](search-limits-quotas-capacity.md#partition-storage-gb) | [After April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date) |
-  | Skillsets using other [built-in](cognitive-search-predefined-skills.md) or custom skills | Standard 2 (S2) and higher | None | [After April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date) |
+  | Skillsets with embedding skills ([integrated vectorization](vector-search-integrated-vectorization.md)) | Basic and higher | [High capacity regions](search-limits-quotas-capacity.md#partition-storage-gb) | [After April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date) |
+  | Skillsets using other [built-in](cognitive-search-predefined-skills.md) or custom skills | Standard 2 (S2) and higher | None | [After April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date) |
 
 + Permissions on both Azure AI Search and the Azure resource:
 
@@ -88,9 +88,9 @@ When evaluating shared private links for your scenario, remember these constrain
   | Azure AI Search | `Microsoft.Search/searchServices/sharedPrivateLinkResources/write`<br> `Microsoft.Search/searchServices/sharedPrivateLinkResources/read`<br> `Microsoft.Search/searchServices/sharedPrivateLinkResources/operationStatuses/read` |
   | Other Azure resource | Permission to approve private endpoint connections. For example, on Azure Storage, you need `Microsoft.Storage/storageAccounts/privateEndpointConnectionsApproval/action`. |
 
-<!-- + For [integrated vectorization](vector-search-integrated-vectorization.md) only, outbound connections through shared private link are supported on all billable tiers, on services [created after April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date), in regions providing [higher capacity](search-limits-quotas-capacity.md#partition-storage-gb).  -->
+<!-- + For [integrated vectorization](vector-search-integrated-vectorization.md) only, outbound connections through shared private link are supported on all billable tiers, on services [created after April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date), in regions providing [higher capacity](search-limits-quotas-capacity.md#partition-storage-gb).  -->
 
-<!-- + For [AI enrichment](cognitive-search-concept-intro.md) and skillset processing, shared private link  that doesn't include an embedding skill and in services [created before April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date), Azure AI Search must be Standard 2 (S2) or higher. -->
+<!-- + For [AI enrichment](cognitive-search-concept-intro.md) and skillset processing, shared private link  that doesn't include an embedding skill and in services [created before April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date), Azure AI Search must be Standard 2 (S2) or higher. -->
 
 <!-- + For all other use cases, that don't involve skillsets, Azure AI Search can be Basic or higher. -->
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Shared Private Link and Skillset Requirements"
}
```

### Explanation
The document "How to Access Private SQL" has been updated to reflect recent changes and provide clearer guidance regarding shared private links and the use of skillsets in Azure AI Search. The main updates include:

1. **New API Version Addition**: A new preview version of the Management REST API (`2025-02-01-preview`) has been added to the list of versions that users can utilize when connecting to preview resources such as Azure Database for MySQL or Azure SQL Managed Instance.

2. **Clarified Indexer Execution Requirements**: An additional note has been included indicating that indexer execution must use a private execution environment that is specific to the search service. It specifies that private endpoint connections are not supported from the multitenant content processing environment.

3. **Updated Skillset and Workload Requirements**: The requirements listed in the workload table have been revised to replace the link references for service creation dates from a prior section to a more relevant section that aligns with the recent changes.

4. **Permissions Requirements Clarification**: The permissions required for Azure AI Search and the Azure resource have been outlined for clarity, indicating the specific actions required to approve private endpoint connections.

5. **Commented Guidance**: Certain commented notes regarding integrated vectorization and other skillset processes have been refined to provide clearer direction on how shared private links function with respect to different service tiers and creation dates.

These changes enhance the usability of the document, ensuring that users have the most current information necessary to effectively utilize shared private links and understand the varying requirements for skillsets within Azure AI Search.

## articles/search/search-indexer-tutorial.md{#item-a3e3ff}

<details>
<summary>Diff</summary>
````diff
@@ -1,14 +1,14 @@
 ---
-title: C# tutorial indexing Azure SQL data
+title: 'C# Tutorial: Index Azure SQL Data'
 titleSuffix: Azure AI Search
-description: In this C# tutorial, connect to Azure SQL Database, extract searchable data, and load it into an Azure AI Search index.
+description: In this C# tutorial, you connect to Azure SQL Database, extract searchable data, and load it into an Azure AI Search index.
 
 manager: nitinme
 author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 03/11/2025
+ms.date: 03/28/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
@@ -17,104 +17,101 @@ ms.custom:
 
 # Tutorial: Index Azure SQL data using the .NET SDK
 
-Configure an [indexer](search-indexer-overview.md) to extract searchable data from Azure SQL Database, sending it to a search index in Azure AI Search. 
+Learn how to configure an [indexer](search-indexer-overview.md) to extract searchable data from Azure SQL Database and send it to a search index in Azure AI Search.
 
-This tutorial uses C# and the [Azure SDK for .NET](/dotnet/api/overview/azure/search) to perform the following tasks:
+In this tutorial, you use C# and the [Azure SDK for .NET](/dotnet/api/overview/azure/search) to:
 
 > [!div class="checklist"]
 > * Create a data source that connects to Azure SQL Database
 > * Create an indexer
 > * Run an indexer to load data into an index
 > * Query an index as a verification step
 
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Prerequisites
 
-* [Azure SQL Database](https://azure.microsoft.com/services/sql-database/) using SQL Server authentication
-* [Visual Studio](https://visualstudio.microsoft.com/downloads/)
-* [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) 
+* An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+* [Azure SQL Database](https://azure.microsoft.com/services/sql-database/) using SQL Server authentication.
+* [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.
+* [Visual Studio](https://visualstudio.microsoft.com/downloads/).
 
 > [!NOTE]
-> You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ## Download files
 
 Source code for this tutorial is in the [DotNetHowToIndexer](https://github.com/Azure-Samples/search-dotnet-getting-started/tree/master/DotNetHowToIndexers) folder in the [Azure-Samples/search-dotnet-getting-started](https://github.com/Azure-Samples/search-dotnet-getting-started) GitHub repository.
 
-## 1 - Create services
+## Create services
 
-This tutorial uses Azure AI Search for indexing and queries, and Azure SQL Database as an external data source. If possible, create both services in the same region and resource group for proximity and manageability. In practice, Azure SQL Database can be in any region.
+This tutorial uses Azure AI Search for indexing and queries and Azure SQL Database as an external data source. If possible, create both services in the same region and resource group for proximity and manageability. In practice, Azure SQL Database can be in any region.
 
 ### Start with Azure SQL Database
 
-This tutorial provides *hotels.sql* file in the sample download to populate the database. Azure AI Search consumes flattened rowsets, such as one generated from a view or query. The SQL file in the sample solution creates and populates a single table.
-
-If you have an existing Azure SQL Database resource, you can add the hotels table to it, starting at the **Open query** step.
+This tutorial provides the *hotels.sql* file in the sample download to populate the database. Azure AI Search consumes flattened rowsets, such as one generated from a view or query. The SQL file in the sample solution creates and populates a single table.
 
-1. Create an Azure SQL database, using the instructions in [Quickstart: Create a single database](/azure/azure-sql/database/single-database-create-quickstart).
+If you have an existing Azure SQL Database resource, you can add the hotels table to it starting at the **Open query** step.
 
-   Server configuration for the database is important.
+1. [Create an Azure SQL database](/azure/azure-sql/database/single-database-create-quickstart). Server configuration for the database is important:
 
    * Choose the SQL Server authentication option that prompts you to specify a username and password. You need this for the ADO.NET connection string used by the indexer.
 
-   * Choose a public connection. It makes this tutorial easier to complete. Public isn't recommended for production and we recommend [deleting this resource](#clean-up-resources) at the end of the tutorial.
+   * Choose a public connection, which makes this tutorial easier to complete. Public isn't recommended for production, and we recommend [deleting this resource](#clean-up-resources) at the end of the tutorial.
 
    :::image type="content" source="media/search-indexer-tutorial/sql-server-config.png" alt-text="Screenshot of server configuration.":::
 
 1. In the Azure portal, go to the new resource.
 
-1. Add a firewall rule to allow access from your client, using the instructions in [Quickstart: Create a server-level firewall rule in Azure portal](/azure/azure-sql/database/firewall-create-server-level-portal-quickstart). You can run `ipconfig` from a command prompt to get your IP address. 
+1. [Add a firewall rule that allows access from your client](/azure/azure-sql/database/firewall-create-server-level-portal-quickstart). You can run `ipconfig` from a command prompt to get your IP address.
 
-1. Use the Query editor to load the sample data. On the navigation pane, select **Query editor (preview)** and enter the user name and password of server admin. 
+1. Use the Query editor to load the sample data. On the navigation pane, select **Query editor (preview)** and enter the username and password of the server admin.
 
-   If you get an access denied error, copy the client IP address from the error message, open the network security page for the server, and add an inbound rule that allows access from your client. 
+   If you get an access denied error, copy the client IP address from the error message, open the network security page for the server, and add an inbound rule that allows access from your client.
 
-1. In Query editor, select **Open query** and navigate to the location of *hotels.sql* file on your local computer. 
+1. In Query editor, select **Open query** and navigate to the location of *hotels.sql* file on your local computer.
 
 1. Select the file and select **Open**. The script should look similar to the following screenshot:
 
    :::image type="content" source="media/search-indexer-tutorial/sql-script.png" alt-text="Screenshot of SQL script in a Query Editor window." border="true":::
 
-1. Select **Run** to execute the query. In the Results pane, you should see a query succeeded message, for three rows.
+1. Select **Run** to execute the query. In the **Results** pane, you should see a query succeeded message for three rows.
 
 1. To return a rowset from this table, you can execute the following query as a verification step:
 
     ```sql
     SELECT * FROM Hotels
     ```
 
-1. Copy the ADO.NET connection string for the database. Under **Settings** > **Connection Strings**, copy the ADO.NET connection string, similar to the example below.
+1. Copy the ADO.NET connection string for the database. Under **Settings** > **Connection Strings**, copy the ADO.NET connection string, which should be similar to the following example:
 
     ```sql
     Server=tcp:<YOUR-DATABASE-NAME>.database.windows.net,1433;Initial Catalog=hotels-db;Persist Security Info=False;User ID=<YOUR-USER-NAME>;Password=<YOUR-PASSWORD>;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;
     ```
 
-You'll need this connection string in the next exercise, setting up your environment.
+You'll need this connection string to set up your environment in the next step.
 
 ### Azure AI Search
 
-The next component is Azure AI Search, which you can [create in the Azure portal](search-create-service-portal.md). You can use the Free tier to complete this walkthrough. 
+The next component is Azure AI Search, which you can [create in the Azure portal](search-create-service-portal.md). You can use the Free tier to complete this tutorial.
 
-### Get an admin api-key and URL for Azure AI Search
+### Get an admin key and URL for Azure AI Search
 
 API calls require the service URL and an access key. A search service is created with both, so if you added Azure AI Search to your subscription, follow these steps to get the necessary information:
 
-1. Sign in to the [Azure portal](https://portal.azure.com), and in your search service **Overview** page, get the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com). On your service **Overview** page, copy the endpoint URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
-1. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
+1. On **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
 
    :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of Azure portal pages showing the HTTP endpoint and access key location for a search service." border="false":::
 
-## 2 - Set up your environment
+## Set up your environment
 
-1. Start Visual Studio and open **DotNetHowToIndexers.sln**.
+1. Start Visual Studio and open *DotNetHowToIndexers.sln*.
 
-1. In Solution Explorer, open **appsettings.json** to provide connection information.
+1. In Solution Explorer, open *appsettings.json* to provide connection information.
 
-1. For `SearchServiceEndPoint`, if the full URL on the service overview page is "https://my-demo-service.search.windows.net", then the value to provide is the entire URL.
+1. For `SearchServiceEndPoint`, if the full URL on your service **Overview** page is `https://my-demo-service.search.windows.net`, provide the entire URL.
 
-1. For `AzureSqlConnectionString`, the string format is similar to this: `"Server=tcp:<your-database-name>.database.windows.net,1433;Initial Catalog=hotels-db;Persist Security Info=False;User ID=<your-user-name>;Password=<your-password>;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;"`
+1. For `AzureSqlConnectionString`, the string format is similar to `"Server=tcp:<your-database-name>.database.windows.net,1433;Initial Catalog=hotels-db;Persist Security Info=False;User ID=<your-user-name>;Password=<your-password>;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;"`.
 
     ```json
     {
@@ -124,19 +121,18 @@ API calls require the service URL and an access key. A search service is created
     }
     ```
 
-1. Replace the user password in the SQL connection string to a valid password. While the database and user names will copy over, the password must be entered manually.
-
-## 3 - Create the pipeline
+1. Replace the user password in the SQL connection string with a valid password. While the database and usernames will copy over, you must enter the password manually.
 
-Indexers require a data source object and an index. Relevant code is in two files:
+## Create the pipeline
 
-* **hotel.cs**, containing a schema that defines the index
+Indexers require a data source object and an index. The relevant code is in two files:
 
-* **Program.cs**, containing functions for creating and managing structures in your service
+* *hotel.cs* contains a schema that defines the index
+* *Program.cs* contains functions for creating and managing structures in your service
 
 ### In hotel.cs
 
-The index schema defines the fields collection, including attributes specifying allowed operations, such as whether a field is full-text searchable, filterable, or sortable as shown in the following field definition for HotelName. A [SearchableField](/dotnet/api/azure.search.documents.indexes.models.searchablefield) is full-text searchable by definition. Other attributes are assigned explicitly.
+The index schema defines the fields collection, including attributes specifying allowed operations, such as whether a field is full-text searchable, filterable, or sortable, as shown in the following field definition for `HotelName`. A [SearchableField](/dotnet/api/azure.search.documents.indexes.models.searchablefield) is, by definition, full-text searchable. Other attributes are explicitly assigned.
 
 ```csharp
 . . . 
@@ -146,11 +142,11 @@ public string HotelName { get; set; }
 . . .
 ```
 
-A schema can also include other elements, including scoring profiles for boosting a search score, custom analyzers, and other constructs. However, for our purposes, the schema is sparsely defined, consisting only of fields found in the sample datasets.
+A schema can also include other elements, such as scoring profiles for boosting a search score and custom analyzers. However, for this tutorial, the schema is sparsely defined, consisting only of fields found in the sample datasets.
 
 ### In Program.cs
 
-The main program includes logic for creating [an indexer client](/dotnet/api/azure.search.documents.indexes.models.searchindexer), an index, a data source, and an indexer. The code checks for and deletes existing resources of the same name, under the assumption that you might run this program multiple times.
+The main program includes logic for creating [an indexer client](/dotnet/api/azure.search.documents.indexes.models.searchindexer), an index, a data source, and an indexer. The code checks for and deletes existing resources of the same name, assuming that you might run this program multiple times.
 
 The data source object is configured with settings that are specific to Azure SQL Database resources, including [partial or incremental indexing](search-how-to-index-sql-database.md#CaptureChangedRows) for using the built-in [change detection features](/sql/relational-databases/track-changes/about-change-tracking-sql-server) of Azure SQL. The source demo hotels database in Azure SQL has a "soft delete" column named **IsDeleted**. When this column is set to true in the database, the indexer removes the corresponding document from the Azure AI Search index.
 
@@ -167,7 +163,7 @@ var dataSource =
 indexerClient.CreateOrUpdateDataSourceConnection(dataSource);
 ```
 
-An indexer object is platform-agnostic, where  configuration, scheduling, and invocation are the same regardless of the source. This example indexer includes a schedule, a reset option that clears indexer history, and calls a method to create and run the indexer immediately. To create or update an indexer, use [CreateOrUpdateIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.createorupdateindexerasync).
+An indexer object is platform agnostic, where configuration, scheduling, and invocation are the same regardless of the source. This example indexer includes a schedule and a reset option that clears the indexer history. It also calls a method to create and run the indexer immediately. To create or update an indexer, use [CreateOrUpdateIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.createorupdateindexerasync).
 
 ```csharp
 Console.WriteLine("Creating Azure SQL indexer...");
@@ -203,7 +199,7 @@ var indexer = new SearchIndexer("hotels-sql-idxr", dataSource.Name, searchIndex.
 await indexerClient.CreateOrUpdateIndexerAsync(indexer);
 ```
 
-Indexer runs are usually scheduled, but during development you might want to run the indexer immediately using [RunIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.runindexerasync).
+Indexer runs are usually scheduled, but during development, you might want to run the indexer immediately using [RunIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.runindexerasync).
 
 ```csharp
 Console.WriteLine("Running Azure SQL indexer...");
@@ -218,35 +214,35 @@ catch (RequestFailedException ex) when (ex.Status == 429)
 }
 ```
 
-## 4 - Build the solution
+## Build the solution
 
-Press F5 to build and run the solution. The program executes in debug mode. A console window reports the status of each operation.
+Select **F5** to build and run the solution. The program executes in debug mode. A console window reports the status of each operation.
 
    :::image type="content" source="media/search-indexer-tutorial/console-output.png" alt-text="Screenshot showing the console output for the program." border="true":::
 
 Your code runs locally in Visual Studio, connecting to your search service on Azure, which in turn connects to Azure SQL Database and retrieves the dataset. With this many operations, there are several potential points of failure. If you get an error, check the following conditions first:
 
-* Search service connection information that you provide is the full URL. If you entered just the service name, operations stop at index creation, with a failure to connect error.
+* Search service connection information that you provide is the full URL. If you only entered the service name, operations stop at index creation, with a failure to connect error.
 
-* Database connection information in **appsettings.json**. It should be the ADO.NET connection string obtained from the Azure portal, modified to include a username and password that are valid for your database. The user account must have permission to retrieve data. Your local client IP address must be allowed inbound access through the firewall.
+* Database connection information in *appsettings.json*. It should be the ADO.NET connection string obtained from the Azure portal, modified to include a username and password that are valid for your database. The user account must have permission to retrieve data. Your local client IP address must be allowed inbound access through the firewall.
 
 * Resource limits. Recall that the Free tier has limits of three indexes, indexers, and data sources. A service at the maximum limit can't create new objects.
 
-## 5 - Search
+## Search
 
-Use Azure portal to verify object creation, and then use **Search explorer** to query the index.
+Use the Azure portal to verify object creation, and then use **Search explorer** to query the index.
 
-1. Sign in to the [Azure portal](https://portal.azure.com), and in your search service left pane, open each page in turn to verify the object is created. **Indexes**, **Indexers**, and **Data Sources** will have "hotels-sql-idx", "hotels-sql-indexer", and "hotels-sql-ds", respectively.
+1. Sign in to the [Azure portal](https://portal.azure.com) and go to your search service. From the left pane, open each page to verify the objects are created. **Indexes**, **Indexers**, and **Data Sources** should have **hotels-sql-idx**, **hotels-sql-indexer**, and **hotels-sql-ds**, respectively.
 
-1. On the Indexes tab, select the hotels-sql-idx index. On the hotels page, **Search explorer** is the first tab.
+1. On the **Indexes** tab, select the **hotels-sql-idx** index. On the hotels page, **Search explorer** is the first tab.
 
 1. Select **Search** to issue an empty query.
 
    The three entries in your index are returned as JSON documents. Search explorer returns documents in JSON so that you can view the entire structure.
 
    :::image type="content" source="media/search-indexer-tutorial/portal-search.png" alt-text="Screenshot of a Search Explorer query for the target index." border="true":::
 
-1. Next, [switch to **JSON View**](search-explorer.md#start-search-explorer) so that you can enter query parameters:
+1. [Switch to **JSON view**](search-explorer.md#start-search-explorer) so that you can enter query parameters.
 
    ```json
    {
@@ -255,9 +251,9 @@ Use Azure portal to verify object creation, and then use **Search explorer** to
    }
    ```
 
-   This query invokes full text search on the term `river`, and the result includes a count of the matching documents. Returning the count of matching documents is helpful in testing scenarios when you have a large index with thousands or millions of documents. In this case, only one document matches the query.
+   This query invokes full text search on the term `river`. The result includes a count of the matching documents. Returning the count of matching documents is helpful in testing scenarios where you have a large index with thousands or millions of documents. In this case, only one document matches the query.
 
-1. Lastly, enter parameters that limit search results to fields of interest: 
+1. Enter parameters that limit search results to fields of interest.
 
    ```json
    {
@@ -285,7 +281,7 @@ You can find and manage resources in the Azure portal, using the All resources o
 
 ## Next steps
 
-Now that you're familiar with the basics of SQL Database indexing, let's take a closer look at indexer configuration.
+Now that you're familiar with the basics of SQL Database indexing, take a closer look at indexer configuration:
 
 > [!div class="nextstepaction"]
 > [Configure a SQL Database indexer](search-how-to-index-sql-database.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhanced C# Tutorial for Indexing Azure SQL Data"
}
```

### Explanation
The modification made to the "C# Tutorial: Index Azure SQL Data" document involves several enhancements and updates aimed at improving clarity, structure, and the overall quality of the tutorial. Key changes include:

1. **Title and Description Updates**: The title has been updated for better readability, and minor adjustments were made to the introductory description to enhance grammatical structure and flow.

2. **Prerequisites Clarification**: The prerequisites section has been restructured to improve clarity and ensure that key requirements, such as having an active Azure subscription, are clearly stated. The mentions of resource creation have been worded more precisely.

3. **Step Numbering Cleaned Up**: The numbering of the steps was normalized for consistency throughout the document. This ensures a clear progression of tasks from service creation through to querying data.

4. **Directives and Notes Enhanced**: Notes and directives, such as those related to using the free tier for services, have been improved for clearer communication of the limitations and important considerations for users.

5. **Environmental Setup Instructions**: Instructions regarding setup in Visual Studio have been slightly reformulated for better readability, and some minor terminology adjustments made.

6. **Code and Configuration Explanations**: Some sections contain enhanced explanations regarding the schema, indexer configuration, and additional context on how to manage connection strings and error handling.

7. **Additional Context in Queries**: When discussing querying the index, the document was updated to provide more detail on what users can expect in terms of output and practical implications when conducting searches.

8. **Next Steps Section Update**: The conclusion now points users toward further resources for configuring SQL Database indexers, encouraging them to continue learning.

These adjustments not only improve the overall experience for readers but also enhance the instructional quality of the tutorial, guiding users more effectively through the process of indexing Azure SQL Data using C#.

## articles/search/search-limits-quotas-capacity.md{#item-3b201a}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 03/11/2025
+ms.date: 04/09/2025
 ms.custom:
   - references_regions
   - build-2024
@@ -47,6 +47,7 @@ Maximum limits on storage, workloads, and quantities of indexes and other object
 | Maximum depth of complex fields |10 |10 |10 |10 |10 |10 |10 |10 |
 | Maximum [suggesters](/rest/api/searchservice/suggesters) per index |1 |1 |1 |1 |1 |1 |1 |1 |
 | Maximum [scoring profiles](/rest/api/searchservice/add-scoring-profiles-to-a-search-index) per index |100 |100 |100 |100 |100 |100 |100 |100 |
+| Maximum [semantic configurations](semantic-how-to-configure.md) per index |100 |100 |100 |100 |100 |100 |100 |100 |
 | Maximum functions per profile |8 |8 |8 |8 |8 |8 |8 |8 |
 | Maximum index size&nbsp;<sup>4</sup> | N/A | N/A | N/A | 1.88&nbsp;TB | 2.34&nbsp;TB | 100 GB| N/A | N/A |
 
@@ -56,7 +57,7 @@ Maximum limits on storage, workloads, and quantities of indexes and other object
 
 <sup>3</sup> An upper limit exists for elements because having a large number of them significantly increases the storage required for your index. An element of a complex collection is defined as a member of that collection. For example, assume a [Hotel document with a Rooms complex collection](search-howto-complex-data-types.md#complex-collection-limits), each room in the Rooms collection is considered an element. During indexing, the indexing engine can safely process a maximum of 3,000 elements across the document as a whole. [This limit](search-api-migration.md#upgrade-to-2019-05-06) was introduced in `api-version=2019-05-06` and applies to complex collections only, and not to string collections or to complex fields.
 
-<sup>4</sup> On most tiers, maximum index size is all available storage on your search service. For S2, S3, and S3 HD, the maximum size of any index is the number provided in the table. Applies to search services created after April 3, 2024.
+<sup>4</sup> For most tiers, the maximum index size is the total available storage on your search service. For S2, S3, and S3 HD services with multiple partitions, and therefore more storage, the maximum size of a single index is provided in the table. Applies to search services created after April 3, 2024.
 
 You might find some variation in maximum limits if your service happens to be provisioned on a more powerful cluster. The limits here represent the common denominator. Indexes built to the above specifications are portable across equivalent service tiers in any region.
 
@@ -81,12 +82,12 @@ When you index documents with vector fields, Azure AI Search constructs internal
 
 Vector limits vary by:
 
-+ [service creation date](vector-search-index-size.md#how-to-check-service-creation-date)
-+ [region](search-region-support.md)
++ [Service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date)
++ [Region](search-region-support.md)
 
-Higher vector limits from April 2024 onwards exist on *new search services* in regions providing the extra capacity, which is most of them.
+Higher vector limits from April 2024 onwards exist on *new search services* in regions providing the extra capacity, which is most of them. If you have an older service in a supported region, check if you can [upgrade your service](search-how-to-upgrade.md) to the higher vector limits.
 
-This table shows the progression of vector quota increases in GB over time. The quota is per partition, so if you scale a new Standard (S1) service to 6 partitions, total vector quota is 35 multiplied by 6.
+This table shows the progression of vector quota increases in GB over time. The quota is per partition, so if you scale a new Standard (S1) service to 6 partitions, the total vector quota is 35 multiplied by 6.
 
 | Service creation date |Basic | S1| S2 | S3/HD | L1 | L2 |
 |-----------------------|------|---|----|----|----|----|
@@ -150,9 +151,9 @@ Indexers can access other Azure resources [over private endpoints](search-indexe
 | Maximum private endpoints | N/A | 10 or 30 | 100 | 400 | 400 | N/A | 20 | 20 |
 | Maximum distinct resource types <sup>3</sup> | N/A | 4 | 7 | 15 | 15 | N/A | 4 | 4 |
 
-<sup>1</sup> AI enrichment and image analysis are computationally intensive and consume disproportionate amounts of available processing power. For this reason, private connections are disabled on lower tiers to ensure the performance and stability of the search service itself. On basic services, private connections to an Azure AI services multi-service resource are unsupported to preserve service stability. For the S1 tier, make sure the service was created with [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) after April 3, 2024. 
+<sup>1</sup> AI enrichment and image analysis are computationally intensive and consume disproportionate amounts of available processing power. For this reason, private connections are disabled on lower tiers to ensure the performance and stability of the search service itself. On Basic services, private connections to an Azure AI services multi-service resource are unsupported to preserve service stability. For the S1 tier, make sure the service was created with [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) after April 3, 2024.
 
-<sup>2</sup> Private connections to an embedding model are supported on basic and S1 high-capacity search services created after April 3, 2024, with the [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) for storage and computational processing. 
+<sup>2</sup> Private connections to an embedding model are supported on Basic and S1 high-capacity search services created after April 3, 2024, with the [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) for storage and computational processing.
 
 <sup>3</sup> The number of distinct resource types are computed as the number of unique `groupId` values used across all shared private link resources for a given search service, irrespective of the status of the resource.
 
@@ -167,7 +168,7 @@ Maximum number of synonym maps varies by tier. Each rule can have up to 20 expan
 
 ## Index alias limits
 
-Maximum number of [index aliases](search-how-to-alias.md) varies by tier and [service creation date](vector-search-index-size.md#how-to-check-service-creation-date). In all tiers, if the service was created after October 2022 the maximum number of aliases is double the maximum number of indexes allowed. If the service was created before October 2022, the limit is the number of indexes allowed.
+Maximum number of [index aliases](search-how-to-alias.md) varies by tier and [service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date). In all tiers, if the service was created after October 2022 the maximum number of aliases is double the maximum number of indexes allowed. If the service was created before October 2022, the limit is the number of indexes allowed.
 
 | Service Creation Date | Free | Basic | S1 | S2 | S3 | S3-HD |L1 | L2 |
 |----------|------|-------|----|----|----|-------|----|----|
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updated Limits and Quotas for Azure AI Search"
}
```

### Explanation
The document "Search Limits, Quotas, and Capacity" has received various updates to enhance its clarity and accuracy regarding the limitations of the Azure AI Search service. The main changes include:

1. **Date Update**: The document's last modified date has been updated to reflect that the content is current as of April 9, 2025.

2. **Added Limits**: A new entry has been added to the table indicating that the maximum number of semantic configurations per index is now set to 100, aligning with other limits.

3. **Clarified Text**: Certain explanatory texts have been refined for better understanding. For example, clarifications about how the maximum index size is defined in relation to service tiers have been specified more clearly.

4. **Terminology Adjustments**: Updates have been made to the terminology regarding service creation dates and key phrases to enhance reading flow and comprehension, including how it pertains to upgrades and service capabilities.

5. **Vector Limits Detail**: Edits were made to clarify how the limits for vector features are related to both service creation dates and region capabilities, providing users with a clearer understanding of their options and restrictions.

6. **Comments and Superscripts**: Some comments regarding the performance of the Azure services and the limitations posed by resource number calculations have been reiterated with minor phrasing adjustments to ensure the technical details are adequately conveyed.

7. **Alias Limits Explanation**: The explanation regarding the maximum number of index aliases has been updated to clarify the dependency on the service creation date and has been made consistent throughout the document.

These modifications serve to provide users with a more precise understanding of the limitations and operational capabilities available within Azure AI Search, thereby improving the document's overall effectiveness as a user resource.

## articles/search/search-manage-azure-cli.md{#item-7fdd08}

<details>
<summary>Diff</summary>
````diff
@@ -11,15 +11,10 @@ ms.custom:
   - devx-track-azurecli
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 03/21/2025
 ---
 
-# Manage your Azure AI Search service with the Azure CLI
-> [!div class="op_single_selector"]
-> * [Portal](search-manage.md)
-> * [PowerShell](search-manage-powershell.md)
-> * [Azure CLI](search-manage-azure-cli.md)
-> * [REST API](search-manage-rest.md)
+# Manage your Azure AI Search service using the Azure CLI
 
 You can run Azure CLI commands and scripts on Windows, macOS, Linux, or in Azure Cloud Shell to create and configure Azure AI Search.
 
@@ -37,7 +32,10 @@ Use the [**az search module**](/cli/azure/search) to perform the following tasks
 
 Occasionally, questions are asked about tasks *not* on the above list.
 
-You can't change a server name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service. 
+You can't change a service name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service.
+
+> [!NOTE]
+> The 2025-02-01-preview supports changing your pricing tier using the [Management REST APIs](search-manage-rest.md#upgrade-a-service) and the [Azure portal](search-capacity-planning.md#change-your-pricing-tier). Currently, you can only move up between Basic and Standard (S1, S2, and S3) tiers, such as going from Basic to S1.
 
 You can't use tools or APIs to transfer content, such as an index, from one service to another. Within a service, programmatic creation of content is through [Search Service REST API](/rest/api/searchservice/) or an SDK such as [Azure SDK for .NET](/dotnet/api/overview/azure/search.documents-readme). While there are no dedicated commands for content migration, you can write script that calls REST API or a client library to create and load indexes on a new service.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarifications and Updates to Azure CLI Management Guide"
}
```

### Explanation
The document titled "Manage your Azure AI Search service using the Azure CLI" has undergone several updates aimed at improving clarity and providing important information regarding the management of Azure AI Search services. The key changes include:

1. **Date Update**: The last modified date for the document has been updated to March 21, 2025, ensuring that users are aware of the most current information.

2. **Title Adjustment**: The title has been slightly modified from "Manage your Azure AI Search service with the Azure CLI" to "Manage your Azure AI Search service using the Azure CLI" to enhance grammatical correctness and flow.

3. **Content Clarifications**: The text has been refined for clarity. For instance, the statement regarding the inability to change a "server name" has been changed to "service name" to provide more accurate terminology.

4. **New Note Added**: A new note section has been introduced, highlighting that the Management REST APIs (as of February 1, 2025) support changing the pricing tier through the Azure portal. This important addition informs users that they can upgrade their service tier, albeit with limitations on movement between tiers.

5. **Consistent Terminology**: The document consistently refers to 'service' instead of 'server' throughout, maintaining uniform terminology.

6. **Structural Adjustments**: Minor formatting and structural enhancements were made in the explanations to ensure that users can easily locate and understand vital pieces of information.

These updates contribute to a clearer understanding of how to manage Azure AI Search services with the Azure CLI, ensuring that users have access to the latest features and guidelines.

## articles/search/search-manage-powershell.md{#item-3c3485}

<details>
<summary>Diff</summary>
````diff
@@ -9,20 +9,15 @@ ms.author: haileytapia
 ms.service: azure-ai-search
 ms.devlang: powershell
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 03/21/2025
 ms.custom:
   - devx-track-azurepowershell
   - ignite-2023
 ---
 
-# Manage your Azure AI Search service with PowerShell
-> [!div class="op_single_selector"]
-> * [Portal](search-manage.md)
-> * [PowerShell](search-manage-powershell.md)
-> * [Azure CLI](search-manage-azure-cli.md)
-> * [REST API](search-manage-rest.md)
+# Manage your Azure AI Search service using PowerShell
 
-You can run PowerShell cmdlets and scripts on Windows, Linux, or in Azure Cloud Shell to create and configure Azure AI Search. 
+You can run PowerShell cmdlets and scripts on Windows, Linux, or in Azure Cloud Shell to create and configure Azure AI Search.
 
 Use the [**Az.Search** module](/powershell/module/az.search/) to perform the following tasks:
 
@@ -38,7 +33,10 @@ Use the [**Az.Search** module](/powershell/module/az.search/) to perform the fol
 
 Occasionally, questions are asked about tasks *not* on the above list.
 
-You can't change a server name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service. 
+You can't change a service name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service.
+
+> [!NOTE]
+> The 2025-02-01-preview supports changing your pricing tier using the [Management REST APIs](search-manage-rest.md#upgrade-a-service) and the [Azure portal](search-capacity-planning.md#change-your-pricing-tier). Currently, you can only move up between Basic and Standard (S1, S2, and S3) tiers, such as going from Basic to S1.
 
 You can't use tools or APIs to transfer content, such as an index, from one service to another. Within a service, programmatic creation of content is through [Search Service REST API](/rest/api/searchservice/) or an SDK such as [Azure SDK for .NET](/dotnet/api/overview/azure/search.documents-readme). While there are no dedicated commands for content migration, you can write script that calls REST API or a client library to create and load indexes on a new service.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements and Clarifications to PowerShell Management Guide"
}
```

### Explanation
The document "Manage your Azure AI Search service using PowerShell" has been updated with several enhancements aimed at improving user clarity and reflecting the latest information. The key revisions include:

1. **Date Update**: The last modified date has been changed to March 21, 2025, ensuring the content is up to date for users.

2. **Title Revision**: The title has been modified from "Manage your Azure AI Search service with PowerShell" to "Manage your Azure AI Search service using PowerShell," enhancing the grammatical structure.

3. **Content Improvement**: The language has been adjusted to refer to "service name" instead of "server name," providing greater accuracy in terminology.

4. **New Note Addition**: A new note has been added to highlight that the Management REST APIs (as of February 1, 2025) allow users to change their pricing tier through the Azure portal. This crucial change informs users about their upgrade options, indicating the ability to move between the Basic and Standard tiers.

5. **Consistent Terminology**: Throughout the document, references have been standardized, ensuring that users consistently see the terminology that best describes the service rather than ambiguous terms.

6. **Clarified Limitations**: Clarifications around capabilities related to changing configurations and transferring content have been retained and emphasized, providing clearer guidance for users.

These updates result in a more coherent and instructive document that helps users effectively manage Azure AI Search services via PowerShell, along with keeping them informed on recent functionalities and guidelines.

## articles/search/search-manage-rest.md{#item-405ec7}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: Manage with REST
+title: Manage using REST
 titleSuffix: Azure AI Search
 description: Create and configure an Azure AI Search service with the Management REST API. The Management REST API is comprehensive in scope, with access to generally available and preview features.
 author: haileytap
@@ -8,23 +8,19 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 03/26/2025
 ---
 
-# Manage your Azure AI Search service with REST APIs
+# Manage your Azure AI Search service using REST APIs
 
-> [!div class="op_single_selector"]
-> * [Portal](search-manage.md)
-> * [PowerShell](search-manage-powershell.md)
-> * [Azure CLI](search-manage-azure-cli.md)
-> * [REST API](search-manage-rest.md)
-
-In this article, learn how to create and configure an Azure AI Search service using the [Management REST APIs](/rest/api/searchmanagement/). Only the Management REST APIs are guaranteed to provide early access to [preview features](/rest/api/searchmanagement/management-api-versions). 
+Learn how to create and configure an Azure AI Search service using the [Management REST APIs](/rest/api/searchmanagement/). Only the Management REST APIs are guaranteed to provide early access to [preview features](/rest/api/searchmanagement/management-api-versions).
 
 The Management REST API is available in stable and preview versions. Be sure to set a preview API version if you're accessing preview features.
 
 > [!div class="checklist"]
 > * [Create or update a service](#create-or-update-a-service)
+> * [Upgrade a service](#upgrade-a-service)
+> * [Change pricing tiers](#change-pricing-tiers)
 > * [Enable Azure role-based access control for data plane](#enable-rbac)
 > * [Enforce a customer-managed key policy](#enforce-cmk)
 > * [Disable semantic ranker](#disable-semantic-ranker)
@@ -33,49 +29,45 @@ The Management REST API is available in stable and preview versions. Be sure to
 > * [Regenerate an admin key](#regenerate-admin-api-keys)
 > * [List private endpoint connections](#list-private-endpoint-connections)
 > * [List search operations](#list-search-operations)
-> * [Delete a search services](#delete-a-search-service)
+> * [Delete a search service](#delete-a-search-service)
 
 All of the Management REST APIs have examples. If a task isn't covered in this article, see the [API reference](/rest/api/searchmanagement/) instead.
 
 ## Prerequisites
 
-* An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-search/).
+* An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
 * [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
-* [Azure CLI](/cli/azure/install-azure-cli) used to get an access token. You must be an owner or administrator in your Azure subscription.
-
-## Get an access token
+* [Azure CLI](/cli/azure/install-azure-cli) to get an access token, as described in the following steps. You must be an owner or administrator in your Azure subscription.
 
-Management REST API calls are authenticated through Microsoft Entra ID. You need to provide an access token on the request, along with permissions to create and configure a resource.
+   Management REST API calls are authenticated through Microsoft Entra ID. You must provide an access token on the request and permissions to create and configure a resource. In addition to the Azure CLI, you can use [Azure PowerShell to create an access token](/azure/azure-resource-manager/management/manage-resources-rest).
 
-You can use the [Azure CLI or Azure PowerShell to create an access token](/azure/azure-resource-manager/management/manage-resources-rest).
+   1. Open a command shell for Azure CLI.
 
-1. Open a command shell for Azure CLI.
+   1. Sign in to your Azure subscription. If you have multiple tenants or subscriptions, make sure you select the correct one.
 
-1. Sign in to your Azure subscription.
+       ```azurecli
+       az login
+       ```
 
-   ```azurecli
-   az login
-   ```
+   1. Get the tenant ID and subscription ID.
 
-1. Get the tenant ID and subscription ID. If you have multiple tenants or subscriptions, make sure you use the correct one.
+      ```azurecli
+      az account show
+      ```
 
-   ```azurecli
-   az account show
-   ````
+   1. Get an access token.
 
-1. Get an access token.
+      ```azurecli
+      az account get-access-token --query accessToken --output tsv
+      ```
 
-   ```azurecli
-   az account get-access-token --query accessToken --output tsv
-   ```
-
-You should have a tenant ID, subscription ID, and bearer token. You'll paste these values into the `.rest` or `.http` file that you create in the next step.
+      You should have a tenant ID, subscription ID, and bearer token. You'll paste these values into the `.rest` or `.http` file that you create in the next step.
 
 ## Set up Visual Studio Code
 
-If you're not familiar with the REST client for Visual Studio Code, this section includes setup so that you can complete the tasks in this quickstart.
+If you're not familiar with the REST client for Visual Studio Code, this section includes setup so that you can complete the tasks in this article.
 
 1. Start Visual Studio Code and select the **Extensions** tile.
 
@@ -129,7 +121,7 @@ If you're not familiar with the REST client for Visual Studio Code, this section
 
 ## Create or update a service
 
-Creates or updates a search service under the current subscription. This example uses variables for the search service name and region, which haven't been defined yet. Either provide the names directly, or add new variables to the collection.
+Creates or updates a search service under the current subscription. This example uses variables for the search service name and region, which haven't been defined yet. Either provide the names directly or add new variables to the collection.
 
 ```http
 ### Create a search service (provide an existing resource group)
@@ -152,6 +144,38 @@ PUT https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups
       }
 ```
 
+## Upgrade a service
+
+Some Azure AI Search capabilities are only available to new services. To avoid service recreation and bring these capabilities to an existing service, you can [upgrade your service](search-how-to-upgrade.md).
+
+```http
+### Upgrade a search service
+@resource-group = my-rg
+@search-service-name = my-search
+POST https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}/upgrade?api-version=2025-02-01-preview  HTTP/1.1
+     Content-type: application/json
+     Authorization: Bearer {{token}}
+```
+
+## Change pricing tiers
+
+If you need more <!-- or less-->capacity, you can [switch to a higher pricing tier](search-capacity-planning.md#change-your-pricing-tier). Currently, you can only move up between Basic and Standard (S1, S2, and S3) tiers. Use the `sku` property to specify the higher <!-- your new -->tier.
+
+```http
+### Change pricing tiers
+@resource-group = my-rg
+@search-service-name = my-search
+PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2025-02-01-preview HTTP/1.1
+     Content-type: application/json
+     Authorization: Bearer {{token}}
+
+    {
+        "sku": {
+            "name": "standard2"
+        }
+    }
+```
+
 ## Create an S3HD service
 
 To create an [S3HD](search-sku-tier.md#tier-descriptions) service, use a combination of `sku` and `hostingMode` properties. Set `sku` to `standard3` and "hostingMode" to `HighDensity`.
@@ -182,7 +206,7 @@ PUT https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups
 
 **Applies to:** Search Index Data Contributor, Search Index Data Reader, Search Service Contributor
 
-In this step, configure your search service to recognize an **authorization** header on data requests that provide an OAuth2 access token.
+Configure your search service to recognize an **authorization** header on data requests that provide an OAuth2 access token.
 
 To use role-based access control for data plane operations, set `authOptions` to `aadOrApiKey` and then send the request.
 
@@ -232,7 +256,7 @@ PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegrou
 [Semantic ranker is enabled](semantic-how-to-enable-disable.md) by default at the free plan that allows up to 1,000 requests per month at no charge. You can lock down the feature at the service level to prevent usage.
 
 ```http
-### disable semantic ranker
+### Disable semantic ranker
 PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2023-11-01 HTTP/1.1
      Content-type: application/json
      Authorization: Bearer {{token}}
@@ -251,7 +275,7 @@ PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegrou
 Azure AI Search [writes to external data sources](search-indexer-securing-resources.md) when updating a knowledge store, saving debug session state, or caching enrichments. The following example disables these workloads at the service level.
 
 ```http
-### disable-external-access
+### Disable external access
 PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2023-11-01 HTTP/1.1
      Content-type: application/json
      Authorization: Bearer {{token}}
@@ -266,7 +290,7 @@ PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegrou
 ## Delete a search service
 
 ```http
-### delete a search service
+### Delete a search service
 DELETE https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2023-11-01 HTTP/1.1
      Content-type: application/json
      Authorization: Bearer {{token}}
@@ -321,7 +345,7 @@ GET https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups
 
 ## Next steps
 
-After a search service is configured, next steps include [create an index](search-how-to-create-search-index.md) or [query an index](search-query-overview.md) using the Azure portal, REST APIs, or an Azure SDK.
+After a search service is configured, your next steps include [creating an index](search-how-to-create-search-index.md) or [querying an index](search-query-overview.md) using the Azure portal, REST APIs, or an Azure SDK.
 
 * [Create an Azure AI Search index in the Azure portal](search-get-started-portal.md)
 * [Set up an indexer to load data from other services](search-indexer-overview.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Improvements and Additions to REST Management Guide"
}
```

### Explanation
The document titled "Manage your Azure AI Search service using REST APIs" has been significantly updated to enhance clarity and provide new functionalities. The main changes include:

1. **Title Change**: The title has been modified from "Manage your Azure AI Search service with REST APIs" to "Manage your Azure AI Search service using REST APIs", improving grammatical structure.

2. **Date Update**: The last modified date has been updated to March 26, 2025, ensuring that users have access to the most current information.

3. **Expanded Content**: The article has added new sections that provide additional instructions, including processes for upgrading an existing service and changing pricing tiers. These enhancements highlight new capabilities and mark essential tasks that users can perform.

4. **Checklist Addition**: The checklist of actions the Management REST APIs can perform has been expanded to include "Upgrade a service" and "Change pricing tiers", providing users quick access to important tasks.

5. **Clarified Descriptions**: Steps for obtaining an access token have been improved for clarity, and formatting has been adjusted for better readability. For instance, the clarity of instructions when using the Azure CLI is enhanced.

6. **Improved Examples**: Code examples for new functionalities such as upgrading a service and changing pricing tiers have been added, making the document more comprehensive and practical for developers.

7. **Consistent Terminology**: Throughout the document, terminology has been standardized, ensuring that users consistently encounter clear and specific language that accurately describes the mechanisms and options available.

8. **Next Steps Guidance**: The guidance for what users should do next after configuring a search service has been refined to promote further exploration and usage of Azure services.

These updates result in a more informative and user-friendly document, equipping users with essential knowledge and tools to better manage their Azure AI Search services using REST APIs.

## articles/search/search-markdown-data-tutorial.md{#item-32ea2a}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index Markdown blobs'
+title: 'Tutorial: Index Markdown Blobs'
 titleSuffix: Azure AI Search
 description: Learn how to index and search Markdown in Azure blobs using Azure AI Search REST APIs.
 
@@ -9,40 +9,40 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: tutorial
-ms.date: 11/19/2024
+ms.date: 03/28/2025
 
 ---
 
 # Tutorial: Index nested Markdown blobs from Azure Storage using REST
 
 [!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
 
-Azure AI Search can index Markdown documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read Markdown data. 
+Azure AI Search can index Markdown documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read Markdown data.
 
-This tutorial shows you to index Markdown files indexed using the `oneToMany` Markdown parsing mode. It uses a REST client and the [Search REST APIs](/rest/api/searchservice/) to perform the following tasks:
+This tutorial shows you how to index Markdown files indexed using the `oneToMany` Markdown parsing mode. It uses a REST client and the [Search REST APIs](/rest/api/searchservice/) to:
 
 > [!div class="checklist"]
 > + Set up sample data and configure an `azureblob` data source
 > + Create an Azure AI Search index to contain searchable content
 > + Create and run an indexer to read the container and extract searchable content
 > + Search the index you just created
 
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Prerequisites
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ [Azure Storage](/azure/storage/common/storage-account-create)
++ [Azure Storage](/azure/storage/common/storage-account-create).
 
-+ [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing Azure AI Search resource](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription.
++ [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.
+
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
 > [!NOTE]
-> You can use the free service for this tutorial. A free search service limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ## Create a Markdown document
 
-Copy and paste the following Markdown into a file named `sample_markdown.md`. The sample data is a single Markdown file containing various Markdown elements. We chose one Markdown file to stay under the storage limits of the free tier.
+Copy and paste the following Markdown into a file named `sample_markdown.md`. The sample data is a single Markdown file containing various Markdown elements. We chose one Markdown file to stay under the storage limits of the Free tier.
 
 ````md
 # Project Documentation
@@ -193,7 +193,7 @@ Thank you for reviewing this example!
 
 ## Copy a search service URL and API key
 
-For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal. For alternative connection methods, see [managed identities](search-howto-managed-identities-data-sources.md).
+For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal. For alternative connection methods, see [Managed identities](search-howto-managed-identities-data-sources.md).
 
 1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the search service **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
@@ -205,7 +205,7 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 1. Start Visual Studio Code and create a new file.
 
-1. Provide values for variables used in the request: 
+1. Provide values for variables used in the request.
 
    ```http
    @baseUrl = PUT-YOUR-SEARCH-SERVICE-ENDPOINT-HERE
@@ -216,7 +216,7 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 1. Save the file using a `.rest` or `.http` file extension.
 
-See [Quickstart: Text search using REST](search-get-started-rest.md) if you need help with the REST client.
+For help with the REST client, see [Quickstart: Keyword search using REST](search-get-started-rest.md).
 
 ## Create a data source
 
@@ -293,19 +293,19 @@ You only need fields for the Markdown elements that the parser supports. These f
 
 - `content`: A string that contains the raw Markdown found in a specific location, based on the header metadata at that point in the document.
 
-- `sections`: An object that contains subfields for the header metadata up to the desired header level. For example, when `markdownHeaderDepth` is set to `h3`, contains string fields `h1`, `h2`, and `h3`. These fields are indexed by mirroring this structure in the index, or through field mappings in the format `/sections/h1`, `sections/h2`, etc. See index and indexer configurations in the following samples for in-context examples. The subfields contained are:
+- `sections`: An object that contains subfields for the header metadata up to the desired header level. For example, when `markdownHeaderDepth` is set to `h3`, contains string fields `h1`, `h2`, and `h3`. These fields are indexed by mirroring this structure in the index, or through field mappings in the format `/sections/h1`, `sections/h2`, etc. For in-context examples, see the index and indexer configurations in the following samples. The subfields contained are:
   - `h1` - A string containing the h1 header value. Empty string if not set at this point in the document.
   - (Optional) `h2`- A string containing the h2 header value. Empty string if not set at this point in the document.
   - (Optional) `h3`- A string containing the h3 header value. Empty string if not set at this point in the document.
   - (Optional) `h4`- A string containing the h4 header value. Empty string if not set at this point in the document.
   - (Optional) `h5`- A string containing the h5 header value. Empty string if not set at this point in the document.
   - (Optional) `h6`- A string containing the h6 header value. Empty string if not set at this point in the document.
 
-- `ordinal_position`: An integer value indicating the position of the section within the document hierarchy. This field is used for ordering the sections in their original sequence as they appear in the document, beginning with an ordinal position of 1 and incrementing sequentially for each content block. 
+- `ordinal_position`: An integer value that indicates the position of the section within the document hierarchy. This field is used for ordering the sections in their original sequence as they appear in the document, beginning with an ordinal position of 1 and incrementing sequentially for each content block.
 
-This implementation leverages [field mappings](search-indexer-field-mappings.md) in the indexer to map from the enriched content to the index. For more information on the parsed one-to-many document structure, see [index markdown blobs](search-how-to-index-markdown-blobs.md).
+This implementation uses [field mappings](search-indexer-field-mappings.md) in the indexer to map from the enriched content to the index. For more information about the parsed one-to-many document structure, see [Index Markdown blobs](search-how-to-index-markdown-blobs.md).
 
-This example provides samples of how to index data both with and without field mappings. In this case, we know that `h1` contains the title of the document, so we can map it to a field named `title`. We'll also be mapping the `h2` and `h3` fields to `h2_subheader` and `h3_subheader` respectively. The `content` and `ordinal_position` fields require no mapping because they are extracted from the Markdown directly into fields using those names. For an example of a full index schema that doesn't require field mappings, see the end of this section.
+This example provides samples of how to index data both with and without field mappings. In this case, we know that `h1` contains the title of the document, so we can map it to a field named `title`. We'll also be mapping the `h2` and `h3` fields to `h2_subheader` and `h3_subheader`, respectively. The `content` and `ordinal_position` fields require no mapping because they're extracted from the Markdown directly into fields using those names. For an example of a full index schema that doesn't require field mappings, see the end of this section.
 
 ```http
 ### Create an index
@@ -326,8 +326,10 @@ POST {{baseUrl}}/indexes?api-version=2024-11-01-preview  HTTP/1.1
     }
 ```
 
-###  Index schema in a configuration with no field mappings
-Field mappings allow you to manipulate and filter enriched content to fit into your desired index shape, but you may just want to take the enriched content directly. In that case, the schema would look like:
+### Index schema in a configuration with no field mappings
+
+Field mappings allow you to manipulate and filter enriched content to fit into your desired index shape. However, you might just want to take the enriched content directly. In that case, the schema would look like:
+
 ```http
 {
   "name": "sample-markdown-index",
@@ -347,9 +349,9 @@ Field mappings allow you to manipulate and filter enriched content to fit into y
 }
 ```
 
-To reiterate, we have subfields up to `h3` in the sections object because `markdownHeaderDepth` is set to `h3`. 
+To reiterate, we have subfields up to `h3` in the sections object because `markdownHeaderDepth` is set to `h3`.
 
-If you choose to use this schema, be sure to adjust later requests accordingly. This will require removing the field mappings from the indexer configuration and updating search queries to use the corresponding field names.
+If you use this schema, be sure to adjust later requests accordingly. This will require removing the field mappings from the indexer configuration and updating search queries to use the corresponding field names.
 
 ## Create and run an indexer
 
@@ -382,11 +384,11 @@ POST {{baseUrl}}/indexers?api-version=2024-11-01-preview  HTTP/1.1
     }
 ```
 
-**Key points**:
+Key points:
 
 + The indexer will only parse headers up to `h3`. Any lower-level headers (`h4`,`h5`,`h6`) will be treated as plain text and show up in the `content` field. This is why the index and field mappings only exist up to a depth of `h3`.
 
-+ The `content` and `ordinal_position` fields require no field mapping as they exist with those names in the enriched content.
++ The `content` and `ordinal_position` fields require no field mapping because they exist with those names in the enriched content.
 
 ## Run queries
 
@@ -404,7 +406,7 @@ POST {{baseUrl}}/indexes/sample-markdown-index/docs/search?api-version=2024-11-0
   }
 ```
 
-Send the request. This is an unspecified full text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
+Send the request. This is an unspecified full-text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
 
 ```json
 HTTP/1.1 200 OK
@@ -431,7 +433,7 @@ Connection: close
 
 ```
 
-Add a `search` parameter to search on a string. 
+Add a `search` parameter to search on a string.
 
 ```http
 ### Query the index
@@ -478,7 +480,8 @@ Connection: close
   ]
 }
 ```
-**Key points**:
+
+Key points:
 
 + Because the `markdownHeaderDepth` is set to `h3`, the `h4`, `h5`, and `h6` headers are treated as plaintext, so they appear in the `content` field.
 
@@ -528,14 +531,14 @@ Connection: close
 }
 ```
 
-For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case-sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
+For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
 
 > [!NOTE]
 > The `$filter` parameter only works on fields that were marked filterable at the creation of your index.
 
 ## Reset and rerun
 
-Indexers can be reset, clearing execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
+Indexers can be reset to clear execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
 
 ```http
 ### Reset the indexer
@@ -562,7 +565,8 @@ When you're working in your own subscription, at the end of a project, it's a go
 You can use the Azure portal to delete indexes, indexers, and data sources.
 
 ## Next steps
-Now that you're familiar with the basics of Azure Blob indexing, let's take a closer look at indexer configuration for Markdown blobs in Azure Storage.
+
+Now that you're familiar with the basics of Azure Blob indexing, take a closer look at indexer configuration for Markdown blobs in Azure Storage:
 
 > [!div class="nextstepaction"]
 > [Configure Markdown blob indexing](search-how-to-index-markdown-blobs.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Markdown Data Tutorial"
}
```

### Explanation
The document "Tutorial: Index Markdown Blobs" has undergone several modifications to enhance clarity, improve instructions, and reflect recent updates in functionality. Key changes include:

1. **Title and Date Update**: The title has been changed to "Tutorial: Index Markdown Blobs" with a new last modified date of March 28, 2025, ensuring the content is current.

2. **Improved Clarity**: The tutorial text has been revised for grammatical improvements and clarity. Phrases such as "this tutorial shows you to index Markdown files" have been corrected to "this tutorial shows you how to index Markdown files," improving readability.

3. **Expanded Prerequisites**: The prerequisites section has been clarified to explicitly mention having an Azure account with an active subscription and improved formatting for links.

4. **Checklist Formatting**: The checklist for tasks has enhanced formatting, making it easier for users to read and follow the workflow for setting up their Azure resources.

5. **New Section for Creating an Indexer**: Details about creating a Markdown document have been streamlined for ease of understanding, and additional guidelines for indexing have been reinforced, emphasizing the importance of the document structure.

6. **Key Points Section Consolidation**: The "Key points" sections have been adjusted for conciseness while retaining crucial information regarding indexer behavior and content parsing.

7. **Consistent Language**: Throughout the document, terminologies have been standardized, and redundant phrases have been removed to create a more uniform language style.

8. **Next Steps Guidance**: The conclusion encourages users to further explore the configuration of Markdown blob indexing, enhancing user engagement with additional resources.

Overall, these updates result in a more structured and user-friendly tutorial, allowing users to effectively index and search Markdown files in Azure Blob Storage using the Azure AI Search REST APIs.

## articles/search/search-performance-tips.md{#item-218e77}

<details>
<summary>Diff</summary>
````diff
@@ -6,12 +6,12 @@ author: mattgotteiner
 ms.author: magottei
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 12/10/2024
+ms.date: 03/21/2025
 ---
 
 # Tips for better performance in Azure AI Search
 
-This article is a collection of tips and best practices for boosting query and indexing performance for keyword search. Knowing which factors are most likely to impact search performance can help you avoid inefficiencies and get the most out of your search service. Some key factors include:
+This article is a collection of tips and best practices for boosting query and indexing performance for keyword search. Knowing which factors are most likely to affect search performance can help you avoid inefficiencies and get the most out of your search service. Some key factors include:
 
 + Index composition (schema and size)
 + Query design
@@ -22,7 +22,7 @@ This article is a collection of tips and best practices for boosting query and i
 
 ## Index size and schema
 
-Queries run faster on smaller indexes. This is partly a function of having fewer fields to scan, but it's also due to how the system caches content for future queries. After the first query, some content remains in memory where it's searched more efficiently. Because index size tends to grow over time, one best practice is to periodically revisit index composition, both schema and documents, to look for content reduction opportunities. However, if the index is right-sized, the only other calibration you can make is to increase capacity: either by [adding replicas](search-capacity-planning.md#adjust-capacity) or upgrading the service tier. The section ["Tip: Upgrade to a Standard S2 tier"](#tip-upgrade-to-a-standard-s2-tier) discusses the scale up versus scale out decision.
+Queries run faster on smaller indexes. This is partly a function of having fewer fields to scan, but it's also due to how the system caches content for future queries. After the first query, some content remains in memory where it's searched more efficiently. Because index size tends to grow over time, one best practice is to periodically revisit index composition, both schema and documents, to look for content reduction opportunities. However, if the index is right-sized, the only other calibration you can make is to increase capacity by [upgrading your service](search-how-to-upgrade.md), [adding replicas](search-capacity-planning.md#add-or-remove-partitions-and-replicas), or [switching to a higher tier](search-capacity-planning.md#change-your-pricing-tier). The section "[Tip: Switch to a Standard S2 tier](#tip-switch-to-a-standard-s2-tier)" discusses the scale up versus scale out decision.
 
 Schema complexity can also adversely affect indexing and query performance. Excessive field attribution builds in limitations and processing requirements. [Complex types](search-howto-complex-data-types.md) take longer to index and query. The next few sections explore each condition.
 
@@ -93,7 +93,7 @@ When query performance is slowing down in general, adding more replicas frequent
 
 One positive side-effect of adding partitions is that slower queries sometimes perform faster due to parallel computing. We've noted parallelization on low selectivity queries, such as queries that match many documents, or facets providing counts over a large number of documents. Since significant computation is required to score the relevancy of the documents, or to count the numbers of documents, adding extra partitions helps queries complete faster.  
 
-To add partitions, use [Azure portal](search-capacity-planning.md#adjust-capacity), [PowerShell](search-manage-powershell.md), [Azure CLI](search-manage-azure-cli.md), or a management SDK.
+To add partitions, use the [Azure portal](search-capacity-planning.md#add-or-remove-partitions-and-replicas), [PowerShell](search-manage-powershell.md), [Azure CLI](search-manage-azure-cli.md), or a management SDK.
 
 ## Service capacity
 
@@ -103,13 +103,13 @@ The tier of your search service and the number of replicas/partitions also have
 
 ### Tip: Create a new high capacity search service
 
-Basic and standard services created [in supported regions](search-limits-quotas-capacity.md#service-limits) after April 3, 2024 have more storage per partition than older services. Before upgrading to a higher tier and a higher billable rate, revisit the [tier service limits](search-limits-quotas-capacity.md#service-limits) to see if the same tier on a newer service gives you the necessary storage.
+Basic and Standard services created [in supported regions](search-limits-quotas-capacity.md#service-limits) after April 3, 2024 have more storage per partition than older services. If you have an older service, check if you can [upgrade your service](search-how-to-upgrade.md) to benefit from more capacity at the same billing rate. If an upgrade isn't available, review the [tier service limits](search-limits-quotas-capacity.md#service-limits) to see if the same tier on a newer service gives you the necessary storage.
 
-### Tip: Upgrade to a Standard S2 tier
+### Tip: Switch to a Standard S2 tier
 
 The Standard S1 search tier is often where customers start. A common pattern for S1 services is that indexes grow over time, which requires more partitions. More partitions lead to slower response times, so more replicas are added to handle the query load. As you can imagine, the cost of running an S1 service has now progressed to levels beyond the initial configuration.
 
-At this juncture, an important question to ask is whether it would be beneficial to move to a higher tier, as opposed to progressively increasing the number of partitions or replicas of the current service. 
+At this juncture, an important question to ask is whether it would be beneficial to [move to a higher tier](search-capacity-planning.md#change-your-pricing-tier), as opposed to progressively increasing the number of partitions or replicas of the current service.
 
 Consider the following topology as an example of a service that has taken on increasing levels of capacity:
 
@@ -133,7 +133,7 @@ However, if the administrator chose to move to a Standard S2 tier the topology w
 
 As this hypothetical scenario illustrates, you can have configurations on lower tiers that result in similar costs as if you had opted for a higher tier in the first place. However, higher tiers come with premium storage, which makes indexing faster. Higher tiers also have much more compute power, as well as extra memory. For the same costs, you could have more powerful infrastructure backing the same index.
 
-An important benefit of added memory is that more of the index can be cached, resulting in lower search latency, and a greater number of queries per second. With this extra power, the administrator may not need to even need to increase the replica count and could potentially pay less than by staying on the S1 service.
+An important benefit of added memory is that more of the index can be cached, resulting in lower search latency, and a greater number of queries per second. With this extra power, the administrator might not need to even need to increase the replica count and could potentially pay less than by staying on the S1 service.
 
 ### Tip: Consider alternatives to regular expression queries
 
@@ -146,5 +146,5 @@ Review these other articles related to service performance:
 + [Analyze performance](search-performance-analysis.md)
 + [Index large data sets in Azure AI Search](search-howto-large-index.md)
 + [Choose a service tier](search-sku-tier.md)
-+ [Plan or add capacity](search-capacity-planning.md#adjust-capacity)
++ [Plan or add capacity](search-capacity-planning.md#add-or-remove-partitions-and-replicas)
 + [Case Study: Use Cognitive Search to Support Complex AI Scenarios](https://techcommunity.microsoft.com/t5/azure-ai/case-study-effectively-using-cognitive-search-to-support-complex/ba-p/2804078)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Performance Tips for Azure AI Search"
}
```

### Explanation
The document "Tips for better performance in Azure AI Search" has been updated to reflect new insights and improvements in content clarity. Key changes include:

1. **Date Update**: The modification date has been updated to March 21, 2025, indicating that the content has been refreshed.

2. **Language Refinement**: Certain phrases have been improved for grammatical accuracy, such as changing “which factors are most likely to impact” to “which factors are most likely to affect,” enhancing the professionalism and clarity of the writing.

3. **Detailed Capacity Management**: The guidance on managing service capacity has been revised to provide clearer instructions on upgrading services and adding storage, with links streamlined for better navigation.

4. **Section Title Modification**: The section previously titled "Tip: Upgrade to a Standard S2 tier" has been renamed to "Tip: Switch to a Standard S2 tier," maintaining consistency in terminology and focusing on the action users should consider.

5. **Flexibility in Costs Evaluation**: The text clarifies that evaluating a move to a higher tier might be more beneficial than continually increasing the number of partitions or replicas in the current service, offering a strategic perspective on capacity management.

6. **Consistent Terminology**: Throughout the document, specific terminologies have been standardized to ensure consistency, such as changing "adding replicas" and "adding partitions" with relevant links for better user understanding.

7. **Enhanced Explanation of Benefits**: The benefits of upgraded services, such as increased cache and reduced search latency, are more clearly articulated, helping users understand the practical implications of service tier selection.

8. **Link Updates**: Some links have been adjusted to guide users more directly to related topics, providing clearer paths for accessing more information on service capacity planning.

These updates ensure that users are equipped with the most accurate and actionable advice to optimize their search performance using Azure AI Search, encouraging effective management and setup of their search services.

## articles/search/search-region-support.md{#item-25b0f1}

<details>
<summary>Diff</summary>
````diff
@@ -9,24 +9,25 @@ ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
 ms.custom: references_regions
-ms.date: 01/27/2025
+ms.date: 04/04/2025
 
 ---
 
 # Azure AI Search regions list
 
-This article identifies the cloud regions in which Azure AI Search is available. It also lists which premium features are available in each region. 
+This article identifies the cloud regions in which Azure AI Search is available. It also lists which premium features are available in each region.
 
 ## Features subject to regional availability
 
 Some features take a dependency on other Azure services or infrastructure that are subject to regional availability. If you need a specific feature, make sure it's available in the desired region.
 
 | Feature | Description | Availability |
 |---------|-------------|--------------|
-| [Extra capacity](search-limits-quotas-capacity.md#service-limits) | Higher capacity partitions became available in selected regions starting in April 2024 with a second wave following in May 2024. Currently, there are just a few regions that *don't* offer higher capacity partitions. If you're using an older search service, create a new search service to benefit from more capacity at the same billing rate. |  Regional support for extra capacity is noted in the footnotes of this article. <p>Check [service age](vector-search-index-size.md#how-to-check-service-creation-date) to see if your search service was created after high capacity partitions became available. <p>To check the capacity of an existing service, [find your search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) and select the **Properties** tab in the middle of the Overview page.|
-| [Availability zones](search-reliability.md#availability-zone-support) | Divides a region's data centers into distinct physical location groups, providing high-availability within the same geo. | Regional support is noted in this article. |
+| [Extra capacity](search-limits-quotas-capacity.md#service-limits) | Higher capacity partitions became available in select regions starting in April 2024, with a second wave following in May 2024. Currently, there are just a few regions that *don't* offer higher capacity partitions. If you have an older search service in a supported region, check if you can [upgrade your service](search-how-to-upgrade.md). Otherwise, create a new search service to benefit from more capacity at the same billing rate. |  Regional support for extra capacity is noted in the footnotes of this article. <p>Check your [service age](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date) to see if your search service was created after high capacity partitions became available. <p>To check the capacity of an existing service, [find your search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) and select the **Properties** tab in the middle of the **Overview** page.|
+| [Availability zones](search-reliability.md#availability-zone-support) | Divides a region's data centers into distinct physical location groups, providing high availability within the same geo. | Regional support is noted in this article. |
 | [Semantic ranker](semantic-search-overview.md) | Takes a dependency on Microsoft-hosted models in specific regions. | Regional support is noted in this article. |
-| [AI service integration](cognitive-search-concept-intro.md) | Refers to [built-in skills](cognitive-search-predefined-skills.md) that make internal calls to Azure AI for enrichment and transformation during indexing. Integration requires that Azure AI Search coexists with an [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) in the same physical region. You can bypass region requirements if you use [identity-based connections](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection), currently in public review. | Regional support is noted in this article. |
+| [Query rewrite](semantic-how-to-query-rewrite.md) | Takes a dependency on Microsoft-hosted models in specific regions. | Regional support is noted in this article. |
+| [AI service integration](cognitive-search-concept-intro.md) | Refers to [built-in skills](cognitive-search-predefined-skills.md) that make internal calls to Azure AI for enrichment and transformation during indexing. Integration requires that Azure AI Search coexists with an [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) in the same physical region. You can bypass region requirements if you use [identity-based connections](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection), currently in public preview. | Regional support is noted in this article. |
 | [Azure OpenAI integration](vector-search-integrated-vectorization.md)  | Refers to the AzureOpenAIEmbedding skill and vectorizer that make internal calls to deployed embedding models on Azure OpenAI. | Check [Azure OpenAI model region availability](/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability) for the most current list of regions for each embedding and chat model. Specific Azure OpenAI models are in fewer regions, so check for model availability first, and then verify Azure AI Search is available in the same region.|
 | [Azure AI Foundry integration](vector-search-integrated-vectorization-ai-studio.md) | Refers to skills and vectorizers that make internal calls to the models hosted in the model catalog. | Check [Azure AI Foundry region availability](/azure/ai-foundry/reference/region-support) for the most current list of regions. |
 | [Azure AI Vision 4.0 multimodal APIs](search-get-started-portal-image-search.md) | Refers to the Azure AI Vision multimodal embeddings skill and vectorizer that call the multimodal embedding API. | Check the [Azure AI Vision region list](/azure/ai-services/computer-vision/overview-image-analysis#region-availability) first, and then verify Azure AI Search is available in the same region.|
@@ -39,97 +40,99 @@ AI service integration refers to internal connections to an Azure AI services mu
 
 ### Americas
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Brazil South​​ ​ | ✅ | ✅ | |  |
+| Brazil South​​ ​| ✅ |  | ✅ |  |
 | Canada Central​​ | ✅ | ✅ | ✅ |  |
-| Canada East​​ ​ |  | ✅ | |  |
-| ​Central US​​ | ✅ | ✅ | ✅ | |
+| Canada East​​ ​|  |  | ✅ |  |
+| ​Central US​​ | ✅ | ✅ | ✅ |  |
 | East US​ | ✅ | ✅ | ✅ |  |
-| East US 2 ​ | ✅ | ✅ | ✅ | |
-| Mexico Central | |  | ✅ | |
-| North Central US​ ​ | ✅ | ✅ | |  | 
-| South Central US​  | ✅ | ✅ | ✅ | |
-| West US​ ​ | ✅ | ✅ | |  |
-| West US 2​ ​ | ✅ | ✅ | ✅ | |
-| West US 3​ | ✅ | ✅ |✅ | |
-| West Central US​ ​ | ✅ | ✅ | | |
+| East US 2 ​ | ✅ | ✅ | ✅ |  |
+| Mexico Central |  | ✅ |  |  |
+| North Central US​ ​| ✅ |  | ✅ |  |
+| South Central US​ | ✅ | ✅ | ✅ |  |
+| West US​​ | ✅ |  | ✅ |  |
+| West US 2​ ​| ✅ | ✅ | ✅ |  |
+| West US 3​ | ✅ | ✅ | ✅ |  |
+| West Central US​ ​ | ✅ |  | ✅ |  |
 
 ### Europe
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| North Europe​​ | ✅ | ✅ | ✅ | S2, S3, L1, L2|
+| North Europe​ <sup>1</sup>​ | ✅ | ✅ | ✅ | ✅ |
 | West Europe​​ | ✅ | ✅ | ✅ |  |
-| France Central​​ | ✅ | ✅ | ✅ | |
-| Germany West Central​ ​| ✅ |  | ✅ | |
-| Italy North​​ |  |  | ✅ | |
-| Norway East​​ | ✅ |  | ✅ |  |
+| France Central​​ | ✅ | ✅ | ✅ |  |
+| Germany West Central​ ​| ✅ | ✅ |  |  |
+| Italy North​​ |  | ✅ |  |  |
+| Norway East​​ | ✅ | ✅ |  |  |
 | Poland Central​​ |  |  |  |  |
-| Spain Central <sup>1</sup> |  |  | ✅  |  |
-| Sweden Central​​ | ✅ |  | ✅ |  |
+| Spain Central <sup>2</sup> |  | ✅ |  |  |
+| Sweden Central​​ | ✅ | ✅ |  |  |
 | Switzerland North​ | ✅ | ✅ | ✅ |  |
 | Switzerland West​ | ✅ | ✅ | ✅ |  |
 | UK South​ | ✅ | ✅ | ✅ |  |
-| UK West​ ​|  | ✅ | |  |
+| UK West​ ​|  |  | ✅ |  |
 
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+<sup>1</sup> This region has capacity constraints on the following tiers: S2, S3, L1, and L2.
+
+<sup>2</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ### Middle East
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Israel Central​ <sup>1</sup> |  |  | ✅  |  |
-| Qatar Central​ <sup>1</sup> |  |  | ✅ | |
-| UAE North​​ | ✅ |  | ✅ |  |
+| Israel Central​ <sup>1</sup> |  | ✅ |  |  |
+| Qatar Central​ <sup>1</sup> |  | ✅ |  |  |
+| UAE North​​ | ✅ | ✅ |  |  |
 
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+<sup>1</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ### Africa
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| South Africa North​ | ✅ |  | ✅ |   |
+| South Africa North​ | ✅ | ✅ |  |  |
 
 ### Asia Pacific
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Australia East​ ​ | ✅ | ✅ | ✅ |   |
-| Australia Southeast​​​ |  | ✅ |  | |
+| Australia East​ ​| ✅ | ✅ | ✅ |  |
+| Australia Southeast​​​ |  |  | ✅ |  |
 | East Asia​ | ✅ | ✅ | ✅ |  |
-| Southeast Asia​ ​ ​ | ✅ | ✅ | ✅ |  |
+| Southeast Asia​​ | ✅ | ✅ | ✅ | ✅ |
 | Central India | ✅ | ✅ | ✅ |  |
-| Jio India West​ ​ | ✅ | ✅ |  |
-| South India <sup>1</sup> |  | | ✅ |
-| Japan East  | ✅ | ✅ | ✅ |
-| Japan West​ | ✅ | ✅ |  |
-| Korea Central | ✅ | ✅ | ✅ |
-| Korea South​ ​ |  | ✅ |  |
-| Taiwan North |  |  |   |  |
+| Jio India West​​ | ✅ |  | ✅ |  |
+| South India <sup>1</sup> |  | ✅ |  |  |
+| Japan East | ✅ | ✅ | ✅ |  |
+| Japan West​ | ✅ |  | ✅ |  |
+| Korea Central | ✅ | ✅ | ✅ |  |
+| Korea South​​ |  |  | ✅ |  |
+| Indonesia Central |  | ✅ |  |  |
 
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+<sup>1</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ## Azure Government regions
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Arizona | ✅ | ✅  | | |
-| Texas |  | ✅ |  | |
-| Virginia | ✅ | ✅  | ✅ | |
+| Arizona | ✅ |  | ✅ |  |
+| Texas |  |  |  |  |
+| Virginia | ✅ | ✅ | ✅ |  |
 
 ## Azure operated by 21Vianet
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| China East |  |  |  |
-| China East 2 <sup>1</sup> | ✅  | | | |
-| China East 3 |  |  |  | |
-| China North |  |  | | |
-| China North 2 <sup>1</sup> |  |  | | |
-| China North 3 | | ✅ | ✅ | |
-
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+| China East |  |  |  |  |
+| China East 2 <sup>1</sup> | ✅ |  |  |  |
+| China East 3 |  |  |  |  |
+| China North |  |  |  |  |
+| China North 2 <sup>1</sup> |  |  |  |  |
+| China North 3 |  | ✅ | ✅ |  |
+
+<sup>1</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ## See also
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Azure AI Search Region Support Document"
}
```

### Explanation
The document on "Azure AI Search regions list" has been modified to provide updated information and improve clarity regarding the availability of Azure AI Search features across different regions. Key changes include:

1. **Date Update**: The last modified date has been updated to April 4, 2025, indicating that the information is recent.

2. **Clarity in Regional Availability**: The descriptions related to features like "Extra capacity" and "Semantic ranker" have been enhanced to clarify the regional dependencies and how users can check the availability of these features in their desired regions.

3. **Addition of New Features**: The "Query rewrite" feature has been added to the regional availability table, expanding the information available to users regarding functionalities dependent on regional services.

4. **Logical Formatting Adjustments**: Modifications include clearer expressions regarding requirements and capabilities, such as specifying that high-capacity partitions are available in select regions, along with guidance on creating or upgrading services to benefit from those capabilities.

5. **Reorganisation of Tables**: The structure of the regional availability table has been adjusted, shifting columns and modifying contents to enhance organization and readability. 

6. **Footnotes Adjustments**: Footnotes have been refined to provide better context on the regional constraints and to clarify when to consider alternative regions for higher storage limits.

7. **Improved Language Precision**: Throughout the document, careful attention has been paid to the precision of language, ensuring that features and their dependencies are clearly defined and accessible for users from various regions.

8. **Regional Breakdown Updates**: The regions are organized into well-structured sections, with each entry clearly delineating the availability of services like AI service integration, availability zones, and semantic ranking, thereby enhancing user navigability and comprehension.

Overall, these modifications aim to make the document more user-friendly, informative, and precise, helping users easily understand the regional capabilities of Azure AI Search and plan their service utilization effectively.

## articles/search/search-reliability.md{#item-3e9b1a}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: mattgotteiner
 ms.author: magottei
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 10/29/2024
+ms.date: 03/21/2025
 ms.custom:
   - subject-reliability
   - references_regions
@@ -21,13 +21,13 @@ Across Azure, [reliability](/azure/reliability/overview) means resiliency and av
 
 + Deploy multiple search services across different geographic regions. All search workloads are fully contained within a single service that runs in a single geographic region, but in a multi-service scenario, you have options for synchronizing content so that it's the same across all services. You can also set up a load balancing solution to redistribute requests or fail over if there's a service outage.
 
-For business continuity and recovery from disasters at a regional level, plan on a cross-regional topology, consisting of multiple search services having identical configuration and content. Your custom script or code provides the *fail over* mechanism to an alternate search service if one suddenly becomes unavailable.
+For business continuity and recovery from disasters at a regional level, plan on a cross-regional topology, consisting of multiple search services having identical configuration and content. Your custom script or code provides the *failover* mechanism to an alternate search service if one suddenly becomes unavailable.
 
 <a name="scale-for-availability"></a>
 
 ## High availability
 
-In Azure AI Search, replicas are copies of your index. A search service is commissioned with at least one replica, and can have up to 12 replicas. [Adding replicas](search-capacity-planning.md#adjust-capacity) allows Azure AI Search to do machine reboots and maintenance against one replica, while query execution continues on other replicas.
+In Azure AI Search, replicas are copies of your index. A search service is commissioned with at least one replica, and can have up to 12 replicas. [Adding replicas](search-capacity-planning.md#add-or-remove-partitions-and-replicas) allows Azure AI Search to do machine reboots and maintenance against one replica, while query execution continues on other replicas.
 
 For each individual search service, Microsoft guarantees at least 99.9% availability for configurations that meet these criteria:
 
@@ -45,7 +45,7 @@ No Service Level Agreement (SLA) is provided for the *Free* tier. For more infor
 
 [Availability zones](/azure/reliability/availability-zones-overview) are an Azure platform capability that divides a region's data centers into distinct physical location groups to provide high availability, within the same region. In Azure AI Search, individual replicas are the units for zone assignment. A search service runs within one region; its replicas run in different physical data centers (or zones) within that region.
 
-Availability zones are used when you add two or more replicas to your search service. Each replica is placed in a different availability zone within the region. If you have more replicas than available zones in the search service region, the replicas are distributed across zones as evenly as possible. There's no specific action on your part, except to [create a search service](search-create-service-portal.md) in a region that provides availability zones, and then to configure the service to [use multiple replicas](search-capacity-planning.md#adjust-capacity).
+Availability zones are used when you add two or more replicas to your search service. Each replica is placed in a different availability zone within the region. If you have more replicas than available zones in the search service region, the replicas are distributed across zones as evenly as possible. There's no specific action on your part, except to [create a search service](search-create-service-portal.md) in a region that provides availability zones, and then to configure the service to [use multiple replicas](search-capacity-planning.md#add-or-remove-partitions-and-replicas).
 
 ### Prerequisites
 
@@ -115,7 +115,7 @@ The goal of a geo-distributed set of search services is to have two or more inde
 You can implement this architecture by creating multiple services and designing a strategy for data synchronization. Optionally, you can include a resource like Azure Traffic Manager for routing requests. 
 
 > [!TIP]
-> For help in deploying multiple search services across multiple regions, see this [Bicep sample on GitHub](https://github.com/Azure-Samples/azure-search-multiple-regions) that deploys a fully configured, multi-regional search solution. The sample gives you two options for index synchronization, and request redirection using Traffic Manager.
+> For help with deploying multiple search services across multiple regions, see this [Bicep sample on GitHub](https://github.com/Azure-Samples/azure-search-multiple-regions) that deploys a fully configured, multi-regional search solution. The sample gives you two options for index synchronization, and request redirection using Traffic Manager.
 
 <a name="data-sync"></a>
 
@@ -166,7 +166,7 @@ Azure Traffic Manager is primarily used for routing network traffic across diffe
 Traffic Manager doesn't provide an endpoint for a direct connection to Azure AI Search, which means you can't put a search service directly behind Traffic Manager. Instead, the assumption is that requests flow to Traffic Manager, then to a search-enabled web client, and finally to a search service on the backend. The client and service are located in the same region. If one search service goes down, the search client starts failing, and Traffic Manager redirects to the remaining client.
 
 > [!NOTE]
-> If you are using Azure Load Balancer [health probes](/azure/load-balancer/load-balancer-custom-probe-overview) on a search service, you must use a HTTPS probe with `/ping` as the path.
+> If you are using Azure Load Balancer [health probes](/azure/load-balancer/load-balancer-custom-probe-overview) on a search service, you must use an HTTPS probe with `/ping` as the path.
 
 ![Diagram of search apps connecting through Azure Traffic Manager.][4]
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Search Reliability Document"
}
```

### Explanation
The "Search Reliability" document has received several modifications to enhance clarity and ensure accuracy of information regarding the high availability and reliability features of Azure AI Search. Key changes include:

1. **Date Update**: The modification date has been updated to March 21, 2025, indicating the recency of the information.

2. **Terminology Standardization**: The term "failover" has been standardized throughout the document to ensure consistent usage, thus improving readability and professionalism.

3. **Link Updates**: References to actions such as adding replicas have been refined to ensure they point to the current contextual documentation, enhancing user navigation. For example, the link has been updated from `search-capacity-planning.md#adjust-capacity` to `search-capacity-planning.md#add-or-remove-partitions-and-replicas`.

4. **Clarification of High Availability Concepts**: The section explaining the use of availability zones has been streamlined for clarity, ensuring users understand how replicas are managed across zones.

5. **Enhanced Deployment Guidance**: The wording in the recommendations has been refined to better assist users in deploying multiple search services across regions, making it clearer and more actionable.

6. **Restructured Instructions**: Instructions regarding the use of Azure Traffic Manager with search services have been clarified to avoid confusion about placement and routing, ensuring users understand the architecture required for optimal service location.

7. **Consistency in Command Styles**: Minor adjustments have been made throughout to ensure a consistent command style, particularly in the tips and notes provided. This helps maintain a professional tone and uniformity across the document.

Overall, these updates contribute to a clearer understanding of the available reliability features and improve the guidance provided for implementing robust Azure AI Search services, allowing users to better plan for high availability and disaster recovery in their configurations.

## articles/search/search-security-manage-encryption-keys.md{#item-db3487}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title:  Encrypt data using customer-managed keys
+title: Encrypt data using customer-managed keys
 titleSuffix: Azure AI Search
 description: Supplement server-side encryption in Azure AI Search using customer managed keys (CMK) or bring your own keys (BYOK) that you create and manage in Azure Key Vault.
 
@@ -8,7 +8,7 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 03/03/2025
+ms.date: 04/07/2025
 ms.custom:
   - references_regions
   - ignite-2023
@@ -110,7 +110,7 @@ Enable the system assigned managed identity for your search service. It's a two-
 
 1. Give the identity a descriptive name.
 
-1. Next, assign the user-managed identity to the search service. This can be done using the latest preview [2024-06-01-preview](/rest/api/searchmanagement/management-api-versions) management API or the previous preview.
+1. Next, assign the user-managed identity to the search service. This can be done using the latest preview [2025-05-01-preview](/rest/api/searchmanagement/management-api-versions) management API or the previous preview.
 
     The identity property takes a type and one or more fully qualified user-assigned identities:
   
@@ -122,7 +122,7 @@ Enable the system assigned managed identity for your search service. It's a two-
     Example of how to assign a user-managed identity to a search service:
   
     ```http
-    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/[search service name]?api-version=2024-06-01-preview
+    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/[search service name]?api-version=2025-05-01-preview
     Content-Type: application/json
 
     {
@@ -213,9 +213,9 @@ In the Azure portal, skillsets are defined in JSON view. Use the JSON shown in t
 
 1. Add a new object. In the object definition, select **Microsoft-managed encryption**.
 
-1. Select **Customer-managed keys** and use the pickers to select the vault, key, and version.
+1. Select **Customer-managed keys** and choose your subscription, vault, key, and version.
 
-:::image type="content" source="media/search-security-manage-encryption-keys/assign-key-vault-portal.png" alt-text="Screenshot of the encryption key page in the Azure portal.":::
+:::image type="content" source="media/search-security-manage-encryption-keys/assign-key-vault.png" alt-text="Screenshot of the encryption key page in the Azure portal.":::
 
 ### [**REST APIs**](#tab/rest)
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Manage Encryption Keys Document"
}
```

### Explanation
The document titled "Encrypt data using customer-managed keys" pertaining to Azure AI Search has been modified to improve clarity and ensure the accuracy of information regarding encryption key management. Key changes include:

1. **Title Formatting**: The title has been slightly reformatted, removing extra spaces for consistency.

2. **Date Update**: The last modified date has been updated to April 7, 2025, indicating the document is current.

3. **Version Updates**: The version number for the management API has been changed from `2024-06-01-preview` to `2025-05-01-preview`, reflecting the latest version users should utilize for assigning user-managed identities.

4. **Instruction Clarity**: The instruction for selecting keys has been revised to use "choose" instead of "use the pickers," simplifying the language and making it easier for users to understand the action required.

5. **Image Reference Update**: The source of the screenshot in the document has been adjusted to point to a more precise file name, enhancing the quality and relevance of visual aids provided for user reference.

6. **Consistent Terminology**: Modifications have been made to ensure consistent terminology throughout the documentation, specifically in how actions are described, aiding in user comprehension.

Overall, these updates aim to refine the instructions related to customer-managed encryption keys, ensuring users have the most accurate and accessible information for securing their data within Azure AI Search services. This enhances user experience and facilitates better understanding of the key management process.

## articles/search/search-security-network-security-perimeter.md{#item-49c0d7}

<details>
<summary>Diff</summary>
````diff
@@ -26,7 +26,7 @@ This article explains how to join an Azure AI Search service to a [network secur
 * Block any data exfiltration from a search service to other services outside the perimeter.
 * Allow access to your search service using inbound and outbound access capabilities of the network security perimeter.
 
-You can add a search service to a network security perimeter in the Azure portal, as described in this article. Alternatively, you can use the [Azure Virtual Network Manager REST API](/rest/api/networkmanager/) to join a search service, and use the [Search Management REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) to view and synchronize the configuration settings.
+You can add a search service to a network security perimeter in the Azure portal, as described in this article. Alternatively, you can use the [Azure Virtual Network Manager REST API](/rest/api/networkmanager/) to join a search service, and use the [Search Management REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2025-05-01-preview&preserve-view=true) to view and synchronize the configuration settings.
 
 ## Limitations and considerations
 
@@ -270,9 +270,9 @@ In order to test your connection through network security perimeter, you need ac
 
 ## View and manage network security perimeter configuration
 
-You can use the [Network Security Perimeter Configuration REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) to review and reconcile perimeter configurations.
+You can use the [Network Security Perimeter Configuration REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2025-05-01preview&preserve-view=true) to review and reconcile perimeter configurations.
 
-Be sure to use preview API version `2024-06-01-preview`. [Learn how to call the Management REST APIs](search-manage-rest.md).
+Be sure to use preview API version `2024-06-01-preview` or a later preview. [Learn how to call the Management REST APIs](search-manage-rest.md).
 
 ## See also
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarifications to Network Security Perimeter Document"
}
```

### Explanation
The document titled "Network security perimeter for Azure AI Search" has been modified to enhance clarity and ensure users are guided with the most current information regarding network security configurations. Key changes include:

1. **Version Update**: The management API version has been updated from `2024-06-01-preview` to `2025-05-01-preview` in two instances. This change ensures that users are directed to utilize the latest APIs for adding a search service to a network security perimeter and managing configurations.

2. **Instruction Clarity**: The text instructing users to use REST APIs for managing perimeter configurations has been refined. It has been changed to specify that users should utilize the latest preview version if available, providing broader guidance.

3. **Consistent Terminology**: Minor adjustments have been made to the language to ensure uniformity throughout the document, particularly in how actions around API usage are described, which improves overall readability.

4. **Enhanced Limitations Note**: The wording was adjusted to inform users of the flexibility in API versions, guiding them to utilize either the older version or a newer preview as applicable.

These updates collectively aim to provide Azure AI Search users with the most relevant and actionable information regarding network security practices, ensuring they can effectively secure their search services while facilitating easier integration and management through updated API references.

## articles/search/search-security-overview.md{#item-6b3f1e}

<details>
<summary>Diff</summary>
````diff
@@ -237,7 +237,7 @@ CMK support was rolled out in two phases. If you created your search service dur
 
 + The second rollout on May 13, 2021 added encryption for temporary disks and extended CMK encryption to [all supported regions](search-region-support.md).
 
-  If you're using CMK from a service created during the first rollout and you also want CMK encryption over temporary disks, you need to create a new search service in your region of choice and redeploy your content. To determine your service creation date, see [How to check service creation date](vector-search-index-size.md#how-to-check-service-creation-date).
+  If you're using CMK from a service created during the first rollout and you also want CMK encryption over temporary disks, you need to create a new search service in your region of choice and redeploy your content. To determine your service creation date, see [How to check service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date).
 
 Enabling CMK encryption will increase index size and degrade query performance. Based on observations to date, you can expect to see an increase of 30-60 percent in query times, although actual performance will vary depending on the index definition and types of queries. Because of the negative performance impact, we recommend that you only enable this feature on indexes that really require it. For more information, see [Configure customer-managed encryption keys in Azure AI Search](search-security-manage-encryption-keys.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to CMK Support Information"
}
```

### Explanation
The document titled "Overview of Search Security" in Azure has been updated to clarify the customer-managed keys (CMK) support details. The changes made are as follows:

1. **Added Clarification**: New information has been added regarding the second rollout of CMK support that occurred on May 13, 2021. This addition highlights that during this rollout, encryption capabilities were extended to temporary disks in all supported regions, enhancing the understanding of the timeline of CMK features.

2. **Link Update**: A hyperlink reference in the document was modified to point to a different section. The link now directs users to "search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date" instead of the previous link, "vector-search-index-size.md#how-to-check-service-creation-date." This ensures that users will access the correct and relevant information regarding checking the service creation date, which may assist in configuration decisions.

These adjustments are part of an effort to improve the accuracy and usefulness of the documentation for users managing encryption keys in Azure Search services, ensuring that they have the most up-to-date and straightforward information available.

## articles/search/search-semi-structured-data.md{#item-d3388d}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index semi-structured data in JSON blobs'
+title: 'Tutorial: Index Semi-Structured Data in JSON Blobs'
 titleSuffix: Azure AI Search
 description: Learn how to index and search semi-structured Azure JSON blobs using Azure AI Search REST APIs.
 
@@ -10,42 +10,42 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 12/10/2024
+ms.date: 03/28/2025
 
 ---
 
 # Tutorial: Index nested JSON blobs from Azure Storage using REST
 
-Azure AI Search can index JSON documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read semi-structured data. Semi-structured data contains tags or markings which separate content within the data. It splits the difference between unstructured data, which must be fully indexed, and formally structured data that adheres to a data model, such as a relational database schema that can be indexed on a per-field basis.
+Azure AI Search can index JSON documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read semi-structured data. Semi-structured data contains tags or markings that separate content within the data. It splits the difference between unstructured data, which must be fully indexed, and formally structured data that adheres to a data model, such as a relational database schema that can be indexed on a per-field basis.
 
-This tutorial shows you to index nested JSON arrays. It uses a REST client and the [Search REST APIs](/rest/api/searchservice/) to perform the following tasks:
+This tutorial shows you how to index nested JSON arrays, using a REST client and the [Search REST APIs](/rest/api/searchservice/) to:
 
 > [!div class="checklist"]
 > + Set up sample data and configure an `azureblob` data source
 > + Create an Azure AI Search index to contain searchable content
 > + Create and run an indexer to read the container and extract searchable content
 > + Search the index you just created
 
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Prerequisites
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ [Azure Storage](/azure/storage/common/storage-account-create)
++ [Azure Storage](/azure/storage/common/storage-account-create).
 
-+ [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing Azure AI Search resource](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription.
++ [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.
+
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
 > [!NOTE]
-> You can use the free service for this tutorial. A free search service limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ### Download files
 
 Download a zip file of the sample data repository and extract the contents. [Learn how](https://docs.github.com/get-started/start-your-journey/downloading-files-from-github).
 
 + [ny-philharmonic-free](https://github.com/Azure-Samples/azure-search-sample-data)
 
-Sample data is a single JSON file containing a JSON array and 1,521 nested JSON elements. Sample data originates from [NY Philharmonic Performance History](https://www.kaggle.com/datasets/nyphil/perf-history) on Kaggle. We chose one JSON file to stay under the storage limits of the free tier.
+The sample data is a single JSON file that contains a JSON array and 1,521 nested JSON elements. The data originates from the [NY Philharmonic Performance History](https://www.kaggle.com/datasets/nyphil/perf-history) on Kaggle. We chose one JSON file to stay under the storage limits of the Free tier.
 
 Here's the first nested JSON in the file. The remainder of the file includes 1,520 other instances of concert performances.
 
@@ -90,7 +90,7 @@ Here's the first nested JSON in the file. The remainder of the file includes 1,5
 
 ### Upload sample data to Azure Storage
 
-1. In Azure Storage, create a new container and name it *ny-philharmonic-free*.
+1. In Azure Storage, create a new container named **ny-philharmonic-free**.
 
 1. [Upload the sample data files](/azure/storage/blobs/storage-quickstart-blobs-portal).
 
@@ -106,7 +106,7 @@ Here's the first nested JSON in the file. The remainder of the file includes 1,5
 
 ### Copy a search service URL and API key
 
-For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal.
+For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal. For alternative connection methods, see [Managed identities](search-howto-managed-identities-data-sources.md).
 
 1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the search service **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
@@ -116,9 +116,9 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 ## Set up your REST file
 
-1. Start Visual Studio Code and create a new file
+1. Start Visual Studio Code and create a new file.
 
-1. Provide values for variables used in the request: 
+1. Provide values for variables used in the request.
 
    ```http
    @baseUrl = PUT-YOUR-SEARCH-SERVICE-ENDPOINT-HERE
@@ -129,7 +129,7 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 1. Save the file using a `.rest` or `.http` file extension.
 
-See [Quickstart: Text search using REST](search-get-started-rest.md) if you need help with the REST client.
+For help with the REST client, see [Quickstart: Keyword search using REST](search-get-started-rest.md).
 
 ## Create a data source
 
@@ -199,7 +199,7 @@ Connection: close
 
 [Create Index (REST)](/rest/api/searchservice/indexes/create) creates a search index on your search service. An index specifies all the parameters and their attributes.
 
-For nested JSON, the index fields must be identical to the source fields. Currently, Azure AI Search doesn't support field mappings to nested JSON. For this reason, field names and data types must match completely. The following index aligns to the JSON elements in the raw content.
+For nested JSON, the index fields must be identical to the source fields. Currently, Azure AI Search doesn't support field mappings to nested JSON, so field names and data types must match completely. The following index aligns to the JSON elements in the raw content.
 
 ```http
 ### Create an index
@@ -235,7 +235,7 @@ POST {{baseUrl}}/indexes?api-version=2024-07-01  HTTP/1.1
     }
 ```
 
-**Key points**:
+Key points:
 
 + You can't use [field mappings](search-indexer-field-mappings.md) to reconcile differences in field names or data types. This index schema is designed to mirror the raw content.
 
@@ -268,7 +268,7 @@ POST {{baseUrl}}/indexers?api-version=2024-07-01  HTTP/1.1
     }
 ```
 
-**Key points**:
+Key points:
 
 + The raw content file contains a JSON array (`"programs"`) with 1,526 nested JSON structures. Set `parsingMode` to `jsonArray` to tell the indexer that each blob contains a  JSON array. Because the nested JSON starts one level down, set `documentRoot` to `/programs`.
 
@@ -290,7 +290,7 @@ POST {{baseUrl}}/indexes/ny-philharmonic-index/docs/search?api-version=2024-07-0
   }
 ```
 
-Send the request. This is an unspecified full text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
+Send the request. This is an unspecified full-text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
 
 ```json
 HTTP/1.1 200 OK
@@ -321,7 +321,7 @@ Connection: close
 }
 ```
 
-Add a `search` parameter to search on a string. Add a `select` parameter to limit the results to fewer fields. Add a `filter` to further narrow the search.
+Add a `search` parameter to search on a string, a `select` parameter to limit the results to fewer fields, and a `filter` to further narrow the search.
 
 ```http
 ### Query the index
@@ -339,14 +339,14 @@ POST {{baseUrl}}/indexes/ny-philharmonic-index/docs/search?api-version=2024-07-0
 
 Two documents are returned in the response.
 
-For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case-sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
+For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case -sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
 
 > [!NOTE]
-> The `$filter` parameter only works on fields that were marked filterable at the creation of your index.
+> The `$filter` parameter only works on fields that were marked filterable during index creation.
 
 ## Reset and rerun
 
-Indexers can be reset, clearing execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
+Indexers can be reset to clear execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
 
 ```http
 ### Reset the indexer
@@ -374,7 +374,7 @@ You can use the Azure portal to delete indexes, indexers, and data sources.
 
 ## Next steps
 
-Now that you're familiar with the basics of Azure Blob indexing, let's take a closer look at indexer configuration for JSON blobs in Azure Storage.
+Now that you're familiar with the basics of Azure Blob indexing, take a closer look at indexer configuration for JSON blobs in Azure Storage:
 
 > [!div class="nextstepaction"]
 > [Configure JSON blob indexing](search-howto-index-json-blobs.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to the Semi-Structured Data Indexing Tutorial"
}
```

### Explanation
The document "Tutorial: Index Semi-Structured Data in JSON Blobs" has undergone significant modifications to enhance clarity, update information, and improve formatting. 

1. **Title Capitalization**: The tutorial's title has been changed to capitalize all major words, aligning with standard title capitalization practices.

2. **Updated Date**: The publication date of the tutorial has been updated from December 10, 2024, to March 28, 2025, reflecting the latest revision date.

3. **Clarifications and Improvements**:
   - The instructional text has been refined for better readability, such as changing "shows you to" to "shows you how to," making it more precise.
   - Section headers and bullet points have improved formatting to enhance understanding without altering meaning; for instance, changing "name it *ny-philharmonic-free*" to "named **ny-philharmonic-free**" emphasizes the name of the container better.
   - Added alternative connection methods in the context of obtaining an endpoint and API key, adding usability for users.

4. **Notes on Limitations**: The notes have been modified to provide clearer guidance on the free service's limitations and the importance of ensuring sufficient resources are available.

5. **Consistent Language and Formatting**: Throughout the document, adjustments were made to ensure consistent language and formatting, thereby enhancing clarity and professionalism.

These updates aim to empower users with a clearer and more effective tutorial on indexing semi-structured data with Azure AI Search, ensuring that all references and instructions are accurate, up-to-date, and easy to navigate.

## articles/search/search-sku-manage-costs.md{#item-6e0122}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: conceptual
-ms.date: 02/25/2025
+ms.date: 03/21/2025
 ---
 
 # Plan and manage costs of an Azure AI Search service
@@ -22,7 +22,7 @@ As a first step, estimate your baseline costs by using the Azure pricing calcula
 Azure provides built-in cost management that cuts across service boundaries to provide inclusive cost monitoring and the ability to set budgets and define alerts. The costs of running a search service will vary depending on capacity and which features you use. After you create your search service, optimize capacity so that you pay only for what you need. 
 
 > [!NOTE]
-> Higher capacity partitions are available at the same billing rate on newer services created after April and May 2024. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits) for partition size upgrades.
+> Higher capacity partitions are available at the same billing rate on newer services created after April and May 2024. For more information about partition size upgrades, see [Service limits](search-limits-quotas-capacity.md#service-limits).
 
 <a name="billable-events"></a>
 
@@ -76,7 +76,7 @@ Follow these guidelines to minimize costs of an Azure AI Search solution.
 
 1. [Scale up](search-capacity-planning.md) for resource-intensive operations like indexing, and then readjust downwards for regular query workloads. If there are predictable patterns to your workloads, you might be able to synchronize scale up to coincide with the expected volume (you would need to write code to automate this).
 
-   When estimating the cost of a search solution, keep in mind that pricing and capacity aren't linear (doubling capacity more than doubles the cost on the same tier). Also, at some point, switching up to a higher tier can give you better and faster performance at roughly the same price point. For more information and an example, see [Upgrade to a Standard S2 tier](search-performance-tips.md#tip-upgrade-to-a-standard-s2-tier).
+   When estimating the cost of a search solution, keep in mind that pricing and capacity aren't linear (doubling capacity more than doubles the cost on the same tier). Also, at some point, switching up to a higher tier can give you better and faster performance at roughly the same price point. For more information and an example, see [Switch to a Standard S2 tier](search-performance-tips.md#tip-switch-to-a-standard-s2-tier).
 
 1. Consider [Azure Web App](/azure/app-service/overview) for your front-end application so that requests and responses stay within the data center boundary.
 
@@ -100,7 +100,7 @@ Search runs as a continuous service. Dedicated resources are always operational,
 
 **Can I change the billing rate (tier) of an existing search service?**
 
-In-place upgrade or downgrade isn't supported. Changing a service tier requires provisioning a new service at the desired tier.
+Existing services can be switched between Basic and Standard (S1, S2, and S3) tiers. Currently, you can only switch from a lower tier to a higher tier, such as going from Basic to S1. For more information, see [Change your pricing tier](search-capacity-planning.md#change-your-pricing-tier).
 
 ## Next steps
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Adjustments to Cost Management for Azure AI Search"
}
```

### Explanation
The document titled "Plan and Manage Costs of an Azure AI Search Service" has been updated with minor modifications aimed at improving clarity and accuracy regarding cost management practices.

1. **Date Update**: The last modified date of the document has been changed from February 25, 2025, to March 21, 2025, reflecting the most current revision.

2. **Enhanced Note on Capacity Partitions**: Clarification in the note about higher capacity partitions emphasizes that detailed information regarding partition size upgrades can now be found in the associated service limits documentation, improving users' ability to find relevant information.

3. **Terminology Consistency**: The terminology was adjusted from "Upgrade to a Standard S2 tier" to "Switch to a Standard S2 tier." This change clarifies the action being referred to and ensures consistency with the updated guidance on managing service tiers.

4. **Clarification on Tier Changes**: The explanation about changing billing rates has been updated. It provides clearer instructions regarding the ability to switch between Basic and Standard tiers and specifies that changes can only occur from lower to higher tiers. This enhancement aids users in understanding the tier management process more thoroughly.

These updates collectively enhance the document's usability, ensuring that users have precise and relevant information to help them manage costs effectively while using Azure AI Search services.

## articles/search/search-sku-tier.md{#item-7686b8}

<details>
<summary>Diff</summary>
````diff
@@ -8,26 +8,27 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 01/15/2025
+ms.date: 03/21/2025
 
 ---
 
 # Choose a service tier for Azure AI Search
 
-Part of [creating a search service](search-create-service-portal.md) is choosing a pricing tier (or SKU) that's fixed for the lifetime of the service. In the Azure portal, tier is specified in the **Select Pricing Tier** page when you create the service. In PowerShell or Azure CLI, the tier is specified through the **`-Sku`** parameter.
+Part of [creating a search service](search-create-service-portal.md) is choosing a pricing tier (or SKU). In the Azure portal, tier is specified in the **Select Pricing Tier** page when you create the service. In PowerShell or Azure CLI, the tier is specified through the `-Sku` parameter.
 
-The tier determines:
+The tier determines the:
 
-+ Maximum number of indexes and other objects allowed on the service
-+ Size and speed of partitions (physical storage)
-+ Billable rate as a fixed monthly cost, but also an incremental cost if you add capacity
++ Maximum number of indexes and other objects allowed on the service.
++ Size and speed of partitions (physical storage).
++ Billable rate as a fixed monthly cost, but also an incremental cost if you add capacity.
++ Workload characteristics. Some tiers are optimized for specific workloads.
 
 In a few instances, the tier you choose determines the availability of [premium features](#feature-availability-by-tier).
 
 Billing rates are shown in the Azure portal's **Select Pricing Tier** page. You can check the [pricing page](https://azure.microsoft.com/pricing/details/search/) for regional rates and review [Plan and manage costs](search-sku-manage-costs.md) to learn more about the billing model.
 
 > [!NOTE]
-> Search services created after April 3, 2024 have larger partitions and higher vector quotas at almost every tier. For more information, see [service limits](search-limits-quotas-capacity.md#service-limits).
+> Search services created after April 3, 2024 have larger partitions and higher vector quotas at almost every tier. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits).
 
 ## Tier descriptions
 
@@ -63,7 +64,7 @@ Currently, several regions are capacity-constrained for specific tiers and can't
 
 ## Feature availability by tier
 
-Most features are available on all tiers, including the free tier. In a few cases, the tier determines the availability of a feature. The following table describes the constraints.
+Most features are available on all tiers, including the Free tier. In a few cases, the tier determines the availability of a feature. The following table describes the constraints.
 
 | Feature | Tier considerations |
 |---------|---------------------|
@@ -88,7 +89,7 @@ Tiers determine the  maximum storage of the service itself, plus the maximum num
 Tier pricing includes details about per-partition storage that ranges from 15 GB for Basic, up to 2 TB for Storage Optimized (L2) tiers. Other hardware characteristics, such as speed of operations, latency, and transfer rates, aren't published, but tiers that are designed for specific solution architectures are built on hardware that has the features to support those scenarios. For more information about partitions, see [Estimate and manage capacity](search-capacity-planning.md) and [Reliability in Azure AI Search](search-reliability.md).
 
 > [!NOTE]
-> Higher capacity partitions became available in selected regions starting in April 2024. A second wave of higher capacity partitions released in May 2024. If you're using an older search service, consider creating a new search service to benefit from more capacity at the same billing rate. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits). To check the age of your search service, see [How to check service creation date](vector-search-index-size.md#how-to-check-service-creation-date).
+> Higher capacity partitions became available in select regions in April 2024. A second wave of higher capacity partitions was released in May 2024. If you have an older search service, you might be able to [upgrade your service](search-how-to-upgrade.md) to benefit from more capacity at the same billing rate.
 
 ## Billing rates
 
@@ -102,22 +103,30 @@ The following example provides an illustration. Assume a hypothetical billing ra
 
 This billing model is based on the concept of applying the billing rate to the number *search units* (SU) used by a search service. All services are initially provisioned at one SU, but you can increase the SUs by adding either partitions or replicas to handle larger workloads. For more information, see [How to estimate costs of a search service](search-sku-manage-costs.md).
 
-## Tier upgrade or downgrade
+## Tier changes
 
-There's no built-in support to upgrade or downgrade tiers. If you want to switch to a different tier, the approach is:
+Services can be switched between Basic and Standard (S1, S2, and S3) tiers. Currently, you can only switch from a lower tier to a higher tier, such as going from Basic to S1. Your region also can't have capacity constraints on the higher tier. For more information, see [Change your pricing tier](search-capacity-planning.md#change-your-pricing-tier).
 
-+ Create a new search service at the new tier.
+If you want to switch to a lower tier or to a different tier than those previously listed, the approach is:
 
-+ Deploy your search content onto the new service. [Follow this checklist](search-howto-move-across-regions.md#prepare-and-move) to make sure you have all of the content.
+1. Create a new search service at the new tier.
 
-+ Delete the old search service once you're sure it's no longer needed.
+1. Deploy your search content onto the new service. [Follow this checklist](search-howto-move-across-regions.md#prepare-and-move) to make sure you have all of the content.
 
-For large indexes that you don't want to rebuild from scratch, consider using one of the backup and restore samples to move them:[backup and restore sample (C#)](https://github.com/Azure-Samples/azure-search-dotnet-utilities/blob/main/index-backup-restore/README.md), [backup and restore sample (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/index-backup-restore/azure-search-backup-and-restore.ipynb), [larget index backup and restore (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/resumable-index-backup-restore/backup-and-restore.ipynb).
+1. Delete the old search service when you're sure it's no longer needed.
+
+For large indexes that you don't want to rebuild from scratch, consider using one of the backup and restore samples to move them:
+
++ [Backup and restore sample (C#)](https://github.com/Azure-Samples/azure-search-dotnet-utilities/blob/main/index-backup-restore/README.md)
++ [Backup and restore sample (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/index-backup-restore/azure-search-backup-and-restore.ipynb)
++ [Largest index backup and restore (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/resumable-index-backup-restore/backup-and-restore.ipynb)
 
 ## Next steps
 
-The best way to choose a pricing tier is to start with a least-cost tier, and then allow experience and testing inform your decision to keep the service or create a new one at a higher tier. For next steps, we recommend that you create a search service at a tier that can accommodate the level of testing you propose to do, and then review the following guidance for recommendations on estimating cost and capacity.
+The best way to choose a pricing tier is to start with a least-cost tier, and then allow experience and testing to inform your decision to keep the service or switch to a higher tier.
+
+For next steps, we recommend that you create a search service at a tier that can accommodate the level of testing you propose to do, and then review the following guidance on estimating cost and capacity:
 
 + [Create a search service](search-create-service-portal.md)
 + [Estimate costs](search-sku-manage-costs.md)
-+ [Estimate capacity](search-sku-manage-costs.md)
++ [Estimate capacity](search-capacity-planning.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revisions to Azure AI Search Service Tier Document"
}
```

### Explanation
The document "Choose a Service Tier for Azure AI Search" has been modified to include various updates aimed at enhancing clarity, accuracy, and detail regarding the service tier options for Azure AI Search.

1. **Date Update**: The last modified date was updated from January 15, 2025, to March 21, 2025, which reflects the most recent changes in the document.

2. **Rephrasing for Clarity**: The phrase “the tier determines” has been changed to “the tier determines the,” making the list of attributes clearer and more grammatically correct.

3. **Additional Attributes Listed**: The document now explicitly states “workload characteristics” as an additional attribute dependent on the service tier, further informing readers about how tiers can affect performance based on specific use cases.

4. **Consistent Terminology**: The term "Free tier" has been standardized throughout the document, ensuring consistency in terminology regarding service tiers.

5. **Notes on Search Service Updates**: The note about larger partitions and higher vector quotas available for services created after April 3, 2024, has been rephrased for clarity, and it now mentions the ability to upgrade older services for better capacity at no additional billing rate.

6. **Change in Tier Management Guidance**: The section on tier upgrades has been revised to provide clear instructions on how to switch between tiers, particularly emphasizing that services can only be switched from a lower tier to a higher tier and includes specific steps for transitioning to a new service. Additional instructions for backing up and restoring large indexes have been moved into a bulleted format for easier reading.

7. **Next Steps Clarification**: The recommendation on choosing a pricing tier has been refined, encouraging users to start with a lower-cost tier and allowing testing to inform future tier decisions. Additionally, a final note about next steps has been provided, with explicit links for users to create a search service and estimate costs and capacity.

These modifications collectively improve the document's coherence, accuracy, and usefulness, ensuring that users have better access to relevant information about managing service tiers effectively in Azure AI Search.

## articles/search/search-synapseml-cognitive-services.md{#item-dcc36f}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index at scale (Spark)'
+title: 'Tutorial: Index at Scale (Spark)'
 titleSuffix: Azure AI Search
 description: Search big data from Apache Spark that's been transformed by SynapseML. Load invoices into data frames, apply machine learning, and then send output to a generated search index.
 
@@ -10,52 +10,52 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 01/30/2025
+ms.date: 03/28/2025
 ---
 
 # Tutorial: Index large data from Apache Spark using SynapseML and Azure AI Search
 
-In this Azure AI Search tutorial, learn how to index and query large data loaded from a Spark cluster. Set up a Jupyter Notebook that performs the following actions:
+In this Azure AI Search tutorial, you learn how to index and query large data loaded from a Spark cluster. You set up a Jupyter Notebook to:
 
 > [!div class="checklist"]
 > + Load various forms (invoices) into a data frame in an Apache Spark session
-> + Analyze them to determine their features
+> + Analyze the forms to determine their features
 > + Assemble the resulting output into a tabular data structure
 > + Write the output to a search index hosted in Azure AI Search
 > + Explore and query over the content you created
 
-This tutorial takes a dependency on [SynapseML](https://microsoft.github.io/SynapseML/), an open source library that supports massively parallel machine learning over big data. In SynapseML, search indexing and machine learning are exposed through *transformers* that perform specialized tasks. Transformers tap into a wide range of AI capabilities. In this exercise, use the **AzureSearchWriter** APIs for analysis and AI enrichment.
+This tutorial takes a dependency on [SynapseML](https://microsoft.github.io/SynapseML/), an open-source library that supports massively parallel machine learning over big data. In SynapseML, search indexing and machine learning are exposed through *transformers* that perform specialized tasks. Transformers tap into a wide range of AI capabilities. In this exercise, you use the **AzureSearchWriter** APIs for analysis and AI enrichment.
 
 Although Azure AI Search has native [AI enrichment](cognitive-search-concept-intro.md), this tutorial shows you how to access AI capabilities outside of Azure AI Search. By using SynapseML instead of indexers or skills, you're not subject to data limits or other constraints associated with those objects.
 
 > [!TIP]
-> Watch a short video of this demo at [https://www.youtube.com/watch?v=iXnBLwp7f88](https://www.youtube.com/watch?v=iXnBLwp7f88). The video expands on this tutorial with more steps and visuals.
+> Watch a [short video of this demo](https://www.youtube.com/watch?v=iXnBLwp7f88). The video expands on this tutorial with more steps and visuals.
 
 ## Prerequisites
 
 You need the `synapseml` library and several Azure resources. If possible, use the same subscription and region for your Azure resources and put everything into one resource group for simple cleanup later. The following links are for portal installs. The sample data is imported from a public site.
 
 + [SynapseML package](https://microsoft.github.io/SynapseML/docs/Get%20Started/Install%20SynapseML/#python) <sup>1</sup>
-+ [Azure AI Search](search-create-service-portal.md) (any tier), with an **API Kind** of `AIServices` <sup>2</sup> 
-+ [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) (any tier) <sup>3</sup>
-+ [Azure Databricks](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal) (any tier) with Apache Spark 3.3.0 runtime<sup>4</sup>
++ [Azure AI Search](search-create-service-portal.md) (any tier) <sup>2</sup> 
++ [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) (any tier) with an **API Kind** of `AIServices` <sup>3</sup>
++ [Azure Databricks](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal) (any tier) with Apache Spark 3.3.0 runtime <sup>4</sup>
 
 <sup>1</sup> This link resolves to a tutorial for loading the package.
 
-<sup>2</sup> You can use the free search tier to index the sample data, but [choose a higher tier](search-sku-tier.md) if your data volumes are large. For billable tiers, provide the [search API key](search-security-api-keys.md#find-existing-keys) in the [Set up dependencies](#step-2-set-up-dependencies) step further on.
+<sup>2</sup> You can use the Free tier to index the sample data, but [choose a higher tier](search-sku-tier.md) if your data volumes are large. For billable tiers, provide the [search API key](search-security-api-keys.md#find-existing-keys) in the [Set up dependencies](#set-up-dependencies) step further on.
 
-<sup>3</sup> This tutorial uses Azure AI Document Intelligence and Azure AI Translator. In the instructions that follow, provide a [multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) key and the region. The same key works for both services. **It's important that you use an Azure AI services multi-service account of API kind of `AIServices` for this tutorial**. You can check the API kind in the Azure portal on the Overview section of your Azure AI services multi-service account page. For more information about API kind, see [Attach an Azure AI services multi-service resource in Azure AI Search](cognitive-search-attach-cognitive-services.md).
+<sup>3</sup> This tutorial uses Azure AI Document Intelligence and Azure AI Translator. In the instructions that follow, provide a [multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) key and the region. The same key works for both services. **For this tutorial, it's important that you use an Azure AI services multi-service account with an API kind of `AIServices`**. You can check the API kind in the Azure portal on the Overview section of your Azure AI services multi-service account page. For more information about API kind, see [Attach an Azure AI services multi-service resource in Azure AI Search](cognitive-search-attach-cognitive-services.md).
 
 <sup>4</sup> In this tutorial, Azure Databricks provides the Spark computing platform. We used the [portal instructions](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal) to set up the cluster and workspace.
 
 > [!NOTE]
-> All of the above Azure resources support security features in the Microsoft Identity platform. For simplicity, this tutorial assumes key-based authentication, using endpoints and keys copied from the Azure portal pages of each service. If you implement this workflow in a production environment, or share the solution with others, remember to replace hard-coded keys with integrated security or encrypted keys.
+> The preceding Azure resources support security features in the Microsoft Identity platform. For simplicity, this tutorial assumes key-based authentication, using endpoints and keys copied from the Azure portal pages of each service. If you implement this workflow in a production environment or share the solution with others, remember to replace hard-coded keys with integrated security or encrypted keys.
 
-## Step 1: Create a Spark cluster and notebook
+## Create a Spark cluster and notebook
 
-In this section, create a cluster, install the `synapseml` library, and create a notebook to run the code.
+In this section, you create a cluster, install the `synapseml` library, and create a notebook to run the code.
 
-1. In Azure portal, find your Azure Databricks workspace and select **Launch workspace**.
+1. In the Azure portal, find your Azure Databricks workspace and select **Launch workspace**.
 
 1. On the left menu, select **Compute**.
 
@@ -67,7 +67,7 @@ In this section, create a cluster, install the `synapseml` library, and create a
 
    :::image type="content" source="media/search-synapseml-cognitive-services/cluster-green-dot.png" alt-text="Screenshot of a Data Bricks compute page with a green dot by the cluster name.":::
 
-1. Install the `synapseml` library after the cluster is created:
+1. After the cluster is created, install the `synapseml` library:
 
    1. Select **Libraries** from the tabs at the top of the cluster's page.
 
@@ -77,7 +77,7 @@ In this section, create a cluster, install the `synapseml` library, and create a
 
    1. Select **Maven**.
 
-   1. In Coordinates, search for or type `com.microsoft.azure:synapseml_2.12:1.0.9`
+   1. In **Coordinates**, search for `com.microsoft.azure:synapseml_2.12:1.0.9`.
 
    1. Select **Install**.
 
@@ -89,17 +89,17 @@ In this section, create a cluster, install the `synapseml` library, and create a
 
 1. Give the notebook a name, select **Python** as the default language, and select the cluster that has the `synapseml` library.
 
-1. Create seven consecutive cells. You use these to paste in code in the following sections.
+1. Create seven consecutive cells. In the following sections, you paste code in these cells.
 
    :::image type="content" source="media/search-synapseml-cognitive-services/create-seven-cells.png" alt-text="Screenshot of the notebook with placeholder cells." border="true":::
 
-## Step 2: Set up dependencies
+## Set up dependencies
 
-Paste the following code into the first cell of your notebook. 
+Paste the following code into the first cell of your notebook.
 
-Replace the placeholders with endpoints and access keys for each resource. Provide a name for a new search index that's created for you. No other modifications are required, so run the code when you're ready.
+Replace the placeholders with endpoints and access keys for each resource. Provide a name for a new search index to be created for you. No other modifications are required, so run the code when you're ready.
 
-This code imports multiple packages and sets up access to the Azure resources used in this workflow.
+This code imports multiple packages and sets up access to the Azure resources used in this tutorial.
 
 ```python
 import os
@@ -115,11 +115,11 @@ search_key = "placeholder-search-service-admin-api-key"
 search_index = "placeholder-for-new-search-index-name"
 ```
 
-## Step 3: Load data into Spark
+## Load data into Spark
 
 Paste the following code into the second cell. No modifications are required, so run the code when you're ready.
 
-This code loads a few external files from an Azure storage account. The files are various invoices, and they're read into a data frame.
+This code loads a few external files from an Azure storage account. The files are various invoices that are read into a data frame.
 
 ```python
 def blob_to_url(blob):
@@ -141,7 +141,7 @@ df2 = (spark.read.format("binaryFile")
 display(df2)
 ```
 
-## Step 4: Add document intelligence
+## Add document intelligence
 
 Paste the following code into the third cell. No modifications are required, so run the code when you're ready.
 
@@ -163,15 +163,15 @@ analyzed_df = (AnalyzeInvoices()
 display(analyzed_df)
 ```
 
-The output from this step should look similar to the next screenshot. Notice how the forms analysis is packed into a densely structured column, which is difficult to work with. The next transformation resolves this issue by parsing the column into rows and columns.
+The output should look similar to the following screenshot. Notice how the forms analysis is packed into a densely structured column, which is difficult to work with. The next transformation resolves this issue by parsing the column into rows and columns.
 
 :::image type="content" source="media/search-synapseml-cognitive-services/analyze-forms-output.png" alt-text="Screenshot of the AnalyzeInvoices output." border="true":::
 
-## Step 5: Restructure document intelligence output
+## Restructure document intelligence output
 
 Paste the following code into the fourth cell and run it. No modifications are required.
 
-This code loads [FormOntologyLearner](https://mmlspark.blob.core.windows.net/docs/0.10.0/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.FormOntologyTransformer), a transformer that analyzes the output of Document Intelligence transformers and infers a tabular data structure. The output of AnalyzeInvoices is dynamic and varies based on the features detected in your content. Furthermore, the transformer consolidates output into a single column. Because the output is dynamic and consolidated, it's difficult to use in downstream transformations that require more structure.
+This code loads [FormOntologyLearner](https://mmlspark.blob.core.windows.net/docs/0.10.0/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.FormOntologyTransformer), a transformer that analyzes the output of Document Intelligence transformers and infers a tabular data structure. The output of AnalyzeInvoices is dynamic and varies based on the features detected in your content. Furthermore, the transformer consolidates the output into a single column. Because the output is dynamic and consolidated, it's difficult to use in downstream transformations that require more structure.
 
 FormOntologyLearner extends the utility of the AnalyzeInvoices transformer by looking for patterns that can be used to create a tabular data structure. Organizing the output into multiple columns and rows makes the content consumable in other transformers, like AzureSearchWriter.
 
@@ -193,11 +193,11 @@ Notice how this transformation recasts the nested fields into a table, which ena
 
 :::image type="content" source="media/search-synapseml-cognitive-services/form-ontology-learner-output.png" alt-text="Screenshot of the FormOntologyLearner output." border="true":::
 
-## Step 6: Add translations
+## Add translations
 
 Paste the following code into the fifth cell. No modifications are required, so run the code when you're ready.
 
-This code loads [Translate](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#translator-sample), a transformer that calls the Azure AI Translator service in Azure AI services. The original text, which is in English in the "Description" column, is machine-translated into various languages. All of the output is consolidated into "output.translations" array.
+This code loads [Translate](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#translator-sample), a transformer that calls the Azure AI Translator service in Azure AI services. The original text, which is in English in the "Description" column, is machine-translated into various languages. All of the output is consolidated into the "output.translations" array.
 
 ```python
 from synapse.ml.cognitive import Translate
@@ -223,11 +223,11 @@ display(translated_df)
 > 
 > :::image type="content" source="media/search-synapseml-cognitive-services/translated-strings.png" alt-text="Screenshot of table output, showing the Translations column." border="true":::
 
-## Step 7: Add a search index with AzureSearchWriter
+## Add a search index with AzureSearchWriter
 
-Paste the following code in the sixth cell and then run it. No modifications are required.
+Paste the following code in the sixth cell and run it. No modifications are required.
 
-This code loads [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample). It consumes a tabular dataset and infers a search index schema that defines one field for each column. Because the translations structure is an array, it's articulated in the index as a complex collection with subfields for each language translation. The generated index has a document key and use the default values for fields created using the [Create Index REST API](/rest/api/searchservice/indexes/create).
+This code loads [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample). It consumes a tabular dataset and infers a search index schema that defines one field for each column. Because the translations structure is an array, it's articulated in the index as a complex collection with subfields for each language translation. The generated index has a document key and uses the default values for fields created using the [Create Index REST API](/rest/api/searchservice/indexes/create).
 
 ```python
 from synapse.ml.cognitive import *
@@ -243,21 +243,21 @@ from synapse.ml.cognitive import *
     ))
 ```
 
-You can check the search service pages in Azure portal to explore the index definition created by AzureSearchWriter.
+To explore the index definition created by AzureSearchWriter, check the search service pages in the Azure portal.
 
 > [!NOTE]
-> If you can't use default search index, you can provide an external custom definition in JSON, passing its URI as a string in the "indexJson" property. Generate the default index first so that you know which fields to specify, and then follow with customized properties if you need specific analyzers, for example.
+> If you can't use the default search index, you can provide an external custom definition in JSON, passing its URI as a string in the "indexJson" property. Generate the default index first so that you know which fields to specify, and then follow with customized properties if you need specific analyzers, for example.
 
-## Step 8: Query the index
+## Query the index
 
-Paste the following code into the seventh cell and then run it. No modifications are required, except that you might want to vary the syntax or try more examples to further explore your content:
+Paste the following code into the seventh cell and run it. No modifications are required, except that you might want to vary the syntax or try more examples to further explore your content:
 
 + [Query syntax](query-simple-syntax.md)
 + [Query examples](search-query-simple-examples.md)
 
 There's no transformer or module that issues queries. This cell is a simple call to the [Search Documents REST API](/rest/api/searchservice/documents/search-post). 
 
-This particular example is searching for the word "door" (`"search": "door"`). It also returns a "count" of the number of matching documents, and selects just the contents of the "Description' and "Translations" fields for the results. If you want to see the full list of fields, remove the "select" parameter.
+This particular example is searching for the word "door" (`"search": "door"`). It also returns a "count" of the number of matching documents and selects just the contents of the "Description' and "Translations" fields for the results. If you want to see the full list of fields, remove the "select" parameter.
 
 ```python
 import requests
@@ -278,7 +278,7 @@ You can find and manage resources in the Azure portal, using the **All resources
 
 ## Next steps
 
-In this tutorial, you learned about the [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample) transformer in SynapseML, which is a new way of creating and loading search indexes in Azure AI Search. The transformer takes structured JSON as an input. The FormOntologyLearner can provide the necessary structure for output produced by the Document Intelligence transformers in SynapseML.
+In this tutorial, you learned about the [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample) transformer in SynapseML, which is a new way of creating and loading search indexes in Azure AI Search. The transformer takes structured JSON as an input. FormOntologyLearner can provide the necessary structure for output produced by the Document Intelligence transformers in SynapseML.
 
 As a next step, review the other SynapseML tutorials that produce transformed content you might want to explore through Azure AI Search:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Update to SynapseML Tutorial for Azure AI Search"
}
```

### Explanation
The document titled "Tutorial: Index at Scale (Spark)" has undergone a series of edits to enhance readability, clarity, and accuracy regarding how to index large datasets using Apache Spark in conjunction with SynapseML and Azure AI Search.

1. **Title Capitalization**: The title of the tutorial has been updated to "Tutorial: Index at Scale (Spark)" for proper capitalization in the title formatting.

2. **Date Update**: The last modified date was updated from January 30, 2025, to March 28, 2025, reflecting the latest changes made to the document.

3. **Increased Clarity in Steps**: Several parts of the tutorial were rephrased for better clarity. For example, phrases like "Analyze them to determine their features" were adjusted to "Analyze the forms to determine their features" to specify the subject clearly.

4. **Consistent Formatting for Links**: Different sections emphasized hyperlinks and terminology for consistent styling, such as changing "open source" to "open-source," ensuring a uniform presentation.

5. **Instructions Refinement**: The wording in instructions was made more straightforward and consistent, enhancing readability. For instance, phrases like “create a notebook to run the code” were revised to integrate the instruction in a more active voice.

6. **Rephrased Explanations of Code Functions**: Descriptions for the code steps were polished for better understanding. For example, the explanation regarding loading files from Azure storage was modified for clarity while emphasizing the purpose of the code.

7. **Simplification of Steps**: Specific action verbs and clearer descriptions were used to describe the steps involved in setting up the Spark cluster, including clearer directives regarding the installation of the `synapseml` library.

8. **Reorganized Information**: Sections that describe dependencies, project setup, and next steps have been streamlined for better flow and to remove redundancy, making the document feel more cohesive.

9. **Next Steps Emphasized**: The concluding section now provides an explicit indication of moving on to other related tutorials, encouraging readers to expand their learning beyond the current tutorial.

These changes enhance the tutorial's overall clarity and usability, making it easier for readers to follow along and understand the process of indexing large datasets using SynapseML and Azure AI Search effectively.

## articles/search/semantic-how-to-configure.md{#item-7a92a6}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 04/04/2025
 ---
 
 # Configure semantic ranker and return captions in search results
@@ -44,6 +44,8 @@ You can specify a semantic configuration on new or existing indexes, using any o
 
 A *semantic configuration* is a section in your index that establishes field inputs for semantic ranking. You can add or update a semantic configuration at any time, no rebuild necessary. If you create multiple configurations, you can specify a default. At query time, specify a semantic configuration on a [query request](semantic-how-to-query-request.md), or leave it blank to use the default.
 
+You can create up to 100 semantic configurations in a single index.
+
 A semantic configuration has a name and the following properties:
 
 | Property | Characteristics |
@@ -158,6 +160,60 @@ SearchIndex searchIndex = new(indexName)
 
 ---
 
+## Opt in for prerelease semantic ranking models
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST APIs](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and in Azure SDKs that provide the property, you can optionally configure an index to use prerelease semantic ranking models if one is deployed in your region. There's no mechanism for knowing if a prerelease is available, or if it was used on specific query. For this reason, we recommend that you use this property in test environments, and only if you're interested in trying out the very latest semantic ranking models.
+
+The configuration property is `"flightingOptIn": true`, and it's set in the semantic configuration section of an index. The property is null or false by default. You can set it true on a create or update request at any time, and it affects semantic queries moving forward, assuming the query stipulates a semantic configuration that includes the property.
+
+```rest
+PUT https://myservice.search.windows.net/indexes('hotels')?allowIndexDowntime=False&api-version=2025-03-01-preview
+
+{
+  "name": "hotels",
+  "fields": [ ],
+  "scoringProfiles": [ ],
+  "defaultScoringProfile": "geo",
+  "suggesters": [ ],
+  "analyzers": [ ],
+  "corsOptions": { },
+  "encryptionKey": { },
+  "similarity": { },
+  "semantic": {
+    "configurations": [
+      {
+        "name": "semanticHotels",
+        "prioritizedFields": {
+          "titleField": {
+            "fieldName": "hotelName"
+          },
+        "prioritizedContentFields": [
+            {
+              "fieldName": "description"
+            },
+            {
+              "fieldName": "description_fr"
+            }
+          ],
+        "prioritizedKeywordsFields": [
+            {
+              "fieldName": "tags"
+            },
+            {
+              "fieldName": "category"
+            }
+          ],
+        "flightingOptIn": true
+        }
+      }
+    ]
+  },
+  "vectorSearch": {  }
+}
+```
+
 ## Next steps
 
 Test your semantic configuration by running a semantic query.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to Semantic Configuration Instructions"
}
```

### Explanation
The document titled "Configure semantic ranker and return captions in search results" has been updated to provide clearer and more comprehensive information regarding semantic configurations for Azure AI Search.

1. **Date Revision**: The document's last modified date was updated from December 10, 2024, to April 4, 2025, indicating the latest changes made.

2. **Additional Configuration Information**: A new point was added stating that users can create up to 100 semantic configurations within a single index. This addition enhances user awareness regarding the limits of configurations available for use.

3. **Opt-In for Prerelease Semantic Ranking Models**: A new section has been included to explain the process for opting into prerelease semantic ranking models. This section provides important context for users who may want to experiment with the latest features in a test environment.

4. **Feature Preview Note**: The new material also references a feature preview inclusion, guiding users toward understanding that the functionality may change and is in a testing phase.

5. **Detailed Configuration Property Usage**: The documentation now includes explicit detail about the property `"flightingOptIn": true`, outlining how users can enable prerelease models by modifying their semantic configuration. This section mentions that the property is null or false by default, promoting a better understanding of behavior upon requesting updates.

6. **REST API Example**: An example of a REST API call is provided to demonstrate how to set the configuration properly in practice. This addition improves clarity and provides users with a hands-on approach to implementing the described changes.

7. **Final Instructions for Next Steps**: The concluding section encourages users to test their semantic configurations by running queries, reinforcing the practical application of the information provided.

Overall, these changes present a thorough enhancement to the documentation, making it easier for users to configure semantic rankings effectively and utilize the latest features within Azure AI Search.

## articles/search/semantic-how-to-enable-disable.md{#item-71ac1e}

<details>
<summary>Diff</summary>
````diff
@@ -10,12 +10,12 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 01/28/2025
+ms.date: 03/31/2025
 ---
 
 # Enable or disable semantic ranker
 
-Semantic ranker is a premium feature billed by usage. By default, semantic ranker is turned on when you create a new billable search service, but anyone with *Contributor* permissions can disable it or change the billing plan. If you don't want anyone to use the feature, you can [disable it service-wide using the management REST API](#disable-semantic-ranker-using-the-rest-api).
+Semantic ranker is a premium feature billed by usage. By default, semantic ranker is enabled on a new billable search service and it's configured for the free plan, but anyone with *Contributor* permissions can disable it or change the billing plan. If you don't want anyone to use the feature, you can [disable it service-wide using the management REST API](#disable-semantic-ranker-using-the-rest-api).
 
 ## Check availability
 
@@ -45,7 +45,7 @@ To enable semantic ranker using the REST API, you can use the [Create or Update
 
 Management REST API calls are authenticated through Microsoft Entra ID. For instructions on how to authenticate, see [Manage your Azure AI Search service with REST APIs](search-manage-rest.md).
 
-* Management REST API version 2023-11-01 provides the configuration property.
+* Management REST API version 2023-11-01 or later provides the configuration property.
 
 * *Owner* or *Contributor* permissions are required to enable or disable features. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Semantic Ranker Enable/Disable Guide"
}
```

### Explanation
The document titled "Enable or disable semantic ranker" has been revised to clarify instructions and update specific details regarding the semantic ranker feature in Azure AI Search.

1. **Date Revision**: The last modified date has been updated from January 28, 2025, to March 31, 2025, reflecting the latest changes made to the document.

2. **Clarification on Default Settings**: The explanation regarding the semantic ranker's default settings has been refined. It now states that the semantic ranker is enabled by default on new billable search services and that it is configured for the free plan. This clarification helps users understand the initial state of the feature more clearly.

3. **Revised API Version Information**: The document now mentions that Management REST API version 2023-11-01 or later provides the configuration property. This change ensures that users are aware that only certain versions contain the necessary features, thereby preventing potential compatibility issues.

4. **Consistency in Terminology**: Minor wording adjustments were made for consistency, such as changing "turned on" to "enabled," promoting a uniform terminology throughout the documentation.

Overall, these changes enhance the clarity and accuracy of the instructions related to enabling and disabling the semantic ranker feature, providing better guidance for users navigating the Azure AI Search service configurations.

## articles/search/semantic-how-to-query-request.md{#item-85530d}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - ignite-2023
   - ignite-2024
 ms.topic: how-to
-ms.date: 02/18/2025
+ms.date: 03/31/2025
 ---
 
 # Add semantic ranking to queries in Azure AI Search
@@ -46,7 +46,7 @@ You can use any of the following tools and SDKs to build a query that uses seman
 
 A few query capabilities bypass relevance scoring, which makes them incompatible with semantic ranking. If your query logic includes the following features, you can't semantically rank your results:
 
-+ A query with `search=*` or an empty search string, such as pure filter-only query, won't work because there's nothing to measure semantic relevance against and so the search scores are zero. The query must provide terms or phrases that can be evaluated during processing.
++ A query with `search=*` or an empty search string, such as pure filter-only query, won't work because there's nothing to measure semantic relevance against and so the search scores are zero. The query must provide terms or phrases that can be evaluated during processing, and that produces search documents that are scored for relevance. Scored results are inputs to the semantic ranker.
 
 + Sorting (orderBy clauses) on specific fields overrides search scores and a semantic score. Given that the semantic score is supposed to provide the ranking, adding an orderby clause results in an HTTP 400 error if you apply semantic ranking over ordered results.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarifications in Query Request for Semantic Ranking"
}
```

### Explanation
The document titled "Add semantic ranking to queries in Azure AI Search" has been updated to incorporate clearer explanations regarding query compatibility with semantic ranking.

1. **Date Revision**: The last modified date has been changed from February 18, 2025, to March 31, 2025, indicating the most recent adjustments made to the content.

2. **Enhanced Explanation of Query Requirements**: The description analyzing queries with `search=*` or an empty search string has been elaborated. The update emphasizes that these queries will not work for semantic ranking because there are no terms or phrases to measure relevance. Additionally, it now clarifies that the results must be scored for relevance, which are necessary inputs for the semantic ranker. This improvement provides users with a better understanding of how to construct valid queries.

3. **Introduction of Sorting Considerations**: A new note has been added explaining that sorting (using `orderBy` clauses) on specific fields can override search scores and the semantic score. This crucial information alerts users to the potential HTTP 400 error that may occur if they attempt to apply semantic ranking to ordered results, ensuring they are aware of constraints when integrating these features.

Overall, these changes enhance the clarity and usability of the instructions for leveraging semantic ranking within queries in Azure AI Search, helping users better navigate query construction and compatibility.

## articles/search/semantic-how-to-query-rewrite.md{#item-3e168f}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.custom:
   - ignite-2024
   - references_regions
 ms.topic: how-to
-ms.date: 11/19/2024
+ms.date: 03/31/2025
 ---
 
 # Rewrite queries with semantic ranker in Azure AI Search (Preview)
@@ -21,7 +21,8 @@ Query rewriting is the process of transforming a user's query into a more effect
 
 Query rewriting improves results from [semantic ranking](search-get-started-semantic.md) by correcting typos or spelling errors in user queries, and expanding queries with synonyms.
 
-Search with query rewriting works like this: 
+Search with query rewriting works like this:
+
 - The user query is sent via the `search` property in the request.
 - The search service sends the search query (or a variation of it) to a generative model that generates alternative queries.
 - The search service uses the original query and the rewritten queries to retrieve search results.
@@ -33,31 +34,25 @@ Query rewriting is an optional feature. Without query rewriting, the search serv
 
 ## Prerequisites
 
-+ A search service, Basic tier or higher.
-
-> [!NOTE]
-> Query rewriting is currently available in the North Europe, and Southeast Asia regions.
-
-+ Your search service must have [semantic ranker enabled](semantic-how-to-enable-disable.md). Review [semantic ranking](semantic-search-overview.md) if you need an introduction to the feature. 
+- A search service, Basic tier or higher, in **North Europe** or **Southeast Asia**.
 
-> [!IMPORTANT]
-> Semantic ranker is currently required for query rewriting.
+- [Semantic ranker must be enabled](semantic-how-to-enable-disable.md). It's enabled by default on newer search services. Review [semantic ranking](semantic-search-overview.md) if you need an introduction to the feature. 
 
-+ An existing search index with a [semantic configuration](semantic-how-to-configure.md) and rich text content. The examples in this guide use the [hotels-sample-index](search-get-started-portal.md) sample data to demonstrate query rewriting. You can use your own data and index to test query rewriting.
+- An existing search index with a [semantic configuration](semantic-how-to-configure.md) and rich text content. The examples in this guide use the [hotels-sample-index](search-get-started-portal.md) sample data to demonstrate query rewriting. You can use your own data and index to test query rewriting.
 
-+ You need a web client that supports REST API requests. The examples in this guide were tested with [Visual Studio Code](https://code.visualstudio.com/download) with the [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) extension. 
+- To follow the instructions in this article, you need a web client that supports REST API requests. The examples in this guide were tested with [Visual Studio Code](https://code.visualstudio.com/download) with the [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) extension. 
 
 > [!TIP]
 > Content that includes explanations or definitions work best for semantic ranking. 
 
 ## Make a search request with query rewrites
 
-In this REST API example, we use [Search Documents](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&branch=searchindex202411&preserve-view=true) to formulate the request. For more information about the request and response properties, see the [API reference documentation](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&branch=searchindex202411&preserve-view=true).
+In this REST API example, use [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2025-03-01-preview&branch=searchindex202503&preserve-view=true) to formulate the request.
 
 1. Paste the following request into a web client as a template. 
 
     ```http
-    POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2024-11-01-preview
+    POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2025-03-01-preview
     {
         "search": "newer hotel near the water with a great restaurant",
         "semanticConfiguration":"en-semantic-config",
@@ -69,24 +64,24 @@ In this REST API example, we use [Search Documents](/rest/api/searchservice/docu
     }
     ```
 
-    - You replace `search-service-name` with your search service name.
-    - You replace `hotels-sample-index` with your index name if it's different. 
-    - We set "search" to a full text search query. The search property is required for query rewriting, unless you specify [vector queries](#vector-queries-with-query-rewrite). If you specify vector queries, then the "search" text must match the `"text"` property of the `"vectorQueries"` object. Your search string can support either the [simple syntax](query-simple-syntax.md) or [full Lucene syntax](query-lucene-syntax.md).
-    - We set "semanticConfiguration" to a [predefined semantic configuration](semantic-how-to-configure.md) embedded in your index.
-    - We set "queryType" to "semantic". We either need to set "queryType" to "semantic" or include a nonempty "semanticQuery" property in the request. [Semantic ranking](semantic-search-overview.md) is required for query rewriting.
-    - We set "queryRewrites" to "generative|count-5" to get up to five query rewrites. You can set the count to any value between 1 and 10. 
-    - Since we requested query rewrites by setting the "queryRewrites" property, we must set "queryLanguage" to the search text language. The Search service uses the same language for the query rewrites. In this example, we use "en-US". The supported locales are: 
+    - Replace `search-service-name` with your search service name.
+    - Replace `hotels-sample-index` with your index name if it's different. 
+    - Set "search" to a full text search query. The search property is required for query rewriting, unless you specify [vector queries](#vector-queries-with-query-rewrite). If you specify vector queries, then the "search" text must match the `"text"` property of the `"vectorQueries"` object. Your search string can support either the [simple syntax](query-simple-syntax.md) or [full Lucene syntax](query-lucene-syntax.md).
+    - Set "semanticConfiguration" to a [predefined semantic configuration](semantic-how-to-configure.md) embedded in your index.
+    - Set "queryType" to "semantic". You either need to set "queryType" to "semantic" or include a nonempty "semanticQuery" property in the request. [Semantic ranking](semantic-search-overview.md) is required for query rewriting.
+    - Set "queryRewrites" to "generative|count-5" to get up to five query rewrites. You can set the count to any value between 1 and 10. 
+    - Since you requested query rewrites by setting the "queryRewrites" property, you must set "queryLanguage" to the search text language. The search service uses the same language for the query rewrites. In this example, you use "en-US". The supported locales are: 
         `en-AU`, `en-CA`, `en-GB`, `en-IN`, `en-US`, `ar-EG`, `ar-JO`, `ar-KW`, `ar-MA`, `ar-SA`, `bg-BG`, `bn-IN`, `ca-ES`, `cs-CZ`, `da-DK`, `de-DE`, `el-GR`, `es-ES`, `es-MX`, `et-EE`, `eu-ES`, `fa-AE`, `fi-FI`, `fr-CA`, `fr-FR`, `ga-IE`, `gl-ES`, `gu-IN`, `he-IL`, `hi-IN`, `hr-BA`, `hr-HR`, `hu-HU`, `hy-AM`, `id-ID`, `is-IS`, `it-IT`, `ja-JP`, `kn-IN`, `ko-KR`, `lt-LT`, `lv-LV`, `ml-IN`, `mr-IN`, `ms-BN`, `ms-MY`, `nb-NO`, `nl-BE`, `nl-NL`, `no-NO`, `pa-IN`, `pl-PL`, `pt-BR`, `pt-PT`, `ro-RO`, `ru-RU`, `sk-SK`, `sl-SL`, `sr-BA`, `sr-ME`, `sr-RS`, `sv-SE`, `ta-IN`, `te-IN`, `th-TH`, `tr-TR`, `uk-UA`, `ur-PK`, `vi-VN`, `zh-CN`, `zh-TW`.
-    - We set "debug" to "queryRewrites" to get the query rewrites in the response. 
+    - Set "debug" to "queryRewrites" to get the query rewrites in the response. 
   
       > [!TIP]
       > Only set `"debug": "queryRewrites"` for testing purposes. For better performance, don't use debug in production.
 
-    - We set "top" to 1 to return only the top search result. 
-    
+    - Set "top" to 1 to return only the top search result. 
+  
 1. Send the request to execute the query and return results.
 
-Next, we evaluate the search results with the query rewrites.
+Next, you evaluate the search results with the query rewrites.
 
 ## Evaluate the response
 
@@ -145,8 +140,9 @@ Here's an example of a response that includes query rewrites:
 ```
 
 Here are some key points to note:
-- Because we set the "debug" property to "queryRewrites" for testing, the response includes a `@search.debug` object with the text input query and query rewrites. 
-- Because we set the "queryRewrites" property to "generative|count-5", the response includes up to five query rewrites.
+
+- Because you set the "debug" property to "queryRewrites" for testing, the response includes a `@search.debug` object with the text input query and query rewrites. 
+- Because you set the "queryRewrites" property to "generative|count-5", the response includes up to five query rewrites.
 - The `"inputQuery"` value is the query sent to the generative model for query rewriting. The input query isn't always the same as the user's `"search"` query.
 
 Here's an example of a response without query rewrites. 
@@ -201,13 +197,13 @@ Here's an example of a response without query rewrites.
 
 You can include vector queries in your search request to combine keyword search and vector search into a single request and a unified response.
 
-Here's an example of a query that includes a vector query with query rewrites. We modify a [previous example](#make-a-search-request-with-query-rewrites) to include a vector query.
+Here's an example of a query that includes a vector query with query rewrites. Modify a [previous example](#make-a-search-request-with-query-rewrites) to include a vector query.
 
-- We add a "vectorQueries" object to the request. This object includes a vector query with the "kind" set to "text". 
+- Add a "vectorQueries" object to the request. This object includes a vector query with the "kind" set to "text". 
 - The "text" value is the same as the "search" value. These values must be identical for query rewriting to work.
 
 ```http
-POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2024-11-01-preview
+POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2025-03-01-preview
 {
     "search": "newer hotel near the water with a great restaurant",
     "vectorQueries": [
@@ -256,6 +252,7 @@ You might observe that the debug (test) response includes an empty array for the
 ```
 
 In the preceding example:
+
 - The response includes a `@search.semanticPartialResponseReason` property with a value of "Transient". This message means that at least one of the queries failed to complete. 
 - The response also includes a `@search.semanticQueryRewriteResultType` property with a value of "OriginalQueryOnly". This message means that the query rewrites are unavailable. Only the original query is used to retrieve search results.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements in Query Rewrite Process Documentation"
}
```

### Explanation
The document titled "Rewrite queries with semantic ranker in Azure AI Search (Preview)" has been significantly modified to improve clarity and detail in the explanation of the query rewriting process.

1. **Date Revision**: The last modified date has been changed from November 19, 2024, to March 31, 2025, reflecting the most current changes.

2. **Expanded Description of Query Rewriting**: The section explaining how the query rewriting process works has been expanded to include a more comprehensive overview. New steps detail the submission of user queries, how the search service interacts with generative models, and the retrieval process for search results. These additions provide users with a clearer idea of the underlying mechanics of query rewriting.

3. **Clarified Prerequisites**: The prerequisites for implementing query rewriting have been simplified and categorized more clearly. It now specifies that a search service must be at least at the Basic tier, available specifically in the North Europe and Southeast Asia regions. Also, it emphasizes that enabling the semantic ranker is essential and mentions that it is enabled by default on newer search services.

4. **Revised API Version Information**: The API version references have been updated from 2024-11-01 to 2025-03-01, ensuring users refer to the most recent and relevant API details.

5. **Streamlined Instruction Language**: The wording throughout the document has been standardized for clarity and readability. Instructions such as replacing specific placeholders within API calls are presented in a more straightforward manner, making it easier for users to follow along.

6. **Updated Response Evaluation**: The section for evaluating responses has been refined, clarifying how to interpret debug information when testing queries with query rewrites. 

Overall, these changes enhance the overall quality of the documentation, making it more user-friendly and informative while ensuring users have access to the latest practices and requirements for utilizing query rewriting effectively within Azure AI Search.

## articles/search/semantic-search-overview.md{#item-b7497b}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: concept-article
-ms.date: 02/18/2025
+ms.date: 03/31/2025
 ---
 
 # Semantic ranking in Azure AI Search
@@ -24,7 +24,7 @@ Semantic ranker is a premium feature, billed by usage. We recommend this article
 
 ## What is semantic ranking?
 
-Semantic ranker calls LLMs at query time. LLms are used to improve the quality of an initial [BM25-ranked](index-similarity-and-scoring.md) or [RRF-ranked](hybrid-search-ranking.md) search result for text-based queries, the text portion of vector queries, and hybrid queries. When you enable it on your search service, semantic ranking extends the query execution pipeline in three ways:
+Semantic ranker calls LLMs at query time. LLms are used to improve the quality of an initial [BM25-ranked](index-similarity-and-scoring.md) or [RRF-ranked](hybrid-search-ranking.md) search result for text-based queries, the text portion of vector queries, and hybrid queries. Semantic ranking extends the query execution pipeline in three ways:
 
 * First, it always adds secondary ranking over an initial result set that was scored using BM25 or Reciprocal Rank Fusion (RRF). This secondary ranking uses multi-lingual, deep learning models adapted from Microsoft Bing to promote the most semantically relevant results.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Semantic Search Overview Documentation"
}
```

### Explanation
The document titled "Semantic Search Overview" has undergone minor updates to improve clarity and accuracy in the explanation of semantic ranking in Azure AI Search.

1. **Date Revision**: The last modified date has been updated from February 18, 2025, to March 31, 2025, reflecting the latest modifications to the article.

2. **Enhanced Definition of Semantic Ranking**: The section defining what semantic ranking is has been slightly refined for conciseness. The mention of how the semantic ranker utilizes large language models (LLMs) at query time remains, reinforcing its role in enhancing the quality of initial search results. The refinement clarifies the text's flow without altering the original meaning.

3. **Clarified Execution Pipeline Explanation**: The explanation regarding how semantic ranking extends the query execution pipeline is now presented more clearly, maintaining the key points without redundant language. This enhances the readability for users trying to understand how the semantic ranking process works in conjunction with traditional ranking methods like BM25 and RRF.

Overall, these adjustments serve to streamline the content, ensuring it remains accessible and informative for users seeking to understand the workings of semantic ranking within Azure AI Search.

## articles/search/toc.yml{#item-c4768f}

<details>
<summary>Diff</summary>
````diff
@@ -201,6 +201,8 @@ items:
       href: search-region-support.md
     - name: Choose a tier
       href: search-sku-tier.md
+    - name: Upgrade a service
+      href: search-how-to-upgrade.md
     - name: Service limits
       href: search-limits-quotas-capacity.md
     - name: Plan and manage capacity
@@ -215,11 +217,11 @@ items:
       href: search-modeling-multitenant-saas-applications.md
     - name: Manage
       items:
-      - name: Manage with PowerShell
+      - name: Manage using PowerShell
         href: search-manage-powershell.md
-      - name: Manage with Azure CLI
+      - name: Manage using Azure CLI
         href: search-manage-azure-cli.md
-      - name: Manage with REST
+      - name: Manage using REST
         href: search-manage-rest.md
       - name: Move service across regions
         href: search-howto-move-across-regions.md
@@ -399,19 +401,21 @@ items:
         href: index-add-language-analyzers.md
       - name: Add a custom analyzer
         href: index-add-custom-analyzers.md
-    - name: Filters
+    - name: Filters and facets
       items:
       - name: Filters in text queries
         displayName: query
         href: search-filters.md
-      - name: Add faceted navigation
-        href: search-faceted-navigation.md
       - name: Understand collection filters
         href: search-query-understand-collection-filters.md
       - name: Troubleshoot collection filters
         href: search-query-troubleshoot-collection-filters.md
       - name: Normalize text for filters
         href: search-normalizers.md
+      - name: Add faceted navigation
+        href: search-faceted-navigation.md
+      - name: Faceted navigation examples
+        href: search-faceted-navigation-examples.md
     - name: Search results
       items:
       - name: Page, sort, and shape results
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Table of Contents for Search Documentation"
}
```

### Explanation
The "toc.yml" file, which serves as the table of contents for the Azure AI Search documentation, has been updated to improve organization and add additional entries.

1. **New Item Addition**: A new entry titled "Upgrade a service" has been added to the table of contents, linking to the document "search-how-to-upgrade.md." This addition provides users with guidance on service upgrades, enhancing the overall utility of the documentation.

2. **Terminology Standardization**: The phrasing for entries regarding management has been standardized from "Manage with" to "Manage using" for PowerShell, Azure CLI, and REST. This change promotes a consistent terminology throughout the documentation, improving clarity for users.

3. **Expanded Filters Section**: The entry for "Filters" has been renamed to "Filters and facets," indicating a broader scope of information being covered. Additionally, a new item titled "Faceted navigation examples" has been introduced, providing users with practical examples related to faceted navigation.

4. **Organization Enhancement**: Minor structural adjustments were made to improve the flow of the table of contents and ensure that related topics are more logically grouped together, facilitating easier navigation for users browsing through the documentation.

Overall, these updates to the table of contents enhance the accessibility and comprehensiveness of the Azure AI Search documentation, ensuring users can find relevant topics more efficiently.

## articles/search/tutorial-create-custom-analyzer.md{#item-ad5520}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: create a custom analyzer'
+title: 'Tutorial: Create a Custom Analyzer'
 titleSuffix: Azure AI Search
 description: Learn how to build a custom analyzer to improve the quality of search results in Azure AI Search.
 author: gmndrg
@@ -8,14 +8,14 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 01/17/2025
+ms.date: 03/28/2025
 ---
 
 # Tutorial: Create a custom analyzer for phone numbers
 
-In search solutions, strings that have complex patterns or special characters can be a challenge to work with because the [default analyzer](search-analyzers.md) strips out or misinterprets meaningful parts of a pattern, resulting in a poor search experience when users can't find the information they expected. Phone numbers are a classic example of strings that are hard to analyze. They come in various formats, and they include special characters that the default analyzer ignores. 
+In search solutions, strings that have complex patterns or special characters can be challenging to work with because the [default analyzer](search-analyzers.md) strips out or misinterprets meaningful parts of a pattern. This results in a poor search experience where users can't find the information they expect. Phone numbers are a classic example of strings that are difficult to analyze. They come in various formats and include special characters that the default analyzer ignores.
 
-With phone numbers as its subject, this tutorial takes a close look at the problems of patterned data, and shows you to solve that problem using a [custom analyzer](index-add-custom-analyzers.md). The approach outlined here can be used as-is for phone numbers, or adapted for fields having the same characteristics (patterned, with special characters), such as URLs, emails, postal codes, and dates.
+With phone numbers as its subject, this tutorial shows you how to solve patterned data problems using a [custom analyzer](index-add-custom-analyzers.md). This approach can be used as is for phone numbers or adapted for fields with the same characteristics (patterned with special characters), such as URLs, emails, postal codes, and dates.
 
 In this tutorial, you use a REST client and the [Azure AI Search REST APIs](/rest/api/searchservice/) to:
 
@@ -27,33 +27,33 @@ In this tutorial, you use a REST client and the [Azure AI Search REST APIs](/res
 
 ## Prerequisites
 
-The following services and tools are required for this tutorial.
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
++ [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. For this tutorial, you can use a free service.
 
-+ [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing Azure AI Search resource](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription. You can use a free service for this quickstart. 
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
 ### Download files
 
-Source code for this tutorial is the [custom-analyzer.rest](https://github.com/Azure-Samples/azure-search-rest-samples/tree/main/custom-analyzers/custom-analyzer.rest) file in the [Azure-Samples/azure-search-rest-samples](https://github.com/Azure-Samples/azure-search-rest-samples) GitHub repository.
+Source code for this tutorial is in the [custom-analyzer.rest](https://github.com/Azure-Samples/azure-search-rest-samples/tree/main/custom-analyzers/custom-analyzer.rest) file in the [Azure-Samples/azure-search-rest-samples](https://github.com/Azure-Samples/azure-search-rest-samples) GitHub repository.
 
-### Copy a key and URL
+### Copy an admin key and URL
 
 The REST calls in this tutorial require a search service endpoint and an admin API key. You can get these values from the Azure portal.
 
-1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com), go to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
 1. Under **Settings** > **Keys**, copy an admin key. Admin keys are used to add, modify, and delete objects. There are two interchangeable admin keys. Copy either one.
 
    :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of the URL and API keys in the Azure portal.":::
 
-A valid API key establishes trust, on a per request basis, between the application sending the request and the search service handling it.
+A valid API key establishes trust, on a per-request basis, between the application sending the request and the search service handling it.
 
 ## Create an initial index
 
 1. Open a new text file in Visual Studio Code.
 
-1. Set variables to the search endpoint and the API key you collected in the previous step.
+1. Set variables to the search endpoint and the API key you collected in the previous section.
 
    ```http
    @baseUrl = PUT-YOUR-SEARCH-SERVICE-URL-HERE
@@ -62,7 +62,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
 
 1. Save the file with a `.rest` file extension.
 
-1. Paste in the following example to create a small index called `phone-numbers-index` with two fields: `id` and `phone_number`. We haven't defined an analyzer yet, so the `standard.lucene` analyzer is used by default.
+1. Paste the following example to create a small index called `phone-numbers-index` with two fields: `id` and `phone_number`. You haven't defined an analyzer yet, so the `standard.lucene` analyzer is used by default.
 
     ```http
     ### Create a new index
@@ -94,7 +94,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       }
     ```
 
-1. Select **Send request**. You should have an `HTTP/1.1 201 Created` response and the response body should include the JSON representation of the index schema.
+1. Select **Send request**. You should have an `HTTP/1.1 201 Created` response, and the response body should include the JSON representation of the index schema.
 
 1. Load data into the index, using documents that contain various phone number formats. This is your test data.
 
@@ -150,7 +150,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       }
     ```
 
-1. Let's try a few queries similar to what a user might type. A user could search for `(425) 555-0100` in any number of formats and still expect results to be returned. Start by searching `(425) 555-0100`:
+1. Try queries similar to what a user might type. For example, a user might search for `(425) 555-0100` in any number of formats and still expect results to be returned. Start by searching `(425) 555-0100`.
 
     ```http  
     ### Search for a phone number
@@ -159,7 +159,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       api-key: {{apiKey}}
     ```
 
-    The query returns **three out of four expected results**, but also returns **two unexpected results**:
+    The query returns three out of four expected results but also returns two unexpected results.
 
     ```json
     {
@@ -188,7 +188,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
     }
     ```
 
-1. Let's try again without any formatting: `4255550100`.
+1. Try again without any formatting: `4255550100`.
 
    ```http  
     ### Search for a phone number
@@ -197,7 +197,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       api-key: {{apiKey}}
     ```
 
-   This query does even worse, returning only **one of four correct matches**.
+   This query does even worse, returning only one of four correct matches.
 
     ```json
     {
@@ -210,48 +210,48 @@ A valid API key establishes trust, on a per request basis, between the applicati
     }
     ```
 
-If you find these results confusing, you're not alone. In the next section, let's dig into why we're getting these results.
+If you find these results confusing, you're not alone. The next section explains why you're getting these results.
 
 <a name="how-analyzers-work"></a>
 
 ## Review how analyzers work
 
-To understand these search results, we need to understand what the analyzer is doing. From there, we can test the default analyzer using the [Analyze API](/rest/api/searchservice/indexes/analyze), providing a foundation for designing an analyzer that better meets our needs.
+To understand these search results, you must understand what the analyzer is doing. From there, you can test the default analyzer using the [Analyze API](/rest/api/searchservice/indexes/analyze), providing a foundation for designing an analyzer that better meets your needs.
 
-An [analyzer](search-analyzers.md) is a component of the [full text search engine](search-lucene-query-architecture.md) responsible for processing text in query strings and indexed documents. Different analyzers manipulate text in different ways depending on the scenario. For this scenario, we need to build an analyzer tailored to phone numbers.
+An [analyzer](search-analyzers.md) is a component of the [full-text search engine](search-lucene-query-architecture.md) responsible for processing text in query strings and indexed documents. Different analyzers manipulate text in different ways depending on the scenario. For this scenario, we need to build an analyzer tailored to phone numbers.
 
 Analyzers consist of three components:
 
 + [**Character filters**](#CharFilters) that remove or replace individual characters from the input text.
-+ A [**Tokenizer**](#Tokenizers) that breaks the input text into tokens, which become keys in the search index.
++ A [**tokenizer**](#Tokenizers) that breaks the input text into tokens, which become keys in the search index.
 + [**Token filters**](#TokenFilters) that manipulate the tokens produced by the tokenizer.
 
-In the following diagram, you can see how these three components work together to tokenize a sentence:
+The following diagram shows how these three components work together to tokenize a sentence.
 
   :::image type="content" source="media/tutorial-create-custom-analyzer/analyzers-explained.png" alt-text="Diagram of Analyzer process to tokenize a sentence":::
 
-These tokens are then stored in an inverted index, which allows for fast, full-text searches.  An inverted index enables full-text search by mapping all unique terms extracted during lexical analysis to the documents in which they occur. You can see an example in the next diagram:
+These tokens are then stored in an inverted index, which allows for fast, full-text searches. An inverted index enables full-text search by mapping all unique terms extracted during lexical analysis to the documents in which they occur. You can see an example in the following diagram:
 
   :::image type="content" source="media/tutorial-create-custom-analyzer/inverted-index-explained.png" alt-text="Example inverted index":::
 
 All of search comes down to searching for the terms stored in the inverted index. When a user issues a query:
 
 1. The query is parsed and the query terms are analyzed.
-1. The inverted index is then scanned for documents with matching terms.
-1. Finally, the retrieved documents are ranked by the [scoring algorithm](index-ranking-similarity.md).
+1. The inverted index is scanned for documents with matching terms.
+1. The [scoring algorithm](index-ranking-similarity.md) ranks the retrieved documents.
 
   :::image type="content" source="media/tutorial-create-custom-analyzer/query-architecture-explained.png" alt-text="Diagram of Analyzer process ranking similarity":::
 
-If the query terms don't match the terms in your inverted index, results aren't returned. To learn more about how queries work, see this article on [full text search](search-lucene-query-architecture.md).
+If the query terms don't match the terms in your inverted index, results aren't returned. To learn more about how queries work, see [Full-text search in Azure AI Search](search-lucene-query-architecture.md).
 
 > [!Note]
-> [Partial term queries](search-query-partial-matching.md) are an important exception to this rule. These queries (prefix query, wildcard query, regex query) bypass the lexical analysis process unlike regular term queries. Partial terms are only lowercased before being matched against terms in the index. If an analyzer isn't configured to support these types of queries, you'll often receive unexpected results because matching terms don't exist in the index.
+> [Partial term queries](search-query-partial-matching.md) are an important exception to this rule. Unlike regular term queries, these queries (prefix query, wildcard query, and regex query) bypass the lexical analysis process. Partial terms are only lowercased before being matched against terms in the index. If an analyzer isn't configured to support these types of queries, you often receive unexpected results because matching terms don't exist in the index.
 
 ## Test analyzers using the Analyze API
 
 Azure AI Search provides an [Analyze API](/rest/api/searchservice/indexes/analyze) that allows you to test analyzers to understand how they process text.
 
-The Analyze API is called using the following request:
+Call the Analyze API using the following request:
 
 ```http
 POST {{baseUrl}}/indexes/phone-numbers-index/analyze?api-version=2024-07-01  HTTP/1.1
@@ -264,7 +264,7 @@ POST {{baseUrl}}/indexes/phone-numbers-index/analyze?api-version=2024-07-01  HTT
   }
 ```
 
-The API returns the tokens extracted from the text, using the analyzer you specified. The standard Lucene analyzer splits the phone number into three separate tokens:
+The API returns the tokens extracted from the text, using the analyzer you specified. The standard Lucene analyzer splits the phone number into three separate tokens.
 
 ```json
 {
@@ -315,23 +315,23 @@ Response:
 }
 ```
 
-Keep in mind that both query terms and the indexed documents undergo analysis. Thinking back to the search results from the previous step, we can start to see why those results were returned.
+Keep in mind that both query terms and the indexed documents undergo analysis. Thinking back to the search results from the previous step, you can start to see why those results are returned.
 
-In the first query, unexpected phone numbers were returned because one of their tokens, `555`, matched one of the terms we searched. In the second query, only the one number was returned because it was the only record that had a token matching `4255550100`.
+In the first query, unexpected phone numbers are returned because one of their tokens, `555`, matched one of the terms you searched. In the second query, only the one number is returned because it's the only record that has a token matching `4255550100`.
 
 ## Build a custom analyzer
 
-Now that we understand the results we're seeing, let's build a custom analyzer to improve the tokenization logic.
+Now that you understand the results you're seeing, build a custom analyzer to improve the tokenization logic.
 
-The goal is to provide intuitive search against phone numbers no matter what format the query or indexed string is in. To achieve this outcome, we'll specify a [character filter](#CharFilters), a [tokenizer](#Tokenizers), and a [token filter](#TokenFilters).
+The goal is to provide intuitive search against phone numbers no matter what format the query or indexed string is in. To achieve this outcome, specify a [character filter](#CharFilters), a [tokenizer](#Tokenizers), and a [token filter](#TokenFilters).
 
 <a name="CharFilters"></a>
 
 ### Character filters
 
-Character filters are used to process text before it's fed into the tokenizer. Common uses of character filters include filtering out HTML elements or replacing special characters.
+Character filters process text before it's fed into the tokenizer. Common uses of character filters are filtering out HTML elements and replacing special characters.
 
-For phone numbers, we want to remove whitespace and special characters because not all phone number formats contain the same special characters and spaces.
+For phone numbers, you want to remove whitespace and special characters because not all phone number formats contain the same special characters and spaces.
 
 ```json
 "charFilters": [
@@ -363,9 +363,9 @@ The filter removes `-` `(` `)` `+` `.` and spaces from the input.
 
 Tokenizers split text into tokens and discard some characters, such as punctuation, along the way. In many cases, the goal of tokenization is to split a sentence into individual words.
 
-For this scenario, we'll use a keyword tokenizer, `keyword_v2`, because we want to capture the phone number as a single term. Note that this isn't the only way to solve this problem. See the [Alternate approaches](#Alternate) section below.
+For this scenario, use a keyword tokenizer, `keyword_v2`, to capture the phone number as a single term. This isn't the only way to solve this problem, as explained in the [Alternate approaches](#Alternate) section.
 
-Keyword tokenizers always output the same text it was given as a single term.
+Keyword tokenizers always output the same text they're given as a single term.
 
 |Input|Output|  
 |-|-|  
@@ -376,9 +376,9 @@ Keyword tokenizers always output the same text it was given as a single term.
 
 ### Token filters
 
-Token filters will filter out or modify the tokens generated by the tokenizer. One common use of a token filter is to lowercase all characters using a lowercase token filter. Another common use is filtering out [stopwords](reference-stopwords.md) such as `the`, `and`, or `is`.
+Token filters modify or filter out the tokens generated by the tokenizer. One common use of a token filter is to lowercase all characters using a lowercase token filter. Another common use is filtering out [stopwords](reference-stopwords.md), such as `the`, `and`, or `is`.
 
-While we don't need to use either of those filters for this scenario, we'll use an nGram token filter to allow for partial searches of phone numbers.
+While you don't need to use either of those filters for this scenario, use an nGram token filter to allow for partial searches of phone numbers.
 
 ```json
 "tokenFilters": [
@@ -395,9 +395,9 @@ While we don't need to use either of those filters for this scenario, we'll use
 
 The [nGram_v2 token filter](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html) splits tokens into n-grams of a given size based on the `minGram` and `maxGram` parameters.
 
-For the phone analyzer, we set `minGram` to `3` because that is the shortest substring we expect users to search. `maxGram` is set to `20` to ensure that all phone numbers, even with extensions, will fit into a single n-gram.
+For the phone analyzer, `minGram` is set to `3` because that's the shortest substring users are expected to search. `maxGram` is set to `20` to ensure that all phone numbers, even with extensions, fit into a single n-gram.
 
- The unfortunate side effect of n-grams is that some false positives will be returned. We'll fix this in a later step by building out a separate analyzer for searches that doesn't include the n-gram token filter.
+The unfortunate side effect of n-grams is that some false positives are returned. You fix this in a later step by building out a separate analyzer for searches that doesn't include the n-gram token filter.
 
 |Input|Output|  
 |-|-|  
@@ -406,7 +406,7 @@ For the phone analyzer, we set `minGram` to `3` because that is the shortest sub
 
 ### Analyzer
 
-With our character filters, tokenizer, and token filters in place, we're ready to define our analyzer.
+With the character filters, tokenizer, and token filters in place, you're ready to define the analyzer.
 
 ```json
 "analyzers": [
@@ -424,26 +424,26 @@ With our character filters, tokenizer, and token filters in place, we're ready t
 ]
 ```
 
-From the Analyze API, given the following inputs, outputs from the custom analyzer are shown in the following table.
+From the Analyze API, given the following inputs, outputs from the custom analyzer are as follows:
 
 |Input|Output|  
 |-|-|  
 |`12345`|`[123, 1234, 12345, 234, 2345, 345]`|  
 |`(321) 555-0199`|`[321, 3215, 32155, 321555, 3215550, 32155501, 321555019, 3215550199, 215, 2155, 21555, 215550, ... ]`|
 
-All of the tokens in the output column exist in the index. If our query includes any of those terms, the phone number is returned.
+All of the tokens in the output column exist in the index. If your query includes any of those terms, the phone number is returned.
 
 ## Rebuild using the new analyzer
 
-1. Delete the current index:
+1. Delete the current index.
 
    ```http
     ### Delete the index
     DELETE {{baseUrl}}/indexes/phone-numbers-index?api-version=2024-07-01 HTTP/1.1
         api-key: {{apiKey}}
     ```
 
-1. Recreate the index using the new analyzer. This index schema adds a custom analyzer definition, and a custom analyzer assignment on the phone number field.
+1. Recreate the index using the new analyzer. This index schema adds a custom analyzer definition and a custom analyzer assignment on the phone number field.
 
     ```http
     ### Create a new index
@@ -513,7 +513,7 @@ All of the tokens in the output column exist in the index. If our query includes
 
 ### Test the custom analyzer
 
-After recreating the index, you can now test out the analyzer using the following request:
+After you recreate the index, test the analyzer using the following request:
 
 ```http
 POST {{baseUrl}}/indexes/tutorial-first-analyzer/analyze?api-version=2024-07-01  HTTP/1.1
@@ -526,7 +526,7 @@ POST {{baseUrl}}/indexes/tutorial-first-analyzer/analyze?api-version=2024-07-01
   }
 ```
 
-You should now see the collection of tokens resulting from the phone number:
+You should now see the collection of tokens resulting from the phone number.
 
 ```json
 {
@@ -558,9 +558,9 @@ You should now see the collection of tokens resulting from the phone number:
 
 ## Revise the custom analyzer to handle false positives
 
-After making some sample queries against the index with the custom analyzer, you'll find that recall has improved and all matching phone numbers are now returned. However, the n-gram token filter causes some false positives to be returned as well. This is a common side effect of an n-gram token filter.
+After using the custom analyzer to make sample queries against the index, you should see that recall has improved and all matching phone numbers are now returned. However, the n-gram token filter also causes some false positives to be returned. This is a common side effect of an n-gram token filter.
 
-To prevent false positives, we'll create a separate analyzer for querying. This analyzer is identical to the previous one, except that it **omits** the `custom_ngram_filter`.
+To prevent false positives, create a separate analyzer for querying. This analyzer is identical to the previous one, except that it omits the `custom_ngram_filter`.
 
 ```json
     {
@@ -574,7 +574,7 @@ To prevent false positives, we'll create a separate analyzer for querying. This
     }
 ```
 
-In the index definition, we then specify both an `indexAnalyzer` and a `searchAnalyzer`.
+In the index definition, specify both an `indexAnalyzer` and a `searchAnalyzer`.
 
 ```json
     {
@@ -589,25 +589,25 @@ In the index definition, we then specify both an `indexAnalyzer` and a `searchAn
     }
 ```
 
-With this change, you're all set. Here are your next steps: 
+With this change, you're all set. Here are your next steps:
 
-1. Delete the index. 
+1. Delete the index.
 
-1. Recreate the index after adding the new custom analyzer (`phone_analyzer-search`) and assigning that analyzer to the `phone-number` field's `searchAnalyzer` property.
+1. Recreate the index after you add the new custom analyzer (`phone_analyzer-search`) and assign that analyzer to the `phone-number` field's `searchAnalyzer` property.
 
 1. Reload the data.
 
-1. Retest the queries to verify the search works as expected. If you're using the sample file, this step creates the third index named `phone-number-index-3`.
+1. Retest the queries to verify that the search works as expected. If you're using the sample file, this step creates the third index named `phone-number-index-3`.
 
 <a name="Alternate"></a>
 
 ## Alternate approaches
 
 The analyzer described in the previous section is designed to maximize the flexibility for search. However, it does so at the cost of storing many potentially unimportant terms in the index.
 
-The following example shows an alternative analyzer that's more efficient in tokenization, but has drawbacks. 
+The following example shows an alternative analyzer that's more efficient in tokenization, but it has drawbacks.
 
-Given an input of `14255550100`, the analyzer can't logically chunk the phone number. For example, it can't separate the country code, `1`, from the area code, `425`. This discrepancy would lead to the phone number not being returned if a user didn't include a country code in their search.
+Given an input of `14255550100`, the analyzer can't logically chunk the phone number. For example, it can't separate the country code, `1`, from the area code, `425`. This discrepancy leads to the phone number not being returned if a user doesn't include a country code in their search.
 
 ```json
 "analyzers": [
@@ -638,7 +638,7 @@ Given an input of `14255550100`, the analyzer can't logically chunk the phone nu
 ]
 ```
 
-You can see in the following example that the phone number is split into the chunks you would normally expect a user to be searching for.
+In the following example, the phone number is split into the chunks you normally expect a user to be search for.
 
 |Input|Output|  
 |-|-|  
@@ -648,7 +648,7 @@ Depending on your requirements, this might be a more efficient approach to the p
 
 ## Takeaways
 
-This tutorial demonstrated the process for building and testing a custom analyzer. You created an index, indexed data, and then queried against the index to see what search results were returned. From there, you used the Analyze API to see the lexical analysis process in action.
+This tutorial demonstrated the process of building and testing a custom analyzer. You created an index, indexed data, and then queried against the index to see what search results were returned. From there, you used the Analyze API to see the lexical analysis process in action.
 
 While the analyzer defined in this tutorial offers an easy solution for searching against phone numbers, this same process can be used to build a custom analyzer for any scenario that shares similar characteristics.
 
@@ -660,7 +660,7 @@ You can find and manage resources in the Azure portal, using the All resources o
 
 ## Next steps
 
-Now that you're familiar with how to create a custom analyzer, let's take a look at all of the different filters, tokenizers, and analyzers available to you to build a rich search experience.
+Now that you know how to create a custom analyzer, take a look at all of the different filters, tokenizers, and analyzers available for building a rich search experience:
 
 > [!div class="nextstepaction"]
-> [Custom Analyzers in Azure AI Search](index-add-custom-analyzers.md)
+> [Custom analyzers in Azure AI Search](index-add-custom-analyzers.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revision of Custom Analyzer Tutorial"
}
```

### Explanation
The tutorial titled "Create a Custom Analyzer" in the Azure AI Search documentation has been thoroughly revised to enhance clarity, improve formatting, and update content. 

1. **Title and Metadata Changes**: The title has been capitalized for consistency, and the last modified date has been updated from January 17, 2025, to March 28, 2025. These adjustments contribute to a polished look and accurate timeline for users.

2. **Content Refinement**: The tutorial's text has been revised to use clearer language and improve readability. For instance, phrases have been restructured for better flow—examples include changing "challenges to work with because" to "challenging to work with because" and clarifying instructions while maintaining a friendly tone.

3. **Prerequisites Update**: The prerequisites section has been reorganized to provide users with clearer instructions on services and tools they need, emphasizing the need for an Azure account and providing more direct links to resources.

4. **Clarification of Technical Instructions**: Several technical instructions and code snippets have received minor wording adjustments for precision and clarity. For example:
   - Detailed prompts have been provided on how to copy the API key and URL, and it has been specified that the index should be recreated without omitting steps.
   - Some steps have been consolidated, and others have been rephrased for a more straightforward understanding.

5. **Explanatory Enhancements**: The underlying concepts of how analyzers work and their components have been emphasized with minor modifications in the explanation to ensure users grasp their importance better. Additionally, the differentiation between using an index analyzer and a search analyzer is explained more clearly in the context of improving results.

6. **Alternate Approaches Section**: Updates were made to the section discussing alternate approaches to highlight the limitations of other analyzers more effectively, providing users with insights for practical applications.

In summary, these modifications together serve to enhance the tutorial's usability and comprehensibility, making it a more effective resource for users looking to implement custom analyzers in Azure AI Search.

## articles/search/tutorial-multiple-data-sources.md{#item-71558f}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ author: haileytap
 ms.author: haileytapia
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 03/10/2025
+ms.date: 03/28/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
@@ -49,7 +49,7 @@ A finished version of the code in this tutorial can be found in the following pr
 > [!NOTE]
 > You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
 
-## 1 - Create services
+## Create services
 
 This tutorial uses Azure AI Search for indexing and queries, Azure Cosmos DB for the first data set, and Azure Blob Storage for the second data set.
 
@@ -99,7 +99,7 @@ This sample uses two small sets of data describing seven fictional hotels. One s
 
 The third component is Azure AI Search, which you can [create in the Azure portal](search-create-service-portal.md) or [find an existing search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your Azure resources.
 
-### Copy an admin API key and URL for Azure AI Search
+### Copy an admin key and URL for Azure AI Search
 
 To authenticate to your search service, you need the service URL and an access key.
 
@@ -111,7 +111,7 @@ To authenticate to your search service, you need the service URL and an access k
 
 Having a valid key establishes trust, on a per-request basis, between the application sending the request and the service handling it.
 
-## 2 - Set up your environment
+## Set up your environment
 
 1. Open Visual Studio.
 
@@ -140,7 +140,7 @@ The first two entries are the URL and admin keys of a search service. Use the fu
 
 The next entries specify account names and connection string information for the Azure Blob Storage and Azure Cosmos DB data sources.
 
-## 3 - Map key fields
+## Map key fields
 
 Merging content requires that both data streams are targeting the same documents in the search index.
 
@@ -155,7 +155,7 @@ Azure AI Search indexers can use field mappings to rename and even reformat data
 > [!NOTE]
 > In most cases, autogenerated document keys, such as those created by default by some indexers, don't make good document keys for combined indexes. In general, use a meaningful, unique key value that already exists in your data sources or can be easily added.
 
-## 4 - Explore the code
+## Explore the code
 
 When the data and configuration settings are in place, the sample program in **/v11/AzureSearchMultipleDataSources.sln** should be ready to build and run.
 
@@ -351,7 +351,7 @@ Because the index is already populated with hotel data from the Azure Cosmos DB
 > [!NOTE]
 > If you have the same non-key fields in both of your data sources, and the data in those fields doesn't match, the index contains the values from whichever indexer ran most recently. In our example, both data sources contain a **HotelName** field. If for some reason the data in this field is different, for documents with the same key value, the **HotelName** data from the most recently indexed data source is the value stored in the index.
 
-## 5 - Search
+## Search
 
 After running the program, you can explore the populated search index using the [**Search explorer**](search-explorer.md) in the Azure portal.
 
@@ -377,7 +377,7 @@ You can find and manage resources in the Azure portal using the All resources or
 
 ## Next step
 
-Now that you're familiar with the concept of ingesting data from multiple sources, take a closer look at indexer configuration, starting with Azure Cosmos DB:
+Now that you're familiar with ingesting data from multiple sources, take a closer look at indexer configuration, starting with Azure Cosmos DB:
 
 > [!div class="nextstepaction"]
 > [Configure an Azure Cosmos DB for NoSQL indexer](search-howto-index-cosmosdb.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to Multiple Data Sources Tutorial"
}
```

### Explanation
The "Multiple Data Sources" tutorial in the Azure AI Search documentation has been updated to improve clarity and ensure consistency throughout the document. 

1. **Date Update**: The last modified date has been changed from March 10, 2025, to March 28, 2025, reflecting the most recent updates to the tutorial.

2. **Section Title Changes**: Various section titles have been modified to simplify wording and improve readability. For example, titles such as "1 - Create services" have been revised to just "Create services,” eliminating unnecessary numbering while maintaining clear headings.

3. **Clarification of Instructions**: Phrases related to the process of copying the API key and URL have been adjusted for clarity, converting "Copy an admin API key and URL for Azure AI Search" to "Copy an admin key and URL for Azure AI Search." This change streamlines the instruction without losing important information.

4. **Consistency in Terminology**: Terms have been standardized throughout the document. For instance, replacing "Set up your environment" from "2 - Set up your environment" helps focus on the activity rather than the numbered step, maintaining a consistent format across sections.

5. **Revised Notes**: The instructional notes have been polished for better comprehension. For example, the statement about autogenerated document keys in the mapping fields section has been clarified to underscore the importance of meaningful key values.

6. **Overall Formatting Improvements**: Additional minor tweaks have been implemented to enhance the tutorial's overall readability, making it easier for users to follow through the steps for using Azure AI Search with multiple data sources.

These modifications collectively enhance the tutorial's clarity and usability, providing users with a more straightforward guide to integrating multiple data sources into Azure AI Search effectively.

## articles/search/tutorial-optimize-indexing-push-api.md{#item-ef0e96}

<details>
<summary>Diff</summary>
````diff
@@ -1,24 +1,24 @@
 ---
-title: 'C# tutorial: Optimize indexing by using the push API'
+title: 'C# Tutorial: Optimize Indexing Using the Push API'
 titleSuffix: Azure AI Search
-description: Learn how to efficiently index data by using Azure AI Search's push API. This tutorial and sample code are in C#.
+description: Learn how to efficiently index data using Azure AI Search's push API. This tutorial and sample code are in C#.
 author: gmndrg
 ms.author: gimondra
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 10/14/2024
+ms.date: 03/28/2025
 ms.custom:
   - devx-track-csharp
   - ignite-2023
 ---
 
-# Tutorial: Optimize indexing by using the push API
+# Tutorial: Optimize indexing using the push API
 
-Azure AI Search supports [two basic approaches](search-what-is-data-import.md) for importing data into a search index: *push* your data into the index programmatically, or *pull* in the data by pointing an [Azure AI Search indexer](search-indexer-overview.md) at a supported data source.
+Azure AI Search supports [two basic approaches](search-what-is-data-import.md) for importing data into a search index: *pushing* your data into the index programmatically, or *pulling* in your data by pointing an [Azure AI Search indexer](search-indexer-overview.md) to a supported data source.
 
-This tutorial explains how to efficiently index data using the [push model](search-what-is-data-import.md#pushing-data-to-an-index) by batching requests and using an exponential backoff retry strategy. You can [download and run the sample application](https://github.com/Azure-Samples/azure-search-dotnet-scale/tree/main/optimize-data-indexing). This article explains the key aspects of the application and what factors to consider when indexing data.
+This tutorial explains how to efficiently index data using the [push model](search-what-is-data-import.md#pushing-data-to-an-index) by batching requests and using an exponential backoff retry strategy. You can [download and run the sample application](https://github.com/Azure-Samples/azure-search-dotnet-scale/tree/main/optimize-data-indexing). This tutorial also explains the key aspects of the application and what factors to consider when indexing data.
 
-This tutorial uses C# and the [Azure.Search.Documents library](/dotnet/api/overview/azure/search) from the Azure SDK for .NET to perform the following tasks:
+In this tutorial, you use C# and the [Azure.Search.Documents library](/dotnet/api/overview/azure/search) from the Azure SDK for .NET to:
 
 > [!div class="checklist"]
 > * Create an index
@@ -29,11 +29,8 @@ This tutorial uses C# and the [Azure.Search.Documents library](/dotnet/api/overv
 
 ## Prerequisites
 
-The following services and tools are required for this tutorial.
-
-+ An Azure subscription. If you don't have one, you can [create a free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
-
-+ [Visual Studio](https://visualstudio.microsoft.com/downloads/), any edition. Sample code and instructions were tested on the free Community edition.
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
++ [Visual Studio](https://visualstudio.microsoft.com/downloads/).
 
 <a name="get-service-info"></a>
 
@@ -43,7 +40,7 @@ Source code for this tutorial is in the [optimize-data-indexing/v11](https://git
 
 ## Key considerations
 
-Factors that affect indexing speeds are listed next. To learn more, see [Index large data sets](search-howto-large-index.md).
+The following factors affect indexing speeds. For more information, see [Index large data sets](search-howto-large-index.md).
 
 + **Service tier and number of partitions/replicas**: Adding partitions or upgrading your tier increases indexing speeds.
 + **Index schema complexity**: Adding fields and field properties lowers indexing speeds. Smaller indexes are faster to index.
@@ -52,21 +49,21 @@ Factors that affect indexing speeds are listed next. To learn more, see [Index l
 + **Retry strategy**: An exponential backoff retry strategy is a best practice for optimum indexing.
 + **Network data transfer speeds**: Data transfer speeds can be a limiting factor. Index data from within your Azure environment to increase data transfer speeds.
 
-## Step 1: Create an Azure AI Search service
+## Create an Azure AI Search service
 
-To complete this tutorial, you need an Azure AI Search service, which you can [create in the Azure portal](search-create-service-portal.md), or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription. We recommend using the same tier you plan to use in production so that you can accurately test and optimize indexing speeds.
+This tutorial requires an Azure AI Search service, which you can [create in the Azure portal](search-create-service-portal.md). You can also [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. To accurately test and optimize indexing speeds, we recommend using the same tier you plan to use in production.
 
 ### Get an admin key and URL for Azure AI Search
 
 This tutorial uses key-based authentication. Copy an admin API key to paste into the *appsettings.json* file.
 
-1. Sign in to the [Azure portal](https://portal.azure.com). Get the endpoint URL from your search service **Overview** page. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com). On your service **Overview** page, copy the endpoint URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
-1. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
+1. On **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
 
     :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of the HTTP endpoint and API key locations.":::
 
-## Step 2: Set up your environment
+## Set up your environment
 
 1. Start Visual Studio and open *OptimizeDataIndexing.sln*.
 
@@ -80,11 +77,11 @@ This tutorial uses key-based authentication. Copy an admin API key to paste into
 }
 ```
 
-## Step 3: Explore the code
+## Explore the code
 
-Once you update *appsettings.json*, the sample program in *OptimizeDataIndexing.sln* should be ready to build and run.
+After you update *appsettings.json*, the sample program in *OptimizeDataIndexing.sln* should be ready to build and run.
 
-This code is derived from the C# section of [Quickstart: Full text search using the Azure SDKs](search-get-started-text.md). You can find more detailed information on the basics of working with the .NET SDK in that article.
+This code is derived from the C# section of [Quickstart: Full text search using the Azure SDKs](search-get-started-text.md), which provides detailed information about the basics of working with the .NET SDK.
 
 This simple C#/.NET console app performs the following tasks:
 
@@ -105,9 +102,9 @@ This simple C#/.NET console app performs the following tasks:
 
 This sample program uses the Azure SDK for .NET to define and create an Azure AI Search index. It takes advantage of the `FieldBuilder` class to generate an index structure from a C# data model class.
 
-The data model is defined by the `Hotel` class, which also contains references to the `Address` class. The FieldBuilder drills down through multiple class definitions to generate a complex data structure for the index. Metadata tags are used to define the attributes of each field, such as whether it's searchable or sortable.
+The data model is defined by the `Hotel` class, which also contains references to the `Address` class. `FieldBuilder` drills down through multiple class definitions to generate a complex data structure for the index. Metadata tags are used to define the attributes of each field, such as whether it's searchable or sortable.
 
-The following snippets from the *Hotel.cs* file show how a single field, and a reference to another data model class, can be specified.
+The following snippets from the *Hotel.cs* file specify a single field and a reference to another data model class.
 
 ```csharp
 . . .
@@ -135,9 +132,9 @@ private static async Task CreateIndexAsync(string indexName, SearchIndexClient i
 
 ### Generate data
 
-A simple class is implemented in the *DataGenerator.cs* file to generate data for testing. The sole purpose of this class is to make it easy to generate a large number of documents with a unique ID for indexing.
+A simple class is implemented in the *DataGenerator.cs* file to generate data for testing. The purpose of this class is to make it easy to generate a large number of documents with a unique ID for indexing.
 
-To get a list of 100,000 hotels with unique IDs, run the following lines of code:
+To get a list of 100,000 hotels with unique IDs, run the following code:
 
 ```csharp
 long numDocuments = 100000;
@@ -147,23 +144,24 @@ List<Hotel> hotels = dg.GetHotels(numDocuments, "large");
 
 There are two sizes of hotels available for testing in this sample: *small* and *large*.
 
-The schema of your index has an effect on indexing speeds. For this reason, it makes sense to convert this class to generate data that best matches your intended index schema after you run through this tutorial.
+The schema of your index affects indexing speeds. After you complete this tutorial, consider converting this class to generate data that best matches your intended index schema.
 
-## Step 4: Test batch sizes
+## Test batch sizes
 
-Azure AI Search supports the following APIs to load single or multiple documents into an index:
+To load single or multiple documents into an index, Azure AI Search supports the following APIs:
 
 + [Documents - Index (REST API)](/rest/api/searchservice/documents)
-+ [IndexDocumentsAction class](/dotnet/api/azure.search.documents.models.indexdocumentsaction) or [IndexDocumentsBatch class](/dotnet/api/azure.search.documents.models.indexdocumentsbatch)
++ [IndexDocumentsAction class](/dotnet/api/azure.search.documents.models.indexdocumentsaction)
++ [IndexDocumentsBatch class](/dotnet/api/azure.search.documents.models.indexdocumentsbatch)
 
-Indexing documents in batches significantly improves indexing performance. These batches can be up to 1,000 documents, or up to about 16 MB per batch.
+Indexing documents in batches significantly improves indexing performance. These batches can be up to 1,000 documents or up to about 16 MB per batch.
 
 Determining the optimal batch size for your data is a key component of optimizing indexing speeds. The two primary factors influencing the optimal batch size are:
 
 + The schema of your index
 + The size of your data
 
-Because the optimal batch size is dependent on your index and your data, the best approach is to test different batch sizes to determine what results in the fastest indexing speeds for your scenario.
+Because the optimal batch size depends on your index and your data, the best approach is to test different batch sizes to determine what results in the fastest indexing speeds for your scenario.
 
 The following function demonstrates a simple approach to testing batch sizes.
 
@@ -203,7 +201,7 @@ public static async Task TestBatchSizesAsync(SearchClient searchClient, int min
 }
 ```
 
-Because not all documents are the same size (although they are in this sample), we estimate the size of the data we're sending to the search service. You can do this by using the following function that first converts the object to json and then determines its size in bytes. This technique allows us to determine which batch sizes are most efficient in terms of MB/s indexing speeds.
+Because not all documents are the same size (although they are in this sample), we estimate the size of the data we're sending to the search service. You can do this by using the following function that first converts the object to JSON and then determines its size in bytes. This technique allows us to determine which batch sizes are most efficient in terms of MB/s indexing speeds.
 
 ```csharp
 // Returns size of object in MB
@@ -232,26 +230,26 @@ The function requires a `SearchClient` plus the number of tries you'd like to te
 await TestBatchSizesAsync(searchClient, numTries: 3);
 ```
 
-When you run the function, you should see an output in your console like the following example:
+When you run the function, you should see an output in your console similar to the following example:
 
 :::image type="content" source="media/tutorial-optimize-data-indexing/test-batch-sizes.png" alt-text="Screenshot of the output of test batch size function.":::
 
-Identify which batch size is most efficient and then use that batch size in the next step of the tutorial. You might see a plateau in MB/s across different batch sizes.
+Identify which batch size is most efficient and use that batch size in the next step of this tutorial. You might see a plateau in MB/s across different batch sizes.
 
-## Step 5: Index the data
+## Index the data
 
 Now that you identified the batch size you intend to use, the next step is to begin to index the data. To index data efficiently, this sample:
 
-+ uses multiple threads/workers
-+ implements an exponential backoff retry strategy
++ Uses multiple threads/workers
++ Implements an exponential backoff retry strategy
 
 Uncomment lines 41 through 49, and then rerun the program. On this run, the sample generates and sends batches of documents, up to 100,000 if you run the code without changing the parameters.
 
 ### Use multiple threads/workers
 
-To take full advantage of Azure AI Search's indexing speeds, use multiple threads to send batch indexing requests concurrently to the service.  
+To take advantage of Azure AI Search's indexing speeds, use multiple threads to send batch indexing requests concurrently to the service.  
 
-Several of the key considerations previously mentioned can affect the optimal number of threads. You can modify this sample and test with different thread counts to determine the optimal thread count for your scenario. However, as long as you have several threads running concurrently, you should be able to take advantage of most of the efficiency gains.
+Several of the [key considerations](#key-considerations) can affect the optimal number of threads. You can modify this sample and test with different thread counts to determine the optimal thread count for your scenario. However, as long as you have several threads running concurrently, you should be able to take advantage of most of the efficiency gains.
 
 As you ramp up the requests hitting the search service, you might encounter [HTTP status codes](/rest/api/searchservice/http-status-codes) indicating the request didn't fully succeed. During indexing, two common HTTP status codes are:
 
@@ -260,11 +258,11 @@ As you ramp up the requests hitting the search service, you might encounter [HTT
 
 ### Implement an exponential backoff retry strategy
 
-If a failure happens, requests should be retried using an [exponential backoff retry strategy](/dotnet/architecture/microservices/implement-resilient-applications/implement-retries-exponential-backoff).
+If a failure happens, you should retry requests using an [exponential backoff retry strategy](/dotnet/architecture/microservices/implement-resilient-applications/implement-retries-exponential-backoff).
 
-Azure AI Search's .NET SDK automatically retries 503s and other failed requests but you should implement your own logic to retry 207s. Open-source tools such as [Polly](https://github.com/App-vNext/Polly) can be useful in a retry strategy.
+Azure AI Search's .NET SDK automatically retries 503s and other failed requests, but you should implement your own logic to retry 207s. Open-source tools like [Polly](https://github.com/App-vNext/Polly) can be useful in a retry strategy.
 
-In this sample, we implement our own exponential backoff retry strategy. We start by defining some variables including the `maxRetryAttempts` and the initial `delay` for a failed request:
+In this sample, we implement our own exponential backoff retry strategy. We start by defining some variables, including the `maxRetryAttempts` and the initial `delay` for a failed request.
 
 ```csharp
 // Create batch of documents for indexing
@@ -279,9 +277,9 @@ TimeSpan delay = delay = TimeSpan.FromSeconds(2);
 int maxRetryAttempts = 5;
 ```
 
-The results of the indexing operation are stored in the variable `IndexDocumentResult result`. This variable is important because it allows you to check if any documents in the batch failed, as shown in the following example. If there's a partial failure, a new batch is created based on the failed documents' ID.
+The results of the indexing operation are stored in the variable `IndexDocumentResult result`. This variable allows you to check if documents in the batch failed, as shown in the following example. If there's a partial failure, a new batch is created based on the failed documents' ID.
 
-`RequestFailedException` exceptions should also be caught as they indicate the request failed completely and should also be retried.
+`RequestFailedException` exceptions should also be caught, as they indicate the request failed completely, and retried.
 
 ```csharp
 // Implement exponential backoff
@@ -339,56 +337,56 @@ do
 
 From here, wrap the exponential backoff code into a function so it can be easily called.
 
-Another function is then created to manage the active threads. For simplicity, that function isn't included here but can be found in *ExponentialBackoff.cs*. The function can be called with the following command where `hotels` is the data we want to upload, `1000` is the batch size, and `8` is the number of concurrent threads:
+Another function is then created to manage the active threads. For simplicity, that function isn't included here but can be found in *ExponentialBackoff.cs*. You can call the function using the following command, where `hotels` is the data we want to upload, `1000` is the batch size, and `8` is the number of concurrent threads.
 
 ```csharp
 await ExponentialBackoff.IndexData(indexClient, hotels, 1000, 8);
 ```
 
-When you run the function, you should see an output:
+When you run the function, you should see an output similar to the following example:
 
 :::image type="content" source="media/tutorial-optimize-data-indexing/index-data-start.png" alt-text="Screenshot that shows the output of an index data function.":::
 
-When a batch of documents fails, an error is printed out indicating the failure and that the batch is being retried:
+When a batch of documents fails, an error is printed indicating the failure and that the batch is being retried.
 
 ```
 [Batch starting at doc 6000 had partial failure]
 [Retrying 560 failed documents]
 ```
 
-After the function is finished running, you can verify that all of the documents were added to the index.
+After the function finishes running, you can verify that all of the documents were added to the index.
 
-## Step 6: Explore the index
+## Explore the index
 
-You can explore the populated search index after the program has run either programmatically or by using the [Search explorer](search-explorer.md) in the Azure portal.
+After the program finishes running, you can explore the populated search index either programmatically or using the [Search explorer](search-explorer.md) in the Azure portal.
 
 ### Programatically
 
-There are two main options for checking the number of documents in an index: the [Count Documents API](/rest/api/searchservice/documents/count) and the [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics). Both paths require time to process so don't be alarmed if the number of documents returned is initially lower than you expect.
+There are two main options for checking the number of documents in an index: the [Count Documents API](/rest/api/searchservice/documents/count) and the [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics). Both paths require time to process, so don't be alarmed if the number of documents returned is initially lower than you expect.
 
 #### Count Documents
 
-The Count Documents operation retrieves a count of the number of documents in a search index:
+The Count Documents operation retrieves a count of the number of documents in a search index.
 
 ```csharp
 long indexDocCount = await searchClient.GetDocumentCountAsync();
 ```
 
 #### Get Index Statistics
 
-The Get Index Statistics operation returns a document count for the current index, plus storage usage. Index statistics take longer than document count to update.
+The Get Index Statistics operation returns a document count for the current index, plus storage usage. Index statistics take longer to update than document count.
 
 ```csharp
 var indexStats = await indexClient.GetIndexStatisticsAsync(indexName);
 ```
 
 ### Azure portal
 
-In the Azure portal, from the left pane, and find the **optimize-indexing** index in the **Indexes** list.
+In the Azure portal, from the left pane, find the **optimize-indexing** index in the **Indexes** list.
 
 :::image type="content" source="media/tutorial-optimize-data-indexing/portal-output.png" alt-text="Screenshow that shows a list of Azure AI Search indexes.":::
 
-The *Document Count* and *Storage Size* are based on [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics) and can take several minutes to update.
+The **Document Count** and **Storage Size** are based on the [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics) and can take several minutes to update.
 
 ## Reset and rerun
 
@@ -406,7 +404,7 @@ You can find and manage resources in the Azure portal, using the **All resources
 
 ## Next step
 
-To learn more about indexing large amounts data, try the following tutorial.
+To learn more about indexing large amounts data, try the following tutorial:
 
 > [!div class="nextstepaction"]
 > [Tutorial: Index large data from Apache Spark using SynapseML and Azure AI Search](search-synapseml-cognitive-services.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Enhancements to Indexing Using the Push API Tutorial"
}
```

### Explanation
The "Optimize Indexing Using the Push API" tutorial in the Azure AI Search documentation has been revised to improve readability, address minor inaccuracies, and ensure consistency throughout the article.

1. **Title and Metadata Changes**: The title has been adjusted from "C# tutorial: Optimize indexing by using the push API" to "C# Tutorial: Optimize Indexing Using the Push API," reflecting proper capitalization and removing unnecessary prepositions for clarity. The publication date was updated from October 14, 2024, to March 28, 2025.

2. **Content Clarity and Consistency**: Several modifications were made to streamline the language. For example, the text has shifted from "programmatically, or *pull* in the data" to "programmatically, or *pulling* in your data," making the structure consistent. This highlights a more conversational tone through uniform verb forms.

3. **Phrase Improvements**: Phrasing has been polished for conciseness, such as changing "You can also [find an existing service]" to "You can also [find an existing service]." Additionally, minor rewording was done to instructions for better clarity, ensuring users better understand the actions they need to take.

4. **Reorganization of Prerequisites**: The prerequisites section was adjusted for clarity, replacing "The following services and tools are required for this tutorial" with "An Azure account with an active subscription." This eliminates redundancy while directing users to important resources succinctly.

5. **Structural Enhancements**: Some section headings were simplified, such as changing "Step 1: Create an Azure AI Search service" to "Create an Azure AI Search service," focusing on the action rather than the numbered step. Similarly, "Step 5: Index the data" changed to "Index the data" for the same reason.

6. **Explaining Procedures**: Instructions related to verifying the completion of tasks were articulated more clearly. For example, mentioning how to check the document count was further emphasized, eliminating redundancy to highlight pertinent information.

7. **Conclusion Enhancements**: A few minor adjustments were made in the summary and next steps sections to maintain a consistent tone and encourage follow-up learning with no substantial changes to the content's intent.

These modifications collectively improve the tutorial's usability and readability, making it a more effective resource for developers looking to optimize data indexing using the Push API in Azure AI Search.

## articles/search/tutorial-rag-build-solution-pipeline.md{#item-25ce01}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: tutorial
-ms.date: 01/09/2025
+ms.date: 03/24/2025
 ---
 
 # Tutorial: Build an indexing pipeline for RAG on Azure AI Search
@@ -29,7 +29,7 @@ In this tutorial, you:
 If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
 
 > [!TIP]
-> You can use the [Import and vectorize data wizard](search-import-data-portal.md) to create your pipeline. Try some quickstarts: [Image search](search-get-started-portal-image-search.md) and [Vector search](search-get-started-portal-import-vectors.md).
+> You can use the [Import and vectorize data wizard](search-import-data-portal.md) to create your pipeline. Try some quickstarts [Image search](search-get-started-portal-image-search.md) or [Vector search](search-get-started-portal-import-vectors.md), to learn more about the pipeline and its moving parts.
 
 ## Prerequisites
 
@@ -115,15 +115,15 @@ print(f"{result.name} created")
 
 ## Create a data source connection
 
-In this step, set up the sample data and a connection to Azure Blob Storage. The indexer retrieves PDFs from a container. You create the container and upload files in this step.
+In this step, set up the sample data and a connection from Azure AI Search to Azure Blob Storage. The indexer retrieves PDFs from a container. You create the container and upload files in this step.
 
 The original ebook is large, over 100 pages and 35 MB in size. We broke it up into smaller PDFs, one per page of text, to stay under the [document limit for indexers](search-limits-quotas-capacity.md#indexer-limits) of 16 MB per API call and also the [AI enrichment data limits](search-limits-quotas-capacity.md#data-limits-ai-enrichment). For simplicity, we omit image vectorization for this exercise.
 
 1. Sign in to the [Azure portal](https://portal.azure.com) and find your Azure Storage account.
 
 1. Create a container and upload the PDFs from [earth_book_2019_text_pages](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/nasa-e-book/earth_book_2019_text_pages).
 
-1. Make sure Azure AI Search has [**Storage Blob Data Reader** permissions](/azure/role-based-access-control/role-assignments-portal) on the resource.
+1. Make sure your [Azure AI Search managed identity](search-howto-managed-identities-data-sources.md) has a [**Storage Blob Data Reader**](/azure/role-based-access-control/role-assignments-portal) role assignment on Azure Storage.
 
 1. Next, in Visual Studio Code, define an indexer data source that provides connection information during indexing.
 
@@ -148,15 +148,15 @@ The original ebook is large, over 100 pages and 35 MB in size. We broke it up in
     print(f"Data source '{data_source.name}' created or updated")
     ```
 
-If you set up a managed identity for Azure AI Search for the connection, the connection string includes a `ResourceId=` suffix. It should look similar to the following example: `"ResourceId=/subscriptions/FAKE-SUBCRIPTION=ID/resourceGroups/FAKE-RESOURCE-GROUP/providers/Microsoft.Storage/storageAccounts/FAKE-ACCOUNT;"`
+If you set up a [managed identity for an Azure AI Search connection to Azure Storage](search-howto-managed-identities-storage.md), the data source connection string includes a `ResourceId=` suffix. It should look similar to the following example: `"ResourceId=/subscriptions/FAKE-SUBSCRIPTION-ID/resourceGroups/FAKE-RESOURCE-GROUP/providers/Microsoft.Storage/storageAccounts/FAKE-ACCOUNT;"`
 
 ## Create a skillset
 
 Skills are the basis for integrated data chunking and vectorization. At a minimum, you want a Text Split skill to chunk your content, and an embedding skill that create vector representations of your chunked content.
 
 In this skillset, an extra skill is used to create structured data in the index. The [Entity Recognition skill](cognitive-search-skill-entity-recognition-v3.md) is used to identify locations, which can range from proper names to generic references, such as "ocean" or "mountain". Having structured data gives you more options for creating interesting queries and boosting relevance.
 
-The AZURE_AI_MULTISERVICE_KEY is needed even if you're using role-based access control. Azure AI Search uses the key for billing purposes and it's required unless your workloads stay under the free limit. You can also a keyless connection if you're using the most recent preview API or beta packages. For more information, see [Attach an Azure AI services multi-service resource to a skillset](cognitive-search-attach-cognitive-services.md).
+The AZURE_AI_MULTISERVICE_KEY is needed even if you're using role-based access control. Azure AI Search uses the key for billing purposes and it's required unless your workloads stay under the free limit. You can also set up a keyless connection if you're using the most recent preview API or beta packages. For more information, see [Attach an Azure AI services multi-service resource to a skillset](cognitive-search-attach-cognitive-services.md).
 
 ```python
 from azure.search.documents.indexes.models import (
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates to RAG Indexing Pipeline Tutorial"
}
```

### Explanation
The tutorial "Build an indexing pipeline for RAG on Azure AI Search" has been updated to improve clarity and ensure consistency in instructions and terminology.

1. **Date Update**: The publication date has been revised from January 9, 2025, to March 24, 2025, ensuring that the content reflects the latest information.

2. **Clarification of Instructions**: The wording in several sections has been refined to enhance clarity. For instance, the suggestion to use the "Import and vectorize data wizard" now includes specifics: "Try some quickstarts [Image search] or [Vector search], to learn more about the pipeline and its moving parts," ensuring users are aware of related resources for deeper understanding.

3. **Connection Setup Details**: Instructions for setting up the connection between Azure AI Search and Azure Blob Storage have been made more explicit. The phrase "connection to Azure Blob Storage" has been changed to "connection from Azure AI Search to Azure Blob Storage," clarifying which service initiates the connection.

4. **Managed Identity Reference**: The explanation regarding permissions has been updated. The original sentence now specifies having a managed identity for Azure AI Search with a relevant role assignment, replacing a more general statement. This distinction clarifies the requirements for ensuring proper access rights.

5. **Connection String Example**: The description of the connection string now includes a hyperlinked reference to managed identities for better context. "If you set up a managed identity" has been modified to clarify the role of a managed identity in establishing the connection.

6. **Skillset Explanation**: Minor rewording in the section about skillsets has been implemented, with phrases restructured for better flow and readability. For instance, "you want a Text Split skill to chunk your content" now emphasizes the need for content chunking more clearly.

7. **Key Requirement Details**: The explanation regarding the AZURE_AI_MULTISERVICE_KEY has been adjusted to improve comprehension. It specifies the necessity of the key even under specific access control models, along with the option of using a keyless connection for certain scenarios, enhancing understanding of connectivity options.

These updates contribute to a more streamlined and user-friendly tutorial, making it easier for users to successfully build and utilize an indexing pipeline for RAG in Azure AI Search.

## articles/search/vector-search-how-to-chunk-documents.md{#item-b79133}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: conceptual
-ms.date: 03/11/2025
+ms.date: 03/31/2025
 ---
 
 # Chunk large documents for vector search solutions in Azure AI Search
@@ -20,7 +20,9 @@ We recommend [integrated vectorization](vector-search-integrated-vectorization.m
 
 ## Common chunking techniques
 
-Chunking is only required if the source documents are too large for the maximum input size imposed by models. Here are some common chunking techniques, associated with built-in features if you use [indexers](search-indexer-overview.md) and [skills](cognitive-search-working-with-skillsets.md).
+Chunking is only required if the source documents are too large for the maximum input size imposed by models, but it's also beneficial if content is poorly represented as a single vector. Consider a wiki page that covers a lot of varied sub-topics. The entire page might be small enough to meet model input requirements, but you might get better results if you chunk at a finer grain.
+
+Here are some common chunking techniques, associated with built-in features if you use [indexers](search-indexer-overview.md) and [skills](cognitive-search-working-with-skillsets.md).
 
 | Approach | Usage | Built-in functionality |
 |----------|-------|-----------------|
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarification and Updates in Document Chunking Techniques for Vector Search"
}
```

### Explanation
The document titled "Chunk large documents for vector search solutions in Azure AI Search" has been updated with clarifications and enhancements to improve the comprehensibility and depth of content regarding document chunking techniques. 

1. **Date Update**: The publication date has been updated from March 11, 2025, to March 31, 2025, ensuring the document reflects the latest information.

2. **Clarification of Chunking Importance**: The explanation regarding the necessity of chunking has been expanded. The new text emphasizes that chunking is not only essential when documents exceed the maximum input size for models but also beneficial when the content is poorly represented as a single vector. For example, a wiki page covering various sub-topics may fit within the input limits but would yield better search results if divided into smaller, more focused chunks.

3. **Formatting Improvements**: New lines and structures were introduced for clarity, following the updated explanation of chunking. This separation highlights common chunking techniques more effectively, making it easier for readers to navigate the content.

4. **Introduction of Common Techniques**: The section remains focused on detailing common chunking techniques while connecting these techniques with built-in features such as [indexers](search-indexer-overview.md) and [skills](cognitive-search-working-with-skillsets.md). This continuity of structure links practical applications seamlessly with the topic discussed.

These revisions enhance the overall understanding of chunking methodologies in vector search solutions, making the guidance more accessible and practical for users aiming to optimize their content for better search performance within Azure AI Search.

## articles/search/vector-search-how-to-quantization.md{#item-744f48}

<details>
<summary>Diff</summary>
````diff
@@ -9,12 +9,12 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 11/19/2024
+ms.date: 03/31/2025
 ---
 
 # Compress vectors using scalar or binary quantization
 
-Azure AI Search supports scalar and binary quantization for reducing the size of vectors in a search index. Quantization is recommended for reducing vector size because it lowers both memory and disk storage consumption for float16 and float32 embeddings. To offset the effects of lossy compression, you can add oversampling and rescoring over uncompressed vectors.
+Azure AI Search supports scalar and binary quantization for reducing the size of vectors in a search index. Quantization is recommended because it reduces both memory and disk storage for float16 and float32 embeddings. To offset the effects of lossy compression, you can add oversampling and rescoring.
 
 To use built-in quantization, follow these steps:
 
@@ -26,15 +26,15 @@ To use built-in quantization, follow these steps:
 > - Create a new vector profile that uses the named configuration
 > - Create a new vector field having the new vector profile
 > - Load the index with float32 or float16 data that's quantized during indexing with the configuration you defined
-> - Optionally, [query quantized data](#query-a-quantized-vector-field-using-oversampling) using the oversampling parameter if you want to override the default
+> - Optionally, [query quantized data](#query-a-quantized-vector-field-using-oversampling) using the oversampling parameter. If the vector field doesn't specify oversampling in its definition, you can add it at query time.
 
 ## Prerequisites
 
-- [Vector fields in a search index](vector-search-how-to-create-index.md) with a `vectorSearch` configuration, using the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (eKNN) algorithms and a new vector profile.
+- [Vector fields in a search index](vector-search-how-to-create-index.md), with a `vectorSearch` configuration specifying either the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (eKNN) algorithm, and a new vector profile.
 
 ## Supported quantization techniques
 
-Quantization applies to vector fields receiving float-type vectors. In the examples in this article, the field's data type is `Collection(Edm.Single)` for incoming float32 embeddings, but float16 is also supported. When the vectors are received on a field with compression configured, the engine automatically performs quantization to reduce the footprint of the vector data in memory and on disk.
+Quantization applies to vector fields receiving float-type vectors. In the examples in this article, the field's data type is `Collection(Edm.Single)` for incoming float32 embeddings, but float16 is also supported. When the vectors are received on a field with compression configured, the engine performs quantization to reduce the footprint of the vector data in memory and on disk.
 
 Two types of quantization are supported:
 
@@ -43,15 +43,38 @@ Two types of quantization are supported:
 - Binary quantization converts floats into binary bits, which takes up 1 bit. This results in up to 28 times reduced vector index size.
 
 >[!Note]
-> While free services support quantization, they may not demonstrate the full storage savings due to the limited storage quota.
+> While free services support quantization, they don't demonstrate the full storage savings due to the limited storage quota.
+
+## Recommended rescoring techniques
+
+Rescoring is a technique used to offset information loss due to vector compression. It uses oversampling to pick up extra vectors, and supplemental information to rescore initial results found by the query. Supplemental information is either uncompressed original full-precision vectors - or for binary quantization only - you have the option of rescoring using the binary quantized document candidates against the query vector. Rescoring options are specified in the index, but you can invoke rescoring at query time if the index supports it.
+
+API versions determine which rescoring behavior is operational for your code. The most recent preview API supports a new rescoring approach for binary quantization. Indexes created with `2025-03-01-preview` can use the new rescoring behaviors.
+
+| API version | Quantization type | Rescoring properties |
+|-------------|-------------------|------------------|
+| [2024-07-01](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-07-01&preserve-view=true) | Scalar and binary quantization, on vector indexes built using Hierarchical Navigable Small World (HNSW) graphs for similarity search | `rerankWithOriginalVectors` |
+| [2024-11-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) | Scalar and binary quantization on HNSW graphs | `rescoringOptions.enableRescoring` and `rescoreStorageMethod.preserveOriginals` |
+| [2025-03-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true) | Binary quantization on HNSW graphs | Previous parameter combinations are still supported but binary quantization can now be rescored if original embeddings are deleted: `rescoringOptions.enableRescoring` and `rescoringOptions.rescoreStorageMethod=discardOriginals` |
+
+Only HNSW graphs allow rescoring. Exhaustive K Nearest Neighbors (eKNN) doesn't support rescoring.
+
+<!-- - In version 2024-11-01-preview, set `rescoringOptions.enableRescoring` and `rescoreStorageMethod.preserveOriginals`
+- In version 2025-03-01-preview, set `rescoringOptions.enableRescoring` and `rescoringOptions.rescoreStorageMethod=preserveOriginals` for scalar or binary quantization, or `rescoringOptions.enableRescoring` and `rescoringOptions.rescoreStorageMethod=discardOriginals` for binary quantization only -->
+
+The generalized process for rescoring is:
+
+1. The vector query executes over compressed vector fields.
+1. The vector query returns the top k oversampled candidates.
+1. Oversampled k candidates are rescored using either the uncompressed original vectors, or the dot product of binary quantization. 1. After rescoring, results are adjusted so that more relevant matches appear first.
 
 ## Add "compressions" to a search index
 
-The following example shows a partial index definition with a fields collection that includes a vector field, and a `vectorSearch.compressions` section.
+This section explains how to specify a `vectorsSearch.compressions` section in the index. The following example shows a partial index definition with a fields collection that includes a vector field.
 
-It includes both `scalarQuantization` or `binaryQuantization`. You can specify as many compression configurations as you need, and then assign the ones you want to a vector profile.
+The compression example includes both `scalarQuantization` or `binaryQuantization`. You can specify as many compression configurations as you need, and then assign the ones you want to a vector profile.
 
-Syntax for `vectorSearch.Compressions` varies between stable and preview REST APIs, with the preview adding new options for storage optimization, plus changes to existing syntax. Backwards compatibility is preserved through internal API mappings, but you should adopt the new syntax in code that targets 2024-11-01-preview and future versions.
+Syntax for `vectorSearch.Compressions` varies between stable and preview REST APIs, with the preview adding more options for storage optimization, plus changes to existing syntax. Backwards compatibility is preserved through internal API mappings, but we recommend adopting the newer properties in code that targets 2024-11-01-preview and future versions.
 
 ### [**2024-07-01**](#tab/2024-07-01)
 
@@ -68,23 +91,109 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-07-01
     { "name": "vectorContent", "type": "Collection(Edm.Single)", "retrievable": false, "searchable": true, "dimensions": 1536,"vectorSearchProfile": "vector-profile-1"},
   ],
   "vectorSearch": {
-        "profiles": [ ],
-        "algorithms": [ ],
+    "profiles": [ 
+      {
+          "name": "vector-profile-1",
+          "algorithm": "use-hnsw",
+          "compression": "use-scalar"
+      }
+    ],
+    "algorithms": [ 
+      {
+        "name": "use-hnsw",
+        "kind": "hnsw",
+        "hnswParameters": { },
+        "exhaustiveKnnParameters": null
+      }
+    ],
+    "compressions": [
+      {
+        "name": "use-scalar",
+        "kind": "scalarQuantization",
+        "scalarQuantizationParameters": {
+          "quantizedDataType": "int8"
+        },
+        "rerankWithOriginalVectors": true,
+        "defaultOversampling": 10
+      },
+      {
+        "name": "use-binary",
+        "kind": "binaryQuantization",
+        "rerankWithOriginalVectors": true,
+        "defaultOversampling": 10
+      }
+    ]
+  }
+}
+```
+
+**Key points**:
+
+- `kind` must be set to `scalarQuantization` or `binaryQuantization`.
+
+- `rerankWithOriginalVectors` uses the original uncompressed vectors to recalculate similarity and rerank the top results returned by the initial search query. The uncompressed vectors exist in the search index even if `stored` is false. This property is optional. Default is true.
+
+- `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
+
+- `quantizedDataType` is optional and applies to scalar quantization only. If you add it, it must be set to `int8`. This is the only primitive data type supported for scalar quantization at this time. Default is `int8`.
+
+### [**2024-11-01-preview**](#tab/2024-11-01-preview)
+
+Use the [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) REST API to configure compression settings.
+
+Changes in this version include new `rescoringOptions` that replace `rerankWithOriginalVectors`, and extend the API with more storage options. Notice that `defaultOversampling` is now a property of `rescoringOptions`.
+
+Rescoring options are used to mitigate the effects of lossy comprehension. You can set `rescoringOptions` for scalar or binary quantization.
+
+```http
+POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-preview
+
+{
+  "name": "my-index",
+  "fields": [
+    { "name": "Id", "type": "Edm.String", "key": true, "retrievable": true, "searchable": true, "filterable": true },
+    { "name": "content", "type": "Edm.String", "retrievable": true, "searchable": true },
+    { "name": "vectorContent", "type": "Collection(Edm.Single)", "retrievable": false, "searchable": true, "dimensions": 1536,"vectorSearchProfile": "vector-profile-1"},
+  ],
+  "vectorSearch": {
+        "profiles": [ 
+          {
+              "name": "vector-profile-1",
+              "algorithm": "use-hnsw",
+              "compression": "use-scalar"
+          }
+        ],
+        "algorithms": [ 
+          {
+            "name": "use-hnsw",
+            "kind": "hnsw",
+            "hnswParameters": { },
+            "exhaustiveKnnParameters": null
+          }
+        ],
         "compressions": [
           {
             "name": "use-scalar",
             "kind": "scalarQuantization",
+            "rescoringOptions": {
+                "enableRescoring": true,
+                "defaultOversampling": 10,
+                "rescoreStorageMethod": "preserveOriginals"
+            },
             "scalarQuantizationParameters": {
               "quantizedDataType": "int8"
             },
-            "rerankWithOriginalVectors": true,
-            "defaultOversampling": 10
+            "truncationDimension": 1024
           },
           {
             "name": "use-binary",
             "kind": "binaryQuantization",
-            "rerankWithOriginalVectors": true,
-            "defaultOversampling": 10
+            "rescoringOptions": {
+                "enableRescoring": true,
+                "defaultOversampling": 10,
+                "rescoreStorageMethod": "preserveOriginals"
+            },
+            "truncationDimension": 1024
           }
         ]
     }
@@ -95,22 +204,28 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-07-01
 
 - `kind` must be set to `scalarQuantization` or `binaryQuantization`.
 
-- `rerankWithOriginalVectors` uses the original uncompressed vectors to recalculate similarity and rerank the top results returned by the initial search query. The uncompressed vectors exist in the search index even if `stored` is false. This property is optional. Default is true.
+- `rescoringOptions` are a collection of properties used to offset lossy compression by rescoring query results using the original full-precision vectors that exist prior to quantization. For rescoring to work, you must have the vector instance that provides this content. Setting `rescoreStorageMethod` to `discardOriginals` prevents you from using `enableRescoring` or `defaultOversampling`. For more information about vector storage, see [Eliminate optional vector instances from storage](vector-search-how-to-storage-options.md).
+
+- `"rescoreStorageMethod": "preserveOriginals"` is the API equivalent of `"rerankWithOriginalVectors": true`. Rescoring vector search results with the original full-precision vectors can result in adjustments to search score and rankings, promoting the more relevant matches as determined by the rescoring step.
 
 - `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
 
 - `quantizedDataType` is optional and applies to scalar quantization only. If you add it, it must be set to `int8`. This is the only primitive data type supported for scalar quantization at this time. Default is `int8`.
 
-### [**2024-11-01-preview**](#tab/2024-11-01-preview)
+- `truncationDimension` is a preview feature that taps inherent capabilities of the text-embedding-3 models to "encode information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks" (see [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)). You can use truncated dimensions with or without rescoring options. For more information about how this feature is implemented in Azure AI Search, see [Truncate dimensions using MRL compression](vector-search-how-to-truncate-dimensions.md).
 
-Use the [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) REST API to configure compression settings.
+### [**2025-03-01-preview**](#tab/2025-03-01-preview)
 
-Changes in this version include new `rescoringOptions` that replace `rerankWithOriginalVectors`, and extend the API with more storage options. Notice that `defaultOversampling` is now a property of `rescoringOptions`.
+Use the [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2025-031-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true) REST API to configure compression settings.
 
-Rescoring options are used to mitigate the effects of lossy comprehension. You can set `rescoringOptions` for scalar or binary quantization.
+Changes in this version include new guidance for *binary quantization*. If you set `enableRescoring` to true, you can set `rescoreStorageMethod` to `discardOriginals` to further reduce storage, without reducing quality. 
+
+Azure AI Search supports a lossy rescoring option on the binary quantized document vectors, which helps close the quality gap between no rescoring and full-precision rescoring when using `binaryQuantization`.
+
+For scalar quantization, there are no rescoring changes in this preview.
 
 ```http
-POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-preview
+POST https://[servicename].search.windows.net/indexes?api-version=2025-03-01-preview
 
 {
   "name": "my-index",
@@ -120,8 +235,21 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-pre
     { "name": "vectorContent", "type": "Collection(Edm.Single)", "retrievable": false, "searchable": true, "dimensions": 1536,"vectorSearchProfile": "vector-profile-1"},
   ],
   "vectorSearch": {
-        "profiles": [ ],
-        "algorithms": [ ],
+        "profiles": [ 
+          {
+              "name": "vector-profile-1",
+              "algorithm": "use-hnsw",
+              "compression": "use-binary"
+          }
+        ],
+        "algorithms": [ 
+          {
+            "name": "use-hnsw",
+            "kind": "hnsw",
+            "hnswParameters": { },
+            "exhaustiveKnnParameters": null
+          }
+        ],
         "compressions": [
           {
             "name": "use-scalar",
@@ -142,7 +270,7 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-pre
             "rescoringOptions": {
                 "enableRescoring": true,
                 "defaultOversampling": 10,
-                "rescoreStorageMethod": "preserveOriginals"
+                "rescoreStorageMethod": "discardOriginals"
             },
             "truncationDimension": 1024
           }
@@ -155,21 +283,21 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-pre
 
 - `kind` must be set to `scalarQuantization` or `binaryQuantization`.
 
-- `rescoringOptions` are a collection of properties used to offset lossy compression by rescoring query results using the original full-precision vectors that exist prior to quantization. For rescoring to work, you must have the vector instance that provides this content. Setting `rescoreStorageMethod` to `discardOriginals` prevents you from using `enableRescoring` or `defaultOversampling`. For more information about vector storage, see [Eliminate optional vector instances from storage](vector-search-how-to-storage-options.md).
+- `rescoringOptions` are a collection of properties used to offset lossy compression by rescoring query results using the original full-precision vectors that exist prior to quantization.
 
-- `"rescoreStorageMethod": "preserveOriginals"` is the API equivalent of `"rerankWithOriginalVectors": true`. Rescoring vector search results with the original full-precision vectors can result in adjustments to search score and rankings, promoting the more relevant matches as determined by the rescoring step.
+- `enableRescoring` rescores the initial results obtained by query execution over compressed data. For scalar quantization, rescoring uses uncompressed vectors to produce more relevant results and takes a dependency on `preserveOriginals`. For binary quantization, rescoring is the same as scalar quantization if you preserve originals, but you can also discard originals and still get rescoring. In this scenario, rescoring is calculated by the dot product of the full precision query and binary quantized data in the index.  
 
-- `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
+- `"rescoreStorageMethod": "discardOriginals"` removes original vectors. These aren't needed for binary quantization.
 
-- `quantizedDataType` is optional and applies to scalar quantization only. If you add it, it must be set to `int8`. This is the only primitive data type supported for scalar quantization at this time. Default is `int8`.
+- `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
 
 - `truncationDimension` is a preview feature that taps inherent capabilities of the text-embedding-3 models to "encode information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks" (see [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)). You can use truncated dimensions with or without rescoring options. For more information about how this feature is implemented in Azure AI Search, see [Truncate dimensions using MRL compression](vector-search-how-to-truncate-dimensions.md).
 
 ---
 
 ## Add the vector search algorithm
 
-You can use HNSW algorithm or exhaustive KNN in the 2024-11-01-preview REST API. For the stable version, use HNSW only.
+You can use HNSW algorithm or exhaustive KNN in the 2024-11-01-preview REST API or later. For the stable version, use HNSW only. If you want rescoring, you must choose HNSW.
 
    ```json
    "vectorSearch": {
@@ -240,15 +368,15 @@ Scalar quantization reduces the resolution of each number within each vector emb
 
 Each component of the vector is mapped to the closest representative value within this set of quantization levels in a process akin to rounding a real number to the nearest integer. In the quantized 8-bit vector, the identifier number stands in place of the original value. After quantization, each vector is represented by an array of identifiers for the bins to which its components belong. These quantized vectors require much fewer bits to store compared to the original vector, thus reducing storage requirements and memory footprint.
 
-## How  binary quantization works in Azure AI Search
+## How binary quantization works in Azure AI Search
 
 Binary quantization compresses high-dimensional vectors by representing each component as a single bit, either 0 or 1. This method drastically reduces the memory footprint and accelerates vector comparison operations, which are crucial for search and retrieval tasks. Benchmark tests show up to 96% reduction in vector index size.
 
-It's particularly effective for embeddings with dimensions greater than 1024. For smaller dimensions, we recommend testing the quality of binary quantization, or trying scalar instead. Additionally, we’ve found BQ performs very well when embeddings are centered around zero. Most popular embedding models such as OpenAI, Cohere, and Mistral are centered around zero.
+It's particularly effective for embeddings with dimensions greater than 1024. For smaller dimensions, we recommend testing the quality of binary quantization, or trying scalar instead. Additionally, we’ve found binary quantization performs very well when embeddings are centered around zero. Most popular embedding models such as OpenAI, Cohere, and Mistral are centered around zero.
 
 ## Query a quantized vector field using oversampling
 
-Query syntax for a compressed or quantized vector field is the same as for noncompressed vector fields, unless you want to override parameters associated with oversampling or rescoring with original vectors.
+Query syntax for a compressed or quantized vector field is the same as for noncompressed vector fields, unless you want to override parameters associated with oversampling and rescoring. You can add an o`versampling` parameter to invoke oversampling and rescoring at query time.
 
 ### [**2024-07-01**](#tab/query-2024-07-01)
 
@@ -302,22 +430,34 @@ POST https://[service-name].search.windows.net/indexes/demo-index/docs/search?ap
 
 **Key points**:
 
-- Applies to vector fields that undergo vector compression, per the vector profile assignment.
+- Oversampling applies to vector fields that undergo vector compression, per the vector profile assignment.
 
-- Overrides the `defaultOversampling` value or introduces oversampling at query time, even if the index's compression configuration didn't specify oversampling or reranking options.
+- Oversampling in the query overrides the `defaultOversampling` value in the index, or invokes oversampling and rescoring at query time, even if the index's compression configuration didn't specify oversampling or reranking options.
 
----
+### [**2025-03-01-preview**](#tab/query-2025-03-01-preview)
+
+The latest preview API is identical to the previous preview API in terms of `vectorQueries` specification. As with the previous version, we recommend oversampling as mitigation for lossy compression.
+
+```http
+POST https://[service-name].search.windows.net/indexes/demo-index/docs/search?api-version=2025-03-01-preview
+
+{    
+    "vectorQueries": [
+        {    
+            "kind": "vector",    
+            "vector": [8, 2, 3, 4, 3, 5, 2, 1],    
+            "fields": "myvector",
+            "oversampling": 12.0,
+            "k": 5   
+        }
+  ]    
+}
+```
 
-<!-- 
-RESCORE WITH ORIGINAL VECTORS -- NEEDS AN H2 or H3
-It's used to rescore search results obtained used compressed vectors.
+**Key points**:
+
+- Oversampling applies to vector fields that undergo vector compression, per the vector profile assignment.
 
-Rescore with original vectors
-After the initial query, rescore results using uncompressed vectors
- 
-For "enableRescoring", we provide true or false options. if it's true, the query will first retrieve using compressed vectors, then rescore results using uncompressed vectors.
+- Oversampling in the query overrides the `defaultOversampling` value in the index, or invokes oversampling and rescoring at query time, even if the index's compression configuration didn't specify oversampling or reranking options.
 
-Step one: Vector query executes using the compressed vectors.
-Step two: Query returns the top oversampling k-matches.
-Step three: Oversampling k-matches are rescored using the uncompressed vectors, adjusting the scores and ranking so that more relevant matches appear first.
- -->
+---
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "Major Revisions and Updates to Vector Quantization Process"
}
```

### Explanation
The document "Compress vectors using scalar or binary quantization" has undergone significant updates to improve its clarity, detail, and guidance on the quantization process within Azure AI Search. This revision aims to enhance users' understanding and capabilities in implementing quantization for vector data.

1. **Date Update**: The publication date has been changed from November 19, 2024, to March 31, 2025, ensuring the content is current.

2. **Enhanced Explanation of Quantization**: The introduction clearly articulates the advantages of quantization, emphasizing its importance not only in size reduction of vector embeddings but also in memory and disk space efficiency. This highlights the significance of quantization for optimizing performance.

3. **New Content Sections**: The document now includes new sections, notably focusing on recommended rescoring techniques, which help mitigate information loss due to vector compression:
   - **Rescoring Techniques**: Detailed explanations provide step-by-step guidance on how to rescore using either original full-precision vectors or binary quantized document candidates, enhancing the quality of search results post-compression.
   - **API Version Specifics**: Clear distinctions on API versions introduce functionalities and properties related to rescoring and quantization, ensuring users understand the implications of their version choices.

4. **Terms and Syntax Update**: The terminology related to rescoring configurations has been standardized, replacing older terms like "rerankWithOriginalVectors" with "rescoringOptions." This update clarifies the API's behavior and expected configurations, aiding in smoother integration for developers.

5. **Details on Quantization Types and Their Performance**: There are refined explanations of both scalar and binary quantization techniques, discussing their respective advantages and use cases, particularly in relation to different embedding sizes and properties to maintain search quality.

6. **Instructions for Configuration**: Added instructions for configuring the `vectorSearch.compressions` sections in indexes are more thorough, providing examples and explaining the implications for scalar and binary quantization setups. This greater detail ensures users can correctly implement quantization settings that meet their specific needs.

7. **Query Syntax Enhancements**: The instructions for querying quantized vector fields have been refined to clarify how to determine oversampling and rescoring parameters. This information is crucial for users wishing to optimize their queries effectively.

8. **Generalization of Rescoring Process**: The generalized process for rescoring has been more clearly articulated, outlining the steps a query takes from initial execution through rescoring. This clarity aids users in understanding the flow and how to leverage rescoring effectively to enhance the relevance of their search results.

Overall, this extensive revision provides a more comprehensive and user-friendly approach to implementing and managing quantization in Azure AI Search, significantly impactful for users looking to optimize their search solutions.

## articles/search/vector-search-how-to-storage-options.md{#item-ee1680}

<details>
<summary>Diff</summary>
````diff
@@ -9,41 +9,44 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 11/19/2024
+ms.date: 03/31/2025
 ---
 
 # Eliminate optional vector instances from storage
 
 Azure AI Search stores multiple copies of vector fields that are used in specific workloads. If you don't need to support a specific behavior, like returning raw vectors in a query response, you can set properties in the index that omit storage for that workload.
 
+Removing storage is irreversible and requires reindexing if you want it back.
+
 ## Prerequisites
 
-- [Vector fields in a search index](vector-search-how-to-create-index.md) with a `vectorSearch` configuration, using the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (KNN) algorithms and a new vector profile.
+- [Vector fields in a search index](vector-search-how-to-create-index.md), with a `vectorSearch` configuration specifying either the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (KNN) algorithm, and a new vector profile.
 
 ## How vector fields are stored
 
-For every vector field, there could be three copies of the vectors, each serving a different purpose:
+For every vector field, there are up to three copies of the vectors, each serving a different purpose:
 
 | Instance | Usage | Controlled using |
-|----------|-------|------------|
-| Source vectors which store the JSON that was received during document indexing (JSON data) | Used for incremental data refresh with `merge` or `mergeOrUpload` during document indexing. Also used if you want "retrievable" vectors returned in the query response. | `stored` property on vector fields |
-| Original full-precision vectors (binary data) | In existing indexes, these are used for internal index operations and for exhaustive KNN search. For vectors using compression, it's also used for rescoring (if enabled) on an oversampled candidate set of results from ANN search on vector fields using [scalar or binary quantization](vector-search-how-to-quantization.md) compression. | `rescoringOptions.rescoreStorageMethod` property in `vectorSearch.compressions`. For *uncompressed* vector fields on indexes created with `2024-11-01-Preview` API versions and later, this will be omitted by default with no impact on search activities nor quality. |
-| Vectors in the [HNSW graph for Approximate Nearest Neighbors (ANN) search](vector-search-overview.md) (HNSW graph) | Used for ANN query execution. Consists of either full-precision vectors (when no compression is applied) or quantized vectors (when compression is applied) | Only applies to HNSW. These data structures are required for efficient ANN search. |
+|----------|-------|------------------|
+| Source vectors received during document indexing (JSON data) | Used for incremental data refresh with `merge` or `mergeOrUpload` indexing action. Also used to return "retrievable" vectors in the query response. | `stored` property on vector fields |
+| Original full-precision vectors (binary data) | Used for internal index operations and for exhaustive KNN search in older API versions. For compressed vectors, it's also used for `preserveOriginals` rescoring on an oversampled candidate set of results from ANN search. This applies to vector fields that undergo [scalar or binary quantization](vector-search-how-to-quantization.md). | `rescoringOptions.rescoreStorageMethod` property in `vectorSearch.compressions`. |
+| Vectors in the [HNSW graph for Approximate Nearest Neighbors (ANN) search](vector-search-overview.md) (HNSW graph) or vectors for exhaustive K Nearest Neighbors (eKNN index) | Used for query execution. Consists of either full-precision vectors (when no compression is applied) or quantized vectors. | Essential. There are no parameters for removing this instance. |
+
+You can set properties that permanently discard the first two instances (JSON data and binary data) from vector storage, but not the last instance.
 
-You can set properties that permanently discard the first two instances (JSON data and binary data) from vector storage.
+To offset lossy compression for HNSW, you can keep the second instance (binary data) for rescoring purposes to improve ANN search quality. For eKNN, only scalar quantization is supported, and rescoring isn't an option. In newer API versions like the latest preview, the second instance isn't kept for eKNN because the third instance provides full-precision vectors in an eKNN index.
 
-The last instance (HNSW graph) is required for ANN vector query execution. If any compression techniques such as [scalar or binary quantization](vector-search-how-to-quantization.md) are used, they are applied to this set of data. If you want to offset lossy compression, you should keep the second instance (binary data) for rescoring purposes to improve ANN search quality.
+### Indexes created with 2024-11-01-preview or later API versions
 
-### Indexes created on or after 2024-11-01-preview API version
-For indexes created with the 2024-11-01-preview API version with uncompressed vector fields, the second and third instances (binary data and HNSW graph) are combined as part of our cost reduction investments, reducing overall storage. The same index created with the 2024-11-01-preview API is functionally equivalent but uses less storage compared to identical indexes created with earlier API versions. Physical data structures are established on a Create Index request, so you must delete and recreate the index to realize the storage reductions.
+For indexes created with the 2024-11-01-preview or a later API with uncompressed vector fields, the second and third instances (binary data and HNSW graph) are combined as part of our cost reduction investments, reducing overall storage. A newer generation index with consolidated vectors is functionally equivalent to older indexes, but uses less storage. Physical data structures are established on a Create Index request, so you must delete and recreate the index to realize the storage reductions.
 
-If you choose to use [vector compression](vector-search-how-to-configure-compression-storage.md), we compress (quantize) the in-memory portion of the vector index. Since memory is often a primary constraint for vector indexes, this allows storing more vectors within the same search service. However, lossy compression results in some information loss, which can impact search quality.
+If you choose [vector compression](vector-search-how-to-configure-compression-storage.md), AI Search compresses (quantizes) the in-memory portion of the vector index. Since memory is often a primary constraint for vector indexes, this practice allows you to store more vectors within the same search service. However, lossy compression equates to less information in the index, which can affect search quality.
 
-To mitigate this, enabling "rescoring" and "oversampling" helps maintain accuracy. This retrieves a larger set of candidate documents from the compressed index and then recomputes similarity scores using the original vectors, which must be retained in storage. As a result, while quantization reduces memory usage (vector index size usage), it slightly increases storage requirements since both compressed and original vectors are stored. The additional storage is approximately equal to the size of the compressed index.
+To mitigate the loss in information, you can [enable "rescoring" and "oversampling" options](vector-search-how-to-quantization.md#recommended-rescoring-techniques) to help maintain quality. The effect is retrieval of a larger set of candidate documents from the compressed index, with recomputation of similarity scores using the original vectors or the dot product. For rescoring to work, original vectors must be retained in storage for certain scenarios. As a result, while quantization reduces memory usage (vector index size usage), it slightly increases storage requirements since both compressed and original vectors are stored. The extra storage is approximately equal to the size of the compressed index.
 
-## Set the `stored` property
+## Remove source vectors (JSON data)
 
-The `stored` property is a boolean property on a vector field definition that determines whether storage is allocated for retrievable vector field content (the source instance). The `stored` property is true by default. If you don't need raw vector content in a query response, you can save up to 50 percent storage per field by changing `stored` to false.
+The `stored` property is a boolean property on a vector field definition that determines whether storage is allocated for retrievable vector field content obtained during indexing (the source instance). The `stored` property is true by default. If you don't need raw vector content in a query response, you can save up to 50 percent storage per field by changing `stored` to false.
 
 Considerations for setting `stored` to false:
 
@@ -52,7 +55,7 @@ Considerations for setting `stored` to false:
 - However, if your indexing strategy includes [partial document updates](search-howto-reindex.md#update-content), such as "merge" or "mergeOrUpload" on an existing document, setting `stored=false` prevents content updates to those fields during the merge. On each "merge" or "mergeOrUpload" operation to a search document, you must provide the vector fields in its entirety, along with the nonvector fields that you're updating, or the vector is dropped.
 
 > [!IMPORTANT]
-> Setting the `stored=false` attribution is irreversible. This property can only be set when you create the index and is only allowed on vector fields. Updating an existing index with new vector fields cannot set this property to `false`. If you want retrievable vector content later, you must drop and rebuild the index, or create and load a new field that has the new attribution.
+> Setting the `stored=false` attribution is irreversible. This property can only be set when you create the index and is only allowed on vector fields. Updating an existing index with new vector fields can't set this property to `false`. If you want retrievable vector content later, you must drop and rebuild the index, or create and load a new field that has the new attribution.
 
 For new vector fields in a search index, set `stored` to false to permanently remove retrievable storage for the vector field. The following example shows a vector field definition with the `stored` property.
 
@@ -86,31 +89,44 @@ PUT https://[service-name].search.windows.net/indexes/demo-index?api-version=202
 
 - Defaults are `stored` set to true and `retrievable` set to false. In a default configuration, a retrievable copy is stored, but it's not automatically returned in results. When `stored` is true, you can toggle `retrievable` between true and false at any time without having to rebuild an index. When `stored` is false, `retrievable` must be false and can't be changed.
 
-## Set the `rescoreStorageMethod` property
+## Remove full-precision vectors (binary data)
 
 [!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
 
-The `rescoreStorageMethod` property controls the storage of full-precision vectors when compression is used.
+When you compress vectors using either scalar or binary quantization, query execution is over the quantized vectors. In this case, you only need the original full-precision vectors (binary data) if you want to rescore.
 
-For *uncompressed* vector fields on indexes created with `2024-11-01-Preview` API versions and later, this will be omitted by default with no impact on search activities nor quality. For existing vector fields created prior to this API version, there is no in-place ability to remove this copy of data.
+If you use newer preview APIs *and* binary quantization, you can safely discard full-precision vectors because rescoring strategies now use the dot product of a binary embedding, which produces high quality search results, without having to reference full-precision vectors in the index.
 
-On a vector compression, the `rescoreStorageMethod` property is set to `preserveOriginals` by default, which retains full-precision vectors for[oversampling and rescoring capabilities](vector-search-how-to-quantization.md#add-compressions-to-a-search-index) to reduce the effect of lossy compression on the HNSW graph. If you don't use these capabilities, you can reduce vector storage by setting `rescoreStorageMethod` to `discardOriginals`.
+The `rescoreStorageMethod` property controls whether full-precision vectors are stored. The guidance for whether to retain full-precision vectors is:
 
-> [!IMPORTANT]
-> Setting the `rescoreStorageMethod` property is irreversible and will have different levels of search quality loss depending on the compression method. This can be set on indexes created with `2024-11-01-Preview` or later, either during index creation or adding new vector fields.
+- For scalar quantization, preserve original full-precision vectors in the index because they're required for rescore.
+- For binary quantization, preserve original full-precision vectors for the highest quality of rescoring, or discard full-precision vectors (requires 2025-03-01-preview) if you want to rescore based on the dot product of the binary embeddings.
+
+Vector storage strategies have been evolving over the last several releases. Index creation date and API version determine your storage options. 
+
+| API version | Applies to | Remove full-precision vectors |
+|-------------|-------------------------------|
+| 2024-07-01 and earlier | Not applicable. | There's no mechanism for removing full-precision vectors. |
+| 2024-11-01-preview | Binary embeddings | Use `rescoreStorageMethod.discardOriginals` to remove full-precision vectors, but doing so prevents rescoring. `enableRescoring` must be false if originals are gone.|
+| 2025-03-01-preview | Binary embeddings | Use `rescoreStorageMethod.discardOriginals` to remove full-precision vectors in the index while still retaining rescore options. In this preview, rescoring is possible because the technique changed. The dot product of the binary embeddings is used on the rescore, producing high quality search results equivalent to or better than earlier techniques based on full-precision vectors. |
 
-If you intend to use scalar or binary quantization, we recommend retaining `rescoreStorageMethod` set to `preserveOriginals` to maximize search quality.
+Notice that scalar isn't listed in the table. If you use scalar quantization, you must retain original full-precision vectors if you want to rescore.
+
+In `vectorSearch.compressions`, the `rescoreStorageMethod` property is set to `preserveOriginals` by default, which retains full-precision vectors for [oversampling and rescoring capabilities](vector-search-how-to-quantization.md#add-compressions-to-a-search-index) to reduce the effect of lossy compression on the HNSW graph. If you don't need full-precision vectors, you can reduce vector storage by setting `rescoreStorageMethod` to `discardOriginals`.
+
+> [!IMPORTANT]
+> Setting the `rescoreStorageMethod` property is irreversible and can adversely affect search quality, although the degree depends on the compression method and any mitigations you apply.
 
 To set this property:
 
-1. Use [Create Index](/rest/api/searchservice/indexes/create?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or [Create or Update Index 2024-11-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) REST APIs, or an Azure SDK beta package providing the feature.
+1. Use [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2025-03-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true) REST APIs, or an Azure SDK beta package providing the feature.
 
 1. Add a `vectorSearch` section to your index with profiles, algorithms, and compressions.
 
-1. Under compressions, add `rescoringOptions` with `enableRescoring` set to true, `defaultOversampling` set to a positive integer, and `rescoreStorageMethod` set to `preserveOriginals`.
+1. Under `vectorSearch.compressions`, add `rescoringOptions` with `enableRescoring` set to true, `defaultOversampling` set to a positive integer, and `rescoreStorageMethod` set to `discardOriginals` for binary quantization and `preserveOriginals` for scalar quantization.
 
     ```http
-    PUT https://[service-name].search.windows.net/indexes/demo-index?api-version=2024-11-01-preview
+    PUT https://[service-name].search.windows.net/indexes/demo-index?api-version=2025-03-01-preview
     
     {
         "name": "demo-index",
@@ -119,9 +135,14 @@ To set this property:
         "vectorSearch": {
             "profiles": [
                 {
-                "name": "myVectorProfile",
+                "name": "myVectorProfile-1",
                 "algorithm": "myHnsw",
                 "compression": "myScalarQuantization"
+                },
+                {
+                "name": "myVectorProfile-2",
+                "algorithm": "myHnsw",
+                "compression": "myBinaryQuantization"
                 }
             ],
             "algorithms": [
@@ -150,6 +171,16 @@ To set this property:
                         "quantizedDataType": "int8"
                     },
                     "truncationDimension": null
+                },
+                {
+                    "name": "myBinaryQuantization",
+                    "kind": "binaryQuantization",
+                    "rescoringOptions": {
+                        "enableRescoring": true,
+                        "defaultOversampling": 10,
+                        "rescoreStorageMethod": "discardOriginals"
+                    },
+                    "truncationDimension": null
                 }
             ]
         }
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Revision of Storage Options for Vector Fields in Azure AI Search"
}
```

### Explanation
The document detailing storage options for vector fields in Azure AI Search has been updated to provide clearer instructions and enhancements on managing vector storage effectively. Key modifications in this version focus on the impact of storage decisions, best practices, and the implications of various properties.

1. **Date Update**: The original publication date has been updated from November 19, 2024, to March 31, 2025, ensuring the documentation is current and reflects the latest guidance.

2. **Clarity on Irreversibility of Storage Decisions**: New content emphasizes that removing storage for vector instances is irreversible and necessitates reindexing if retrieval options need to be restored, providing users with clearer warnings about the implications of their choices.

3. **Detailed Prerequisites and Vector Storage Explanation**: The prerequisites section has been enhanced for clarity by rephrasing and specifying the requirements more rigorously. It now includes improved descriptions of the three potential copies of vector data (source vectors, original full-precision vectors, and HNSW graph vectors), including their specific usage and how users can control their storage.

4. **Redundancy Eliminated**: The updated explanation removes redundant content regarding how properties can remove two instance types (JSON data and binary data) while keeping the HNSW graph, thereby streamlining information for better comprehension.

5. **New Indexing Guidelines**: Adjustments in guidance related to indexes created with newer API versions (2024-11-01-preview and later) elucidate how data structures for binary and graph vectors are combined for cost efficiency, reducing overall storage requirements, and the necessity of index recreation for such benefits.

6. **Improvements on Compressed Vector Handling**: The document details the handling of quantized vectors, emphasizing the impact of both scalar and binary quantization on storage needs and search quality. It introduces clearer guidance on using "rescoring" and "oversampling" to maintain result accuracy while receiving larger document sets.

7. **Refinements on the Stored and Rescore Storage Method Properties**: Clarifications enhance user understanding on the `stored` property, which determines whether vector content should be retrievable and outline the implications of setting the `rescoreStorageMethod` property for optimal efficiency. 

8. **Structure and Movement of Content**: The organization was improved to foster better navigation through the document, especially regarding the properties affecting storage and operational behaviors in Azure AI Search.

Overall, these changes reflect a commitment to clear, actionable guidelines for managing vector storage, enhancing the user experience for developers utilizing Azure AI Search.

## articles/search/vector-search-index-size.md{#item-bb2846}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 01/09/2025
+ms.date: 03/20/2025
 ---
 
 # Vector index size and staying under limits
@@ -24,7 +24,7 @@ For each vector field, Azure AI Search constructs an internal vector index using
 > [Vector optimization techniques](vector-search-how-to-configure-compression-storage.md) are now generally available. Use capabilities like narrow data types, scalar and binary quantization, and elimination of redundant storage to reduce your vector quota and storage quota consumption.
 
 > [!NOTE]
-> Not all algorithms consumes vector index size quota. Vector quotas are established based on memory requirements of approximate nearest neighbor search. Vector fields created with the Hierarchical Navigable Small World (HNSW) algorithm need to reside in memory during query execution because of the random-access nature of graph-based traversals. Vector fields using exhaustive KNN algorithm are loaded into memory dynamically in pages during query execution, and as a result do not consume vector quota.
+> Not all algorithms consume vector index size quota. Vector quotas are established based on memory requirements of approximate nearest neighbor search. Vector fields created with the Hierarchical Navigable Small World (HNSW) algorithm need to reside in memory during query execution because of the random-access nature of graph-based traversals. Vector fields using exhaustive KNN algorithm are loaded into memory dynamically in pages during query execution, and as a result do not consume vector quota.
 
 ## Key points about quota and vector index size
 
@@ -38,29 +38,11 @@ For each vector field, Azure AI Search constructs an internal vector index using
 
 If you aren't sure what your search service limits are, here are two ways to get that information:
 
-+ In the Azure portal, in the search service **Overview** page, both the **Properties** tab and **Usage** tab show partition size and storage, and also vector quota and vector index size.
++ In the Azure portal, on the search service **Overview** page, both the **Properties** tab and **Usage** tab show partition size and storage, and also vector quota and vector index size.
 
-+ In the Azure portal, in the **Scale** page, you can review the number and size of partitions.
++ In the Azure portal, on the **Scale** page, you can review the number and size of partitions.
 
-## How to check service creation date
-
-Newer services created after April 3, 2024 offer five to ten times more vector storage as older ones at the same tier billing rate. If your service is older, consider creating a new service and migrating your content.
-
-1. In Azure portal, open the resource group that contains your search service.
-
-1. On the leftmost pane, under **Settings**, select **Deployments**.
-
-1. Locate your search service deployment. If there are many deployments, use the filter to look for "search".
-
-1. Select the deployment. If you have more than one, click through to see if it resolves to your search service.
-
-    :::image type="content" source="media/vector-search-index-size/resource-group-deployments.png" lightbox="media/vector-search-index-size/resource-group-deployments.png" alt-text="Screenshot of a filtered deployments list.":::
-
-1. Expand deployment details. You should see *Created* and the creation date.
-
-   :::image type="content" source="media/vector-search-index-size/deployment-details.png" lightbox="media/vector-search-index-size/deployment-details.png" alt-text="Screenshot of the deployment details showing creation date.":::
-
-1. Now that you know the age of your search service, review the vector quota limits based on service creation: [Vector index size limits](search-limits-quotas-capacity.md#vector-index-size-limits).
+Your vector limit varies depending on your [service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date).
 
 ## How to get vector index size
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates on Vector Index Size Limitations and Guidelines"
}
```

### Explanation
The document concerning vector index size and maintaining compliance with limitations in Azure AI Search has been refined with several updates aimed at clarifying guidelines and improving user comprehension. The changes made include:

1. **Date Revision**: The last modified date has been changed from January 9, 2025, to March 20, 2025, ensuring that the information is timely and relevant.

2. **Clarification of Algorithm Behavior**: The note regarding algorithms and their impact on vector size quota has been slightly modified for grammatical accuracy, removing an unnecessary pluralization. This enhances clarity without changing the intended meaning, which explains how different algorithms interact with the vector quota system.

3. **Streamlined Instructions for Quota Review**: The instructions detailing how to check service limits within the Azure portal have been simplified. The phrasing is adjusted to enhance readability and conciseness, particularly in directions relating to the **Overview** and **Scale** pages.

4. **Removal of the Service Creation Check Section**: A section describing how to check the service creation date has been removed. This reduction suggests a shift towards simplification, possibly because users can now access this information through a single link reference, which consolidates the guidance into a more focused format.

5. **Simplified Reference to Vector Limits**: The modification incorporates a streamlined reference to the variability of vector limits based on the service creation date. This information remains essential for users to understand their storage capabilities without the previous lengthy explanation.

These updates reflect an effort to provide more concise, clear, and user-friendly documentation, which is crucial for users managing vector indexes within Azure AI Search. Overall, the changes enhance the document’s utility by focusing on the most relevant information and streamlining the structure for better navigation.

## articles/search/vector-search-overview.md{#item-56e5fa}

<details>
<summary>Diff</summary>
````diff
@@ -67,7 +67,7 @@ Azure AI Search supports [hybrid scenarios](hybrid-search-overview.md) that run
 
 Vector search is available as part of all Azure AI Search tiers in all regions at no extra charge.
 
-Newer services created after April 3, 2024 support [higher quotas for vector indexes](vector-search-index-size.md).
+Newer services created after April 3, 2024 support [higher quotas for vector indexes](vector-search-index-size.md). If you have an older service, you might be able to [upgrade your service](search-how-to-upgrade.md) for higher vector quotas.
 
 Vector search is available in:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Clarification on Vector Index Quotas and Service Upgrades"
}
```

### Explanation
The document providing an overview of vector search in Azure AI Search has been updated to improve clarity regarding indexing quotas and service upgrade options. The following changes were made:

1. **Expanded Information on Service Upgrades**: The statement about newer services supporting higher quotas for vector indexes has been enhanced to indicate that users with older service instances may have the opportunity to upgrade for higher vector quotas. This addition clarifies options available to users and enhances the understanding of service capabilities.

2. **Minimal Changes in Syntax**: The update comprises minor editing, where the structure of the sentence has been slightly rephrased while retaining the overall meaning. This editing fosters better readability without altering the content's intent.

These modifications aim to inform users more effectively about their options regarding vector indexes and the potential for upgrading their services, thus enhancing the overall utility of the document.

## articles/search/vector-store.md{#item-db9b8c}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: concept-article
-ms.date: 03/11/2025
+ms.date: 03/21/2025
 ---
 
 # Vector storage in Azure AI Search
@@ -160,11 +160,11 @@ The following screenshot shows an S1 service configured with one partition and o
 
 :::image type="content" source="media/vector-search-overview/usage-tiles-storage-vector-index.png" alt-text="Screenshot of usage tiles showing storage, vector index, and index count.":::
 
-Vector index limits and estimations are covered in [another article](vector-search-index-size.md), but two points to emphasize up front is that maximum storage varies by service tier, and also by when the search service was created. Newer same-tier services have significantly more capacity for vector indexes. For these reasons, take the following actions:
+Vector index limits and estimations are covered in [another article](vector-search-index-size.md), but two points to emphasize are that maximum storage varies by service tier and by when the search service was created. Newer same-tier services have significantly more capacity for vector indexes. For these reasons, take the following actions:
 
-+ [Check the deployment date of your search service](vector-search-index-size.md#how-to-check-service-creation-date). If it was created before April 3, 2024, consider creating a new search service for greater capacity.
++ [Check the deployment date of your search service](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date). If it was created before April 3, 2024, you might be able to [upgrade your service](search-how-to-upgrade.md) for greater capacity.
 
-+ [Choose a scalable tier](search-sku-tier.md) if you anticipate fluctuations in vector storage requirements. The Basic tier is fixed at one partition on older search services. Consider Standard 1 (S1) and above for more flexibility and faster performance, or create a new search service that uses higher limits and more partitions at every nillable tier.
++ [Choose a scalable tier](search-sku-tier.md) if you anticipate fluctuations in vector storage requirements. The Basic tier is fixed at one partition on older search services. Consider Standard 1 (S1) and above for more flexibility and faster performance. In the 2025-02-01-preview, you can also [switch from a lower tier to a higher tier](search-capacity-planning.md#change-your-pricing-tier).
 
 ## Basic operations and interaction
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Updates on Vector Storage Capacity and Service Management"
}
```

### Explanation
The document on vector storage in Azure AI Search has undergone several minor updates aimed at enhancing clarity and providing more comprehensive guidance to users. The changes made include:

1. **Date Update**: The "ms.date" has been changed from March 11, 2025, to March 21, 2025, ensuring users receive the most current information regarding the document's revision.

2. **Improved Clarity in Messaging**: The phrasing around vector index limits and service creation dates has been streamlined for better readability. The sentence structure has been improved to emphasize that the maximum storage is determined by both the service tier and the creation date of the search service.

3. **Adjusted Links for Service Assessment**: The reference for checking the deployment date of the search service has been updated to provide users the ability to assess if they are eligible for an upgrade instead of solely suggesting the creation of a new service. This reflects a more user-friendly approach for managing service limitations.

4. **Enhanced Information on Tier Flexibility**: Users are now informed of the option to switch from a lower to a higher tier in the 2025-02-01-preview, providing additional options for managing vector storage capabilities. This addition represents a proactive approach to help users optimize their configurations based on future needs.

5. **Consistent Terminology**: Minor adjustments were made to ensure consistency in terminology and punctuation throughout the document, thus enhancing overall coherence.

These modifications collectively aim to refine the document's usability by allowing clearer communication of critical information and providing actionable steps for users managing vector storage in Azure AI Search.

## articles/search/whats-new.md{#item-fa71b4}

<details>
<summary>Diff</summary>
````diff
@@ -7,156 +7,98 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: overview
-ms.date: 02/28/2025
+ms.date: 03/31/2025
 ms.custom:
   - references_regions
   - ignite-2024
 ---
 
 # What's new in Azure AI Search
 
-**Azure Cognitive Search is now Azure AI Search**. Learn about the latest updates to Azure AI Search functionality, docs, and samples.
+Learn about the latest updates to Azure AI Search functionality, docs, and samples.
 
 > [!NOTE]
 > Preview features are announced here, but we also maintain a [preview features list](search-api-preview.md) so you can find them in one place.
 
-## February 2025
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [**Customer-managed keys support for Managed HSM**](search-security-manage-encryption-keys.md) | Security | Use either Azure Key Vault or Azure Key Vault Managed HSM (Hardware Security Module) to store customer-managed keys for extra encryption of sensitive content. |
-
-## December 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [**RAG chat with Azure AI Search + Python**](https://azure.github.io/ai-app-templates/repo/azure-samples/azure-search-openai-demo/) | Template | An AI application template for building a RAG solution using Azure AI Search and Python. |
-
-## November 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [**Network security perimeter**](search-security-network-security-perimeter.md) | Security | Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. The Azure portal and the Management REST APIs in the [2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) can be used to view and reconcile network security perimeter configurations. |
-| [**Shared private link support for Azure AI service connections**](search-indexer-howto-access-private.md) | Security  | Connections to Azure AI for built-in skills processing can now be private using a shared private link on the connection. |
-| [**Rescoring options for compressed vectors**](/azure/search/vector-search-how-to-quantization?tabs=2024-11-01-preview%2Cquery-2024-07-01#add-compressions-to-a-search-index) | Relevance | You can set options to rescore with original vectors instead of compressed vectors. Applies to HNSW and exhaustive KNN vector algorithms, using binary and scalar compression. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**Store fewer vector instances**](vector-search-how-to-storage-options.md) | vector search | In vector compression scenarios, you can omit storage of full precision vectors if you don't need them for rescoring. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**Query rewrite in the semantic reranker**](semantic-how-to-query-rewrite.md) | Relevance | You can set options on a semantic query to rewrite the query input into a revised or expanded query that generates more relevant results from the L2 ranker. Available in the [Search Documents (2024-11-01-preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature.|
-| [**New semantic ranker models**](semantic-search-overview.md) | Relevance | Semantic ranker runs with improved models in all supported regions. There is no change to APIs or the Azure portal experience. |
-| [**Document Layout skill**](cognitive-search-skill-document-intelligence-layout.md) | Applied AI (skills) | A new skill used to analyze a document for structure and provide [structure-aware (paragraph) chunking](search-how-to-semantic-chunking.md). This skill calls Document Intelligence and uses the Document Intelligence layout model. Available in selected regions through the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature.|
-| [**Keyless billing for Azure AI skills processing**](cognitive-search-attach-cognitive-services.md) | Applied AI (skills) | You can now use a managed identity and roles for a keyless connection to Azure AI services for built-in skills processing. This capability removes restrictions for having both search and AI services in the same region. Available in the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**Markdown parsing mode**](search-how-to-index-markdown-blobs.md) | Indexer data source |  With this parsing mode, indexers can generate one-to-one or one-to-many search documents from Markdown files in Azure Storage and OneLake. Available in the [Create or Update Indexer (2024-11-01-preview)](/rest/api/searchservice/indexers/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**2024-11-01-preview**](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-11-01-preview&preserve-view=true) | API | Preview release of REST APIs for query rewrite, Document Layout skill, keyless billing for skills processing, Markdown parsing mode, and rescoring options for compressed vectors. |
-| [**Portal support for structured data**](search-get-started-portal-import-vectors.md) | Feature | The **Import and vectorize data** wizard now supports Azure SQL, Azure Cosmos DB, and Azure Table Storage.|
-
-## October 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Lower the dimension requirements for MRL-trained text embedding models on Azure OpenAI](vector-search-how-to-truncate-dimensions.md) | Feature | Text-embedding-3-small and Text-embedding-3-large are trained using Matryoshka Representation Learning (MRL). This allows you to truncate the embedding vectors to fewer dimensions, and adjust the balance between vector index size usage and retrieval quality. A new `truncationDimension` in the [2024-09-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true) enables access to MRL compression in text embedding models. This can only be configured for new vector fields. |
-| [Unpack `@search.score` to view subscores in hybrid search results](hybrid-search-ranking.md#unpack-a-search-score-into-subscores-preview) | Feature | You can investigate Reciprocal Rank Fusion (RRF) ranked results by viewing the individual query subscores of the final merged and scored result. A new `debug` property unpacks the search score. `QueryResultDocumentSubscores`, `QueryResultDocumentRerankerInput`, and `QueryResultDocumentSemanticField` provide the extra detail. These definitions are available in the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
-| [Target filters in a hybrid search to just the vector queries](hybrid-search-how-to-query.md#hybrid-search-with-filters-targeting-vector-subqueries-preview) | Feature | A filter on a hybrid query involves all subqueries on the request, regardless of type. You can override the global filter to scope the filter to a specific subquery. The new `filterOverride` parameter is available on hybrid queries using the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
-| [Text Split skill (token chunking)](cognitive-search-skill-textsplit.md) | Applied AI (skills) | This skill has new parameters that improve data chunking for embedding models. A new `unit` parameter lets you specify token chunking. You can now chunk by token length, setting the length to a value that makes sense for your embedding model. You can also specify the tokenizer and any tokens that shouldn't be split during data chunking. The new `unit` parameter and query subscore definitions are found in the [2024-09-01-preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
-| [2024-09-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-09-01-preview&preserve-view=true) | API | Preview release of REST APIs for truncated dimensions in text-embedding-3 models, targeted vector filtering for hybrid queries, RRF subscore details for debugging, and token chunking for Text Split skill.|
-| [Portal support for customer-managed key encryption (CMK)](search-security-manage-encryption-keys.md#step-4-encrypt-content) | Feature | When you create new objects in the Azure portal, you can now specify CMK-encryption and select an Azure Key Vault to provide the key. |
-
-## August 2024
+## March 2025
 
 | Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
 |-----------------------------|------|--------------|
-| [Debug Session improvements](cognitive-search-debug-session.md) | feature | There are two important improvements. First, you can now debug integrated vectorization and data chunking workloads. Second, Debug Sessions is redesigned for a more streamlined presentation of skills and mappings. You can select an object in the flow, and view or edit its details in a side panel. The previous tabbed layout is fully replaced with more context-sensitive information on the page. |
-| [2024-07-01](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-07-01&preserve-view=true) | API | Stable release of REST APIs for generally available vector data types, vector compression, and integrated vectorization during indexing and queries. |
-| [Integrated vectorization](vector-search-integrated-vectorization.md) | Feature | Announcing general availability. Skills-driven data chunking and embedding during indexing. |
-| [Vectorizers](vector-search-how-to-configure-vectorizer.md) | Feature  | Announcing general availability. Text-to-vector conversion during query execution. Both [Azure OpenAI vectorizer](vector-search-vectorizer-azure-open-ai.md) and [custom Web API vectorizer](vector-search-vectorizer-custom-web-api.md) are generally available. |
-| [AzureOpenAIEmbedding skill](cognitive-search-skill-azure-openai-embedding.md) | Feature | Announcing general availability. A skill type that calls an Azure OpenAI embedding model to generate embeddings during indexing.  |
-| [Index projections](index-projections-concept-intro.md) | Feature | Announcing general availability. A component of a skillset definition that defines the shape of a secondary index, supporting a one-to-many index pattern, where content from an enrichment pipeline can target multiple indexes. |
-| [Binary and Scalar quantization](vector-search-how-to-quantization.md)  | Feature | Announcing general availability. Compress vector index size in memory and on disk using built-in quantization. |
-| [Narrow data types](vector-search-how-to-assign-narrow-data-types.md) | Feature  | Announcing general availability. Assign a smaller data type on vector fields, assuming incoming data is of that data type. |
-| [Import and vectorize data wizard](search-get-started-portal-import-vectors.md) | Azure portal | Announcing general availability. A wizard that creates a full indexing pipeline that includes data chunking and vectorization. The wizard creates all necessary objects and configurations. This release adds wizard support for Azure Data Lake in Azure Storage.|
-| [stored property](vector-search-how-to-storage-options.md) | Feature  | Announcing general availability. Boolean that reduces storage of vector indexes by *not* storing retrievable vectors. |
-| [vectorQueries.Weight property](vector-search-how-to-query.md#vector-weighting) | Feature  | Announcing general availability. Specify the relative weight of each vector query in a search operation. |
-
-## July 2024
+| [Service upgrade (preview)](search-how-to-upgrade.md) | Service | Upgrade your search service to higher storage limits in your region. With a one-time upgrade, you no longer need to recreate your service. Available in [Upgrade Service (2025-02-01-preview)](/rest/api/searchmanagement/services/upgrade?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true) and the Azure portal. |
+| [Pricing tier change (preview)](search-capacity-planning.md#change-your-pricing-tier) | Service | Change the [pricing tier](search-sku-tier.md) of your search service. This provides flexibility to scale storage, increase request throughput, and decrease latency based on your needs. In this preview, you can only change between Basic and Standard (S1, S2, and S3) tiers. Available in [Update Service (2025-02-01-preview)](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#searchupdateservicewithsku) and the Azure portal. |
+| [Facet hierarchies, aggregations, and facet filters (preview)](search-faceted-navigation-examples.md) | Queries | New facet query parameters support nested facets. For numeric facetable fields, you can sum the values of each field. You can also specify filters on a facet to add inclusion or exclusion criteria. Available in [Search Documents (2025-03-01-preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and the Azure portal.|
+| [Rescore vector queries over binary quantization using full precision vectors (preview)](vector-search-how-to-quantization.md#recommended-rescoring-techniques) | Queries | For vector indexes that contain binary quantization, you can rescore query results using a full precision vector query. The query engine uses the dot product of the binary embeddings and the vector query for rescoring, which improves the quality of search results.  Set `enableRescoring` and `discardOriginals` to use this feature, and call the latest preview API version on the request.|
+| [Semantic ranker pre-release models (preview)](semantic-how-to-configure.md#opt-in-for-prerelease-semantic-ranking-models) | Index | Opt in to use pre-release semantic ranker models if one happens to be available in your region. Available in [Create or Update Index (2025-03-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview#semanticconfiguration&preserve-view=true).|
+| [Search Service REST 2025-03-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2025-03-01-preview&preserve-view=true) | REST | Public preview release of REST APIs for data plane operations. Adds support for multi-vector embeddings, hierarchical facets, facet aggregation, and facet filters. |
+| [Search Management 2025-02-01-preview](/rest/api/searchmanagement/management-api-versions?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true) | REST | Public review release of REST APIs for control plane operations. Adds support for in-place upgrade to higher capacity partitions, in-place upgrade to higher tiers, and Azure Confidential Compute. |
 
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Chat with your data](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator) | Accelerator| A solution accelerator for the RAG pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to create conversational search experiences. The code with sample data is available for use case scenarios such as financial advisor and contract review and summarization.|
-| [Conversational Knowledge Mining](https://github.com/microsoft/Customer-Service-Conversational-Insights-with-Azure-OpenAI-Services) | Accelerator| A solution accelerator built on top of Azure AI Search, Azure Speech and Azure OpenAI services that allows customers to extract actionable insights from post-contact center conversations. |
-| [Build your own copilot](https://github.com/microsoft/Build-your-own-AI-Assistant-Solution-Accelerator) | Accelerator| Create your own custom copilot solution that empowers [Client Advisor](https://github.com/microsoft/Build-your-own-copilot-Solution-Accelerator/blob/main/ClientAdvisor/README.md) to harness the power of generative AI across both structured and unstructured data. Help our customers to optimize daily tasks and foster better interactions with more clients.  |
-
-## June 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Image search in the Azure portal](search-get-started-portal-image-search.md) | Feature | Search explorer now supports image search. In a vector index that has vectorized image content, you can drop images into Search Explorer to query for a match.
-
-## May 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Higher capacity and more vector quota at every tier (same billing rate)](search-limits-quotas-capacity.md#service-limits) | Infrastructure | For most regions, partition sizes are now even larger for Standard 2 (S2), Standard 3 (S3), and Standard 3 High Density (S3 HD) for services created after April 3, 2024. To get the larger partitions, create a new service in a [region that provides newer infrastructure](search-region-support.md). <br><br>Storage Optimized tiers (L1 and L2) also have more capacity. L1 and L2 customers must create a new service to benefit from the higher capacity. There's no in-place upgrade at this time. <br><br>Extra capacity is now available in [more regions](search-limits-quotas-capacity.md#service-limits): Germany North​, Germany West Central​, South Africa North​, Switzerland West​, and Azure Government (Texas, Arizona, and Virginia).|
-| [OneLake integration (preview)](search-how-to-index-onelake-files.md) | Feature | New indexer for OneLake files and OneLake shortcuts. If you use Microsoft Fabric and OneLake for data access to Amazon Web Services (AWS) and Google data sources, use this indexer to import external data into a search index. This indexer is available through the Azure portal, the [2024-05-01-preview REST API](/rest/api/searchservice/data-sources/create-or-update?view=rest-searchservice-2024-05-01-preview&preserve-view=true), and Azure SDK beta packages. |
-| [Vector relevance](vector-search-how-to-query.md) <br>[hybrid query relevance](hybrid-search-how-to-query.md) | Feature | Four enhancements improve vector and hybrid search relevance. <br><br>First, you can now set thresholds on vector search results to exclude low-scoring results. <br><br>Second, changes in the query architecture apply scoring profiles at the end of the query pipeline for every query type. Document boosting is a common scoring profile, and it now works as expected on vector and hybrid queries.<br><br>Third, you can set [`MaxTextRecallSize` and `countAndFacetMode`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) in hybrid queries to control the quantity of BM25-ranked search results that flow into the hybrid ranking model. <br><br>Fourth, for vector and hybrid search, you can weight a vector query to have boost or diminish its importance in a multiquery request. |
-| [Binary vectors support](/rest/api/searchservice/supported-data-types) | Feature | `Collection(Edm.Byte)` is a new supported data type. This data type opens up integration with the [Cohere v3 binary embedding models](https://cohere.com/blog/int8-binary-embeddings) and custom binary quantization. Narrow data types lower the cost of large vector datasets. See [Index binary data for vector search](vector-search-how-to-index-binary-data.md) for more information.| 
-| [Azure AI Vision multimodal embeddings skill (preview)](cognitive-search-skill-vision-vectorize.md) | Skill | New skill that's bound to the [multimodal embeddings API of Azure AI Vision](/azure/ai-services/computer-vision/concept-image-retrieval). You can generate embeddings for text or images during indexing. This skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true).|
-| [Azure AI Vision vectorizer (preview)](vector-search-vectorizer-ai-services-vision.md) | Vectorizer | New vectorizer connects to an Azure AI Vision resource using the [multimodal embeddings API](/azure/ai-services/computer-vision/concept-image-retrieval) to generate embeddings at query time. This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
-| [Azure AI Foundry model catalog vectorizer (preview)](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) | Vectorizer | New vectorizer connects to an embedding model deployed from the [Azure AI Foundry model catalog](/azure/ai-foundry/how-to/model-catalog-overview). This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). <br><br>[**How to implement integrated vectorization using models from Azure AI Foundry**](vector-search-integrated-vectorization-ai-studio.md).|
-| [AzureOpenAIEmbedding skill (preview) supports more models on Azure OpenAI](cognitive-search-skill-azure-openai-embedding.md) | Skill | Now supports text-embedding-3-large and text-embedding-3-small, along with text-embedding-ada-002 from the previous update. New `dimensions` and `modelName` properties make it possible to specify the various embedding models on Azure OpenAI. Previously, the dimensions limits were fixed at 1,536 dimensions, applicable to text-embedding-ada-002 only. The updated skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true).|
-| Azure portal updates | Portal | [Import and vectorize data wizard](search-get-started-portal-import-vectors.md) now supports OneLake indexers as a data source. For embeddings, it also supports connections to Azure AI Vision multimodal, Azure AI Foundry model catalog, and more embedding models on Azure OpenAI. <br><br>When adding a field to an index, you can choose a [binary data type](vector-search-how-to-index-binary-data.md). <br><br>[Search explorer](search-explorer.md) now defaults to 2024-05-01-preview and supports the new preview features for vector and hybrid queries.  |
-| [2024-05-01-preview](/rest/api/searchservice/search-service-api-versions#2024-05-01-preview) | API | New preview version of the Search REST APIs provides new skills and vectorizers, new binary data type, OneLake files indexer, and new query parameters for more relevant results. See [Upgrade REST APIs](search-api-migration.md) if you have existing code written against the 2023-07-01-preview and need to migrate to this version.|
-| Azure SDK beta packages | API | Review the changelogs of the following Azure SDK beta packages for new feature support: [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/CHANGELOG.md), [Azure SDK for .NET](https://github.com/Azure/azure-sdk-for-net/blob/Azure.Search.Documents_11.6.0-beta.4/sdk/search/Azure.Search.Documents/CHANGELOG.md), [Azure SDK for Java](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/search/azure-search-documents/CHANGELOG.md) |
-| [Python code samples](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/readme.md)  | Samples | New end-to-end samples demonstrate [integration with Cohere Embed v3](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/community-integration/cohere/azure-search-cohere-embed-v3-sample.ipynb), [integration with OneLake and cloud data platforms on Google and AWS](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/e2e-demos/azure-ai-search-e2e-build-demo.ipynb), and [integration with Azure AI Vision multimodal APIs](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/embeddings/multimodal-embeddings/multimodal-embeddings.ipynb). |
-
-## April 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Security update addressing information disclosure](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2024-29063) | API | GET responses [no longer return connection strings or keys](search-api-migration.md#breaking-changes-for-client-code-that-reads-connection-information). Applies to GET Skillset, GET Index, and GET Indexer. This change helps protect your Azure assets integrated with AI Search from unauthorized access. |
-| [More storage on Basic and Standard tiers](search-limits-quotas-capacity.md#service-limits) | Infrastructure |  Basic now supports up to three partitions and three replicas. Basic and Standard (S1, S2, S3) tiers have significantly more storage per partition, at the same per-partition billing rate. Extra capacity is subject to [regional availability](search-limits-quotas-capacity.md#service-limits) and applies to new search services created after April 3, 2024. Currently, there's no in-place upgrade, so you must create a new search service to get the extra storage. |
-| [More quota for vectors](search-limits-quotas-capacity.md#vector-index-size-limits) | Infrastructure | Vector quotas are also higher on new services created after April 3, 2024 in selected regions. |
-| [Vector quantization, narrow vector data types, and a new `stored` property (preview)](vector-search-how-to-configure-compression-storage.md) | Feature | Collectively, these three features add vector compression and smarter storage options. First, *scalar quantization* reduces vector index size in memory and on disk. Second, [narrow data types](/rest/api/searchservice/supported-data-types) reduce per-field storage by storing smaller values. Third, you can use `stored` to opt-out of storing the extra copy of a vector that's used only for search results. If you don't need vectors in a query response, you can set `stored` to false to save on space. |
-| [2024-03-01-preview Search REST API](/rest/api/searchservice/search-service-api-versions#2024-03-01-preview) | API | New preview version of the Search REST APIs for the new data types, vector compression properties, and vector storage options. |
-| [2024-03-01-preview Management REST API](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true) | API | New preview version of the Management REST APIs for control plane operations.  |
-| [2023-07-01-preview deprecation announcement](/rest/api/searchservice/search-service-api-versions#2023-07-01-preview) | API | Deprecation announced on April 8, 2024. It becomes unsupported on July 8, 2024. This was the first REST API that offered vector search support. Newer API versions have a different vector configuration. You should [migrate to a newer version](search-api-migration.md) as soon as possible. |
-
-## February 2024
+## February 2025
 
 | Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
 |-----------------------------|------|--------------|
-| New dimension limits | Feature | For vector fields, maximum dimension limits are now `3072`, up from `2048`. |
+| [Customer-managed keys support for Managed HSM](search-security-manage-encryption-keys.md) | Security | Use either Azure Key Vault or Azure Key Vault Managed HSM (Hardware Security Module) to store customer-managed keys for extra encryption of sensitive content. |
 
-## 2023 announcements
+## 2024 announcements
 
 | Month | Type | Announcement |
 |-------|------|-------------|
-| November | Feature | [**Vector search, generally available**](vector-search-overview.md). The previous restriction on customer-managed keys (CMK) is now lifted. [Prefiltering](vector-search-how-to-query.md) and [exhaustive K-nearest neighbor algorithm](vector-search-ranking.md) are also now generally available. |
-| November | Feature | [**Semantic ranker, generally available**](semantic-search-overview.md)|
-| November | Feature | [**Integrated vectorization (preview)**](vector-search-integrated-vectorization.md) adds data chunking and text-to-vector conversions during indexing, and also adds text-to-vector conversions at query time. |
-| November | Feature | [**Import and vectorize data wizard (preview)**](search-get-started-portal-import-vectors.md) automates data chunking and vectorization. It targets the [2023-10-01-Preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-10-01-preview&preserve-view=true) REST API. | 
-| November | Feature | [**Index projections (preview)**](index-projections-concept-intro.md) defines the shape of a secondary index, used for a one-to-many index pattern, where content from an enrichment pipeline can target multiple indexes. | 
-| November | API | [**2023-11-01 Search REST API**](/rest/api/searchservice/search-service-api-versions#2023-11-01) is stable version of the Search REST APIs for [vector search](vector-search-overview.md) and [semantic ranking](semantic-how-to-query-request.md). See [Upgrade REST APIs](search-api-migration.md) for migration steps to generally available features.|
-| November | API | [**2023-11-01 Management REST API**](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2023-11-01&preserve-view=true) adds APIs that [enable or disable semantic ranker](/rest/api/searchmanagement/services/create-or-update#searchsemanticsearch). |
-| November | Skill | [**Azure OpenAI Embedding skill (preview)**](cognitive-search-skill-azure-openai-embedding.md) connects to a deployed embedding model on your Azure OpenAI resource to generate embeddings during skillset execution.|
-| November | Skill | [**Text Split skill (preview)**](cognitive-search-skill-textsplit.md) updated in [2023-10-01-Preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-10-01-preview&preserve-view=true) to support native data chunking. |
-| November | Video | [**How vector search and semantic ranking improve your GPT prompts**](https://www.youtube.com/watch?v=Xwx1DJ0OqCk) explains how hybrid retrieval gives you optimal grounding data for generating useful AI responses and enables search over both concepts and keywords. |
-| November | Sample | [**Role-based access control in Generative AI applications**](https://techcommunity.microsoft.com/t5/azure-ai-services-blog/access-control-in-generative-ai-applications-with-azure/ba-p/3956408) explains how to use Microsoft Entra ID and Microsoft Graph API to roll out granular user permissions on chunked content in your index. |
-| October | Sample  | [**"Chat with your data" solution accelerator**](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator). End-to-end RAG pattern that uses Azure AI Search as a retriever. It provides indexing, data chunking, and orchestration. |
-| October | Feature | [**Exhaustive  K-Nearest Neighbors (KNN)**](vector-search-overview.md#eknn) scoring algorithm for similarity search in vector space. Available in the 2023-10-01-Preview REST API only. |
-| October | Feature | [**Prefilters in vector search**](vector-search-how-to-query.md) evaluate filter criteria before query execution, reducing the amount of content that needs to be searched. Available in the 2023-10-01-Preview REST API only, through a new `vectorFilterMode` property on the query that can be set to `preFilter` (default) or `postFilter`, depending on your requirements. |
-| October | API | [**2023-10-01-Preview Search REST API**](/rest/api/searchservice/search-service-api-versions#2023-10-01-Preview), breaking changes the definition for [vector fields](vector-search-how-to-create-index.md) and [vector queries](vector-search-how-to-query.md).|
-| August | Feature | [**Enhanced semantic ranking**](semantic-search-overview.md).  Upgraded models are rolling out for semantic reranking, and availability is extended to more regions. Maximum unique token counts doubled from 128 to 256.|
-| July | Sample | [**Vector demo (Azure SDK for JavaScript)**](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-javascript/readme.md). Uses Node.js and the **@azure/search-documents 12.0.0-beta.2** library to generate embeddings, create and load an index, and run several vector queries. |
-| July | Sample | [**Vector demo (Azure SDK for .NET)**](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-dotnet/DotNetVectorDemo/readme.md).  Uses the **Azure.Search.Documents 11.5.0-beta.3** library to generate embeddings, create and load an index, and run several vector queries. You can also try [this sample](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/search/Azure.Search.Documents/samples/Sample07_VectorSearch.md) from the Azure SDK team.|
-| July | Sample | [**Vector demo (Azure SDK for Python)**](https://github.com/Azure/azure-search-vector-samples/tree/main/demo-python) Uses the latest beta release of the **azure.search.documents** to generate embeddings, create and load an index, and run several vector queries. Visit the [azure-search-vector-samples/demo-python](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python) repo for more vector search demos. |
-| June | Feature | [**Vector search public preview**](vector-search-overview.md). |
-| June | Feature | [**Semantic search availability**](semantic-search-overview.md), available on the Basic tier.|
-| June | API | [**2023-07-01-Preview Search REST API**](/rest/api/searchservice/index-preview). Support for vector search. |
-| May | Feature | [**Azure RBAC (role-based access control, generally available)**](search-security-rbac.md). |
-| May | API | [**2022-09-01 Management REST API**](/rest/api/searchmanagement), with support for configuring search to use Azure roles. The **Az.Search** module of Azure PowerShell and **Az search** module of the Azure CLI are updated to support search service authentication options. You can also use the [**Terraform provider**](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/search_service) to configure authentication options (see this [Terraform quickstart](search-get-started-terraform.md) for details). | 
-| April | Sample |  [**Multi-region deployment of Azure AI Search for business continuity and disaster recovery**](https://github.com/Azure-Samples/azure-search-multiple-regions). Deployment scripts that fully configure a multi-regional solution for Azure AI Search, with options for synchronizing content and request redirection if an endpoint fails. |
-| March |  Sample | [**ChatGPT + Enterprise data with Azure OpenAI and Azure AI Search (GitHub)**](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/README.md). Python code and a template for combining Azure AI Search with the large language models in OpenAI. For background, see this Tech Community blog post: [Revolutionize your Enterprise Data with ChatGPT](https://techcommunity.microsoft.com/t5/ai-applied-ai-blog/revolutionize-your-enterprise-data-with-chatgpt-next-gen-apps-w/ba-p/3762087). <br><br>Key points: <br><br>Use Azure AI Search to consolidate and index searchable content.</br> <br>Query the index for initial search results.</br> <br>Assemble prompts from those results and send to the gpt-35-turbo (preview) model in Azure OpenAI.</br> <br>Return a cross-document answer and provide citations and transparency in your customer-facing app so that users can assess the response.</br> |
+| December | Template | [RAG chat with Azure AI Search + Python](https://azure.github.io/ai-app-templates/repo/azure-samples/azure-search-openai-demo/). An AI application template for building a RAG solution using Azure AI Search and Python. |
+| November | Security | [Network security perimeter](search-security-network-security-perimeter.md).  Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. The Azure portal and the Management REST APIs in the [2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) can be used to view and reconcile network security perimeter configurations. |
+| November | Security | [Shared private link support for Azure AI service connections](search-indexer-howto-access-private.md). Connections to Azure AI for built-in skills processing can now be private using a shared private link on the connection. |
+| November | Relevance | [Rescoring options for compressed vectors](/azure/search/vector-search-how-to-quantization?tabs=2024-11-01-preview%2Cquery-2024-07-01#add-compressions-to-a-search-index). You can set options to rescore with original vectors instead of compressed vectors. Applies to HNSW and exhaustive KNN vector algorithms, using binary and scalar compression. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Vector search | [Store fewer vector instances](vector-search-how-to-storage-options.md). In vector compression scenarios, you can omit storage of full precision vectors if you don't need them for rescoring. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Relevance | [Query rewrite in the semantic reranker](semantic-how-to-query-rewrite.md). You can set options on a semantic query to rewrite the query input into a revised or expanded query that generates more relevant results from the L2 ranker. Available in the [Search Documents (2024-11-01-preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature.|
+| November | Relevance | [New semantic ranker models](semantic-search-overview.md). Semantic ranker runs with improved models in all supported regions. There's no change to APIs or the Azure portal experience. |
+| November | Applied AI (skills) | [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md). A new skill used to analyze a document for structure and provide [structure-aware (paragraph) chunking](search-how-to-semantic-chunking.md). This skill calls Document Intelligence and uses the Document Intelligence layout model. Available in selected regions through the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Applied AI (skills) | [Keyless billing for Azure AI skills processing](cognitive-search-attach-cognitive-services.md). You can now use a managed identity and roles for a keyless connection to Azure AI services for built-in skills processing. This capability removes restrictions for having both search and AI services in the same region. Available in the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Indexer data source | [Markdown parsing mode](search-how-to-index-markdown-blobs.md). With this parsing mode, indexers can generate one-to-one or one-to-many search documents from Markdown files in Azure Storage and OneLake. Available in the [Create or Update Indexer (2024-11-01-preview)](/rest/api/searchservice/indexers/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | API | [2024-11-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-11-01-preview&preserve-view=true). Preview release of REST APIs for query rewrite, Document Layout skill, keyless billing for skills processing, Markdown parsing mode, and rescoring options for compressed vectors. |
+| November | Feature | [Portal support for structured data](search-get-started-portal-import-vectors.md). The **Import and vectorize data** wizard now supports Azure SQL, Azure Cosmos DB, and Azure Table Storage. |
+| October | Feature | [Lower the dimension requirements for MRL-trained text embedding models on Azure OpenAI](vector-search-how-to-truncate-dimensions.md). Text-embedding-3-small and Text-embedding-3-large are trained using Matryoshka Representation Learning (MRL). This allows you to truncate the embedding vectors to fewer dimensions, and adjust the balance between vector index size usage and retrieval quality. A new `truncationDimension` in the [2024-09-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true) enables access to MRL compression in text embedding models. This can only be configured for new vector fields. |
+| October | Feature | [Unpack `@search.score` to view subscores in hybrid search results](hybrid-search-ranking.md#unpack-a-search-score-into-subscores-preview). You can investigate Reciprocal Rank Fusion (RRF) ranked results by viewing the individual query subscores of the final merged and scored result. A new `debug` property unpacks the search score. `QueryResultDocumentSubscores`, `QueryResultDocumentRerankerInput`, and `QueryResultDocumentSemanticField` provide the extra detail. These definitions are available in the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
+| October | Feature | [Target filters in a hybrid search to just the vector queries](hybrid-search-how-to-query.md#hybrid-search-with-filters-targeting-vector-subqueries-preview). A filter on a hybrid query involves all subqueries on the request, regardless of type. You can override the global filter to scope the filter to a specific subquery. The new `filterOverride` parameter is available on hybrid queries using the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
+| October | Applied AI (skills) | [Text Split skill (token chunking)](cognitive-search-skill-textsplit.md). This skill has new parameters that improve data chunking for embedding models. A new `unit` parameter lets you specify token chunking. You can now chunk by token length, setting the length to a value that makes sense for your embedding model. You can also specify the tokenizer and any tokens that shouldn't be split during data chunking. The new `unit` parameter and query subscore definitions are found in the [2024-09-01-preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
+| October | API | [2024-09-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-09-01-preview&preserve-view=true). Preview release of REST APIs for truncated dimensions in text-embedding-3 models, targeted vector filtering for hybrid queries, RRF subscore details for debugging, and token chunking for Text Split skill.|
+| October | Feature | [Portal support for customer-managed key encryption (CMK)](search-security-manage-encryption-keys.md#step-4-encrypt-content). When you create new objects in the Azure portal, you can now specify CMK-encryption and select an Azure Key Vault to provide the key. |
+| August | Feature | [Debug Session improvements](cognitive-search-debug-session.md). There are two important improvements. First, you can now debug integrated vectorization and data chunking workloads. Second, Debug Sessions is redesigned for a more streamlined presentation of skills and mappings. You can select an object in the flow, and view or edit its details in a side panel. The previous tabbed layout is fully replaced with more context-sensitive information on the page. |
+| August | API | [2024-07-01](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-07-01&preserve-view=true). Stable release of REST APIs for generally available vector data types, vector compression, and integrated vectorization during indexing and queries. |
+| August | Feature | [Integrated vectorization](vector-search-integrated-vectorization.md), Announcing general availability. Skills-driven data chunking and embedding during indexing. |
+| August | Feature | [Vectorizers](vector-search-how-to-configure-vectorizer.md). Announcing general availability. Text-to-vector conversion during query execution. Both [Azure OpenAI vectorizer](vector-search-vectorizer-azure-open-ai.md) and [custom Web API vectorizer](vector-search-vectorizer-custom-web-api.md) are generally available. |
+| August | Feature | [AzureOpenAIEmbedding skill](cognitive-search-skill-azure-openai-embedding.md). Announcing general availability. A skill type that calls an Azure OpenAI embedding model to generate embeddings during indexing. |
+| August | Feature | [Index projections](index-projections-concept-intro.md). Announcing general availability. A component of a skillset definition that defines the shape of a secondary index, supporting a one-to-many index pattern, where content from an enrichment pipeline can target multiple indexes. |
+| August | Feature | [Binary and Scalar quantization](vector-search-how-to-quantization.md). Announcing general availability. Compress vector index size in memory and on disk using built-in quantization. |
+| August | Feature | [Narrow data types](vector-search-how-to-assign-narrow-data-types.md). Announcing general availability. Assign a smaller data type on vector fields, assuming incoming data is of that data type. |
+| August | Feature | [Import and vectorize data wizard](search-get-started-portal-import-vectors.md). Announcing general availability. A wizard that creates a full indexing pipeline that includes data chunking and vectorization. The wizard creates all necessary objects and configurations. This release adds wizard support for Azure Data Lake in Azure Storage. |
+| August | Feature | [stored property](vector-search-how-to-storage-options.md). Announcing general availability. Boolean that reduces storage of vector indexes by *not* storing retrievable vectors. |
+| August | Feature | [vectorQueries.Weight property](vector-search-how-to-query.md#vector-weighting). Announcing general availability. Specify the relative weight of each vector query in a search operation. |
+| July | Accelerator | [Chat with your data](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator). A solution accelerator for the RAG pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to create conversational search experiences. The code with sample data is available for use case scenarios such as financial advisor and contract review and summarization. |
+| July | Accelerator | [Conversational Knowledge Mining](https://github.com/microsoft/Customer-Service-Conversational-Insights-with-Azure-OpenAI-Services). A solution accelerator built on top of Azure AI Search, Azure Speech and Azure OpenAI services that allows customers to extract actionable insights from post-contact center conversations. |
+| July | Accelerator | [Build your own copilot](https://github.com/microsoft/Build-your-own-AI-Assistant-Solution-Accelerator). Create your own custom copilot solution that empowers [Client Advisor](https://github.com/microsoft/Build-your-own-copilot-Solution-Accelerator/blob/main/ClientAdvisor/README.md) to harness the power of generative AI across both structured and unstructured data. Help our customers to optimize daily tasks and foster better interactions with more clients. |
+| June | Feature | [Image search in the Azure portal](search-get-started-portal-image-search.md). Search explorer now supports image search. In a vector index that contains vectorized image content, you can drop images into Search Explorer to query for a match. |
+| May | Service limits| [Higher capacity and more vector quota at every tier (same billing rate)](search-limits-quotas-capacity.md#service-limits). For most regions, partition sizes are now even larger for Standard 2 (S2), Standard 3 (S3), and Standard 3 High Density (S3 HD) for services created after April 3, 2024. To get the larger partitions, create a new service in a [region that provides newer infrastructure](search-region-support.md). <br><br>Storage Optimized tiers (L1 and L2) also have more capacity. L1 and L2 customers must create a new service to benefit from the higher capacity. There's no in-place upgrade at this time. <br><br>Extra capacity is now available in [more regions](search-limits-quotas-capacity.md#service-limits): Germany North​, Germany West Central​, South Africa North​, Switzerland West​, and Azure Government (Texas, Arizona, and Virginia). |
+| May | Feature | [OneLake integration (preview)](search-how-to-index-onelake-files.md). New indexer for OneLake files and OneLake shortcuts. If you use Microsoft Fabric and OneLake for data access to Amazon Web Services (AWS) and Google data sources, use this indexer to import external data into a search index. This indexer is available through the Azure portal, the [2024-05-01-preview REST API](/rest/api/searchservice/data-sources/create-or-update?view=rest-searchservice-2024-05-01-preview&preserve-view=true), and Azure SDK beta packages. |
+| May | Feature | [Vector relevance](vector-search-how-to-query.md) <br>[hybrid query relevance](hybrid-search-how-to-query.md). Four enhancements improve vector and hybrid search relevance. <br><br>First, you can now set thresholds on vector search results to exclude low-scoring results. <br><br>Second, changes in the query architecture apply scoring profiles at the end of the query pipeline for every query type. Document boosting is a common scoring profile, and it now works as expected on vector and hybrid queries.<br><br>Third, you can set [`MaxTextRecallSize` and `countAndFacetMode`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) in hybrid queries to control the quantity of BM25-ranked search results that flow into the hybrid ranking model. <br><br>Fourth, for vector and hybrid search, you can weight a vector query to have boost or diminish its importance in a multiquery request. |
+| May | Feature | [Binary vectors support](/rest/api/searchservice/supported-data-types). `Collection(Edm.Byte)` is a new supported data type. This data type opens up integration with the [Cohere v3 binary embedding models](https://cohere.com/blog/int8-binary-embeddings) and custom binary quantization. Narrow data types lower the cost of large vector datasets. See [Index binary data for vector search](vector-search-how-to-index-binary-data.md) for more information. |
+| May | Skill | [Azure AI Vision multimodal embeddings skill (preview)](cognitive-search-skill-vision-vectorize.md). New skill that's bound to the [multimodal embeddings API of Azure AI Vision](/azure/ai-services/computer-vision/concept-image-retrieval). You can generate embeddings for text or images during indexing. This skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
+| May | Vectorizer | [Azure AI Vision vectorizer (preview)](vector-search-vectorizer-ai-services-vision.md). New vectorizer connects to an Azure AI Vision resource using the [multimodal embeddings API](/azure/ai-services/computer-vision/concept-image-retrieval) to generate embeddings at query time. This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
+| May | Vectorizer | [Azure AI Foundry model catalog vectorizer (preview)](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md). New vectorizer connects to an embedding model deployed from the [Azure AI Foundry model catalog](/azure/ai-foundry/how-to/model-catalog-overview). This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). <br><br>[**How to implement integrated vectorization using models from Azure AI Foundry**](vector-search-integrated-vectorization-ai-studio.md).|
+| May | Skill | [AzureOpenAIEmbedding skill (preview) supports more models on Azure OpenAI](cognitive-search-skill-azure-openai-embedding.md). Now supports text-embedding-3-large and text-embedding-3-small, along with text-embedding-ada-002 from the previous update. New `dimensions` and `modelName` properties make it possible to specify the various embedding models on Azure OpenAI. Previously, the dimensions limits were fixed at 1,536 dimensions, applicable to text-embedding-ada-002 only. The updated skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
+| May | Portal | [Import and vectorize data wizard](search-get-started-portal-import-vectors.md) now supports OneLake indexers as a data source. For embeddings, it also supports connections to Azure AI Vision multimodal, Azure AI Foundry model catalog, and more embedding models on Azure OpenAI. <br><br>When adding a field to an index, you can choose a [binary data type](vector-search-how-to-index-binary-data.md). <br><br>[Search explorer](search-explorer.md) now defaults to 2024-05-01-preview and supports the new preview features for vector and hybrid queries. |
+| May | API | [2024-05-01-preview](/rest/api/searchservice/search-service-api-versions#2024-05-01-preview). New preview version of the Search REST APIs provides new skills and vectorizers, new binary data type, OneLake files indexer, and new query parameters for more relevant results. See [Upgrade REST APIs](search-api-migration.md) if you have existing code written against the 2023-07-01-preview and need to migrate to this version. |
+| May | API | Azure SDK beta packages. Review the changelogs of the following Azure SDK beta packages for new feature support: [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/CHANGELOG.md), [Azure SDK for .NET](https://github.com/Azure/azure-sdk-for-net/blob/Azure.Search.Documents_11.6.0-beta.4/sdk/search/Azure.Search.Documents/CHANGELOG.md), [Azure SDK for Java](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/search/azure-search-documents/CHANGELOG.md) |
+| May | Samples | [Python code samples](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/readme.md). New end-to-end samples demonstrate [integration with Cohere Embed v3](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/community-integration/cohere/azure-search-cohere-embed-v3-sample.ipynb), [integration with OneLake and cloud data platforms on Google and AWS](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/e2e-demos/azure-ai-search-e2e-build-demo.ipynb), and [integration with Azure AI Vision multimodal APIs](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/embeddings/multimodal-embeddings/multimodal-embeddings.ipynb). |
+| April | API | [Security update addressing information disclosure](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2024-29063). GET responses [no longer return connection strings or keys](search-api-migration.md#breaking-changes-for-client-code-that-reads-connection-information). Applies to GET Skillset, GET Index, and GET Indexer. This change helps protect your Azure assets integrated with AI Search from unauthorized access. |
+| April | API | [2024-03-01-preview Search REST API](/rest/api/searchservice/search-service-api-versions#2024-03-01-preview) |
+| April | API | [2024-03-01-preview Management REST API](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true) |
+| April | API | [2023-07-01-preview deprecation announcement](/rest/api/searchservice/search-service-api-versions#2023-07-01-preview). This version is no longer supported as of July 8, 2024. Newer API versions have a different vector configuration. You should [migrate to a newer version](search-api-migration.md) as soon as possible. |
+| April | Service limits | [Basic and Standard tiers](search-limits-quotas-capacity.md#service-limits) offer more storage per partition, at the same per-partition billing rate. Extra capacity is subject to [regional availability](search-limits-quotas-capacity.md#service-limits) and applies to new search services created after April 3, 2024. Basic now supports up to three partitions and three replicas. |
+| April | Service limits | [Vector quotas are higher](search-limits-quotas-capacity.md#vector-index-size-limits) on new services created after April 3, 2024 in selected regions. |
+| April | Feature | [Vector quantization, narrow vector data types, and a new `stored` property (preview)](vector-search-how-to-configure-compression-storage.md). Collectively, these three features minimize storage and costs.|
+| February | Feature | New dimension limits on vector fields. Maximum dimension limits are now `3072`, up from `2048`.|
 
 ## Previous year's announcements
 
++ [2023 announcements](/previous-versions/azure/search/search-whats-new-2023)
 + [2022 announcements](/previous-versions/azure/search/search-whats-new-2022)
 + [2021 announcements](/previous-versions/azure/search/search-whats-new-2021)
 + [2020 announcements](/previous-versions/azure/search/search-whats-new-2020)
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "Major Rewrite and Update of What's New in Azure AI Search"
}
```

### Explanation
The document detailing the latest updates in Azure AI Search has been significantly revised, updating not only the content but also its structure and presentation. Here are the main details regarding the modifications made:

1. **Date Update**: The publication date has been updated from February 28, 2025, to March 31, 2025, reflecting the document's current relevance.

2. **Removal of Specific Monthly Updates**: The structure has shifted from providing detailed monthly updates for earlier months (e.g., February, December, November, etc.) to a focus on a broader section titled "March 2025". This change eliminates previously established sections for each month and consolidates the information into periods that now highlight major updates.

3. **Enhanced Content Overview**: The introductory section now provides a summary of the changes rather than detailing each month's feature independently. This makes the document more concise, focusing on important functionalities without overwhelming users with excessive detail.

4. **Expanded List of Features and Updates**: A total of 67 additions have been made, incorporating numerous new features, enhancements, and conditional capabilities, including:
   - New skills, such as the Document Layout skill for indexing structured documents.
   - New functionalities for vector search, offering better compression and dynamic query handling.
   - Support for customer-managed keys to enhance security.
   - The announcement of various newly available preview features to improve integration and user convenience.

5. **Streamlined Structure**: The document has an overall clearer and more professional layout, making it easier for readers to navigate through the updates and find critical information quickly.

6. **Headings and Formatting Adjustments**: The headings have been revised to better match the new content structure, facilitating easier scanning and understanding of each feature or update.

Ultimately, these changes represent a major overhaul aimed at making the document more user-friendly and relevant, promoting better understanding of the significant improvements and features within the Azure AI Search environment while also shifting from a monthly to a more thematic organizational approach.


