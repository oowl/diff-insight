---
date: '2025-04-10'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:24b31df...MicrosoftDocs:a9279bd
summary: |-
  此次代码更新涉及多个文档的修改，主要包括格式调整、内容更新和新功能的支持。更新的重点包括：

  1. 新增了一些图片文件，提高了文档的直观性和可读性。
  2. 对语义排名、语义查询和量化相关文档进行了重大更新，新增了支持预发布模型及量化技术的功能说明。
  3. 重构了`whats-new.md`文档，以优化信息的准确性和用户查阅体验。

  此外，该更新还包含了一些破坏性改变，特别是在`whats-new.md`中的内容重组。多个文档在格式和措辞上进行细微调整，以提升可读性和一致性，并更新了一些API版本，以确保与最新功能相一致。

  总体来说，这次更新旨在提升Azure AI Search相关文档的质量，提供更完整的内容和以用户为中心的体验，尤其是在引入视觉信息和更新新功能方面，帮助用户更好地理解和使用系统。
title: '[zh_CN] Diff Insight Report - search'

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:24b31df...MicrosoftDocs:a9279bd){target="_blank"}

# Highlights

这次代码更新涉及多个文档的修改，包括格式调整、内容更新、新功能支持等。这些更新主要集中在如下几个方面：

## New features

- 在`articles/search/media`等目录中新增了一些图片文件，以提高文档的直观性和可读性。
- 对语义排名、语义查询和量化等相关文档进行了重大更新，新增了支持预发布语义排名模型及量化技术的功能说明。
- `whats-new.md`文档进行了重构，以优化信息的准确性和用户查阅体验。

## Breaking changes

- `whats-new.md`文档的重构带来了破坏性改变，主要是通过内容重组和新的链接合并来提供最新功能和服务的概述。

## Other updates

- 多个文档进行了格式和措辞的细微调整，以提升整体的可读性和一致性。
- 更新了一些API版本，确保文档内容与最新的服务和API功能相一致。
- 在文档中补充了一些重要的说明和注意事项，帮助用户更好地理解Azure AI Search的相关功能。

# Insights

这次更新主要是为了提升Azure AI Search相关文档的整体质量，包括内容的完整性、最新功能的覆盖、以及用户体验的优化。特别是新增图片和详细说明的引入，使得技术文档对此类复杂操作的描述变得更加直观，便于用户更快地上手和应用。

- **视觉信息的引入**：通过在关键文档中增加图片和图示，用户可以更直观地理解操作步骤和流程，这对于初学者尤其重要。

- **新功能的支持**：通过对语义排名、量化技术等功能的更新，文档为用户提供了探索最新功能的指导，使得Azure AI Search在技术发展中的最新阵地能被更好地应用。

- **文档结构优化**：通过对多个文档的结构和内容进行调整，不仅提高了查阅的效率，也使得用户能更迅速地找到相应的信息，这点在`whats-new.md`的重构中得到了极大的体现。

这样的更新显示了Azure团队致力于持续改善文档质量，使用户能够及时获得最新功能信息，并以更高效的方式使用Azure AI Search服务。这种细节的优化，对于提升开发者体验和生产力至关重要。

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [.openpublishing.redirection.search.json](#item-8b66f9) | minor update | 更新 JSON 文件的格式: 结束换行符 | modified | 1 | 1 | 2 | 
| [cognitive-search-attach-cognitive-services.md](#item-68eaec) | minor update | 更新说明以包含额外信息 | modified | 1 | 1 | 2 | 
| [cognitive-search-skill-document-intelligence-layout.md](#item-62c06f) | minor update | 更新文档以反映日期和支持区域 | modified | 8 | 7 | 15 | 
| [cognitive-search-tutorial-blob-dotnet.md](#item-ba889d) | minor update | 更新标题格式和日期 | modified | 13 | 13 | 26 | 
| [cognitive-search-tutorial-blob.md](#item-ff148f) | minor update | 更新标题格式和发布日期 | modified | 10 | 10 | 20 | 
| [cognitive-search-tutorial-debug-sessions.md](#item-7e10e9) | minor update | 标题格式和发布日期更新 | modified | 27 | 27 | 54 | 
| [preview-generic.md](#item-51bbcc) | minor update | 更新文件标题、描述和发布日期 | modified | 4 | 4 | 8 | 
| [add-two-each.png](#item-56b26e) | new feature | 新增图片文件 add-two-each.png | added | 0 | 0 | 0 | 
| [change-pricing-tier.png](#item-f71721) | new feature | 新增图片文件 change-pricing-tier.png | added | 0 | 0 | 0 | 
| [initial-values.png](#item-465304) | new feature | 新增图片文件 initial-values.png | added | 0 | 0 | 0 | 
| [portal-notifications.png](#item-4bd098) | new feature | 新增图片文件 portal-notifications.png | added | 0 | 0 | 0 | 
| [pricing-tier-list.png](#item-ff6b12) | new feature | 新增图片文件 pricing-tier-list.png | added | 0 | 0 | 0 | 
| [provisioning-status.png](#item-ede201) | new feature | 新增图片文件 provisioning-status.png | added | 0 | 0 | 0 | 
| [updating-message.png](#item-fc9f1b) | new feature | 新增图片文件 updating-message.png | added | 0 | 0 | 0 | 
| [portal-add-facetable-field.png](#item-d0b7a4) | new feature | 新增图片文件 portal-add-facetable-field.png | added | 0 | 0 | 0 | 
| [portal-facet-query.png](#item-57be1f) | new feature | 新增图片文件 portal-facet-query.png | added | 0 | 0 | 0 | 
| [search-data-source.png](#item-e559ff) | new feature | 新增图片文件 search-data-source.png | added | 0 | 0 | 0 | 
| [service-creation-upgrade-metadata.png](#item-d1251d) | new feature | 新增图片文件 service-creation-upgrade-metadata.png | added | 0 | 0 | 0 | 
| [upgrade-button.png](#item-894e31) | new feature | 新增图片文件 upgrade-button.png | added | 0 | 0 | 0 | 
| [upgrade-confirmation.png](#item-880793) | new feature | 新增图片文件 upgrade-confirmation.png | added | 0 | 0 | 0 | 
| [upgrade-panel.png](#item-0c9673) | new feature | 新增图片文件 upgrade-panel.png | added | 0 | 0 | 0 | 
| [assign-key-vault.png](#item-e19e19) | new feature | 新增图片文件 assign-key-vault.png | added | 0 | 0 | 0 | 
| [resource-training.md](#item-07788d) | minor update | 更新 resource-training.md 文档 | modified | 2 | 3 | 5 | 
| [search-api-migration.md](#item-07297b) | minor update | 更新 search-api-migration.md 文档 | modified | 2 | 2 | 4 | 
| [search-api-preview.md](#item-511f5d) | minor update | 更新 search-api-preview.md 文档 | modified | 9 | 4 | 13 | 
| [search-capacity-planning.md](#item-0dd6c9) | minor update | 更新 search-capacity-planning.md 文档 | modified | 80 | 32 | 112 | 
| [search-create-service-portal.md](#item-f90159) | minor update | 更新 search-create-service-portal.md 文档 | modified | 6 | 6 | 12 | 
| [search-faceted-navigation-examples.md](#item-2b1158) | new feature | 新增搜索的分面导航示例文档 | added | 721 | 0 | 721 | 
| [search-faceted-navigation.md](#item-f29d1e) | minor update | 更新 search-faceted-navigation.md 文档 | modified | 112 | 167 | 279 | 
| [search-faq-frequently-asked-questions.yml](#item-eab2ba) | minor update | 更新 Azure AI Search 常见问题解答文档 | modified | 24 | 7 | 31 | 
| [search-how-to-index-sql-database.md](#item-86d873) | minor update | 更新 SQL 数据库索引文档 | modified | 50 | 39 | 89 | 
| [search-how-to-large-index.md](#item-d34e42) | minor update | 更新 Azure AI Search 大规模索引文档 | modified | 4 | 4 | 8 | 
| [search-how-to-load-search-index.md](#item-a72573) | minor update | 更新 Azure AI Search 索引加载文档 | modified | 2 | 2 | 4 | 
| [search-how-to-semantic-chunking.md](#item-4a1d07) | minor update | 更新语义切块文档 | modified | 7 | 5 | 12 | 
| [search-how-to-upgrade.md](#item-990225) | new feature | 新增 Azure AI Search 服务升级指南 | added | 115 | 0 | 115 | 
| [search-howto-index-encrypted-blobs.md](#item-a7097a) | minor update | 更新索引加密 Blob 的教程 | modified | 43 | 41 | 84 | 
| [search-howto-managed-identities-data-sources.md](#item-edf98d) | minor update | 更新 Azure AI Search 受管身份数据源配置指南 | modified | 6 | 6 | 12 | 
| [search-howto-reindex.md](#item-46738a) | minor update | 更新重建索引文档 | modified | 3 | 3 | 6 | 
| [search-howto-run-reset-indexers.md](#item-fb10c8) | minor update | 更新重置索引器操作文档 | modified | 26 | 20 | 46 | 
| [search-indexer-how-to-access-private-sql.md](#item-1bd4cc) | minor update | 更新私有 SQL 访问文档中的 API 版本 | modified | 1 | 1 | 2 | 
| [search-indexer-howto-access-private.md](#item-73d30d) | minor update | 更新访问私有内容的文档 | modified | 5 | 5 | 10 | 
| [search-indexer-tutorial.md](#item-a3e3ff) | minor update | 更新 C# 教程以索引 Azure SQL 数据 | modified | 54 | 58 | 112 | 
| [search-limits-quotas-capacity.md](#item-3b201a) | minor update | 更新搜索限制、配额和容量文档 | modified | 10 | 9 | 19 | 
| [search-manage-azure-cli.md](#item-7fdd08) | minor update | 更新 Azure CLI 管理 Azure AI 搜索服务文档 | modified | 6 | 8 | 14 | 
| [search-manage-powershell.md](#item-3c3485) | minor update | 更新 PowerShell 管理 Azure AI 搜索服务文档 | modified | 7 | 9 | 16 | 
| [search-manage-rest.md](#item-405ec7) | minor update | 更新 REST 管理 Azure AI 搜索服务文档 | modified | 63 | 39 | 102 | 
| [search-markdown-data-tutorial.md](#item-32ea2a) | minor update | 更新 Markdown 数据教程文档 | modified | 34 | 30 | 64 | 
| [search-performance-tips.md](#item-218e77) | minor update | 优化 Azure AI 搜索性能提示文档 | modified | 9 | 9 | 18 | 
| [search-region-support.md](#item-25b0f1) | minor update | 更新 Azure AI Search 区域支持文档 | modified | 62 | 59 | 121 | 
| [search-reliability.md](#item-3e9b1a) | minor update | 更新 Azure AI 搜索可靠性文档 | modified | 6 | 6 | 12 | 
| [search-security-manage-encryption-keys.md](#item-db3487) | minor update | 更新用户管理加密密钥的管理文档 | modified | 6 | 6 | 12 | 
| [search-security-network-security-perimeter.md](#item-49c0d7) | minor update | 更新网络安全边界配置文档 | modified | 3 | 3 | 6 | 
| [search-security-overview.md](#item-6b3f1e) | minor update | 更新客户管理加密密钥文档链接 | modified | 1 | 1 | 2 | 
| [search-semi-structured-data.md](#item-d3388d) | minor update | 更新半结构化数据索引教程 | modified | 25 | 25 | 50 | 
| [search-sku-manage-costs.md](#item-6e0122) | minor update | 更新Azure AI搜索服务成本管理文档 | modified | 4 | 4 | 8 | 
| [search-sku-tier.md](#item-7686b8) | minor update | 更新Azure AI搜索服务SKU选择文档 | modified | 26 | 17 | 43 | 
| [search-synapseml-cognitive-services.md](#item-dcc36f) | minor update | 更新SynapseML与Azure AI Search的指南 | modified | 39 | 39 | 78 | 
| [semantic-how-to-configure.md](#item-7a92a6) | new feature | 添加对预发布语义排名模型的支持 | modified | 57 | 1 | 58 | 
| [semantic-how-to-enable-disable.md](#item-71ac1e) | minor update | 更新语义排名器启用与禁用的说明 | modified | 3 | 3 | 6 | 
| [semantic-how-to-query-request.md](#item-85530d) | minor update | 更新语义排名查询请求的说明 | modified | 2 | 2 | 4 | 
| [semantic-how-to-query-rewrite.md](#item-3e168f) | minor update | 更新语义查询重写的说明 | modified | 27 | 30 | 57 | 
| [semantic-search-overview.md](#item-b7497b) | minor update | 更新语义排名概述的细节 | modified | 2 | 2 | 4 | 
| [toc.yml](#item-c4768f) | minor update | 更新搜索文档目录 | modified | 10 | 6 | 16 | 
| [tutorial-create-custom-analyzer.md](#item-ad5520) | minor update | 更新自定义分析器创建教程 | modified | 62 | 62 | 124 | 
| [tutorial-multiple-data-sources.md](#item-71558f) | minor update | 更新多个数据源教程 | modified | 8 | 8 | 16 | 
| [tutorial-optimize-indexing-push-api.md](#item-ef0e96) | minor update | 更新优化索引的推送 API 教程 | modified | 54 | 56 | 110 | 
| [tutorial-rag-build-solution-pipeline.md](#item-25ce01) | minor update | 更新 RAG 建立解决方案管道教程 | modified | 6 | 6 | 12 | 
| [vector-search-how-to-chunk-documents.md](#item-b79133) | minor update | 更新向量搜索文档分块教程 | modified | 4 | 2 | 6 | 
| [vector-search-how-to-quantization.md](#item-744f48) | major update | 更新量化教程以支持新功能和更详细的说明 | modified | 186 | 46 | 232 | 
| [vector-search-how-to-storage-options.md](#item-ee1680) | major update | 更新存储选项文档以增强清晰度和功能 | modified | 58 | 27 | 85 | 
| [vector-search-index-size.md](#item-bb2846) | minor update | 更新向量索引大小文档以修正措辞和增加信息 | modified | 5 | 23 | 28 | 
| [vector-search-overview.md](#item-56e5fa) | minor update | 增加关于服务升级以提高向量配额的说明 | modified | 1 | 1 | 2 | 
| [vector-store.md](#item-db9b8c) | minor update | 更新向量存储文档以提供更多服务升级信息 | modified | 4 | 4 | 8 | 
| [whats-new.md](#item-fa71b4) | breaking change | 重构Whats New文档以提升信息准确性和可用性 | modified | 67 | 125 | 192 | 


# Modified Contents
## articles/search/.openpublishing.redirection.search.json{#item-8b66f9}

<details>
<summary>Diff</summary>
````diff
@@ -382,4 +382,4 @@
             "redirect_document_id": false
         }
     ]
-}
\ No newline at end of file
+  }
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 JSON 文件的格式: 结束换行符"
}
```

### Explanation
此次修改涉及对 `articles/search/.openpublishing.redirection.search.json` 文件的轻微更新。具体来说，修改内容包括在文件的末尾添加了一个换行符，从而使文件格式更符合标准。此更改包括一行的新增和一行的删除，导致总的更改次数为两次。虽然此修订不影响文件的功能性或内容，但它确保了良好的代码风格和可读性。详细的变更可以通过链接访问：[查看更改](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2F.openpublishing.redirection.search.json)。

## articles/search/cognitive-search-attach-cognitive-services.md{#item-68eaec}

<details>
<summary>Diff</summary>
````diff
@@ -50,7 +50,7 @@ Using the Azure portal or newer preview REST APIs and beta SDK packages, you can
 
 1. On your Azure AI services multi-service resource, [assign the identity](/azure/role-based-access-control/role-assignments-portal) to the **Cognitive Services User** role.
 
-1. Using the Azure portal, or the [Skillset 2024-11-01-preview REST API](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), or an Azure SDK beta package that provides the syntax, configure a skillset to use an identity:
+1. Using the Azure portal, or the [Skillset 2024-11-01-preview REST API](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or later, or an Azure SDK beta package that provides the syntax, configure a skillset to use an identity:
 
    + The managed identity used on the connection belongs to the search service. It can be system managed or user assigned.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新说明以包含额外信息"
}
```

### Explanation
此次修改对 `articles/search/cognitive-search-attach-cognitive-services.md` 文件进行了轻微的更新。主要变化是对第50行内容的修改，原句中的“或者”重新排列以通过“或后续的”来引入新的信息，提示用户可以使用更新的 API 版本或未来的 SDK beta 包。这一变动提高了说明的准确性，确保用户能获得最新使用方法的指引。在功能上没有改变，但提供了更清晰的指示。详细的变更记录可以通过以下链接查看：[查看更改](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fcognitive-search-attach-cognitive-services.md)。

## articles/search/cognitive-search-skill-document-intelligence-layout.md{#item-62c06f}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - references_regions
   - ignite-2024
 ms.topic: reference
-ms.date: 02/13/2025
+ms.date: 04/07/2025
 ---
 
 # Document Layout skill
@@ -22,12 +22,13 @@ The **Document Layout** skill analyzes a document to extract regions of interest
 
 This article is the reference documentation for the Document Layout skill. For usage information, see [Structure-aware chunking and vectorization](search-how-to-semantic-chunking.md).
 
-The **Document Layout** skill calls the [Document Intelligence Public preview version 2024-07-31-preview](/rest/api/aiservices/operation-groups?view=rest-aiservices-v4.0%20(2024-07-31-preview)&preserve-view=true). It's currently only available in the following Azure regions:
+The **Document Layout** skill calls the [Document Intelligence Public preview version 2024-07-31-preview](/rest/api/aiservices/operation-groups?view=rest-aiservices-v4.0%20(2024-07-31-preview)&preserve-view=true). 
 
-+ East US
-+ West US2
-+ West Europe
-+ North Central US
+Supported regions varies by modality:
+
++ In code, your skillset can call Document Intelligence through an Azure AI multi-service resource in any region that provides both AI enrichment and Document Intelligence. See [Product availability by region](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/table) to find regions that provide both *AI enrichment* in Azure AI Search and *Document Intelligence* under Azure AI services.
+
++ In the [Import and vectorize data](search-import-data-portal.md) wizard in the Azure portal, you can enable document layout detection in the data source connection step. Document layout detection in the portal is available in the following Azure regions: **East US**, **West Europe**, **North Central US**. Create an Azure AI multi-service resource in one of these three regions to get the portal experience.
 
 Supported file formats include:
 
@@ -59,7 +60,7 @@ Microsoft.Skills.Util.DocumentIntelligenceLayoutSkill
 
 ## Supported languages
 
-Refer to [Azure AI Document Intelligence layout model supported languages](/azure/ai-services/document-intelligence/language-support/ocr?view=doc-intel-3.1.0&tabs=read-print%2Clayout-print%2Cgeneral#layout) for printed text.
+Refer to [Azure AI Document Intelligence layout model supported languages](/azure/ai-services/document-intelligence/language-support/ocr?view=doc-intel-3.1.0&tabs=read-print%2Clayout-print%2Cgeneral#layout&preserve-view=true) for printed text.
 
 ## Limitations
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新文档以反映日期和支持区域"
}
```

### Explanation
此次修改对 `articles/search/cognitive-search-skill-document-intelligence-layout.md` 文件进行了更新，主要内容包括对一些信息的修改和补充。首先，将文件中与日期相关的部分更新为 2025 年 4 月 7 日。其次，为了提供更清晰的指导，增加了有关支持区域的说明，明确提到不同的区域取决于技能的使用方式。具体来说，更新内容强调了在代码中如何调用 Document Intelligence 以及在 Azure 门户的导入和向量化数据向导中如何启用文档布局检测。最后，还提供了有关文件格式和支持语言的链接更新，使其更具可读性和准确性。这些修改增强了文档的清晰度和实用性，确保用户获得最新的信息。详细更改可通过以下链接查看：[查看更改](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fcognitive-search-skill-document-intelligence-layout.md)。

## articles/search/cognitive-search-tutorial-blob-dotnet.md{#item-ba889d}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Skillsets using C#'
+title: 'Tutorial: Skillsets Using C#'
 titleSuffix: Azure AI Search
 description: Use C# and the Azure SDK for .NET to create skillsets. This skillset applies AI transformations and analyses to create searchable content from images and unstructured text.
 
@@ -9,7 +9,7 @@ manager: nitinme
 
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 01/17/2025
+ms.date: 03/31/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
@@ -18,19 +18,17 @@ ms.custom:
 
 # C# Tutorial: Use skillsets to generate searchable content in Azure AI Search
 
-In this tutorial, learn how to use the [Azure SDK for .NET](https://www.nuget.org/packages/Azure.Search.Documents/) to create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
+Learn how to use the [Azure SDK for .NET](https://www.nuget.org/packages/Azure.Search.Documents/) to create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
 
-Skillsets add AI processing to raw content, making that content more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations: from image analysis, to natural language processing, to customized processing that you provide externally.
+Skillsets add AI processing to raw content, making it more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations, from image analysis to natural language processing to customized processing that you provide externally.
 
-This tutorial helps you learn how to:
+In this tutorial, you:
 
 > [!div class="checklist"]
 > + Define objects in an enrichment pipeline.
 > + Build a skillset. Invoke OCR, language detection, entity recognition, and key phrase extraction.
 > + Execute the pipeline. Create and load a search index.
-> + Check the results using full text search.
-
-If you don't have an Azure subscription, open a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
+> + Check the results using full-text search.
 
 ## Overview
 
@@ -42,16 +40,18 @@ Once content is extracted, the [skillset](cognitive-search-working-with-skillset
 
 ## Prerequisites
 
-+ [Visual Studio](https://visualstudio.microsoft.com/downloads/)
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+
++ [Azure Storage](/azure/storage/common/storage-account-create).
 
-+ [Azure.Search.Documents 11.x NuGet package](https://www.nuget.org/packages/Azure.Search.Documents) 
++ [Azure AI Search](search-create-app-portal.md).
 
-+ [Azure Storage](/azure/storage/common/storage-account-create)
++ [Azure.Search.Documents 11.x NuGet package](https://www.nuget.org/packages/Azure.Search.Documents).
 
-+ [Azure AI Search](search-create-app-portal.md)
++ [Visual Studio](https://visualstudio.microsoft.com/downloads/).
 
 > [!NOTE]
-> You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ### Download files
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新标题格式和日期"
}
```

### Explanation
此次修改对 `articles/search/cognitive-search-tutorial-blob-dotnet.md` 文件进行了轻微更新，主要内容涉及标题格式调整和日期更新。文件标题从“Tutorial: Skillsets using C#”更改为“Tutorial: Skillsets Using C#”，确保标题首字母大写，从而提升了文档的专业性。同时，文档的发布日期从 2025 年 1 月 17 日更新为 2025 年 3 月 31 日，以反映最新的信息。

此外，还对文中的一些句子进行了轻微的文字调整，以提高语句的流畅性和可读性，例如删除了一些不必要的词汇，使得内容结构更加简洁。文档中对操作步骤和必要条件的描述进行了调整，以增强其逻辑性和条理性。总体而言，这次修改提高了文档的清晰度和可用性，让读者更容易理解操作流程和所需条件。详细变更可通过以下链接查看：[查看更改](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fcognitive-search-tutorial-blob-dotnet.md)。

## articles/search/cognitive-search-tutorial-blob.md{#item-ff148f}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Skillsets using REST'
+title: 'Tutorial: Skillsets Using REST'
 titleSuffix: Azure AI Search
 description: Use the Search REST APIs to create skillsets. This skillset applies AI transformations and analyses to create searchable content from images and unstructured text.
 
@@ -9,25 +9,23 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 01/17/2025
+ms.date: 03/31/2025
 ---
 
 # REST Tutorial: Use skillsets to generate searchable content in Azure AI Search
 
-In this tutorial, learn how to call REST APIs that create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
+Learn how to call REST APIs that create an [AI enrichment pipeline](cognitive-search-concept-intro.md) for content extraction and transformations during indexing.
 
-Skillsets add AI processing to raw content, making that content more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations: from image analysis, to natural language processing, to customized processing that you provide externally.
+Skillsets add AI processing to raw content, making it more uniform and searchable. Once you know how skillsets work, you can support a broad range of transformations, from image analysis to natural language processing to customized processing that you provide externally.
 
-This tutorial helps you learn how to:
+In this tutorial, you:
 
 > [!div class="checklist"]
 > + Define objects in an enrichment pipeline.
 > + Build a skillset. Invoke OCR, language detection, entity recognition, and key phrase extraction.
 > + Execute the pipeline. Create and load a search index.
 > + Check the results using full text search.
 
-If you don't have an Azure subscription, open a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Overview
 
 This tutorial uses a REST client and the [Azure AI Search REST APIs](/rest/api/searchservice/) to create a data source, index, indexer, and skillset.
@@ -38,20 +36,22 @@ Once content is extracted, the [skillset](cognitive-search-working-with-skillset
 
 ## Prerequisites
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client)
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
 + [Azure Storage](/azure/storage/common/storage-account-create)
 
 + [Azure AI Search](search-create-app-portal.md)
 
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client)
+
 > [!NOTE]
-> You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ### Download files
 
 Download a zip file of the sample data repository and extract the contents. [Learn how](https://docs.github.com/get-started/start-your-journey/downloading-files-from-github).
 
-+ [Sample data files (mixed media)](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/ai-enrichment-mixed-media). 
++ [Sample data files (mixed media)](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/ai-enrichment-mixed-media).
 
 + [Sample REST file](https://github.com/Azure-Samples/azure-search-rest-samples/tree/main/skillset-tutorial)
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新标题格式和发布日期"
}
```

### Explanation
这次修改对 `articles/search/cognitive-search-tutorial-blob.md` 文件进行了小幅更新，主要涵盖了标题格式和发布日期的调整。具体而言，标题“Tutorial: Skillsets using REST”已更改为“Tutorial: Skillsets Using REST”，使得标题更加规范化，符合书写标准。同时，发布日期也从 2025 年 1 月 17 日更新为 2025 年 3 月 31 日，以确保读者获取到最新的信息。

在内容部分，修改了几处句子的结构，使其更简洁流畅，例如，将“在本教程中，您将学习如何调用 REST API”修改为“您将学习如何调用 REST API”。此外，还调整了所需的前提条件部分，增加了“拥有有效的 Azure 订阅”这一条目，并重新排列了所需工具和资源的顺序，旨在确保读者能够更容易理解开始操作的步骤。

这些修改增强了文档的一致性和可读性，使得用户在学习如何使用 Azure AI Search 的 REST API 时能更顺利地进行操作。详细变更可通过以下链接查看：[查看更改](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fcognitive-search-tutorial-blob.md)。

## articles/search/cognitive-search-tutorial-debug-sessions.md{#item-7e10e9}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Debug skillsets'
+title: 'Tutorial: Debug Skillsets'
 titleSuffix: Azure AI Search
 description: Practice creating and completing a debug session on an Azure AI Search skillset. This tutorial provides a buggy sample skillset that you resolve in a debug session.
 
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 12/03/2024
+ms.date: 03/31/2025
 ---
 
 # Tutorial: Fix a skillset using Debug Sessions
@@ -19,15 +19,15 @@ In Azure AI Search, a skillset coordinates the actions of skills that analyze, t
 
 **Debug Sessions** is an Azure portal tool that provides a holistic visualization of a skillset that executes on Azure AI Search. Using this tool, you can drill down to specific steps to easily see where an action might be falling down.
 
-In this article, use **Debug Sessions** to find and fix missing inputs and outputs. The tutorial is all-inclusive. It provides sample data, a REST file that creates objects, and instructions for debugging problems in the skillset.
-
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
+In this tutorial, you use **Debug Sessions** to find and fix missing inputs and outputs. The tutorial is all-inclusive. It provides sample data, a REST file that creates objects, and instructions for debugging problems in the skillset.
 
 ## Prerequisites
 
-+ Azure AI Search. [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription. You can use a free service for this tutorial. The free tier doesn't provide managed identity support for an Azure AI Search service. You must use keys for connections to Azure Storage.
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+
++ Azure AI Search. [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. You can use a free service for this tutorial. The Free tier doesn't provide managed identity support for an Azure AI Search service. You must use keys for connections to Azure Storage.
 
-+ Azure Storage account with [Blob storage](/azure/storage/blobs/), used for hosting sample data, and for persisting cached data created during a debug session. If you're using a free search service, the storage account must have shared access keys enabled and it must allow public network access.
++ Azure Storage account with [Blob storage](/azure/storage/blobs/), used for hosting sample data and for persisting cached data created during a debug session. If you're using a free search service, the storage account must have shared access keys enabled and it must allow public network access.
 
 + [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
@@ -44,29 +44,29 @@ This section creates the sample data set in Azure Blob Storage so that the index
 
 1. [Download sample data (clinical-trials-pdf-19)](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/_ARCHIVE/clinical-trials/clinical-trials-pdf-19), consisting of 19 files.
 
-1. [Create an Azure Storage account](/azure/storage/common/storage-account-create?tabs=azure-portal) or [find an existing account](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Storage%2storageAccounts/). 
+1. [Create an Azure Storage account](/azure/storage/common/storage-account-create?tabs=azure-portal) or [find an existing account](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Storage%2storageAccounts/).
 
    + Choose the same region as Azure AI Search to avoid bandwidth charges.
 
    + Choose the StorageV2 (general purpose V2) account type.
 
-1. Navigate to the Azure Storage services pages in the Azure portal and create a Blob container. Best practice is to specify the access level "private". Name your container `clinicaltrialdataset`.
+1. Go to the Azure Storage services pages in the Azure portal and create a Blob container. Best practice is to specify the access level "private". Name your container `clinicaltrialdataset`.
 
 1. In container, select **Upload** to upload the sample files you downloaded and unzipped in the first step.
 
-1. While in the Azure portal, copy the connection string for Azure Storage. You can get the connection string from **Settings** > **Access Keys** in the Azure portal.
+1. In the Azure portal, select **Settings** > **Access Keys** and copy the connection string for Azure Storage.
 
 ## Copy a key and URL
 
 This tutorial uses API keys for authentication and authorization. You need the search service endpoint and an API key, which you can get from the Azure portal.
 
-1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com), go to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
 1. Under **Settings** > **Keys**, copy an admin key. Admin keys are used to add, modify, and delete objects. There are two interchangeable admin keys. Copy either one.
 
    :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of the URL and API keys in the Azure portal.":::
 
-A valid API key establishes trust, on a per request basis, between the application sending the request and the search service handling it.
+A valid API key establishes trust, on a per-request basis, between the application sending the request and the search service handling it.
 
 ## Create data source, skillset, index, and indexer
 
@@ -82,9 +82,9 @@ In this section, create a "buggy" workflow that you can fix in this tutorial.
 
 ## Check results in the Azure portal
 
-The sample code intentionally creates a buggy index as a consequence of problems that occurred during skillset execution. The problem is that the index is missing data. 
+The sample code intentionally creates a buggy index as a consequence of problems that occurred during skillset execution. The problem is that the index is missing data.
 
-1. In Azure portal, on the search service **Overview** page, select the **Indexes** tab. 
+1. In Azure portal, on the search service **Overview** page, select the **Indexes** tab.
 
 1. Select *clinical-trials*.
 
@@ -98,15 +98,15 @@ The sample code intentionally creates a buggy index as a consequence of problems
 
 1. Run the query. You should see empty values for `organizations` and `locations`.
 
-    These fields should have been populated through the skillset's [Entity Recognition skill](cognitive-search-skill-entity-recognition-v3.md), used to detect organizations and locations anywhere within the blob's content. In the next exercise, you'll debug the skillset to determine what went wrong.
+    These fields should have been populated through the skillset's [Entity Recognition skill](cognitive-search-skill-entity-recognition-v3.md), used to detect organizations and locations anywhere within the blob's content. In the next exercise, you debug the skillset to determine what went wrong.
 
 Another way to investigate errors and warnings is through the Azure portal.
 
-1. Open the **Indexers** tab and select *clinical-trials-idxr*.
+1. On the **Indexers** tab, select *clinical-trials-idxr*.
 
    Notice that while the indexer job succeeded overall, there were warnings.
 
-1. Select **Success** to view the warnings (if there were mostly errors, the detail link would be **Failed**). You'll see a long list of every warning emitted by the indexer.
+1. Select **Success** to view the warnings. If there are mostly errors, the detail link is **Failed**. You should see a long list of every warning emitted by the indexer.
 
    :::image type="content" source="media/cognitive-search-debug/portal-indexer-errors-warnings.png" alt-text="Screenshot of view warnings." :::
 
@@ -116,27 +116,27 @@ Another way to investigate errors and warnings is through the Azure portal.
 
 1. Select **+ Add Debug Session**.
 
-1. Give the session a name. 
+1. Give the session a name.
 
 1. In Indexer template, provide the indexer name. The indexer has references to the data source, the skillset, and index.
 
 1. Select the storage account.
 
-1. **Save** the session. 
+1. **Save** the session.
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-create-session.png" lightbox="media/cognitive-search-debug/debug-tutorial-create-session.png" alt-text="Screenshot of Debug session definition page." :::
   
 1. A debug session opens to the settings page. You can make modifications to the initial configuration and override any defaults. A debug session only works with a single document. The default is to accept the first document in the collection as the basis of your debug sessions. You can [choose a specific document to debug](cognitive-search-how-to-debug-skillset.md#create-a-debug-session) by providing its URI in Azure Storage.
 
-1. When the debug session has finished initializing, you should see a skills workflow with mappings and a search index. The enriched document data structure appears in a details pane on the side. We excluded it from the following screenshot so that you could see more of the workflow.
+1. When the debug session finishes initializing, you should see a skills workflow with mappings and a search index. The enriched document data structure appears in a details pane on the side. We excluded it from the following screenshot so that you could see more of the workflow.
 
    :::image type="content" source="media/cognitive-search-debug/debug-execution-complete1.png" lightbox="media/cognitive-search-debug/debug-execution-complete1.png" alt-text="Screenshot of Debug Session visual editor." :::
 
 ## Find issues with the skillset
 
 Any issues reported by the indexer are indicated as **Errors** and **Warnings**.
 
-Notice that the number of errors and warning is a much smaller list than the one displayed earlier because this list is only detailing the errors for a single document. Like the list displayed by the indexer, you can select on a warning message and see the details of this warning.
+Notice that the number of errors and warnings is a smaller list than the one displayed earlier because this list is only detailing the errors for a single document. Like the list displayed by the indexer, you can select on a warning message and see the details of this warning.
 
 Select **Warnings** to review the notifications. You should see four:
 
@@ -163,7 +163,7 @@ Because all four notifications are about this skill, your next step is to debug
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-skill-detail.png" alt-text="Screenshot of the skill details pane.":::
 
-1. Hover over each input (or select an input) to show the values in the **Expression evaluator**. Notice that the displayed result for this input doesn’t look like a text input. It looks like a series of new line characters `\n \n\n\n\n` instead of text. The lack of text means that no entities can be identified, so either this document fails to meet the prerequisites of the skill, or there's another input that should be used instead.
+1. Hover over each input (or select an input) to show the values in the **Expression evaluator**. Notice that the displayed result for this input doesn’t look like a text input. It looks like a series of new line characters `\n \n\n\n\n` instead of text. The lack of text means that no entities can be identified. Either this document doesn't meet the prerequisites of the skill, or there's another input that should be used instead.
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-skill-input-null.png" alt-text="Screenshot of skill input showing null values.":::
 
@@ -177,7 +177,7 @@ Because all four notifications are about this skill, your next step is to debug
 
    :::image type="content" source="media/cognitive-search-debug/debug-tutorial-edit-skill.png" alt-text="Screenshot of Expression Evaluator for fixed merged_content input." :::
 
-1. Select **Run** in the session's window menu. This kicks off another execution of the skillset using the document. 
+1. Select **Run** in the session's window menu. This kicks off another execution of the skillset using the document.
 
 1. Once the debug session execution completes, notice that the warnings count has reduced by one. Warnings show that the error for text input is gone, but the other warnings remain. The next step is to address the warning about the missing or empty value `/document/languageCode`.
 
@@ -221,11 +221,11 @@ The messages say to check the 'outputFieldMappings' property of your indexer, so
 
 1. Select **Run**.
 
-All of the errors have been resolved.
+All of the errors are resolved.
 
 ## Commit changes to the skillset
 
-When the debug session was initiated, the search service created a copy of the skillset. This was done to protect the original skillset on your search service. Now that you have finished debugging your skillset, the fixes can be committed (overwrite the original skillset). 
+When the debug session was initiated, the search service created a copy of the skillset. This was done to protect the original skillset on your search service. Now that you debugged your skillset, the fixes can be committed (overwrite the original skillset).
 
 Alternatively, if you aren't ready to commit changes, you can save the debug session and reopen it later.
 
@@ -243,7 +243,7 @@ Alternatively, if you aren't ready to commit changes, you can save the debug ses
 
 1. Select **Refresh** to show the status of the reset and run commands.
 
-When the indexer has finished running, there should be a green checkmark and the word Success next to the time stamp for the latest run in the **Execution history** tab. To ensure that the changes have been applied:
+When the indexer finishes running, there should be a green checkmark and the word Success next to the time stamp for the latest run in the **Execution history** tab. To ensure that the changes are applied:
 
 1. In the left pane, open **Indexes**.
 
@@ -263,7 +263,7 @@ The free service is limited to three indexes, indexers, and data sources. You ca
 
 ## Next steps
 
-This tutorial touched on various aspects of skillset definition and processing. To learn more about concepts and workflows, refer to the following articles:
+This tutorial touched on various aspects of skillset definition and processing. To learn more about concepts and workflows, see the following articles:
 
 + [How to map skillset output fields to fields in a search index](cognitive-search-output-field-mapping.md)
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "标题格式和发布日期更新"
}
```

### Explanation
此次修改涉及 `articles/search/cognitive-search-tutorial-debug-sessions.md` 文档的内容更新，主要包括标题格式的调整和发布日期的更新。具体来说，标题由“Tutorial: Debug skillsets”更新为“Tutorial: Debug Skillsets”，遵循更一致的书写规范。文件的发布日期也由 2024 年 12 月 3 日更改为 2025 年 3 月 31 日，以反映最新的时间信息。

在内容方面，文中对一些句子进行了重新组织和简化，以提高流畅性。例如，将“在本教程中，使用 **Debug Sessions** 查找和修复缺失的输入和输出”简化为“您使用 **Debug Sessions** 查找和修复缺失的输入和输出”。此外，增加了一些关于前提条件的说明，确保用户在开始之前具有所需的 Azure 账户和服务设置。

本次更新不仅提高了文档的可读性，还确保读者能准确地理解使用 Debug Sessions 进行技能集调试的步骤和要求。修改后的内容更加清晰地描述了调试过程和操作步骤，从而提升了用户体验。详细更改信息可通过以下链接查看：[查看更改](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fcognitive-search-tutorial-debug-sessions.md)。

## articles/search/includes/previews/preview-generic.md{#item-51bbcc}

<details>
<summary>Diff</summary>
````diff
@@ -1,13 +1,13 @@
 ---
-title: include file
-description: include file
+title: Include file
+description: Include file
 author: eric-urban
 ms.author: eur
 ms.service: azure-ai-speech
 ms.topic: include
-ms.date: 9/20/2024
+ms.date: 03/19/2024
 ms.custom: include, ignite-2024
 ---
 
 > [!NOTE]
-> This feature is currently in public preview. This preview is provided without a service-level agreement, and is not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
+> This feature is currently in public preview. This preview is provided without a service-level agreement and isn't recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新文件标题、描述和发布日期"
}
```

### Explanation
此次修改对 `articles/search/includes/previews/preview-generic.md` 文件进行了小幅更新，主要修改了文件的标题、描述及发布日期。具体来说，标题和描述均从“include file”更改为“Include file”，以确保格式化的一致性。同时，发布日期从 2024 年 9 月 20 日更新为 2024 年 3 月 19 日，以反映最新的信息。

此外，文中包含的注意事项内容也经过调整，移除了句中的某些标点符号，以提高流畅性。例如，将“服务水平协议，并且不推荐用于生产工作负载。”简化为“服务水平协议并且不推荐用于生产工作负载。”，使得表述更为顺畅。

这些变更使文档在格式上更为统一，并确保用户获取到最新的文件发布日期和相关信息，提升了文档的整体可读性和专业性。详细变更信息可通过以下链接查看：[查看更改](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fincludes%2Fpreviews%2Fpreview-generic.md)。

## articles/search/media/search-capacity-planning/add-two-each.png{#item-56b26e}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 add-two-each.png"
}
```

### Explanation
此次修改是对 `articles/search/media/search-capacity-planning/add-two-each.png` 文件的新增，表示引入了一幅新图像。这张图片可能用于支持与搜索容量规划相关的文档内容，为用户提供更直观的理解。

随着这幅新图片的添加，文档将变得更加丰富，有助于读者更好地掌握搜索容量规划的概念和应用场景。新增的图片文件没有进行删除或变化，因此没有其他影响或更新。

整体而言，这种添加图片的修改，提升了文档的可视化效果，使得信息传递更加清晰明了，能够更好地辅助用户理解相关内容。详细信息可查看图片文件：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-capacity-planning%2Fadd-two-each.png)。

## articles/search/media/search-capacity-planning/change-pricing-tier.png{#item-f71721}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 change-pricing-tier.png"
}
```

### Explanation
此次修改涉及到对 `articles/search/media/search-capacity-planning/change-pricing-tier.png` 文件的新增。这张图片可能用于展示与搜索容量规划中的定价层相关的内容，增强文档的可视化效果。

通过添加这张新图片，文档能够为读者提供更清晰的视角，助力其了解如何改变定价层及其影响。这对于用户在实际操作中作出决策来说，提供了更直观、易于理解的信息。

此次变更没有删除或更改其他内容，因此其影响主要集中在新增内容的可用性上。这种格式的图片添加，有助于提升整体文档质量，使得信息呈现更加丰富与生动。详细信息可查看图片文件：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-capacity-planning%2Fchange-pricing-tier.png)。

## articles/search/media/search-capacity-planning/initial-values.png{#item-465304}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 initial-values.png"
}
```

### Explanation
此次修改对 `articles/search/media/search-capacity-planning/initial-values.png` 文件进行了新增。这张图片旨在为文章提供可视化信息，可能与搜索容量规划中的初始值设置相关，帮助读者更好地理解相关概念。

通过这幅新增的图片，文档内容将变得更加生动和易于理解，使用户能够更轻松地获取有关如何设置初始值的关键信息。这种图像的添加有助于提升文档的整体质量和用户体验。

此次更新没有删除或更改任何现有内容，因此其影响主要集中在新内容的引入。这种理论与实践的结合方式，增强了读者获取信息的有效性。详细信息可查看图片文件：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-capacity-planning%2Finitial-values.png)。

## articles/search/media/search-capacity-planning/portal-notifications.png{#item-4bd098}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 portal-notifications.png"
}
```

### Explanation
此次修改涉及对 `articles/search/media/search-capacity-planning/portal-notifications.png` 文件的新增。这张图片可能与搜索容量规划中的门户通知功能相关，旨在帮助用户更好地理解这一功能的使用方式和效果。

通过增加这张新图片，文档的可视化组件得到了丰富，有助于读者快速识别门户通知的关键特点和操作步骤，这对于提升用户的操作体验和信息获取效率非常重要。

此次变更没有删除或更改其他现有内容，主要影响集中在新的视觉元素引入上，从而加强了文档信息的传达能力。用户可以通过以下链接查看该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-capacity-planning%2Fportal-notifications.png)。

## articles/search/media/search-capacity-planning/pricing-tier-list.png{#item-ff6b12}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 pricing-tier-list.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-capacity-planning/pricing-tier-list.png` 文件。这张图片可能展示了有关搜索容量规划的定价层级列表，旨在为用户提供清晰的定价结构和可选方案的视觉参考。

这幅新增的图片增强了文档的信息传达能力，使用户能够更直观地理解不同定价层级的选择及其相应的服务特点。通过这种视觉化的呈现方式，用户在做出决策时可以更容易地比较各个层级的服务内容和价格。

此次更新没有涉及删除或更改其他内容，专注于引入新的视觉元素，以便更好地辅助用户理解相关信息。用户可通过以下链接访问该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-capacity-planning%2Fpricing-tier-list.png)。

## articles/search/media/search-capacity-planning/provisioning-status.png{#item-ede201}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 provisioning-status.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-capacity-planning/provisioning-status.png` 文件。这张图片可能用于展示搜索容量规划中的配置状态，旨在帮助用户了解当前资源的配置情况和进度。

通过增加这张图片，文档的内容变得更加丰富，提供了直观的视觉工具来跟踪和理解系统中资源的配置状态。这能够提高用户在管理和监控过程中对信息的获取效率，进而优化他们的决策。

此次更新未对其他现有内容进行删除或更改，主要专注于新增这一视觉元素，以增强用户体验。用户可通过以下链接查看该图片：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-capacity-planning%2Fprovisioning-status.png)。

## articles/search/media/search-capacity-planning/updating-message.png{#item-fc9f1b}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 updating-message.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-capacity-planning/updating-message.png` 文件。这张图片可能用于显示搜索容量规划中正在更新的消息，帮助用户理解在更新过程中的反馈信息和状态。

通过引入这幅图像，文档中关于更新过程的描述变得更加直观，用户可以更快速地把握更新操作的进展情况。这一视觉补充旨在提升用户体验，使他们能够更清晰地理解系统在执行更新时传达的信息。

此次更新没有删减或更改其他内容，专注于增强文档的视觉表现。用户可以通过以下链接访问该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-capacity-planning%2Fupdating-message.png)。

## articles/search/media/search-faceted-navigation/portal-add-facetable-field.png{#item-d0b7a4}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 portal-add-facetable-field.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-faceted-navigation/portal-add-facetable-field.png` 文件。这张图片可能用于展示如何在门户中添加可过滤字段，旨在帮助用户更好地理解和使用搜索的分面导航功能。

通过提供这一视觉素材，文档的表达更加生动，用户可以通过图形更直观地掌握添加可过滤字段的步骤和方法。这种新功能的引入将有助于提高用户在实施和管理分面搜索时的效率和准确性。

此次更新未对现有内容进行删除或更改，专注于通过新增图像提升文档的实用性和用户体验。用户可通过以下链接查看该图片：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-faceted-navigation%2Fportal-add-facetable-field.png)。

## articles/search/media/search-faceted-navigation/portal-facet-query.png{#item-57be1f}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 portal-facet-query.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-faceted-navigation/portal-facet-query.png` 文件。这张图片可能用于展示门户的分面查询界面，帮助用户更好地理解如何在搜索中利用分面功能进行过滤和查询。

通过引入此图像，文档的指导变得更加具体，用户可以更直观地看到如何使用分面查询，提高了操作的可理解性和实用性。该图片旨在增强用户对分面导航的掌握，使他们在使用此功能时能够更加得心应手。

此次更新没有对其他内容进行删除或更改，能够为用户提供一个更加完整的学习和操作支持。用户可以通过以下链接访问该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-faceted-navigation%2Fportal-facet-query.png)。

## articles/search/media/search-how-to-index-sql-database/search-data-source.png{#item-e559ff}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 search-data-source.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-how-to-index-sql-database/search-data-source.png` 文件。这张图片可能用于说明如何为 SQL 数据库设置数据源，以便进行索引，从而帮助用户更好地理解相关流程。

通过增加这一视觉辅助材料，文档的内容变得更加丰富与易懂，使用户在进行 SQL 数据库索引时能够更直观地掌握步骤与操作要求。这将极大地提升用户的学习体验，让他们在实际应用中更加高效和准确。

此次更新未对现有内容进行任何删除或更改，专注于为用户提供新的信息资源。用户可以通过以下链接查看该图片的详细内容：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-how-to-index-sql-database%2Fsearch-data-source.png)。

## articles/search/media/search-how-to-upgrade/service-creation-upgrade-metadata.png{#item-d1251d}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 service-creation-upgrade-metadata.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-how-to-upgrade/service-creation-upgrade-metadata.png` 文件。这张图片可能用于说明服务创建升级过程中的元数据要求，旨在帮助用户更清楚地理解在升级过程中所需的重要信息。

通过引入这一新图像，文档得以提升其视觉效果和信息可理解性，使得用户在进行服务升级时能够更加直观地把握每一步所需注意的细节。这对于希望顺利完成服务升级的用户来说将是一个极大的帮助。

此次更新没有对其他内容进行删除或更改，专注于增强现有文档的资源。用户可以通过以下链接查看该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-how-to-upgrade%2Fservice-creation-upgrade-metadata.png)。

## articles/search/media/search-how-to-upgrade/upgrade-button.png{#item-894e31}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 upgrade-button.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-how-to-upgrade/upgrade-button.png` 文件。这张图片可能用于展示在服务升级过程中所需要点击的“升级”按钮，以帮助用户在执行升级操作时提供明确的视觉指导。

通过插入该图像，文档的可读性和用户体验得到了提升，使得用户更易于理解如何在界面中找到和使用相关按钮，从而顺利地完成升级流程。这对于需要进行服务升级的用户来说，将提供重要的帮助和参考。

此次更新未对现有内容进行删除或更改，专注于添加新的视觉资源。用户可以通过以下链接查看该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-how-to-upgrade%2Fupgrade-button.png)。

## articles/search/media/search-how-to-upgrade/upgrade-confirmation.png{#item-880793}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 upgrade-confirmation.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-how-to-upgrade/upgrade-confirmation.png` 文件。这张图片可能用于显示在服务升级完成后用户需要确认的界面，帮助用户理解在升级过程中的最后确认步骤。

通过添加这张图像，文档的可视化效果得到了进一步加强，使用户能够更清晰地识别在升级完成后所需的确认操作。这一补充对于提升用户体验和减少潜在的操作错误具备重要意义。

此次更新没有对其他内容进行删除或更改，专注于增强现有文档的辅助材料。用户可以通过以下链接查看该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-how-to-upgrade%2Fupgrade-confirmation.png)。

## articles/search/media/search-how-to-upgrade/upgrade-panel.png{#item-0c9673}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 upgrade-panel.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-how-to-upgrade/upgrade-panel.png` 文件。这张图片可能用于展示用户在进行服务升级时所看到的升级面板，帮助用户更好地理解在界面中需要执行的操作步骤。

通过添加这张图像，文档进一步增强了内容的直观性，使用户能够在进行升级时更容易识别和理解每一步的操作界面。这一更新旨在提高用户的操作便利性，并减少在升级过程中的疑惑或错误。

此次更新专注于添加视觉材料，没有对现有内容进行删除或更改。用户可以通过以下链接查看该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-how-to-upgrade%2Fupgrade-panel.png)。

## articles/search/media/search-security-manage-encryption-keys/assign-key-vault.png{#item-e19e19}

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增图片文件 assign-key-vault.png"
}
```

### Explanation
此次修改新增了 `articles/search/media/search-security-manage-encryption-keys/assign-key-vault.png` 文件。这张图片用于展示如何在安全管理加密密钥时分配密钥保管库，旨在帮助用户理解具体的操作流程。

通过引入这张图片，文档提供了更直观的视觉指导，使用户在使用密钥保管库进行加密密钥管理时能够更快速地获取相关信息。这一更新不仅提升了文档的可读性，还增强了用户的使用体验，减少了在操作过程中的不确定性。

此次更新专注于添加图像内容，未对其他部分进行修改或删除。用户可以通过以下链接查看该图片的详细信息：[查看图片](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fmedia%2Fsearch-security-manage-encryption-keys%2Fassign-key-vault.png)。

## articles/search/resource-training.md{#item-07788d}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ author: haileytap
 ms.author: haileytapia
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 10/28/2024
+ms.date: 04/07/2025
 ---
 
 # Training - Azure AI Search
@@ -27,10 +27,9 @@ Learning paths are a collection of training modules that are organized around sp
 
 | Module | Learning path |
 |--------|---------------|
-[Fundamentals of Knowledge Mining and Azure AI Search](/training/modules/intro-to-azure-search/) | [Microsoft Azure AI Fundamentals:](/training/paths/document-intelligence-knowledge-mining/) |
+[Fundamentals of Knowledge Mining and Azure AI Search](/training/modules/intro-to-azure-search/) | [Microsoft Azure AI Fundamentals](/training/paths/document-intelligence-knowledge-mining/) |
 | [Create an Azure AI Search solution](/training/modules/create-azure-cognitive-search-solution/) | [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
 | [Create a custom skill for Azure AI Search](/training/modules/create-azure-ai-custom-skill/) | [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
-| [Build an Azure Machine Learning custom skill for Azure AI Search](/training/modules/build-azure-machine-learn-custom-skill-for-azure-cognitive-search/) | |
 | [Enrich your data with Azure AI Language](/training/modules/enrich-search-index-using-language-studio/) | |
 | [Create a knowledge store with Azure AI Search](/training/modules/create-knowledge-store-azure-cognitive-search/) | [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
 | [Implement advanced search features in Azure AI Search](/training/modules/implement-advanced-search-features-azure-cognitive-search/)| [Implement knowledge mining](/training/paths/implement-knowledge-mining-azure-cognitive-search/) |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 resource-training.md 文档"
}
```

### Explanation
此次修改涉及对 `articles/search/resource-training.md` 文档的更新，主要包括日期的更新及一些文本的微调。具体改动包括：

1. **日期更新**：文档中的 `ms.date` 从原来的 `10/28/2024` 更新为 `04/07/2025`，这表明文档的最新更新时间。

2. **文本调整**：在关于学习模块与学习路径的表格中，对部分文本进行了小的修改，例如将“Microsoft Azure AI Fundamentals:” 简化为“Microsoft Azure AI Fundamentals”。

3. **行数变化**：此次修改中增加了 2 行，删除了 3 行，总体变更为 5 行，这表明文本进行了小幅的调整，但未增加新的内容。

整体现修改旨在使文档更加准确和易读，通过对日期和表格内容的微调，以确保提供给用户的信息是最新的。用户可以通过以下链接查看文档的详细信息：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fresource-training.md)。

## articles/search/search-api-migration.md{#item-07297b}

<details>
<summary>Diff</summary>
````diff
@@ -12,7 +12,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 02/14/2025
+ms.date: 03/10/2025
 ---
 
 # Upgrade to the latest REST API in Azure AI Search
@@ -24,7 +24,7 @@ Use this article to migrate to newer versions of the [**Search Service REST APIs
 | Data plane | [`2024-07-01`](/rest/api/searchservice/search-service-api-versions#2024-07-01) | Stable |
 | Data plane | [`2024-11-01-preview`](/rest/api/searchservice/search-service-api-versions#2024-11-01-preview) | Preview |
 | Control plane | [`2023-11-01`](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2023-11-0&preserve-view=true1) | Stable |
-| Control plane | [`2024-03-01-preview`](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true) | Preview |
+| Control plane | [`2025-02-01-preview`](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true) | Preview |
 
 Upgrade instructions focus on code changes that get you through breaking changes from previous versions so that existing code runs the same as before, but on the newer API version. Once your code is in working order, you can decide whether to adopt newer features. To learn more about new features, see [vector code samples](https://github.com/Azure/azure-search-vector-samples) and [What's New](whats-new.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 search-api-migration.md 文档"
}
```

### Explanation
此次修改对 `articles/search/search-api-migration.md` 文档进行了更新，主要包括日期的更改以及一处内容的文本调整。具体改动包括：

1. **日期更新**：文档中 `ms.date` 从原来的 `02/14/2025` 更新为 `03/10/2025`，反映了最新的更新时间。

2. **内容调整**：在关于控制平面的 API 版本表格中，原先的一个版本 `2024-03-01-preview` 被替换为 `2025-02-01-preview`，表示有新的预览版本可供用户使用。

3. **行数变化**：此次修改增加了 2 行，删除了 2 行，总体变更为 4 行，表明内容进行了小幅度的调整，确保提供的信息是最新的和准确的。

该文档的目的是指导用户如何迁移到 Azure AI Search 的最新 REST API 版本，确保用户的代码在升级后依然可以正常运作。此更新旨在提高文档的有效性并提供相关的使用信息。用户可以通过以下链接查看文档的详细信息：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-api-migration.md)。

## articles/search/search-api-preview.md{#item-511f5d}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 11/05/2024
+ms.date: 03/31/2025
 ---
 
 # Preview features in Azure AI Search
@@ -26,6 +26,9 @@ Preview features are removed from this list if they're retired or transition to
 
 |Feature&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  | Category | Description | Availability  |
 |---------|------------------|-------------|---------------|
+| [**flightingOptIn parameter in a semantic configuration**](semantic-how-to-configure.md#opt-in-for-prerelease-semantic-ranking-models) | Queries| You can opt in to use prerelease semantic ranking models if one is available in a search service region. | [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true). |
+| [**Rescore vector queries over binary embeddings using full precision vectors**](vector-search-how-to-quantization.md#recommended-rescoring-techniques) | Relevance (scoring) | For vector indexes that contain quantized binary embeddings, you can rescore query results using a full precision query vector. The query engine uses the dot product for rescoring, which improves the quality of search results. Set `enableRescoring` and `discardOriginals` to use this feature.| [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true). |
+| [**Facet hierarchies, aggregations, and facet filters**](search-faceted-navigation-examples.md) | Queries| New facet query parameters support nested facets. For numeric facetable fields, you can sum the values of each field. You can also specify filters on a facet to add inclusion or exclusion criteria. | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2025-03-01-preview&preserve-view=true). |
 | [**Query rewrite in the semantic reranker**](semantic-how-to-query-rewrite.md) | Relevance (scoring) | You can set options on a semantic query to rewrite the query input into a revised or expanded query that generates more relevant results from the L2 ranker. | [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&preserve-view=true).|
 | [**Document Layout skill**](cognitive-search-skill-document-intelligence-layout.md) | Applied AI (skills) | A new skill used to analyze a document for structure and provide [structure-aware chunking](search-how-to-semantic-chunking.md). | [Create or Update Skillset (preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true). |
 | [**Keyless billing for Azure AI skills processing**](cognitive-search-attach-cognitive-services.md). | Applied AI (skills) | You can now use a managed identity and roles for a keyless connection to Azure AI services for built-in skills processing. This capability removes restrictions for having both search and AI services in the same region.  | [Create or Update Skillset  (preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true).|
@@ -57,7 +60,9 @@ Preview features are removed from this list if they're retired or transition to
 
 |Feature&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  | Category | Description | Availability  |
 |---------|------------------|-------------|---------------|
-| [**Network security perimeter**](search-security-network-security-perimeter.md) | Service | Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. | The Azure portal and the [Network Security Perimeter APIs 2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true). |
+| [**Service upgrade**](search-how-to-upgrade.md) | Feature | Upgrade your search service to higher storage limits in your region. With a one-time upgrade, you no longer need to recreate your service. | The Azure portal and [Upgrade Service (2025-02-01-preview)](/rest/api/searchmanagement/services/upgrade?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true). |
+| [**Pricing tier change**](search-capacity-planning.md#change-your-pricing-tier) | Feature | Change the [pricing tier](search-sku-tier.md) of your search service. This provides flexibility to scale storage, increase request throughput, and decrease latency based on your needs. In this preview, you can only change between Basic and Standard (S1, S2, and S3) tiers. | The Azure portal and [Update Service (2025-02-01-preview)](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#searchupdateservicewithsku). |
+| [**Network security perimeter**](search-security-network-security-perimeter.md) | Service | Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. | The Azure portal and the [Network Security Perimeter APIs 2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) or the latest preview version. |
 | [**Search service under a user-assigned managed identity**](search-howto-managed-identities-data-sources.md) | Service | Configures a search service to use a previously created user-assigned managed identity. | [Services - Update](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true#identity), 2021-04-01-preview, or the latest preview version. We recommend using the latest preview version. |
 
 ## Preview features in Azure SDKs
@@ -91,10 +96,10 @@ For data plane operation on content, [**`2024-05-01-preview`**](/rest/api/search
 GET {endpoint}/indexes('{indexName}')?api-version=2024-05-01-Preview
 ```
 
-For management operations on the search service, [**`2024-06-01-preview`**](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) is the most recent preview version. The following example shows the syntax for Update Service 2024-06-01-preview version.
+For management operations on the search service, [**`2025-05-01-preview`**](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-05-01-preview&preserve-view=true) is the most recent preview version. The following example shows the syntax for Update Service 2025-05-01-preview version.
 
 ```rest
-PATCH https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2024-06-01-preview
+PATCH https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2025-05-01-preview
 
 {
   "tags": {
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 search-api-preview.md 文档"
}
```

### Explanation
此次修改对 `articles/search/search-api-preview.md` 文档进行了更新，主要包括日期更改、新增功能描述及若干文本调整。具体改动包括：

1. **日期更新**：文档中 `ms.date` 从原来的 `11/05/2024` 更新为 `03/31/2025`，显式地标识了最新的更新时间。

2. **新增功能描述**：在文档中，针对预览功能区域新增了多个条目，这些条目描述了新的查询参数和功能，包括：
   - 允许使用先行发布的语义排名模型的 `flightingOptIn` 参数。
   - 对于包含量化二进制嵌入的向量索引，可以使用全精度查询向量重新评分的功能。
   - 支持嵌套的面状查询参数的新功能，包括新旧面状字段的值求和。

3. **内容移动与修改**：有条目被移至不同的位置，同时对部分功能的描述进行了优化，确保信息的准确性和可读性。

4. **行数变化**：本次修改增加了 9 行，删除了 4 行，整体变更为 13 行，反映出内容的增强与更新。

这些修改的目的是为用户提供最新的 Azure AI Search 预览功能信息，确保用户了解新增的功能和来自 API 的变化。用户可以通过以下链接查看文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-api-preview.md)。

## articles/search/search-capacity-planning.md{#item-0dd6c9}

<details>
<summary>Diff</summary>
````diff
@@ -11,14 +11,14 @@ ms.custom:
   - ignite-2023
   - ignite-2024
 ms.topic: conceptual
-ms.date: 03/11/2025
+ms.date: 03/31/2025
 ---
 
 # Estimate and manage capacity of a search service
 
 In Azure AI Search, capacity is based on *replicas* and *partitions* that can be scaled to your workload. Replicas are copies of the search engine. Partitions are units of storage. Each new search service starts with one each, but you can add or remove replicas and partitions independently to accommodate fluctuating workloads. Adding capacity increases the [cost of running a search service](search-sku-manage-costs.md#billable-events).
 
-The physical characteristics of replicas and partitions, such as processing speed and disk IO, vary by [service tier](search-sku-tier.md). On a standard search service, the replicas and partitions are faster and larger than those of a basic service.
+The physical characteristics of replicas and partitions, such as processing speed and disk IO, vary by [pricing tier](search-sku-tier.md). On a standard search service, the replicas and partitions are faster and larger than those of a basic service.
 
 Changing capacity isn't instantaneous. It can take up to an hour to commission or decommission partitions, especially on services with large amounts of data.
 
@@ -30,73 +30,121 @@ When scaling a search service, you can choose from the following tools and appro
 + [Management REST API](/rest/api/searchmanagement/services/create-or-update)
 
 > [!NOTE]
-> Higher capacity partitions are available at the same billing rate on newer services created after April and May 2024. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits) for partition size upgrades.
+> If your service was created before April or May 2024, a one-time upgrade to higher storage limits might be available at no extra cost. For more information, see [Upgrade your search service](search-how-to-upgrade.md).
 
 ## Concepts: search units, replicas, partitions
 
 Capacity is expressed in *search units* that can be allocated in combinations of *partitions* and *replicas*.  
 
 | Concept  | Definition|
 |----------|-----------|
-|*Search unit* | A single increment of total available capacity (36 units). A minimum of one unit is required to run the service. The first replica and partition pair is the first search unit. However, each extra instance of a replica *or* a partition consumes an extra search unit. For example, you start with one replica and partition (one search unit), add a second replica, you are now consuming two search units. A search unit is also the billing unit for an Azure AI Search service. |
+|*Search unit* | A single increment of total available capacity (36 units). A minimum of one unit is required to run the service. The first replica and partition pair is the first search unit. However, each extra instance of a replica *or* a partition consumes an extra search unit. For example, you start with one replica and partition (one search unit), add a second replica, you're now consuming two search units. A search unit is also the billing unit for an Azure AI Search service. |
 |*Replica* | Instances of the search service, used primarily to load balance query operations. Each replica hosts one copy of an index. If you allocate three replicas, you have three copies of an index available for servicing query requests.|
 |*Partition* | Physical storage and I/O for read/write operations (for example, when rebuilding or refreshing an index). Each partition has a slice of the total index. If you allocate three partitions, your index is divided into thirds. |
 
-Review the [partitions and replicas table](#partition-and-replica-combinations) for possible combinations that stay under the 36 unit limit. 
+Review the [partitions and replicas table](#partition-and-replica-combinations) for possible combinations that stay under the 36 unit limit.
 
 ## When to add capacity
 
-Initially, a service is allocated a minimal level of resources consisting of one partition and one replica. The [tier you choose](search-sku-tier.md) determines partition size and speed, and each tier is optimized around a set of characteristics that fit various scenarios. If you choose a higher-end tier, you might [need fewer partitions](search-performance-tips.md#service-capacity) than if you go with S1. One of the questions you'll need to answer through self-directed testing is whether a larger and more expensive partition yields better performance than two cheaper partitions on a service provisioned at a lower tier.
+Initially, a service is allocated a minimal level of resources consisting of one partition and one replica. The [tier you choose](search-sku-tier.md) determines partition size and speed, and each tier is optimized around a set of characteristics that fit various scenarios. If you choose a higher-end tier, you might [need fewer partitions](search-performance-tips.md#service-capacity) than if you go with S1. One of the questions you need to answer through self-directed testing is whether a larger and more expensive partition yields better performance than two cheaper partitions on a service provisioned at a lower tier.
 
-A single service must have sufficient resources to handle all workloads (indexing and queries). Neither workload runs in the background. You can schedule indexing for times when query requests are naturally less frequent, but the service won't otherwise prioritize one task over another. Additionally, a certain amount of redundancy smooths out query performance when services or nodes are updated internally.
+A single service must have sufficient resources to handle all workloads (indexing and queries). Neither workload runs in the background. You can schedule indexing for times when query requests are naturally less frequent, but the service doesn't otherwise prioritize one task over another. Additionally, a certain amount of redundancy smooths out query performance when services or nodes are updated internally.
 
-Some guidelines for determining whether to add capacity include:
+Guidelines for determining whether to add capacity include:
 
-+ Meeting the high availability criteria for service level agreement
-+ The frequency of HTTP 503 errors is increasing
-+ Large query volumes are expected
++ Meeting the high availability criteria for service-level agreement.
++ The frequency of HTTP 503 errors is increasing.
++ Large query volumes are expected.
++ A [one-time upgrade](#how-to-upgrade-capacity) to newer infrastructure and larger partitions isn’t sufficient.
++ The current number of partitions isn’t adequate for indexing workloads.
 
-As a general rule, search applications tend to need more replicas than partitions, particularly when the service operations are biased toward query workloads. Each replica is a copy of your index, allowing the service to load balance requests against multiple copies. All load balancing and replication of an index is managed by Azure AI Search and you can alter the number of replicas allocated for your service at any time. You can allocate up to 12 replicas in a Standard search service and 3 replicas in a Basic search service. Replica allocation can be made either from the [Azure portal](search-create-service-portal.md) or one of the programmatic options.
+As a general rule, search applications tend to need more replicas than partitions, particularly when the service operations are biased toward query workloads. Each replica is a copy of your index, allowing the service to load balance requests against multiple copies. Azure AI Search manages all load balancing and replication of an index, and you can alter the number of replicas allocated for your service at any time. You can allocate up to 12 replicas in a Standard search service and 3 replicas in a Basic search service. Replica allocation can be made either from the [Azure portal](search-create-service-portal.md) or one of the programmatic options.
 
 Extra partitions are helpful for intensive indexing workloads. Extra partitions spread read/write operations across a larger number of compute resources.
 
-Finally, larger indexes take longer to query. As such, you might find that every incremental increase in partitions requires a smaller but proportional increase in replicas. The complexity of your queries and query volume will factor into how quickly query execution is turned around.
+Finally, larger indexes take longer to query. As such, you might find that every incremental increase in partitions requires a smaller but proportional increase in replicas. The complexity of your queries and query volume factors into how quickly query execution is turned around.
 
 > [!NOTE]
-> Adding more replicas or partitions increases the cost of running the service, and can introduce slight variations in how results are ordered. Be sure to check the [pricing calculator](https://azure.microsoft.com/pricing/calculator/) to understand the billing implications of adding more nodes. The [chart below](#chart) can help you cross-reference the number of search units required for a specific configuration. For more information on how additional replicas impact query processing, see [Ordering results](search-pagination-page-layout.md#ordering-results).
+> Adding more replicas or partitions increases the cost of running the service, and can introduce slight variations in how results are ordered. Be sure to check the [pricing calculator](https://azure.microsoft.com/pricing/calculator/) to understand the billing implications of adding more nodes. The [chart below](#chart) can help you cross-reference the number of search units required for a specific configuration. For more information on how extra replicas affect query processing, see [Ordering results](search-pagination-page-layout.md#ordering-results).
 
 <a name="adjust-capacity"></a>
 
+## How to upgrade capacity
+
+Some Azure AI Search capabilities are only available to new services. One such capability is higher storage capacity, which applies to [services created after April 2024](search-limits-quotas-capacity.md#service-limits). However, if you created your service before April 2024, you can get higher capacity without recreating your service by performing a one-time upgrade. For more information, see [Upgrade your search service](search-how-to-upgrade.md).
+
 ## How to change capacity
 
-To increase or decrease the capacity of your search service, add or remove partitions and replicas.
+To increase or decrease the capacity of your service, you have two options:
+
++ [Add or remove partitions and replicas](#add-or-remove-partitions-and-replicas)
++ [Change your pricing tier](#change-your-pricing-tier)
 
-1. Sign in to the [Azure portal](https://portal.azure.com/) and select the search service.
+### Add or remove partitions and replicas
 
-1. Under **Settings**, open the **Scale** page to modify replicas and partitions. 
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.
+
+1. From the left pane, select **Settings** > **Scale**.
 
    The following screenshot shows a Standard service provisioned with one replica and partition. The formula at the bottom indicates how many search units are being used (1). If the unit price was $100 (not a real price), the monthly cost of running this service would be $100 on average.
 
-   :::image type="content" source="media/search-capacity-planning/1-initial-values.png" alt-text="Scale page showing current values" border="true":::
+   :::image type="content" source="media/search-capacity-planning/initial-values.png" alt-text="Screenshot of the Scale page showing the current replica and partition values." border="true":::
 
 1. Use the slider to increase or decrease the number of partitions. Select **Save**.
 
    This example adds a second replica and partition. Notice the search unit count; it's now four because the billing formula is replicas multiplied by partitions (2 x 2). Doubling capacity more than doubles the cost of running the service. If the search unit cost was $100, the new monthly bill would now be $400.
 
-   For the current per unit costs of each tier, visit the [Pricing page](https://azure.microsoft.com/pricing/details/search/).
+   For the current per unit costs of each tier, visit the [pricing page](https://azure.microsoft.com/pricing/details/search/).
+
+   :::image type="content" source="media/search-capacity-planning/add-two-each.png" alt-text="Screenshot of the Scale page with added replicas and partitions." border="true":::
 
-   :::image type="content" source="media/search-capacity-planning/2-add-2-each.png" alt-text="Add replicas and partitions" border="true":::
+1. Check your notifications to confirm that the operation started.
 
-1. After saving, you can check notifications to confirm the action succeeded.
+   :::image type="content" source="media/search-capacity-planning/portal-notifications.png" alt-text="Screenshot of the notification of the scaling operation in the Azure portal." border="true":::
 
-   :::image type="content" source="media/search-capacity-planning/3-save-confirm.png" alt-text="Save changes" border="true":::
+   This operation can take several hours to complete. You can’t cancel the process after it starts, and there’s no real-time monitoring of replica and partition adjustments. However, the following message displays while changes are underway.
 
-   Changes in capacity can take anywhere from 15 minutes up to several hours to complete. You can't cancel once the process has started and there's no real-time monitoring for replica and partition adjustments. However, the following message remains visible while changes are underway.
+   :::image type="content" source="media/search-capacity-planning/updating-message.png" alt-text="Screenshot of the Updating message in the Azure portal." border="true":::
 
-   :::image type="content" source="media/search-capacity-planning/4-updating.png" alt-text="Status message in the Azure portal" border="true":::
+### Change your pricing tier
 
 > [!NOTE]
-> After a service is provisioned, it cannot be upgraded to a higher tier. You must create a search service at the new tier and reload your indexes. See [Create an Azure AI Search service in the Azure portal](search-create-service-portal.md) for help with service provisioning.
+> The 2025-02-01-preview supports changes between Basic and Standard (S1, S2, and S3) tiers. Currently, you can only switch from a lower tier to a higher tier, such as going from Basic to S1. Your region also can't have [capacity constraints on the higher tier](search-region-support.md).
+
+Your [pricing tier](search-sku-tier.md) determines the maximum storage of your search service. If you need more <!-- or less capacity -->capacity, you can switch to a different pricing tier that accommodates your storage needs.
+
+In addition to capacity, changing your pricing tier affects the workload and maximum limits of your service. Before you proceed, compare the [service limits](search-limits-quotas-capacity.md) of your current tier and your desired tier. These include limits on:
+
++ Partition storage
++ Indexes
++ Vectors
++ Indexers
++ Shared private link resources
++ Synonyms
++ Index aliases
++ Semantic ranker throttling
+
+Generally, switching to a higher tier increases your [storage limit](search-limits-quotas-capacity.md#service-limits) and [vector limit](search-limits-quotas-capacity.md#vector-index-size-limits), increases request throughput, and decreases latency<!-- , while switching to a lower tier decreases your storage limit and vector limit, decreases request throughput, and increases latency -->.
+
+To change your pricing tier:
+
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.
+
+1. From the left pane, select **Settings** > **Scale**.
+
+1. Under your current tier, select **Change Pricing Tier**.
+
+   :::image type="content" source="media/search-capacity-planning/change-pricing-tier.png" alt-text="Screenshot of the Change Pricing Tier button in the Azure portal." border="true":::
+
+1. On the **Select Pricing Tier** page, choose a higher tier from the list. Currently, you can only move up between Basic, S1, S2, and S3. Other pricing tiers are unavailable and appear dimmed.
+
+1. To switch to the higher tier, select **Select**.
+
+   :::image type="content" source="media/search-capacity-planning/pricing-tier-list.png" alt-text="Screenshot of the Select Pricing Tier page and the list of higher tiers in the Azure portal." border="true":::
+
+   This operation can take several hours to complete. You can’t cancel the process after it starts, and there’s no real-time monitoring of tier changes. However, on the **Overview** page, a **Provisioning** status indicates the operation is underway for your service.
+
+   :::image type="content" source="media/search-capacity-planning/provisioning-status.png" alt-text="Screenshot of the service Overview page with a Provisioning status." border="true":::
 
 ## How scale requests are handled
 
@@ -143,25 +191,25 @@ The following chart applies to Standard tier and higher. It shows all possible c
 
 Basic search services have lower search unit counts.
 
-+ On search services created before April 3, 2024, a basic search service can have exactly one partition and up to three replicas, for a maximum limit of three SUs. The only adjustable resource is replicas. 
++ On search services created before April 3, 2024, Basic services can have exactly one partition and up to three replicas for a maximum limit of three SUs. The only adjustable resource is replicas. However, you might be able to increase your partition count by [upgrading your service](search-how-to-upgrade.md).
 
-+ On search services created after April 3, 2024 in [supported regions](search-limits-quotas-capacity.md#service-limits), basic services can have up to three partitions and three replicas. The maximum SU limit is nine to support a full complement of partitions and replicas.
++ On search services created after April 3, 2024 in [supported regions](search-limits-quotas-capacity.md#service-limits), Basic services can have up to three partitions and three replicas. The maximum SU limit is nine to support a full complement of partitions and replicas.
 
 For search services on any billable tier, regardless of creation date, you need a minimum of two replicas for high availability on queries.
 
 For billing rates per tier and currency, see the [Azure AI Search pricing page](https://azure.microsoft.com/pricing/details/search/).
 
 ## Estimate capacity using a billable tier
 
-Storage needs are determined by the size of the indexes you expect to build. There are no solid heuristics or generalities that help with estimates. The only way to determine the size of an index is [build one](search-what-is-an-index.md). Its size is based on tokenization and embeddings, and whether you enable suggesters, filtering, and sorting, or can take advantage of [vector compression](vector-search-how-to-quantization.md).
+The size of the indexes you expect to build determines storage needs. There are no solid heuristics or generalities that help with estimates. The only way to determine the size of an index is [build one](search-what-is-an-index.md). Its size is based on tokenization and embeddings, and whether you enable suggesters, filtering, and sorting, or can take advantage of [vector compression](vector-search-how-to-quantization.md).
 
 We recommend estimating on a billable tier, Basic or above. The Free tier runs on physical resources shared by multiple customers and is subject to factors beyond your control. Only the dedicated resources of a billable search service can accommodate larger sampling and processing times for more realistic estimates of index quantity, size, and query volumes during development. 
 
 1. [Review service limits at each tier](search-limits-quotas-capacity.md#service-limits) to determine whether lower tiers can support the number of indexes you need. Consider whether you need multiple copies of an index for active development, testing, and production. 
 
    A search service is subject to object limits (maximum number of indexes, indexers, skillsets, etc.) and storage limits. Whichever limit is reached first is the effective limit. 
 
-1. [Create a service at a billable tier](search-create-service-portal.md). Tiers are optimized for certain workloads. For example, Storage Optimized tier has a limit of 10 indexes because it's designed to support a low number of very large indexes.
+1. [Create a service at a billable tier](search-create-service-portal.md). Tiers are optimized for certain workloads. For example, the Storage Optimized tier has a limit of 10 indexes because it's designed to support a low number of large indexes.
 
     + Start low, at Basic or S1, if you're not sure about the projected load.
 
@@ -187,11 +235,11 @@ Storage requirements can be inflated if you include data that will never be sear
 
 ## Service-level agreement considerations
 
-The Free tier and preview features aren't covered by [service-level agreements (SLAs)](https://azure.microsoft.com/support/legal/sla/search/v1_0/). For all billable tiers, SLAs take effect when you provision sufficient redundancy for your service. 
+The Free tier and preview features aren't covered by [service-level agreements (SLAs)](https://azure.microsoft.com/support/legal/sla/search/v1_0/). For all billable tiers, SLAs take effect when you provision sufficient redundancy for your service.
 
-+ Two or more replicas satisfy query (read) SLAs. 
++ Two or more replicas satisfy query (read) SLAs.
 
-+ Three or more replicas satisfy query and indexing (read-write) SLAs. 
++ Three or more replicas satisfy query and indexing (read-write) SLAs.
 
 The number of partitions doesn't affect SLAs.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 search-capacity-planning.md 文档"
}
```

### Explanation
此次修改对 `articles/search/search-capacity-planning.md` 文档进行了大幅更新，主要包括对日期的更新、内容的扩充、结构的调整以及一些术语的规范化。具体改动如下：

1. **日期更新**：文档中的 `ms.date` 从 `03/11/2025` 更新为 `03/31/2025`，说明文档是最新的。

2. **内容扩充**：添加了多个新的段落与说明，这些新增内容涵盖有关如何在较早版本的服务中进行一次性升级的说明，以及如何在 Azure 门户中管理搜索服务容量的更详细的步骤。

3. **术语规范化**：将文档中的“服务级别”改为“定价级别”，确保术语使用一致，并反映了定价策略的变化。

4. **改善结构**：修改了部分段落的顺序和格式，使得内容更清晰易读。此外，详细说明了添加或删除分区与副本的步骤，帮助用户更好地理解如何管理其搜索服务的容量。

5. **行数变化**：本次修改增加了 80 行，删除了 32 行，总体变更为 112 行，展现了内容的增强性与信息的准确性。

该文档旨在指导用户如何评估和管理 Azure AI Search 服务的容量，以更好地满足工作负载的需求。用户可以通过以下链接查看最新版本的文档：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-capacity-planning.md)。

## articles/search/search-create-service-portal.md{#item-f90159}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - references_regions
   - build-2024
 ms.topic: how-to
-ms.date: 02/20/2025
+ms.date: 03/21/2025
 ---
 
 # Create an Azure AI Search service in the Azure portal
@@ -33,13 +33,13 @@ You can also use:
 
 ## Before you start
 
-Some properties are fixed for the lifetime of the search service. Before creating your service, decide on the following properties:
+Some properties are fixed for the lifetime of the search service. Before you create your service, decide on the following properties:
 
 | Property | Description |
 |--|--|
 | [Name](#name-your-service) | Becomes part of the URL endpoint. The name must be unique and follow naming rules. |
 | [Region](search-region-support.md) | Determines data residency and availability of certain features. For example, semantic ranker and Azure AI integration have region requirements. Choose a region that supports the features you need. |
-| [Tier](search-sku-tier.md) | Determines infrastructure, service limits, and billing. Some features aren't available on lower or specialized tiers. |
+| [Tier](search-sku-tier.md) | Determines infrastructure, service limits, and billing. Some features aren't available on lower or specialized tiers. In the 2025-02-01-preview, you can also [switch from a lower tier to a higher tier](search-capacity-planning.md#change-your-pricing-tier). |
 
 ## Subscribe to Azure
 
@@ -131,7 +131,7 @@ Currently, the following regions offer cross-regional availability for Azure AI
 + Americas: West US, East US
 + Europe: Switzerland North, Sweden Central
 
-This list isn't definitive, and depending on your tier, you might have more choices. Region status can also change quickly, so confirm your region choice before creating your search service.
+This list isn't definitive, and depending on your tier, you might have more choices. Region status can also change quickly, so confirm your region choice before you create your search service.
 
 ## Choose a tier
 
@@ -149,8 +149,8 @@ The Basic and Standard tiers are the most common for production workloads, but m
 :::image type="content" source="media/search-create-service-portal/select-pricing-tier.png" lightbox="media/search-create-service-portal/select-pricing-tier.png" alt-text="Screenshot of the Select Pricing Tier page in the Azure portal." border="true":::
 
 > [!NOTE]
-> + You can't change the tier after creating your search service, so choose carefully.
-> + Search services created after April 3, 2024 have larger partitions and higher vector quotas at every billable tier.
+> + After you create your service, you can move up between Basic and Standard (S1, S2, and S3) tiers. Switching to a lower tier isn't currently supported. For more information, see [Change your pricing tier](search-capacity-planning.md#change-your-pricing-tier).
+> + Services created after April 3, 2024 have larger partitions and higher vector quotas at every billable tier.
 
 ## Create your service
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 search-create-service-portal.md 文档"
}
```

### Explanation
此次对 `articles/search/search-create-service-portal.md` 文档的修改包括更新日期、调整文本表述以及增加有关定价层更改的说明。具体改动如下：

1. **日期更新**：文档中的 `ms.date` 从 `02/20/2025` 更新为 `03/21/2025`，确保读者获得最新的信息。

2. **表述调整**：在描述创建服务的步骤时，对某些句子的语序进行了修改，比如将“Before creating your service”修改为“Before you create your service”，使得表述更加流畅。

3. **内容增强**：在关于服务层的部分新增了重要说明，例如在创建服务后，可以在基础层和标准层之间进行升级，而降级未被支持，从而帮助用户在选择服务层时做出更明智的决策。

4. **行数变化**：本次修改增加了 6 行，删除了 6 行，整体变更为 12 行。这些变更反映了内容的精简与必要扩展，同时保持文档的准确性和可读性。

通过这些改动，文档更好地指导用户如何在 Azure 门户中创建 Azure AI Search 服务。用户可以通过以下链接查看文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-create-service-portal.md)。

## articles/search/search-faceted-navigation-examples.md{#item-2b1158}

<details>
<summary>Diff</summary>
````diff
@@ -0,0 +1,721 @@
+---
+
+title: Faceted navigation examples
+titleSuffix: Azure AI Search
+description: Examples that demonstrate query syntax for facet hierarchies, distinct counts, facet aggregations, and facet filters.
+
+manager: nitinme
+author: HeidiSteen
+ms.author: heidist
+ms.service: azure-ai-search
+ms.topic: how-to
+ms.date: 04/04/2025
+---
+
+# Faceted navigation examples
+
+This section extends [faceted navigation configuration](search-faceted-navigation.md) with examples that demonstrate basic usage and other scenarios.
+
+Facetable fields are defined in an index, but facet parameters and expressions are defined in query requests. If you have an index with facetable fields, you can try new preview features like [facet hierarchies](#facet-hierarchy-example), [facet aggregations](#facet-aggregation-example), and [facet filters](#facet-filtering-example) on existing indexes.
+
+## Facet parameters and syntax
+
+Depending on the API, a facet query is usually an array of facet expressions that are applied to search results. Each facet expression contains a facetable field name, optionally followed by a comma-separated list of name-value pairs.
+
++ *facet query* is a query request that includes a facet property.
++ *facetable field* is a field definition in the search index attributed with the `facetable` property.
++ *count* is the number of matches for each facet found in the search results.
+
+The following table describes facet parameters used in the examples.
+
+| Facet parameter | Description | Usage | Example |
+|-----------------|-------------|-------|---------|
+| `count` | Maximum number of facet terms per structure.| Integer. Default is 10. There's no upper limit, but higher values degrade performance, especially if the faceted field contains a large number of unique terms. This is due to the way facet queries are distributed across shards. You can set `count` to zero or to a value that's greater than or equal to the number of unique values in the facetable field to get an accurate count across all shards. The tradeoff is increased latency. | `Tags,count:5` limits the faceted navigation response to 5 facet buckets that containing the most facet counts, but they can be in any order. |
+| `sort` | Determines order of facet buckets. | Valid values are `count`, `-count`, `value`, `-value`. Use `count` to list facets from greatest to smallest. Use `-count` to sort in ascending order (smallest to greatest). Use `value` to sort alphanumerically by facet value in ascending order. Use `-value` to sort descending by value. | `"facet=Category,count:3,sort:count"` gets the top three facet buckets in search results, listed in descending order by the number of matches in each Category. If the top three categories are Budget, Extended-Stay, and Luxury, and Budget has 5 hits, Extended-Stay has 6, and Luxury has 4, then the facet buckets are ordered as Extended-Stay, Budget, Luxury. Another example is`"facet=Rating,sort:-value"`. It produces facets for all possible ratings, in descending order by value. If ratings are from 1 to 5, the facets are ordered 5, 4, 3, 2, 1, irrespective of how many documents match each rating. |
+| `values` | Provides values for facet labels. | Set to pipe-delimited numeric or `Edm.DateTimeOffset` values specifying a dynamic set of facet entry values. The values must be listed in sequential, ascending order to get the expected results. | `"facet=baseRate,values:10 | 20"` produces three facet buckets: one for base rate 0 up to but not including 10, one for 10 up to but not including 20, and one for 20 and higher. A string `"facet=lastRenovationDate,values:2024-02-01T00:00:00Z"` produces two facet buckets: one for hotels renovated before February 2024, and one for hotels renovated February 1, 2024 or later. |
+| `interval` | Provides an interval sequence for facets that can be grouped into intervals. | An integer interval greater than zero for numbers, or minute, hour, day, week, month, quarter, year for date time values. | `"facet=baseRate,interval:100"` produces facet buckets based on base rate ranges of size 100. If base rates are all between $60 and $600, there are facet buckets for 0-100, 100-200, 200-300, 300-400, 400-500, and 500-600. The string `"facet=lastRenovationDate,interval:year"` produces one facet bucket for each year a hotel was renovated. |
+| `timeoffset` | Specifies the UTC time offset to account for in setting time boundaries. | Set to (`[+-]hh:mm, [+-]hhmm, or [+-]hh`). If used, the `timeoffset` parameter must be combined with the interval option, and only when applied to a field of type `Edm.DateTimeOffset`. | `"facet=lastRenovationDate,interval:day,timeoffset:-01:00"` uses the day boundary that starts at 01:00:00 UTC (midnight in the target time zone). |
+
+`count` and `sort` can be combined in the same facet specification, but they can't be combined with `interval` or `values`.
+
+`interval` and `values` can't be combined together.
+
+Interval facets on date time are computed based on the UTC time if `timeoffset` isn't specified. For example, for `"facet=lastRenovationDate,interval:day"`, the day boundary starts at 00:00:00 UTC.
+
+## Basic facet example
+
+The following facet queries work against the [hotels sample index](search-get-started-portal.md). You can use **JSON view** in Search Explorer to paste in the JSON query. For help with getting started, see [Add faceted navigation to search results](search-faceted-navigation.md).
+
+This first query retrieves facets for Categories, Ratings, Tags, and rooms with baseRate values in specific ranges. Notice the last facet is on a subfield of the Rooms collection. Facets count the parent document (Hotels) and not intermediate subdocuments (Rooms), so the response determines the number of *hotels* that have any rooms in each pricing category.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "ocean view",  
+  "facets": [ "Category", "Rating", "Tags", "Rooms/BaseRate,values:80|150|220" ],
+  "count": true 
+}  
+```
+
+This second example uses a filter to narrow down the previous faceted query result after the user selects Rating 3 and category "Motel".
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "water view",  
+  "facets": [ "Tags", "Rooms/BaseRate,values:80|150|220" ],
+  "filter": "Rating eq 3 and Category eq 'Motel'",
+  "count": true  
+} 
+```
+
+The third example sets an upper limit on unique terms returned in a query. The default is 10, but you can increase or decrease this value using the count parameter on the facet attribute. This example returns facets for city, limited to 5.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "view",  
+  "facets": [ "Address/City,count:5" ],
+  "count": true
+} 
+```
+
+This example shows three facets for "Category", "Tags", and "Rating", with a count override on "Tags" and a range override for "Rating", which is otherwise stored as a double in the index.
+
+```http
+POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
+{
+    "search": "*",
+    "facets": [ 
+        "Category", 
+        "Tags,count:5", 
+        "Rating,values:1|2|3|4|5"
+    ],
+    "count": true
+}
+```
+
+For each faceted navigation tree, there's a default limit of the top 10 facet instances found by the query. This default makes sense for navigation structures because it keeps the values list to a manageable size. You can override the default by assigning a value to "count". For example, `"Tags,count:5"` reduces the number of tags under the Tags section to the top five.
+
+For Numeric and DateTime values only, you can explicitly set values on the facet field (for example, `facet=Rating,values:1|2|3|4|5`) to separate results into contiguous ranges (either ranges based on numeric values or time periods). Alternatively, you can add "interval", as in `facet=Rating,interval:1`. 
+
+Each range is built using 0 as a starting point, a value from the list as an endpoint, and then trimmed of the previous range to create discrete intervals.
+
+## Distinct values example
+
+You can formulate a query that returns a distinct value count for each facetable field. This example formulates an empty or unqualified query (`"search": "*"`) that matches on all documents, but by setting `top` to zero, you get just the counts, with no results.
+
+For brevity, this query includes just two fields marked as `facetable` in the hotels sample index.
+
+```http
+POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
+{
+    "search": "*",
+    "count": true,
+    "top": 0,
+    "facets": [ 
+        "Category", "Address/StateProvince""
+    ]
+}
+```
+
+Results from this query are as follows:
+
+```json
+{
+  "@odata.count": 50,
+  "@search.facets": {
+    "Address/StateProvince": [
+      {
+        "count": 9,
+        "value": "WA"
+      },
+      {
+        "count": 6,
+        "value": "CA "
+      },
+      {
+        "count": 4,
+        "value": "FL"
+      },
+      {
+        "count": 3,
+        "value": "NY"
+      },
+      {
+        "count": 3,
+        "value": "OR"
+      },
+      {
+        "count": 3,
+        "value": "TX"
+      },
+      {
+        "count": 2,
+        "value": "GA"
+      },
+      {
+        "count": 2,
+        "value": "MA"
+      },
+      {
+        "count": 2,
+        "value": "TN"
+      },
+      {
+        "count": 1,
+        "value": "AZ"
+      }
+    ],
+    "Category": [
+      {
+        "count": 13,
+        "value": "Budget"
+      },
+      {
+        "count": 12,
+        "value": "Suite"
+      },
+      {
+        "count": 7,
+        "value": "Boutique"
+      },
+      {
+        "count": 7,
+        "value": "Resort and Spa"
+      },
+      {
+        "count": 6,
+        "value": "Extended-Stay"
+      },
+      {
+        "count": 5,
+        "value": "Luxury"
+      }
+    ]
+  },
+  "value": []
+}
+```
+
+## Facet hierarchy example
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and available in the Azure portal, you can configure a facet hierarchy using the `>` and `;` operators.
+
+The nesting (hierarchical) operator `>` denotes a parent–child relationship, and the semicolon operator `;` denotes multiple fields at the same nesting level, which are all children of the same parent. The parent must contain only one field. Both the parent and child fields must be `facetable`. 
+
+The order of operations in a facet expression that includes facet hierarchies are:
+
+* options operator (comma `,`) that separates facet parameters for the facet field, such as the comma in `Rooms/BaseRate,values`
+* parentheses, such as the ones enclosing `(Rooms/BaseRate,values:50 ; Rooms/Type)`.
+* nesting operator (angled bracket `>`)
+* append operator (semicolon `;`), demonstrated in a second example `"Tags>(Rooms/BaseRate,values:50 ; Rooms/Type)"` in this section, where two child facets are peers under the Tags parent.
+
+There are several examples for facet hierarchies. The first example is a query that returns just a few documents, which is helpful for viewing a full response. Facets count the parent document (Hotels) and not intermediate subdocuments (Rooms), so the response determines the number of *hotels* that have any rooms in each facet bucket.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{
+  "search": "ocean",  
+  "facets": ["Address/StateProvince>Address/City", "Tags>Rooms/BaseRate,values:50"],
+  "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+  "count": true 
+}
+```
+
+Results from this query are as follows. Both hotels have pools. For other tags, only one hotel provides the amenity.
+
+```json
+{
+  "@odata.count": 2,
+  "@search.facets": {
+    "Tags": [
+      {
+        "value": "pool",
+        "count": 2,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 2
+            }
+          ]
+        }
+      },
+      {
+        "value": "air conditioning",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "bar",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "restaurant",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "view",
+        "count": 1,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 1
+            }
+          ]
+        }
+      }
+    ],
+    "Address/StateProvince": [
+      {
+        "value": "FL",
+        "count": 1,
+        "@search.facets": {
+          "Address/City": [
+            {
+              "value": "Tampa",
+              "count": 1
+            }
+          ]
+        }
+      },
+      {
+        "value": "HI",
+        "count": 1,
+        "@search.facets": {
+          "Address/City": [
+            {
+              "value": "Honolulu",
+              "count": 1
+            }
+          ]
+        }
+      }
+    ]
+  },
+  "value": [
+    {
+      "@search.score": 1.6076145,
+      "HotelName": "Ocean Water Resort & Spa",
+      "Description": "New Luxury Hotel for the vacation of a lifetime. Bay views from every room, location near the pier, rooftop pool, waterfront dining & more.",
+      "Tags": [
+        "view",
+        "pool",
+        "restaurant"
+      ],
+      "Address": {
+        "City": "Tampa",
+        "StateProvince": "FL"
+      }
+    },
+    {
+      "@search.score": 1.0594962,
+      "HotelName": "Windy Ocean Motel",
+      "Description": "Oceanfront hotel overlooking the beach features rooms with a private balcony and 2 indoor and outdoor pools. Inspired by the natural beauty of the island, each room includes an original painting of local scenes by the owner. Rooms include a mini fridge, Keurig coffee maker, and flatscreen TV. Various shops and art entertainment are on the boardwalk, just steps away.",
+      "Tags": [
+        "pool",
+        "air conditioning",
+        "bar"
+      ],
+      "Address": {
+        "City": "Honolulu",
+        "StateProvince": "HI"
+      }
+    }
+  ]
+}
+```
+
+This second example extends the previous one, demonstrating multiple top-level facets with multiple children. Notice the semicolon (`;`) operator separates each child.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{  
+  "search": "+ocean",  
+  "facets": ["Address/StateProvince > Address/City", "Tags > (Rooms/BaseRate,values:50 ; Rooms/Type)"],
+  "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+  "count": true 
+}  
+```
+
+A partial response, trimmed for brevity, shows Tags with child facets for the rooms base rate and type. In the hotels sample index, both hotels that match to `+ocean` have rooms in each type and a pool.
+
+```json
+{
+  "@odata.count": 2,
+  "@search.facets": {
+    "Tags": [
+      {
+        "value": "pool",
+        "count": 2,
+        "@search.facets": {
+          "Rooms/BaseRate": [
+            {
+              "to": 50,
+              "count": 0
+            },
+            {
+              "from": 50,
+              "count": 2
+            }
+          ],
+          "Rooms/Type": [
+            {
+              "value": "Budget Room",
+              "count": 2
+            },
+            {
+              "value": "Deluxe Room",
+              "count": 2
+            },
+            {
+              "value": "Standard Room",
+              "count": 2
+            },
+            {
+              "value": "Suite",
+              "count": 2
+            }
+          ]
+        }}]},
+  ...
+}
+```
+
+This last example shows precedence rules for parentheses that affects nesting levels. Suppose you want to return a facet hierarchy in this order.
+
+```
+Address/StateProvince
+  Address/City
+    Category
+    Rating
+```
+
+To return this hierarchy, create a query where Category and Rating are siblings under Address/City.
+
+```json
+  { 
+    "search": "beach",  
+    "facets": [
+        "Address/StateProvince > (Address/City > (Category ; Rating))"
+        ],
+    "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+    "count": true 
+  }
+```
+
+If you remove the innermost parentheses, Category and Rating are no longer siblings because the precedence rules mean that the `>` operator is evaluated before `;`.
+
+```json
+  { 
+    "search": "beach",  
+    "facets": [
+        "Address/StateProvince > (Address/City > Category ; Rating)"
+        ],
+    "select": "HotelName, Description, Tags, Address/StateProvince, Address/City",
+    "count": true 
+  }
+```
+
+The top-level parent is still Address/StateProvince, but now Address/City and Rating are on same level.
+
+```
+Address/StateProvince
+  Rating
+  Address/City
+    Category
+```
+
+## Facet filtering example
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and available in the Azure portal, you can configure facet filters.
+
+Facet filtering enables you to constrain the facet values returned to those matching a specified regular expression. Two new parameters accept a regular expression that is applied to the facet field:
+
+* `includeTermFilter` filters the facet values to those that match the regular expression
+* `excludeTermFilter` filters the facet values to those that don't match the regular expression 
+
+If a facet string satisfies both conditions, the `excludeTermFilter` takes precedence because the set of bucket strings is first evaluated with `includeTermFilter` and then excluded with `excludeTermFilter`.
+
+Only those facet values that match the regular expression are returned. You can combine these parameters with other facet options (for example, `count`, `sort`, and [hierarchical faceting](#facet-hierarchy-example)) on string fields.
+
+Because the regular expression is nested within a JSON string value, you must escape both the double quote (`"`) and the backslash (`\`) characters. The regular expression itself is delimited by the forward slash (`/`). For more information about escape patterns, see [Regular expression search](query-lucene-syntax.md#bkmk_regex).
+
+The following example shows how to escape special characters in your regular expression such as backslash, double quotes, or regular expression syntax characters. 
+
+```json
+{
+    "search": "*", 
+    "facets": ["name,includeTermFilter:/EscapeBackslash\\\OrDoubleQuote\\"OrRegexCharacter\\(/"] 
+}
+```
+
+Here's an example of a facet filter that matches on Budget and Extended-Stay hotels, with Rating as a child of each hotel category.
+
+```http
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+{ 
+    "search": "*", 
+    "facets": ["(Category,includeTermFilter:/(Budget|Extended-Stay)/)>Rating,values:1|2|3|4|5"],
+    "select": "HotelName, Category, Rating",
+    "count": true 
+} 
+```
+
+The following example is an abbreviated response (hotel documents are omitted for brevity).
+
+```json
+{
+  "@odata.count": 50,
+  "@search.facets": {
+    "Category": [
+      {
+        "value": "Budget",
+        "count": 13,
+        "@search.facets": {
+          "Rating": [
+            {
+              "to": 1,
+              "count": 0
+            },
+            {
+              "from": 1,
+              "to": 2,
+              "count": 0
+            },
+            {
+              "from": 2,
+              "to": 3,
+              "count": 4
+            },
+            {
+              "from": 3,
+              "to": 4,
+              "count": 5
+            },
+            {
+              "from": 4,
+              "to": 5,
+              "count": 4
+            },
+            {
+              "from": 5,
+              "count": 0
+            }
+          ]
+        }
+      },
+      {
+        "value": "Extended-Stay",
+        "count": 6,
+        "@search.facets": {
+          "Rating": [
+            {
+              "to": 1,
+              "count": 0
+            },
+            {
+              "from": 1,
+              "to": 2,
+              "count": 0
+            },
+            {
+              "from": 2,
+              "to": 3,
+              "count": 4
+            },
+            {
+              "from": 3,
+              "to": 4,
+              "count": 1
+            },
+            {
+              "from": 4,
+              "to": 5,
+              "count": 1
+            },
+            {
+              "from": 5,
+              "count": 0
+            }
+          ]
+        }
+      }
+    ]
+  }, 
+  "value": [  ALL 50 HOTELS APPEAR HERE ]
+}
+```
+
+## Facet aggregation example
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and available in the Azure portal, you can aggregate facets.
+
+Facet aggregations allow you to compute metrics from facet values. The aggregation capability works alongside the existing faceting options. The only supported metric is `sum`. Adding `metric: sum` to a numeric facet aggregates all the values of each bucket. 
+
+You can add a default value to use if a document contains a null for that field: `"facets": [ "Rooms/SleepsCount, metric: sum, default:2"]`. If a room has a null value for the Rooms/SleepsCount field, the default substitutes for the missing value.
+
+You can sum any facetable field of a numeric data type (except vectors and geographic coordinates). 
+
+Here's an example using the hotels-sample-index. The Rooms/SleepsCount field is facetable and numeric, so we choose this field to demonstrate sum. If we sum that field, we get the sleep count for the entire hotel. Recall that facets count the parent document (Hotels) and not intermediate subdocuments (Rooms), so the response sums the SleepsCount of all rooms for the entire hotel. In this query, we add a filter to sum the SleepsCount for just one hotel.
+
+```rest
+POST /indexes/hotels-sample-index/docs/search?api-version=2025-03-01-Preview
+
+{ 
+      "search": "*",
+      "filter": "HotelId eq '41'",
+      "facets": [ "Rooms/SleepsCount, metric: sum"],
+      "select": "HotelId, HotelName, Rooms/Type, Rooms/SleepsCount",
+      "count": true
+}
+```
+
+A response for the query might look like the following example. Windy Ocean Model can accommodate a total of 40 guests.
+
+```json
+{
+  "@odata.count": 1,
+  "@search.facets": {
+    "Rooms/SleepsCount": [
+      {
+        "sum": 40.0
+      }
+    ]
+  },
+  "value": [
+    {
+      "@search.score": 1.0,
+      "HotelId": "41",
+      "HotelName": "Windy Ocean Motel",
+      "Rooms": [
+        {
+          "Type": "Suite",
+          "SleepsCount": 4
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Budget Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Budget Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 4
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 4
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Suite",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Deluxe Room",
+          "SleepsCount": 2
+        },
+        {
+          "Type": "Standard Room",
+          "SleepsCount": 2
+        }
+      ]
+    }
+  ]
+}
+```
+
+## Next steps
+
+Revisit [facet navigation configuration](search-faceted-navigation.md) for tools and APIs, and review [best practices](search-faceted-navigation.md#best-practices-for-working-with-facets) for working with facets in code.
+
+We recommend the [C#: Add search to web apps](tutorial-csharp-overview.md) for an example of faceted navigation that includes code for the presentation layer. The sample also includes filters, suggestions, and autocomplete. It uses JavaScript and React for the presentation layer.
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增搜索的分面导航示例文档"
}
```

### Explanation
此次修改新增了 `search-faceted-navigation-examples.md` 文档，内容详尽地介绍了分面导航的示例，包括如何配置和使用分面功能。该文档的主要内容如下：

1. **文档结构与标题**：文档标题为“Faceted navigation examples”，提供了关于分面导航的示例和配置的指导，旨在帮助用户理解如何在 Azure AI Search 中利用分面功能。

2. **基本概念介绍**：文档开始对分面参数及其语法进行了详细说明，解释了不同的分面参数（如 `count`、`sort` 和 `values`）的用法和示例，帮助用户理解如何构建查询请求。

3. **示例查询**：提供了多个示例查询，展示了如何在不同场景下使用分面功能。例如，如何对酒店信息进行分类，通过设置过滤条件缩小搜索结果的范围，并展示了如何进行独特值计数、分面层次结构的配置等。

4. **高级功能**：引入了分面过滤和分面聚合等高级功能，使得用户能够依据正则表达式过滤特定的分面值，并计算分面值的度量。

5. **操作步骤**：文档清晰地列出了每个功能的使用示例，并解释了查询结果的结构，以便用户能够快速理解输出结果的意义。

6. **行数与内容**：新增的文档包含721行内容，全面覆盖了分面导航的各种用例和示例，为开发人员提供了丰富的资源，帮助其在实际应用中实现分面功能。

通过这篇文档，用户能够更好地利用 Azure AI Search 实现复杂的搜索需求，并有效地进行分面导航的配置和操作。用户可以通过以下链接查看详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-faceted-navigation-examples.md)。

## articles/search/search-faceted-navigation.md{#item-f29d1e}

<details>
<summary>Diff</summary>
````diff
@@ -1,37 +1,45 @@
 ---
-title: Add a faceted navigation category hierarchy
+title: Add facets to a query
 titleSuffix: Azure AI Search
-description: Add faceted navigation for self-directed filtering in applications that integrate with Azure AI Search.
+description: Add faceted navigation for self-directed navigation in applications that integrate with Azure AI Search.
 
 manager: nitinme
 author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
-ms.topic: concept-article
-ms.date: 02/26/2025
+ms.topic: how-to
+ms.date: 04/04/2025
 ---
 
-# Add faceted navigation to a search app
+# Add faceted navigation to search results
 
-Faceted navigation is used for self-directed drill-down filtering on query results in a search app, where your application offers form controls for scoping search to groups of documents (for example, categories or brands), and Azure AI Search provides the data structures and filters to back the experience.
+Faceted navigation is used for self-directed filtering on query results in a search app, where your application offers form controls for scoping search to groups of documents (for example, categories or brands), and Azure AI Search provides the data structures and filters to back the experience.
 
-In this article, learn how to return a faceted navigation structure in Azure AI Search.
+In this article, learn the steps for returning a faceted navigation structure in Azure AI Search. Once you're familiar with basic concepts and clients, continue to [Facet examples](search-faceted-navigation-examples.md) for syntax about various use cases, including basic faceting and distinct counts. 
+
+More facet capabilities are available through preview APIs:
+
++ hierarchical facet structures
++ facet filtering
++ facet aggregations
+
+[Facet navigation examples](search-faceted-navigation-examples.md) provide the syntax and usage for the preview features.
 
 ## Faceted navigation in a search page
 
-Facets are dynamic and returned on a query. A search response brings with it all of the facet categories used to navigate the documents in the result. The query executes first, and then facets are pulled from the current results and assembled into a faceted navigation structure.
+Facets are dynamic because they're based on each specific query result set. A search response brings with it all of the facet buckets used to navigate the documents in the result. The query executes first, and then facets are pulled from the current results and assembled into a faceted navigation structure.
 
-In Azure AI Search, facets are one layer deep and can't be hierarchical. If you aren't familiar with faceted navigation structures, the following example shows one on the left. Counts indicate the number of matches for each facet. The same document can be represented in multiple facets.
+In Azure AI Search, facets are one layer deep and can't be hierarchical unless you use the preview API. If you aren't familiar with faceted navigation structures, the following example shows one on the left. Counts indicate the number of matches for each facet. The same document can be represented in multiple facets.
 
 :::image source="media/search-faceted-navigation/azure-search-facet-nav.png" alt-text="Screenshot of faceted search results.":::
 
 Facets can help you find what you're looking for, while ensuring that you don't get zero results. As a developer, facets let you expose the most useful search criteria for navigating your search index.
 
 ## Faceted navigation in code
 
-Facets are enabled on supported fields in an index, and then specified on a query. At query time, a faceted navigation structure is returned at the top of the response.
+Facets are enabled on supported fields in an index, and then specified on a query. The faceted navigation structure is returned at the beginning of the response, followed by the results.
 
-The following REST example is an unqualified query (`"search": "*"`) that is scoped to the entire index (see the [built-in hotels sample](search-get-started-portal.md)). It returns a faceted navigation structure for the "Category" field.
+The following REST example is an empty query (`"search": "*"`) that is scoped to the entire index (see the [built-in hotels sample](search-get-started-portal.md)). The `facets` parameter specifies the "Category" field.
 
 ```http
 POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
@@ -47,7 +55,7 @@ POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-
 }
 ```
 
-The response for the example includes the faceted navigation structure at the top. The structure consists of "Category" values and a count of the hotels for each one. It's followed by the rest of the search results, trimmed here to just one document for brevity. This example works well for several reasons. The number of facets for this field fall under the limit (default is 10) so all of them appear, and every hotel in the index of 50 hotels is represented in exactly one of these categories.
+The response for the example starts with the faceted navigation structure. The structure consists of "Category" values and a count of the hotels for each one. It's followed by the rest of the search results, trimmed here to just one document for brevity. This example works well for several reasons. The number of facets for this field fall under the limit (default is 10) so all of them appear, and every hotel in the index of 50 hotels is represented in exactly one of these categories.
 
 ```json
 {
@@ -94,214 +102,142 @@ The response for the example includes the faceted navigation structure at the to
                 "concierge"
             ],
             "ParkingIncluded": false,
-        }
+        },
+        . . . 
     ]
 }
 ```
 
 ## Enable facets on fields
 
-During [index creation or update](search-how-to-create-search-index.md), facets are enabled when you set `"facetable": true` on new fields that you add to an index. Although it's not strictly required, it's a best practice to also set the "filterable" attribute so that you can build the necessary filters that back the faceted navigation experience in your search application.
+You can add facets to new fields that contain plain text or numeric content. Supported data types include strings, dates, boolean fields, and numeric fields (but not vectors).
 
-Here's a JSON example of the hotels sample index, showing "facetable" and "filterable" on low cardinality fields that contain single values or short phrases: "Category", "Tags", "Rating".
+You can use the Azure portal, REST APIs, Azure SDKs or any method that supports the creation or update of index schemas in Azure AI Search. As a first step, identify which fields to use for faceting.
 
-```json
-{
-  "name": "hotels",  
-  "fields": [
-    { "name": "hotelId", "type": "Edm.String", "key": true, "searchable": false, "sortable": false, "facetable": false },
-    { "name": "Description", "type": "Edm.String", "filterable": false, "sortable": false, "facetable": false },
-    { "name": "HotelName", "type": "Edm.String", "facetable": false },
-    { "name": "Category", "type": "Edm.String", "filterable": true, "facetable": true },
-    { "name": "Tags", "type": "Collection(Edm.String)", "filterable": true, "facetable": true },
-    { "name": "Rating", "type": "Edm.Int32", "filterable": true, "facetable": true },
-    { "name": "Location", "type": "Edm.GeographyPoint" }
-  ]
-}
-```
+### Choose which fields to attribute
 
-### Prerequisites
+Facets can be calculated over single-value fields and collections. Fields that work best in faceted navigation have these characteristics:
 
-Add faceting to new fields that contain plain text or numeric content. Supported data types include strings, dates, boolean fields, and numeric fields (but not vectors).
+* Human readable (nonvector) content.
+* Low cardinality (a few distinct values that repeat throughout documents in your search corpus).
+* Short descriptive values (one or two words) that render nicely in a navigation tree.
+
+The values within a field, and not the field name itself, produce the facets in a faceted navigation structure. If the facet is a string field named *Color*, facets are blue, green, and any other value for that field. Review field values to ensure there are no typos, nulls, or casing differences. Consider [assigning a normalizer](search-normalizers.md) to a filterable and facetable field to smooth out minor variations in the text. For example, "Canada", "CANADA", and "canada" would all be normalized to one bucket.
+
+### Avoid unsupported fields
 
 You can't set facets on existing fields, on vector fields, or fields of type `Edm.GeographyPoint` or `Collection(Edm.GeographyPoint)`.
 
-On complex fields, "facetable" must be null.
+On complex field collections, "facetable" must be null. 
 
 ### Start with new field definitions
 
-Because attribution determines how a field is indexed, many attributes can only be set when fields are created. This restriction applies to facets and filters. If your index already exists and you add a new field definition, existing documents in the index get a null value for the new field. This null value is replaced the next time you [refresh the index](search-howto-reindex.md).
+Attributes that affect how a field is indexed can only be set when fields are created. This restriction applies to facets and filters. 
 
-### Choosing which fields to attribute
+If your index already exists, you can add a new field definition that provides facets. Existing documents in the index get a null value for the new field. This null value is replaced the next time you [refresh the index](search-howto-reindex.md).
 
-Facets can be calculated over single-value fields and collections. Fields that work best in faceted navigation have these characteristics:
+#### [**Azure portal**](#tab/portal-facet)
 
-* Human readable (nonvector) content.
+1. In the search services page of the [Azure portal](https://portal.azure.com), go to the **Fields** tab of the index and select **Add field**.
 
-* Low cardinality (a few distinct values that repeat throughout documents in your search corpus).
+1. Provide a name, data type, and attributes. We recommend adding filterable because it's common to set filters based on a facet bucket in the response. We recommend sortable because filters produce unordered results, and you might want to sort them in your application.
 
-* Short descriptive values (one or two words) that render nicely in a navigation tree.
+   You can also set searchable if you also want to support full text search on the field, and retrievable if you want to include the field in the search response.
 
-The values within a field, and not the field name itself, produce the facets in a faceted navigation structure. If the facet is a string field named *Color*, facets are blue, green, and any other value for that field.
+   :::image type="content" source="media/search-faceted-navigation/portal-add-facetable-field.png" alt-text="Screenshot of the Add fields page in the Azure portal." border="true" lightbox="media/search-faceted-navigation/portal-add-facetable-field.png":::
 
-### Defaults in REST and Azure SDKs
+1. Save the field definition.
 
-If you're using one of the Azure SDKs, your code must explicitly set the "facetable" attribute on a field.
+#### [**REST**](#tab/rest-facet)
 
-The REST API has defaults for field attributes based on the [data type](/rest/api/searchservice/supported-data-types). The following data types are "filterable" and "facetable" by default:
+When you define an index schema, facets are enabled when you set `"facetable": true` on new fields that you add to an index. Although it's not strictly required, it's a best practice to also set the "filterable" attribute so that you can build the necessary filters that back the faceted navigation experience in your search application.
+
+Start with [Create or Update Index](search-how-to-create-search-index.md) request and specify the fields collection.
+
+  Here's a JSON example of the hotels sample index, showing "facetable" and "filterable" on low cardinality fields that contain single values or short phrases: "Category", "Tags", "Rating".
+
+  ```json
+  {
+    "name": "hotels",  
+    "fields": [
+      { "name": "hotelId", "type": "Edm.String", "key": true, "searchable": false, "sortable": false, "facetable": false },
+      { "name": "Description", "type": "Edm.String", "filterable": false, "sortable": false, "facetable": false },
+      { "name": "HotelName", "type": "Edm.String", "facetable": false },
+      { "name": "Category", "type": "Edm.String", "filterable": true, "facetable": true },
+      { "name": "Tags", "type": "Collection(Edm.String)", "filterable": true, "facetable": true },
+      { "name": "Rating", "type": "Edm.Int32", "filterable": true, "facetable": true },
+      { "name": "Location", "type": "Edm.GeographyPoint" }
+    ]
+  }
+  ```
+
+#### Defaults in REST
+
+Both the Azure portal and the REST API have defaults for field attributes based on the [data type](/rest/api/searchservice/supported-data-types). The following data types are "filterable" and "facetable" by default:
 
 * `Edm.String` and `Collection(Edm.String)`
 * `Edm.DateTimeOffset` and `Collection(Edm.DateTimeOffset)`
 * `Edm.Boolean` and`Collection(Edm.Boolean)`
 * `Edm.Int32`, `Edm.Int64`, `Edm.Double`, and their collection equivalents
 
-## Return facets in a query
+#### [**Azure SDKs**](#tab/sdk-facet)
 
-Recall that facets are calculated from results in a query response. You only get facets for documents found by the current query. 
+If you're using one of the Azure SDKs, your code must explicitly set facetable on a field.
 
-1. Facets are configured at query-time. Use the [Search POST](/rest/api/searchservice/documents/search-post) or [Search GET](/rest/api/searchservice/documents/search-get) request, or an equivalent Azure SDK API, to specify facets. 
+Assign the facet property to fields using APIs that create or update an index.
 
-1. Set facet query parameters in the request. In Search POST, `facets` are an array of facet expressions to apply to the search query. Each facet expression contains a field name, optionally followed by a comma-separated list of name-value pairs. Valid facet parameters are `count`, `sort`, `values`, `interval`, and `timeoffset`.
+* [Azure SDK for .NET: SearchIndex.Fields Property](/dotnet/api/azure.search.documents.indexes.models.searchindex.fields)
+* [Azure SDK for Python: SearchField Class](/python/api/azure-search-documents/azure.search.documents.indexes.models.searchfield)
+* [Azure SDK for Java: SearchField Class](/java/api/com.azure.search.documents.indexes.models.searchfield)
+* [Azure SDK for JavaScript: Simple Field interface](/javascript/api/@azure/search-documents/simplefield)
 
-    | Facet parameter | Description and usage |
-    |-----------------|-----------------------|
-    | `count` | Maximum number of facet terms; default is 10. An example is `Tags,count:5`. There's no upper limit on the number of terms, but higher values degrade performance, especially if the faceted field contains a large number of unique terms. This is due to the way faceting queries are distributed across shards. You can set count to zero or to a value that's greater than or equal to the number of unique values in the "facetable" field to get an accurate count across all shards. The tradeoff is increased latency.
-    | `sort` | Set to "count", "-count", "value", "-value". Use `count` to sort descending by count. Use `-count` to sort ascending by count. Use `value` to sort ascending by value. Use `-value` to sort descending by value (for example, `"facet=category,count:3,sort:count"` gets the top three categories in facet results in descending order by the number of documents with each city name). If the top three categories are Budget, Motel, and Luxury, and Budget has five hits, Motel has 6, and Luxury has 4, then the buckets are in the order Motel, Budget, Luxury. For `-value`, `"facet=rating,sort:-value"` produces buckets for all possible ratings, in descending order by value (for example, if the ratings are from 1 to 5, the buckets are ordered 5, 4, 3, 2, 1, irrespective of how many documents match each rating). |
-    | `values` | Set to pipe-delimited numeric or `Edm.DateTimeOffset` values specifying a dynamic set of facet entry values. For example, `"facet=baseRate,values:10 | 20"` produces three buckets: one for base rate 0 up to but not including 10, one for 10 up to but not including 20, and one for 20 and higher. A string `"facet=lastRenovationDate,values:2010-02-01T00:00:00Z"` produces two buckets: one for hotels renovated before February 2010, and one for hotels renovated February 1, 2010 or later. The values must be listed in sequential, ascending order to get the expected results. |
-    | `interval` | An integer interval greater than zero for numbers, or minute, hour, day, week, month, quarter, year for date time values. For example, `"facet=baseRate,interval:100"` produces buckets based on base rate ranges of size 100. If base rates are all between $60 and $600, there are buckets for 0-100, 100-200, 200-300, 300-400, 400-500, and 500-600. The string `"facet=lastRenovationDate,interval:year"` produces one bucket for each year when hotels were renovated. |
-    | `timeoffset` | Can be set to (`[+-]hh:mm, [+-]hhmm, or [+-]hh`). If used, the timeoffset parameter must be combined with the interval option, and only when applied to a field of type `Edm.DateTimeOffset`. The value specifies the UTC time offset to account for in setting time boundaries. For example: `"facet=lastRenovationDate,interval:day,timeoffset:-01:00"` uses the day boundary that starts at 01:00:00 UTC (midnight in the target time zone). |
+---
 
-`count` and `sort` can be combined in the same facet specification, but they can't be combined with interval or values, and interval and values can't be combined together.
+## Return facets in a query
 
-Interval facets on date time are computed based on the UTC time if `timeoffset` isn't specified. For example, for `"facet=lastRenovationDate,interval:day"`, the day boundary starts at 00:00:00 UTC.
+Recall that facets are dynamically calculated from results in a query response. You only get facets for documents found by the current query.
 
-### Basic facet example
+#### [**Azure portal**](#tab/portal-facet-response)
 
-The following example works against the [hotels sample index](search-get-started-portal.md). You can use **JSON view** in Search Explorer to paste in the JSON query. This example shows three facets for "Category", "Tags", and "Rating", with a count override on "Tags" and a range override for "Rating", which is otherwise stored as a double in the index.
+Use JSON view in Search Explorer to set facet parameters in the [Azure portal](https://portal.azure.com).
 
-```http
-POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
-{
-    "search": "*",
-    "facets": [ 
-        "Category", 
-        "Tags,count:5", 
-        "Rating,values:1|2|3|4|5"
-    ],
-    "count": true
-}
-```
+1. Select an index and open Search Explorer in JSON View.
+1. Provide a query in JSON. You can type it out, copy the JSON from a REST example, or use intellisense to help with syntax. Refer to the REST example in the next tab for reference on facet expressions.
+1. Select **Search** to return faceted results, articulated in JSON.
 
-For each faceted navigation tree, there's a default limit of the top 10 facet instances found by the query. This default makes sense for navigation structures because it keeps the values list to a manageable size. You can override the default by assigning a value to "count". For example, `"Tags,count:5"` reduces the number of tags under the Tags section to the top five.
+Here's a screenshot of the [basic facet query example](search-faceted-navigation-examples.md#basic-facet-example) on the [hotels sample index](search-get-started-portal.md). You can paste in other examples in this article to return the results in Search Explorer.
 
-For Numeric and DateTime values only, you can explicitly set values on the facet field (for example, `facet=Rating,values:1|2|3|4|5`) to separate results into contiguous ranges (either ranges based on numeric values or time periods). Alternatively, you can add "interval", as in `facet=Rating,interval:1`. 
+:::image type="content" source="media/search-faceted-navigation/portal-facet-query.png" alt-text="Screenshot of the Search Explorer page in the Azure portal." border="true" lightbox="media/search-faceted-navigation/portal-facet-query.png":::
 
-Each range is built using 0 as a starting point, a value from the list as an endpoint, and then trimmed of the previous range to create discrete intervals.
+#### [**REST**](#tab/rest-facet-response)
 
-### Distinct values example
+1. Facets are configured at query-time. Use the [Search POST](/rest/api/searchservice/documents/search-post) or [Search GET](/rest/api/searchservice/documents/search-get) request, or an equivalent Azure SDK API, to specify facets. 
 
-You can formulate a query that returns a distinct value count for each "facetable" field. This example formulates an empty or unqualified query (`"search": "*"`) that matches on all documents, but by setting `top` to zero, you get just the counts, with no results.
+1. Set facet query parameters in the request. In Search POST, `facets` are an array of facet expressions to apply to the search query. Each facet expression contains a field name, optionally followed by a comma-separated list of name-value pairs. Valid facet parameters are `count`, `sort`, `values`, `interval`, and `timeoffset`.
 
-For brevity, this query includes just two fields marked as "facetable" in the hotels sample index.
+    | Facet parameter | Description and usage |
+    |-----------------|-----------------------|
+    | `count` | Maximum number of facet terms per structure; default is 10. An example is `Tags,count:5`. There's no upper limit on the number of terms, but higher values degrade performance, especially if the faceted field contains a large number of unique terms. This is due to the way faceting queries are distributed across shards. You can set count to zero or to a value that's greater than or equal to the number of unique values in the "facetable" field to get an accurate count across all shards. The tradeoff is increased latency.
+    | `sort` | Set to `count`, `-count`, `value`, `-value`. Use `count` to sort descending by count. Use `-count` to sort ascending by count. Use `value` to sort ascending by value. Use `-value` to sort descending by value (for example, `"facet=category,count:3,sort:count"` gets the top three categories in facet results in descending order by the number of documents with each Category name). If the top three categories are Budget, Motel, and Luxury, and Budget has five hits, Motel has 6, and Luxury has 4, then the buckets are in the order Motel, Budget, Luxury. For `-value`, `"facet=rating,sort:-value"` produces buckets for all possible ratings, in descending order by value (for example, if the ratings are from 1 to 5, the buckets are ordered 5, 4, 3, 2, 1, irrespective of how many documents match each rating). |
+    | `values` | Set to pipe-delimited numeric or `Edm.DateTimeOffset` values specifying a dynamic set of facet entry values. For example, `"facet=baseRate,values:10 | 20"` produces three buckets: one for base rate 0 up to but not including 10, one for 10 up to but not including 20, and one for 20 and higher. A string `"facet=lastRenovationDate,values:2010-02-01T00:00:00Z"` produces two buckets: one for hotels renovated before February 2010, and one for hotels renovated February 1, 2010 or later. The values must be listed in sequential, ascending order to get the expected results. |
+    | `interval` | An integer interval greater than zero for numbers, or minute, hour, day, week, month, quarter, year for date time values. For example, `"facet=baseRate,interval:100"` produces buckets based on base rate ranges of size 100. If base rates are all between $60 and $600, there are buckets for 0-100, 100-200, 200-300, 300-400, 400-500, and 500-600. The string `"facet=lastRenovationDate,interval:year"` produces one bucket for each year when hotels were renovated. |
+    | `timeoffset` | Can be set to (`[+-]hh:mm, [+-]hhmm, or [+-]hh`). If used, the `timeoffset` parameter must be combined with the interval option, and only when applied to a field of type `Edm.DateTimeOffset`. The value specifies the UTC time offset to account for in setting time boundaries. For example: `"facet=lastRenovationDate,interval:day,timeoffset:-01:00"` uses the day boundary that starts at 01:00:00 UTC (midnight in the target time zone). |
 
-```http
-POST https://{{service_name}}.search.windows.net/indexes/hotels/docs/search?api-version={{api_version}}
-{
-    "search": "*",
-    "count": true,
-    "top": 0,
-    "facets": [ 
-        "Category", "Address/StateProvince""
-    ]
-}
-```
+`count` and `sort` can be combined in the same facet specification, but they can't be combined with `interval` or `values`, and `interval` and `values` can't be combined together.
 
-Results from this query are as follows:
+Interval facets on date time are computed based on the UTC time if `timeoffset` isn't specified. For example, for `"facet=lastRenovationDate,interval:day"`, the day boundary starts at 00:00:00 UTC.
 
-```json
-{
-  "@odata.count": 50,
-  "@search.facets": {
-    "Address/StateProvince": [
-      {
-        "count": 9,
-        "value": "WA"
-      },
-      {
-        "count": 6,
-        "value": "CA "
-      },
-      {
-        "count": 4,
-        "value": "FL"
-      },
-      {
-        "count": 3,
-        "value": "NY"
-      },
-      {
-        "count": 3,
-        "value": "OR"
-      },
-      {
-        "count": 3,
-        "value": "TX"
-      },
-      {
-        "count": 2,
-        "value": "GA"
-      },
-      {
-        "count": 2,
-        "value": "MA"
-      },
-      {
-        "count": 2,
-        "value": "TN"
-      },
-      {
-        "count": 1,
-        "value": "AZ"
-      }
-    ],
-    "Category": [
-      {
-        "count": 13,
-        "value": "Budget"
-      },
-      {
-        "count": 12,
-        "value": "Suite"
-      },
-      {
-        "count": 7,
-        "value": "Boutique"
-      },
-      {
-        "count": 7,
-        "value": "Resort and Spa"
-      },
-      {
-        "count": 6,
-        "value": "Extended-Stay"
-      },
-      {
-        "count": 5,
-        "value": "Luxury"
-      }
-    ]
-  },
-  "value": []
-}
-```
+---
 
 ## Best practices for working with facets
 
 This section is a collection of tips and workarounds that are helpful for application development.
 
-### Initialize a faceted navigation structure
+We recommend the [C#: Add search to web apps](tutorial-csharp-overview.md) for an example of faceted navigation that includes code for the presentation layer. The sample also includes filters, suggestions, and autocomplete. It uses JavaScript and React for the presentation layer.
+
+### Initialize a faceted navigation structure with an unqualified or empty search string
 
-It's useful to initialize a search page with an open query (`"search": "*"`) to completely fill in the faceted navigation structure. As soon as you pass query terms in the request, the faceted navigation structure is scoped to just the matches in the results, rather than the entire index.
+It's useful to initialize a search page with an open query (`"search": "*"`) to completely fill in the faceted navigation structure. As soon as you pass query terms in the request, the faceted navigation structure is scoped to just the matches in the results, rather than the entire index. This practice is helpful for verifying facet and filter behaviors during testing. If you include match criteria in the query, the response excludes documents that don't match, which has the potential downstream effect of excluding facets.
 
 ### Clear facets
 
@@ -315,7 +251,15 @@ Remember that you can't use `Edm.GeographyPoint` or `Collection(Edm.GeographyPoi
 
 ### Check for bad data
 
-As you prepare data for indexing, check fields for null values, misspellings or case discrepancies, and single and plural versions of the same word. By default, filters and facets don't undergo lexical analysis or [spell check](speller-how-to-add.md), which means that all values of a "facetable" field are potential facets, even if the words differ by one character. Optionally, you can [assign a normalizer](search-normalizers.md) to a "filterable" and "facetable" field to smooth out variations in casing and characters.
+As you prepare data for indexing, check fields for null values, misspellings or case discrepancies, and single and plural versions of the same word. By default, filters and facets don't undergo lexical analysis or [spell check](speller-how-to-add.md), which means that all values of a "facetable" field are potential facets, even if the words differ by one character. 
+
+[Normalizers](search-normalizers.md) can mitigate data discrepancies, correcting for casing and character differences. Otherwise, to inspect your data, you can check fields at their source, or run queries that return values from the index.
+
+An index isn't the best place to fix nulls or invalid values. You should fix data problems in your source, assuming it's a database or persistent storage, or in a data cleansing step that you perform prior to indexing. 
+
+### Ordering facet buckets
+
+Although you can sort within a bucket, there's no parameters for controlling the order of facet buckets in the navigation structure as a whole. If you want facet buckets in a specific order, you must provide it in application code.
 
 ### Discrepancies in facet counts
 
@@ -346,4 +290,5 @@ Content type
 
 ## Next steps
 
-We recommend the [C#: Add search to web apps](tutorial-csharp-overview.md) for an example of faceted navigation that includes code for the presentation layer. The sample also includes filters, suggestions, and autocomplete. It uses JavaScript and React for the presentation layer.
+> [!div class="nextstepaction"]
+> [Facet navigation examples](search-faceted-navigation-examples.md)
\ No newline at end of file
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 search-faceted-navigation.md 文档"
}
```

### Explanation
此次对 `articles/search/search-faceted-navigation.md` 文档的修改主要包括内容的精简、日期和主题的更新，以及对面向开发者的指导进行优化。具体改动包括：

1. **标题与描述更新**：文档的标题从“Add a faceted navigation category hierarchy”修改为“Add facets to a query”，描述也相应变化，使其更清晰地指向如何在查询中添加分面。

2. **内容简化与精炼**：减少了不少冗余的内容，将部分说明进行了合并或提炼，使文档更加简洁，确保开发者可以快速获取所需的信息。例如，关于如何返回分面结构的说明被重写为更易懂的语言。

3. **增加新功能提示**：文档中新增了对预览 API 的描述，包括分面层级结构、分面过滤、分面聚合等功能的指引，鼓励开发者利用新功能进行更复杂的查询和数据分析。

4. **示例与代码优化**：更改了代码示例的顺序，使得返回分面结构的过程更为直观，同时也去掉了不必要的代码行，保持了代码的整洁性。对于 REST API 示例进行了简化，强调了 `facets` 参数。

5. **强调最佳实践**：新增的段落内容解释了如何有效初始化和处理分面导航，提醒开发者注意数据质量的问题，建议在数据索引前解决潜在的空值和错误值的问题。

6. **行数变化**：本次修改增加了 112 行，删除了 167 行，主要是通过精简和合并内容来提升文档的可读性和有效性。

通过这些优化，文档现在能够更好地支持开发者理解和实现 Azure AI Search 中的分面导航，从而提升用户体验。用户可以通过以下链接查看文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-faceted-navigation.md)。

## articles/search/search-faq-frequently-asked-questions.yml{#item-eab2ba}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ metadata:
   ms.author: haileytapia
   ms.service: azure-ai-search
   ms.topic: faq
-  ms.date: 01/16/2025
+  ms.date: 03/21/2025
 title: Azure AI Search Frequently Asked Questions
 summary:  Find answers to commonly asked questions about Azure AI Search.
 
@@ -36,7 +36,7 @@ sections:
         answer: |
           For vectors, the embedding models you use determines the linguistic experience. 
           
-          For nonvector strings and numbers, the default analyzer used for tokenization is standard Lucene and it's language agnostic. Otherwise, language support is expressed through [language analyzers](index-add-language-analyzers.md#supported-language-analyzers) that apply linguistic rules to inbound (indexing) and outbound (queries) content. Some features, such as [speller](speller-how-to-add.md#supported-languages) and [query rewrite](semantic-how-to-query-rewrite.md), are limited to a subset of languages.
+          For nonvector strings and numbers, the default analyzer used for tokenization is standard Lucene, which is language agnostic. Otherwise, language support is expressed through [language analyzers](index-add-language-analyzers.md#supported-language-analyzers) that apply linguistic rules to inbound (indexing) and outbound (queries) content. Some features, such as [speller](speller-how-to-add.md#supported-languages) and [query rewrite](semantic-how-to-query-rewrite.md), are limited to a subset of languages.
 
       - question: |
           How do I integrate search into my solution?
@@ -56,15 +56,27 @@ sections:
           You can't pause a search service. In Azure AI Search, computing resources are allocated when the service is created. It's not possible to release and reclaim those resources on-demand.
 
       - question: |
-          Can I upgrade, downgrade, rename or move the service?
+          Can I upgrade or downgrade the service?
         answer: |
-          Service tier, name, and region are fixed for the lifetime of the service.
+          Services created before April 2024 in select regions can be [upgraded to higher capacity clusters](search-how-to-upgrade.md). Downgrading your service isn't supported. 
+          
+          To get more capacity, you can also [switch to a higher pricing tier](search-capacity-planning.md#change-your-pricing-tier). Your region can't have [capacity constraints on the higher tier](search-region-support.md), and you can only move up between Basic and Standard (S1, S2, and S3) tiers, such as going from Basic to S1. Currently, you can't switch to a lower tier.
+          
+      - question: |
+          Can I rename or move the service?
+        answer: |
+          Service name and region are fixed for the lifetime of the service.
           
       - question: |
           If I migrate my search service to another subscription or resource group, should I expect any downtime?
         answer: |
           As long as you follow the [checklist before moving resources](/azure/azure-resource-manager/management/move-resource-group-and-subscription) and make sure each step is completed, there shouldn't be any downtime.
 
+      - question: |
+          Why do I see different storage limits for same-tier search services?
+        answer: |
+          Storage limits can vary by service creation date. In most supported regions, [newer services have higher storage limits than older services](search-limits-quotas-capacity.md#partition-storage-gb), even if they're on the same tier. However, you might be able to [upgrade your old service](search-how-to-upgrade.md) to access the new limits.
+
   - name: Indexing 
     questions:
       - question: |
@@ -130,7 +142,12 @@ sections:
       - question: |
           Why do I see different vector index size limits between my new search services and existing search services?
         answer: |
-          Azure AI Search rolled out improved vector index size limits worldwide for new search services, but [some regions experience capacity constraints](search-region-support.md), and some regions don't have the required infrastructure. New search services created in supported regions should see increased vector index size limits. Unfortunately, we can't migrate existing services to the new limits. Also, only vector indexes that use the Hierarchical Navigable Small World (HNSW) algorithm report on vector index size in the Azure portal. If your index uses exhaustive KNN, vector index size is reported as zero, even though the index contains vectors. 
+          Azure AI Search rolled out improved vector index size limits worldwide for new search services, but [some regions experience capacity constraints](search-region-support.md), and some regions don't have the required infrastructure. New search services created after May 2024 in supported regions should see increased vector index size limits. Alternatively, if you have an existing service in a supported region, you can [upgrade your service](search-how-to-upgrade.md) to access the new limits.
+          
+      - question: |
+          Why does my vector index show zero storage?
+        answer: |    
+          Only vector indexes that use the Hierarchical Navigable Small World (HNSW) algorithm report on vector index size in the Azure portal. If your index uses exhaustive KNN, vector index size is reported as zero, even though the index contains vectors. 
 
       - question: |
           How do I enable vector search on a search index?
@@ -141,7 +158,7 @@ sections:
           
           * Add a "vectorSearch" section to the index schema specifying the configuration used by vector search fields, including the parameters of the Approximate Nearest Neighbor algorithm used, like HNSW.
           
-          * Use the latest stable version[**2024-07-01**](/rest/api/searchservice), or an Azure SDK to create or update the index, load documents, and issue queries. For more information, see [Create a vector index](vector-search-how-to-create-index.md).
+          * Use the latest stable version, [**2024-07-01**](/rest/api/searchservice), or an Azure SDK to create or update the index, load documents, and issue queries. For more information, see [Create a vector index](vector-search-how-to-create-index.md).
 
   - name: Queries
     questions:
@@ -189,7 +206,7 @@ sections:
       - question: |
           Does Azure AI Search process customer data in other regions?
         answer: |
-          Processing (vectorization or applied AI transformations) is performed in the Geo that hosts the Azure AI services used by skills, or the Azure apps or functions hosting custom skills, or the Azure OpenAI or Azure AI Foundry region that hosts your deployed models. These resources are specified by you, so you can choose whether to provision them in the same Geo as your search service or not
+          Processing (vectorization or applied AI transformations) is performed in the Geo that hosts the Azure AI services used by skills, or the Azure apps or functions hosting custom skills, or the Azure OpenAI or Azure AI Foundry region that hosts your deployed models. These resources are specified by you, so you can choose whether to deploy them in the same Geo as your search service or not.
           
           If you send data to external (non-Azure) models or services, the processing location is determined by the external service. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure AI Search 常见问题解答文档"
}
```

### Explanation
此次修改对 `search-faq-frequently-asked-questions.yml` 文档进行了部分更新，主要包括以下几个方面的变化：

1. **日期更新**：文档的日期更新为 2025 年 3 月 21 日，反映了内容的最新修订时间。

2. **内容精简与修改**：对某些问题的回答进行了更精简和明确的表述。例如：
   - 修改了关于如何升级或降级服务的问题，将其表述从“Can I upgrade, downgrade, rename or move the service?”简化为“Can I upgrade or downgrade the service?”，并将重命名和移动服务的相关内容分开为新的问题。
   - 对存储限制和向上升级服务的相关问题进行了详细说明，使之更加清晰易懂。

3. **新增问题**：增加了一些新的常见问题，以满足用户的需求。例如新增了关于“为什么我看到相同等级的搜索服务有不同的存储限制”和“为什么我的向量索引显示零存储”这两个问题，提供了关于向量索引的限制和报告机制的解释。

4. **结构优化**：文档整体结构得到了优化，使得问题与回答的层次关系更加清晰。通过增加小标题，使得内容更易于阅读并快速找到信息。

5. **格式与语言统一**：确保所有回答采用统一的格式，并增强了语言的流畅性。例如，修正了一些语句，使其更符合自然语言表达。

6. **减少冗余**：删除了一些不必要的内容，使文档更加简洁。例如，针对如何整合搜索到解决方案的内容进行了修正，使其更贴合实际需求。

本次修改通过清晰化问题与答案的表达，以及对常见问题的补充，增强了用户在使用 Azure AI Search 时的体验。用户可以通过以下链接查看文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-faq-frequently-asked-questions.yml)。

## articles/search/search-how-to-index-sql-database.md{#item-86d873}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 11/20/2024
+ms.date: 03/18/2025
 ---
 
 # Index data from Azure SQL Database
@@ -30,13 +30,17 @@ This article also provides:
 
 ## Prerequisites
 
-+ An [Azure SQL database](/azure/azure-sql/database/sql-database-paas-overview) with data in a single table or view, or a [SQL Managed Instance with a public endpoint](search-how-to-index-sql-managed-instance.md).
++ An [Azure SQL database](/azure/azure-sql/database/sql-database-paas-overview) or a [SQL Managed Instance with a public endpoint](search-how-to-index-sql-managed-instance.md).
 
-  Use a table if your data is large or if you need [incremental indexing](#CaptureChangedRows) using SQL's native change detection capabilities.
++ A single table or view.
 
-  Use a view if you need to consolidate data from multiple tables. Large views aren't ideal for SQL indexer. A workaround is to create a new table just for ingestion into your Azure AI Search index. You can use SQL integrated change tracking to track new and changed rows, which is easier to implement than High Water Mark.
+  Use a table if your data is large or if you need incremental indexing using SQL's native change detection capabilities ([SQL integrated change tracking](#indexing-new-changed-and-deleted-rows)) to reflect new, changed, and deleted rows in the search index.
 
-+ Read permissions. Azure AI Search supports SQL Server authentication, where the user name and password are provided on the connection string. Alternatively, you can [set up a managed identity and use Azure roles](search-howto-managed-identities-sql.md).
+  Use a view if you need to consolidate data from multiple tables. Large views aren't ideal for SQL indexer. A workaround is to create a new table just for ingestion into your Azure AI Search index. If you choose to go with a view, you can use [High Water Mark](#indexing-new-changed-and-deleted-rows) for change detection, but must use a workaround for deletion detection.
+
++ Primary key must be single-valued. On a table, it must also be non-clustered for full SQL integrated change tracking.
+
++ Read permissions. Azure AI Search supports SQL Server authentication, where the user name and password are provided on the connection string. Alternatively, you can [set up a managed identity and use Azure roles](search-howto-managed-identities-sql.md) with membership in **SQL Server Contributor** or **SQL DB Contributor** roles.
 
 To work through the examples in this article, you need the Azure portal or a [REST client](search-get-started-rest.md). If you're using Azure portal, make sure that access to all public networks is enabled in the Azure SQL firewall and that the client has access via an inbound rule. For a REST client that runs locally, configure the SQL Server firewall to allow inbound access from your device IP address. Other approaches for creating an Azure SQL indexer include Azure SDKs.
 
@@ -52,12 +56,12 @@ Use these instructions to create and load a table in Azure SQL Database for test
 
 1. On your Azure SQL database, select **Query editor (preview)** and then select **New Query**.
 
-1. Paste in and then run the T-SQL script that creates the hotels table.
+1. Paste in and then run the T-SQL script that creates the hotels table. A non-clustered primary key is a requirement for SQL integrated change tracking.
 
    ```tsql
    CREATE TABLE tbl_hotels
     (
-        Id TINYINT PRIMARY KEY,
+        Id TINYINT PRIMARY KEY NONCLUSTERED,
         Modified DateTime NULL DEFAULT '0000-00-00 00:00:00',
         IsDeleted TINYINT,
         HotelName VARCHAR(40),
@@ -93,9 +97,9 @@ Use these instructions to create and load a table in Azure SQL Database for test
    SELECT Description FROM tbl_hotels;
     ```
 
-You should see results similar to the following screenshot.
+   You should see results similar to the following screenshot.
 
-:::image type="content" source="media/search-how-to-index-sql-database/tsql-query-results.png" alt-text="Screenshot of query results showing the description field.":::
+   :::image type="content" source="media/search-how-to-index-sql-database/tsql-query-results.png" alt-text="Screenshot of query results showing the description field.":::
 
 The Description field provides the most verbose content. You should target this field for full text search and optional vectorization.
 
@@ -104,39 +108,42 @@ Now that you have a database table, you can use the Azure portal, REST client, o
 > [!TIP]
 > Another resource that provides sample content and code can be found on [Azure-Samples/SQL-AI-samples](https://github.com/Azure-Samples/SQL-AI-samples/tree/main/AzureSQLACSSamples/src).
 
-## Use the Azure portal
-
-You can use either the **Import data** wizard or **Import and vectorize data** wizard to automate indexing from an SQL database table or view. The data source configuration is similar for both wizards.
-
-1. [Start the wizard](search-import-data-portal.md#starting-the-wizards).
-
-1. On **Connect to your data**, select or verify that the data source type is either *Azure SQL Database* or *SQL database*.
+## Set up the indexer pipeline
 
-   The data source name refers to the data source connection object in Azure AI Search. If you use the vector wizard, your data source name is autogenerated using a custom prefix specified at the end of the wizard workflow.
+In this step, specify the data source, index, and indexer.
 
-1. Specify the server name, database name, and table or view name.
+### [**Azure portal**](#tab/portal-sql)
 
-   the Azure portal validates the connection. If the database is paused due to inactivity, navigate to the database server page and make sure database status is *online*. You can run a query on any table to activate the database.
+1. Make sure your SQL database is active and not paused due to inactivity. In the Azure portal, navigate to the database server page and verify the database status is *online*. You can run a query on any table to activate the database.
 
    :::image type="content" source="media/search-how-to-index-sql-database/database-online.png" alt-text="Screenshot of the database status page in the Azure portal.":::
 
-1. Specify an authentication method, either a SQL Server login defined during server setup, or a managed identity.
+1. Make sure you have a table or view that meets the requirements for indexers and change detection.
 
-   If you [configure Azure AI Search to use a managed identity](search-howto-managed-identities-data-sources.md), and you create a role assignment on the database server that grants **SQL Server Contributor** or **SQL DB Contributor** permissions to the identity, your indexer can connect to Azure SQL using Microsoft Entra ID and roles.
+   First, you can only pull from a single table or view. We recommend tables because they support SQL integrated change tracking policy, which detects new, updated, and deleted rows. A high water mark policy doesn't support row deletion and is harder to implement.
 
-1. For the **Import and vectorize data** wizard, you can specify options for change and deletion tracking.
+   Second, the primary key must be a single value (compound keys aren't supported) and non-clustered.
 
-   + Deletion tracking is based on [soft delete using custom metadata](#soft-delete-column-deletion-detection-policy).
+1. Switch to your search service and create a data source. Under **Search management** > **Data sources**, select **Add data source**:
 
-   + Change tracking is based on [SQL Server integrated change tracking](#sql-integrated-change-tracking-policy) or [high water mark change tracking](#high-water-mark-change-detection-policy).
+   1. For data source type, choose *Azure SQL Database*.
+   1. Provide a name for the data source object on Azure AI Search.
+   1. Use the dropdowns to select the subscription, account type, server, database, table or view, schema, and table name.
+   1. For change tracking we recommend **SQL Integrated Change Tracking Policy**.
+   1. For authentication, we recommend connecting with a [managed identity](search-howto-managed-identities-data-sources.md). Your search service must have **SQL Server Contributor** or **SQL DB Contributor** role membership on the database.
+   1. Select **Create** to create the data source.
 
-1. Continue with the remaining steps to complete the wizard:
+   :::image type="content" source="media/search-how-to-index-sql-database/search-data-source.png" alt-text="Screenshot of the data source creation page in the Azure portal.":::
 
-   + [Quickstart: Import data wizard](search-get-started-portal.md)
+1. Start the **Import data** wizard to create the index and indexer.
 
-   + [Quickstart: Import and vectorize data wizard](search-get-started-portal-import-vectors.md)
+   1. On the Overview page, select **Import data**.
+   1. Select the data source you just created, and select **Next**.
+   1. Skip the **Add cognitive skills (Optional)** page.
+   1. On **Customize target index**, name the index, set the key to your primary key in the table, and then group select *Retrievable* and *Searchable* for all fields, and optionally add *Filterable* and *Sortable* for short strings or numeric values.
+   1. On **Create an indexer**, name the indexer and select **Submit**.
 
-## Use the REST APIs
+### [**REST**](#tab/test-sql)
 
 This section demonstrates the REST API calls that create a data source, index, and indexer.
 
@@ -178,6 +185,7 @@ The data source definition specifies the data to index, credentials, and policie
    + Alternatively, you can specify a managed identity connection string that doesn't include database secrets with the following format: `Initial Catalog|Database=<your database name>;ResourceId=/subscriptions/<your subscription ID>/resourceGroups/<your resource group name>/providers/Microsoft.Sql/servers/<your SQL Server name>/;Connection Timeout=connection timeout length;`.
 
     For more information, see [Connect to Azure SQL Database indexer using a managed identity](search-howto-managed-identities-sql.md).
+
 > [!NOTE]
 > For the container name property, the value is restricted to only allow letters, numbers, underscores (_), dots (.), single dashes (-), and square brackets ([])
 
@@ -280,6 +288,8 @@ Once the index and data source have been created, you're ready to create the ind
 
 An indexer runs automatically when it's created. You can prevent this by setting "disabled" to true. To control indexer execution, [run an indexer on demand](search-howto-run-reset-indexers.md) or [put it on a schedule](search-howto-schedule-indexers.md).
 
+---
+
 ## Check indexer status
 
 To monitor the indexer status and execution history, check the indexer execution history in the Azure portal, or send a [Get Indexer Status](/rest/api/searchservice/indexers/get-status) REST API request
@@ -350,22 +360,23 @@ For Azure SQL indexers, there are two change detection policies:
 
 + "SqlIntegratedChangeTrackingPolicy" (applies to tables only)
 
-+ "HighWaterMarkChangeDetectionPolicy" (works for tables and views)
++ "HighWaterMarkChangeDetectionPolicy" (works for views)
 
 ### SQL Integrated Change Tracking Policy
 
 We recommend using "SqlIntegratedChangeTrackingPolicy" for its efficiency and its ability to identify deleted rows.
 
 Database requirements:
 
-+ SQL Server 2012 SP3 and later, if you're using SQL Server on Azure VMs
-+ Azure SQL Database or SQL Managed Instance
-+ Tables only (no views)
-+ On the database, [enable change tracking](/sql/relational-databases/track-changes/enable-and-disable-change-tracking-sql-server) for the table
-+ No composite primary key (a primary key containing more than one column) on the table
-+ No clustered indexes on the table. As a workaround, any clustered index would have to be dropped and re-created as nonclustered index, however, performance might be affected in the source compared to having a clustered index
++ Azure SQL Database or SQL Managed Instance. SQL Server 2016 or later if you're using an Azure VM.
++ Database must have [change tracking enabled](/sql/relational-databases/track-changes/enable-and-disable-change-tracking-sql-server)
++ Tables only (no views).
++ Tables can't be clustered. To meet this requirement, drop the clustered index and recreate it as non-clustered index. This workaround often degrades performance. Duplicating content in a second table that's dedicated to indexer processing can be a helpful mitigation. 
++ Tables can't be empty. If you use TRUNCATE TABLE to clear rows, a reset and rerun of the indexer won't remove the corresponding search documents. To remove orphaned search documents, you must [index them with a delete action](search-howto-reindex.md#delete-orphan-documents).
++ Primary key can't be a compound key (containing more than one column).
++ Primary key must be non-clustered if you want deletion detection.
 
-Change detection policies are added to data source definitions. To use this policy, create or update your data source like this:
+Change detection policies are added to data source definitions. To use this policy, edit the data source definition in the Azure portal, or use REST to update your data source like this:
 
 ```http
 POST https://myservice.search.windows.net/datasources?api-version=2024-07-01
@@ -382,10 +393,10 @@ api-key: admin-key
     }
 ```
 
-When using SQL integrated change tracking policy, don't specify a separate data deletion detection policy. The SQL integrated change tracking policy has built-in support for identifying deleted rows. However, for the deleted rows to be detected automatically, the document key in your search index must be the same as the primary key in the SQL table. 
+When using SQL integrated change tracking policy, don't specify a separate data deletion detection policy. The SQL integrated change tracking policy has built-in support for identifying deleted rows. However, for the deleted rows to be detected automatically, the document key in your search index must be the same as the primary key in the SQL table, and the primary key must be non-clustered.
 
-> [!NOTE]  
-> When using [TRUNCATE TABLE](/sql/t-sql/statements/truncate-table-transact-sql) to remove a large number of rows from a SQL table, the indexer needs to be [reset](/rest/api/searchservice/indexers/reset) to reset the change tracking state to pick up row deletions.
+<!-- > [!NOTE]  
+> When using [TRUNCATE TABLE](/sql/t-sql/statements/truncate-table-transact-sql) to remove a large number of rows from a SQL table, the indexer needs to be [reset](/rest/api/searchservice/indexers/reset) to reset the change tracking state to pick up row deletions. -->
 
 <a name="HighWaterMarkPolicy"></a>
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 SQL 数据库索引文档"
}
```

### Explanation
此次对 `search-how-to-index-sql-database.md` 文档进行了更新，主要改动如下：

1. **日期更新**：文档发布日期从 2024 年 11 月 20 日更新为 2025 年 3 月 18 日，表明内容最近经过了修改。

2. **前提条件简化**：对前提条件部分进行了调整，明确要求使用 Azure SQL 数据库或带有公共终结点的 SQL 托管实例，并且强调必须使用单值主键，并且在表上需为非聚集索引，以支持完整的 SQL 集成变化跟踪。

3. **内容精炼与逻辑调整**：对关于表和视图的使用说明进行了改写，使其更为简洁明了。例如，表的使用被重新表述为“使用单表或视图。对于需要增量索引的较大数据，建议使用表”，而视图的描述则进行了逻辑上的拆分，增强了可读性。

4. **新内容的增加**：增加了关于数据库必须具备的一些详细要求，特别是在使用 SQL 集成变化跟踪时，强调必须满足的条件，包括主键不能为复合键等。

5. **索引和数据源创建步骤调整**：对使用 Azure 门户创建数据源的步骤进行了优化和重新排列，确保顺序更加符合用户的实际操作流程。

6. **改进图像说明**：文档中对图像的使用依旧保持，但对描述文字进行了小幅调整，以提高其准确性和一致性。

7. **删除冗余信息**：去掉了一些重复和不必要的内容，使得文档更为精炼和易懂。

通过这些改动，该文档现在更加清晰地指导开发者如何从 SQL 数据库创建索引，并满足使用 Azure AI Search 时的技术要求。用户可以通过以下链接查看文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-how-to-index-sql-database.md)。

## articles/search/search-how-to-large-index.md{#item-d34e42}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: conceptual
-ms.date: 02/25/2025
+ms.date: 03/21/2025
 ---
 
 # Index large data sets in Azure AI Search
@@ -21,7 +21,7 @@ These strategies assume familiarity with the [two basic approaches for importing
 
 This article complements [Tips for better performance](search-performance-tips.md), which offers best practices on index and query design. A well-designed index that includes only the fields and attributes you need is an important prerequisite for large-scale indexing.
 
-We recommend using a newer search service, created after April 3, 2024, for [higher storage per partition](search-limits-quotas-capacity.md#service-limits).
+We recommend using a search service created after April 3, 2024 for [higher storage per partition](search-limits-quotas-capacity.md#service-limits). Older services can also be [upgraded to benefit from higher partition storage](search-how-to-upgrade.md).
 
 > [!NOTE]
 > The strategies described in this article assume a single large data source. If your solution requires indexing from multiple data sources, see [Index multiple data sources in Azure AI Search](/samples/azure-samples/azure-search-dotnet-scale/multiple-data-sources/) for a recommended approach.
@@ -46,7 +46,7 @@ Because the optimal batch size depends on your index and your data, the best app
 
 ### Manage threads and a retry strategy
 
-Indexers have built-in thread management, but when you're using the push APIs, your application code needs to manage threads. Make sure there are sufficient threads to make full use of the available capacity, especially if you've recently increased partitions or upgraded to a higher tier search service. 
+Indexers have built-in thread management, but when you're using the push APIs, your application code needs to manage threads. Make sure there are sufficient threads to make full use of the available capacity, especially if you recently [upgraded your service](search-how-to-upgrade.md), [switched to a higher tier](search-capacity-planning.md#change-your-pricing-tier), or [increased partitions](search-capacity-planning.md#add-or-remove-partitions-and-replicas).
 
 1. [Increase the number of concurrent threads](tutorial-optimize-indexing-push-api.md#use-multiple-threadsworkers) in your client code.
 
@@ -97,7 +97,7 @@ When there are no longer any new or updated documents in the data source, indexe
 For more information about setting schedules, see [Create Indexer REST API](/rest/api/searchservice/indexers/create) or see [Schedule indexers for Azure AI Search](search-howto-schedule-indexers.md).
 
 > [!NOTE]
-> Some indexers that run on an older runtime architecture have a 24-hour rather than 2-hour maximum processing window. The two-hour limit is for newer content processors that run in an [internally managed multitenant environment](search-howto-run-reset-indexers.md#indexer-execution-environment). Whenever possible, Azure AI Search tries to offload indexer and skillset processing to the multi-tenant environment. If the indexer can't be migrated, it runs in the private environment and it can run for as long as 24 hours. If you're scheduling an indexer that exhibits these characteristics, assume a 24-hour processing window.
+> Some indexers that run on an older runtime architecture have a 24-hour rather than 2-hour maximum processing window. The two-hour limit is for newer content processors that run in an [internally managed multitenant environment](search-howto-run-reset-indexers.md#indexer-execution-environment). Whenever possible, Azure AI Search tries to offload indexer and skillset processing to the multitenant environment. If the indexer can't be migrated, it runs in the private environment and it can run for as long as 24 hours. If you're scheduling an indexer that exhibits these characteristics, assume a 24-hour processing window.
 
 <a name="parallel-indexing"></a>
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure AI Search 大规模索引文档"
}
```

### Explanation
此次对 `search-how-to-large-index.md` 文档进行了更新，主要的修改包括：

1. **日期更新**：文档的更新日期由 2025 年 2 月 25 日修改为 2025 年 3 月 21 日，表明信息已经最近进行了更新。

2. **服务推荐调整**：在关于使用新搜索服务的推荐部分，更新了建议的表达方式，使其更为简洁，如“建议使用 2024 年 4 月 3 日后创建的搜索服务以获得更高的每分区存储”，并增加了说明“旧服务也可以通过升级受益于更高的分区存储”。

3. **多线程管理与重试策略**：在处理索引任务时，为了更好地利用服务的可用容量，增加了对服务升级、切换到更高等级和增加分区的参考，这些内容从原来的较为简单的提示扩展为更详细的建议，帮助用户理解在不同情况下的使用策略。

4. **内容结构优化**：在文档中，对某些段落的信息进行了微调，使得表述更加清晰，如关于并发线程的建议，优化了其逻辑流畅性，以便用户更容易理解和实施。

5. **保留重要提示**：对关于旧运行架构的索引器最大处理时间的说明进行了保留，并且保持信息的一致性，确保用户了解相关的运行环境及影响。

本次修改通过明确的建议和优化的信息结构，提升了用户对如何在 Azure AI Search 中高效执行大规模索引的理解。用户可以通过以下链接查看文档的详细更新内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-how-to-large-index.md)。

## articles/search/search-how-to-load-search-index.md{#item-a72573}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.author: heidist
 
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 10/31/2024
+ms.date: 03/21/2025
 ---
 
 # Load data into a search index in Azure AI Search
@@ -28,7 +28,7 @@ You can prepare these documents yourself, but if content resides in a [supported
 
 Once data is indexed, the physical data structures of the index are locked in. For guidance on what can and can't be changed, see [Update and rebuild an index](search-howto-reindex.md).
 
-Indexing isn't a background process. A search service will balance indexing and query workloads, but if [query latency is too high](search-performance-analysis.md#impact-of-indexing-on-queries), you can either [add capacity](search-capacity-planning.md#adjust-capacity) or identify periods of low query activity for loading an index.
+Indexing isn't a background process. A search service will balance indexing and query workloads, but if [query latency is too high](search-performance-analysis.md#impact-of-indexing-on-queries), you can either [add capacity](search-capacity-planning.md#add-or-remove-partitions-and-replicas) or identify periods of low query activity for loading an index.
 
 For more information, see [Data import strategies](search-what-is-data-import.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure AI Search 索引加载文档"
}
```

### Explanation
此次对 `search-how-to-load-search-index.md` 文档进行了小幅修改，主要包括以下几点：

1. **日期更新**：文档的更新日期由 2024 年 10 月 31 日修改为 2025 年 3 月 21 日，反映了最近的内容更新。

2. **内容修正**：在关于 query 的部分，调整了对扩展容量的描述，使其更准确。从原来的“添加容量”改为“增加或移除分区和副本”，这样可以让用户更清晰地理解其选项，并且提供更具体的指导。 

3. **信息结构优化**：调整了一处关于查询延迟的说明，明确了在加载索引时，高延迟情况下需要的解决方案，使其更加一致且易理解。

通过这些改动，文档的准确性和可读性得到了增强，为用户提供了更清晰的指引，帮助他们更好地理解如何在 Azure AI Search 中加载数据索引。用户可以通过以下链接查看文档的详细更新内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-how-to-load-search-index.md)。

## articles/search/search-how-to-semantic-chunking.md{#item-4a1d07}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: rawan
 ms.author: rawan
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 11/22/2024
+ms.date: 04/07/2025
 ms.custom:
   - references_regions
   - ignite-2024
@@ -36,13 +36,15 @@ For illustration purposes, this article uses the [sample health plan PDFs](https
 
 + [An indexer-based indexing pipeline](search-indexer-overview.md) with an index that accepts the output. The index must have fields for receiving headings and content.
 
++ [An index projection](search-how-to-define-index-projections.md) for one-to-many indexing.
+
 + [A supported data source](search-indexer-overview.md#supported-data-sources) having text content that you want to chunk.
 
-+ [A skillset with Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) that splits documents based on paragraph boundaries.
++ A skillset with these two skills:
 
-+ [An Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) that generates vector embeddings.
+  + [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md) that splits documents based on paragraph boundaries. This skill has region requirements. An Azure AI multi-service resource must be in the same region as Azure AI Search with AI Enrichment.
 
-+ [An index projection](search-how-to-define-index-projections.md) for one-to-many indexing.
+  + [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md) that generates vector embeddings. This skill also has region requirements. The model must be in the same region as Azure AI Search.
 
 ## Prepare data files
 
@@ -52,7 +54,7 @@ The raw inputs must be in a [supported data source](search-indexer-overview.md#s
 
 + Supported indexers can be any indexer that can handle the supported file formats. These indexers include [Blob indexers](search-howto-indexing-azure-blob-storage.md), [OneLake indexers](search-how-to-index-onelake-files.md), [File indexers](search-file-storage-integration.md).
 
-+ Supported regions for this feature include: East US, West US2, West Europe, North Central US. Be sure to [check this list](search-region-support.md#azure-public-regions) for updates on regional availability.
++ Supported regions for the portal experience of this feature include: East US, West Europe, North Central US. If your setting up your skillset programmatically, you can use any Document Intelligence region that also provides the AI enrichment feature of Azure AI Search. For more information, see [Product availability by region](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/table).
 
 You can use the Azure portal, REST APIs, or an Azure SDK package to [create a data source](search-howto-indexing-azure-blob-storage.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新语义切块文档"
}
```

### Explanation
在 `search-how-to-semantic-chunking.md` 文档中进行了小幅修改，主要更新内容如下：

1. **日期更新**：文档的更新日期由 2024 年 11 月 22 日修改为 2025 年 4 月 7 日，以反映最新的修改时间。

2. **技能集描述重构**：对涉及技能集的部分进行了调整，明确了所需的技能如下：
   - 增添了两项技能的描述：
     - “文档布局技能（Document Layout skill）”，用于在段落边界上拆分文档。
     - “Azure OpenAI 嵌入技能（Azure OpenAI Embedding skill）”，用于生成向量嵌入，并强调了这些技能的区域要求，即必须在与 Azure AI Search 同一地区内。

3. **支持的数据源和区域更新**：更新了有关支持数据源和区域的内容，增加了对如何程序性设置技能集的说明，允许使用任何具有 AI 丰富功能的文档智能区域。此外，准确列出支持该功能的地区。

4. **数据文件准备部分的扩展**：为使用中的原始输入和支持的索引器提供了更多详细信息，帮助用户更清楚地理解如何准备数据和使用兼容的索引器。

通过此次修改，文档不仅在内容上更加准确、全面，还提升了可读性，帮助用户更好地理解如何在 Azure AI Search 中进行语义切块操作。用户可以通过以下链接查看文档的详细更新内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-how-to-semantic-chunking.md)。

## articles/search/search-how-to-upgrade.md{#item-990225}

<details>
<summary>Diff</summary>
````diff
@@ -0,0 +1,115 @@
+---
+title: Service Upgrade in the Azure Portal
+titleSuffix: Azure AI Search
+description: Learn how to upgrade your existing Azure AI Search service to high-capacity storage and processors in your region.
+manager: nitinme
+author: haileytap
+ms.author: haileytapia
+ms.service: azure-ai-search
+ms.topic: how-to
+ms.custom: references_regions
+ms.date: 04/04/2025
+---
+
+# Upgrade your Azure AI Search service in the Azure portal
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+An upgrade brings older search services to the capabilities of new services created in the same region. Specifically, it upgrades the computing power of the underlying service. This one-time operation doesn't introduce breaking changes to your application, and you shouldn't need to change any code.
+
+For [eligible services](#upgrade-eligibility), an upgrade increases the [partition storage](#higher-storage-limits) and [vector index size](#higher-vector-limits) on the same tier at no extra cost.
+
+> [!TIP]
+> Looking to [change your pricing tier](search-capacity-planning.md#change-your-pricing-tier)? You can now move up between Basic and Standard (S1, S2, and S3) tiers.
+
+This article describes how to upgrade your service in the [Azure portal](https://portal.azure.com/). Alternatively, you can use the [Search Management REST APIs](/rest/api/searchmanagement/) to upgrade your service programmatically. For more information, see [Manage your search service using REST](search-manage-rest.md#upgrade-a-service).
+
+## About service upgrades
+
+In April 2024, Azure AI Search increased the [storage capacity](search-limits-quotas-capacity.md#service-limits) of newly created search services. Services created before April 2024 saw no capacity changes, so if you wanted larger and faster partitions, you had to create a new service. However, some older services can now be upgraded to benefit from the higher capacity partitions.
+
+In this preview, an upgrade only increases the [storage limit](#higher-storage-limits) and [vector index size](#higher-vector-limits) of [eligible services](#upgrade-eligibility).
+
+### Upgrade eligibility
+
+To qualify for an upgrade, your service:
+
+> [!div class="checklist"]
+> + Must have been created before April 2024. Services created after April 2024 should already have higher capacity. To see when you created your service, [check your service creation date](#check-your-service-creation-or-upgrade-date).
+> + Must be in a region where higher capacity is enabled.
+> + Must be in one of the following regions:
+>   + East US
+>   + North Central US
+>   + West Central US
+>   + UK South
+
+<!-- Check the footnotes in the [list of supported regions](search-region-support.md). -->
+
+### Higher storage limits
+
+For [eligible services](#upgrade-eligibility), the following table compares the storage limit (per partition) before and after an upgrade.
+
+| | Basic <sup>1</sup> | S1 | S2 | S3/HD | L1 | L2 |
+|-|-|-|-|-|-|-|
+| **Limit before upgrade** | 2 GB | 25 GB | 100 GB | 200 GB | 1 TB | 2 TB |
+| **Limit after upgrade** | 15 GB | 160 GB | 512 GB | 1 TB | 2 TB | 4 TB |
+
+<sup>1</sup> Basic services created before April 3, 2024 were originally limited to one partition, which increases to three partitions after an upgrade. [Partition counts for all other pricing tiers](search-limits-quotas-capacity.md#service-limits) stay the same.
+
+### Higher vector limits
+
+For [eligible services](#upgrade-eligibility), the following table compares the vector index size (per partition) before and after an upgrade.
+
+| | Basic | S1 | S2 | S3/HD | L1 | L2 |
+|-|-|-|-|-|-|-|
+| **Limit before upgrade** | 0.5 GB <sup>1</sup> or 1 GB <sup>2</sup> | 1 GB <sup>1</sup> or 3 GB <sup>2</sup> | 6 GB <sup>1</sup> or 12 GB <sup>2</sup> | 12 GB <sup>1</sup> or 36 GB <sup>2</sup> | 12 GB | 36 GB |
+| **Limit after upgrade** | 5 GB | 35 GB | 150 GB | 300 GB | 150 GB | 300 GB |
+
+<sup>1</sup> Applies to services created before July 1, 2023.
+
+<sup>2</sup> Applies to services created between July 1, 2023 and April 3, 2024 in all regions except Germany West Central, Qatar Central, and West India, to which the <sup>1</sup> limits apply.
+
+## Check your service creation or upgrade date
+
+On the **Overview** page, you can view various metadata about your search service, including the **Create date (UTC)** and **Upgrade date (UTC)**.
+
+:::image type="content" source="media/search-how-to-upgrade/service-creation-upgrade-metadata.png" alt-text="Screenshot of the service creation and service upgrade dates in the Azure portal." border="true":::
+
+The date you created your service partially determines its [upgrade eligibility](#upgrade-eligibility). If your service has never been upgraded, the **Upgrade date (UTC)** doesn't appear.
+
+## Upgrade your service
+
+You can’t undo a service upgrade. Before you proceed, make sure that you want to permanently increase the [storage limit](#higher-storage-limits) and [vector index size](#higher-vector-limits) of your search service. We recommend that you test this operation in a nonproduction environment.
+
+To upgrade your service:
+
+1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.
+
+1. On the **Overview** page, select **Upgrade** from the command bar.
+
+   :::image type="content" source="media/search-how-to-upgrade/upgrade-button.png" alt-text="Screenshot of the Upgrade button on the command bar in the Azure portal." border="true":::
+
+   If this button appears dimmed, an upgrade isn’t available for your service. Your service either has the [latest upgrade](#check-your-service-creation-or-upgrade-date) or is in an [unsupported region](#upgrade-eligibility).
+
+1. Review the upgrade details for your service, and then select **Upgrade**.
+
+   :::image type="content" source="media/search-how-to-upgrade/upgrade-panel.png" alt-text="Screenshot of your service upgrade details in the Azure portal." border="true":::
+
+   A confirmation appears reminding you that the upgrade can't be undone.
+
+1. To permanently upgrade your service, select **Upgrade**.
+
+   :::image type="content" source="media/search-how-to-upgrade/upgrade-confirmation.png" alt-text="Screenshot of the upgrade confirmation in the Azure portal." border="true":::
+
+1. Check your notifications to confirm that the operation started.
+
+   The upgrade is an asynchronous operation, so you can continue using your service. Depending on the size of your service, the upgrade can take several hours to complete.
+
+   If the upgrade fails, your service returns to its original state.
+
+## Next step
+
+After you upgrade your search service, you might want to reconsider your scale configuration:
+
+> [!div class="nextstepaction"]
+> [Estimate and manage capacity of an Azure AI Search service](search-capacity-planning.md)
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "新增 Azure AI Search 服务升级指南"
}
```

### Explanation
在 `search-how-to-upgrade.md` 文档中添加了一份完整的升级指南，主要涵盖以下内容：

1. **文档概述**：文档详细介绍了如何在 Azure 门户中将现有 Azure AI Search 服务升级为更高容量的存储和处理器能力，并确保该操作不会引入破坏性更改。

2. **升级的必要性**：升级可以提升旧版搜索服务到新服务的功能，尤其是计算能力的增强，并且用户不需要更改任何代码。

3. **升级资格**：文档明确了服务升级的资格要求，包括创建日期、区域要求和支持的地区（如东部美国、北中部美国等）。

4. **更高的存储和向量限制**：提供详细的表格对比了在升级前后不同定价层的存储限制和向量索引大小，帮助用户了解升级后所能获得的新能力。

5. **检查服务创建或升级日期**：用户可以在 Azure 门户的概述页面中查看其服务的创建和升级日期，以确认其升级资格。

6. **具体的升级步骤**：详细说明了如何在 Azure 门户中执行升级操作，确保用户能够轻松跟随步骤完成升级。同时强调升级是不可逆的，因此用户需确保其确实想要进行此操作。

7. **后续步骤提示**：升级完成后，鼓励用户重新考虑其规模配置，并提供相关链接以供管理和估算服务的容量。

通过这些内容的加入，该文档为用户提供了更为全面的指引，确保他们能够顺利地对 Azure AI Search 服务进行升级。用户可以通过以下链接查看该指南的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-how-to-upgrade.md)。

## articles/search/search-howto-index-encrypted-blobs.md{#item-a7097a}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index encrypted blobs'
+title: 'Tutorial: Index Encrypted Blobs'
 titleSuffix: Azure AI Search
 description: Learn how to index and extract text from encrypted documents in Azure Blob Storage with Azure AI Search.
 
@@ -11,110 +11,112 @@ ms.custom:
   - ignite-2023
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 02/24/2025
+ms.date: 03/28/2025
 ---
 
 # Tutorial: Index and enrich encrypted blobs for full-text search in Azure AI Search
 
-This tutorial shows you how to use [Azure AI Search](search-what-is-azure-search.md) to index documents that have been previously encrypted with a customer-managed key in [Azure Blob Storage](/azure/storage/blobs/storage-blobs-introduction). 
+Learn how to use [Azure AI Search](search-what-is-azure-search.md) to index documents that were encrypted with a customer-managed key in [Azure Blob Storage](/azure/storage/blobs/storage-blobs-introduction).
 
-Normally, an indexer can't extract content from blobs that have been encrypted using the [client-side encryption](/azure/storage/blobs/client-side-encryption) of the Azure Blob Storage client library because the indexer doesn't have access to the customer-managed encryption key in [Azure Key Vault](/azure/key-vault/general/overview). However, by leveraging the [DecryptBlobFile custom skill](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile), followed by the [Document Extraction skill](cognitive-search-skill-document-extraction.md), you can provide controlled access to the key to decrypt the files and then extract content from them. This unlocks the ability to index and enrich these documents without compromising the encryption status of your stored documents.
+Normally, an indexer can't extract content from blobs that were encrypted using [client-side encryption](/azure/storage/blobs/client-side-encryption) in the Azure Blob Storage client library. This is because the indexer doesn't have access to the customer-managed encryption key in [Azure Key Vault](/azure/key-vault/general/overview). However, using the [DecryptBlobFile custom skill](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile) and the [Document Extraction skill](cognitive-search-skill-document-extraction.md), you can provide controlled access to the key to decrypt the files and then extract content from them. This unlocks the ability to index and enrich these documents without compromising the encryption status of your stored documents.
 
-Starting with previously encrypted whole documents (unstructured text) such as PDF, HTML, DOCX, and PPTX in Azure Blob Storage, this tutorial uses a REST client and the Search REST APIs to perform the following tasks:
+Starting with previously encrypted whole documents (unstructured text) such as PDF, HTML, DOCX, and PPTX in Azure Blob Storage, this tutorial uses a REST client and the Search REST APIs to:
 
 > [!div class="checklist"]
-> + Define a pipeline that decrypts the documents and extracts text from them.
-> + Define an index to store the output.
-> + Execute the pipeline to create and load the index.
-> + Explore results using full text search and a rich query syntax.
-
-If you don't have an Azure subscription, open a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
+> + Define a pipeline that decrypts the documents and extracts text from them
+> + Define an index to store the output
+> + Execute the pipeline to create and load the index
+> + Explore results using full-text search and a rich query syntax
 
 ## Prerequisites
 
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+
 + [Azure AI Search](search-create-service-portal.md) on any tier or region.
 
 + [Azure Storage](https://azure.microsoft.com/services/storage/), Standard performance (general-purpose v2).
 
-+ Blobs encrypted with a customer-managed key. See [Tutorial: Encrypt and decrypt blobs using Azure Key Vault](/azure/storage/blobs/storage-encrypt-decrypt-blobs-key-vault) if you need to create sample data.
++ Blobs encrypted with a customer-managed key. To create sample data, see [Tutorial: Encrypt and decrypt blobs using Azure Key Vault](/azure/storage/blobs/storage-encrypt-decrypt-blobs-key-vault).
 
 + [Azure Key Vault](https://azure.microsoft.com/services/key-vault/) in the same subscription as Azure AI Search. The key vault must have **soft-delete** and **purge protection** enabled.
 
-Custom skill deployment creates an Azure Function app and an Azure Storage account. Since these resources are created for you, they aren't listed as a prerequisite. When you're finished with this tutorial, remember to clean up the resources so that you aren't billed for services you're not using.
+Custom skill deployment creates an Azure Function app and an Azure Storage account. These resources are created for you, so they aren't listed as a prerequisite. When you finish this tutorial, remember to clean up the resources so that you aren't billed for services you're not using.
 
 > [!NOTE]
-> Skillsets often require [attaching an Azure AI services multi-service resource](cognitive-search-attach-cognitive-services.md). As written, this skillset has no dependency on Azure AI services and thus no key is required. If you later add enrichments that invoke built-in skills, remember to update your skillset accordingly.
+> Skillsets often require [attaching an Azure AI services multi-service resource](cognitive-search-attach-cognitive-services.md). As written, this skillset has no dependency on Azure AI services, so no key is required. If you later add enrichments that invoke built-in skills, remember to update your skillset accordingly.
 
 ## Deploy the custom skill
 
-This example uses the sample [DecryptBlobFile](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile) project from the [Azure Search Power Skills](https://github.com/Azure-Samples/azure-search-power-skills) GitHub repository. In this section, you deploy the skill to an Azure Function so that it can be used in a skillset. A built-in deployment script creates an Azure Function resource with a **psdbf-function-app-** prefix and loads the skill. You are prompted to provide a subscription and resource group. Be sure to choose the same subscription that your Azure Key Vault instance lives in.
+This tutorial uses the sample [DecryptBlobFile](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile) project from the [Azure Search Power Skills](https://github.com/Azure-Samples/azure-search-power-skills) GitHub repository. In this section, you deploy the skill to an Azure Function so that it can be used in a skillset. A built-in deployment script creates an Azure Function resource with a **psdbf-function-app-** prefix and loads the skill. You're prompted to provide a subscription and resource group. Be sure to choose the subscription that contains your Azure Key Vault instance.
 
-Operationally, the DecryptBlobFile skill takes the URL and SAS token for each blob as inputs, and it outputs the downloaded, decrypted file using the file reference contract that Azure AI Search expects. Recall that DecryptBlobFile needs the encryption key to perform the decryption. As part of setup, you also create an access policy that grants DecryptBlobFile function access to the encryption key in Azure Key Vault.
+Operationally, the DecryptBlobFile skill takes the URL and SAS token for each blob as inputs. It outputs the downloaded, decrypted file using the file reference contract that Azure AI Search expects. Recall that DecryptBlobFile needs the encryption key to perform the decryption. As part of setup, you also create an access policy that grants DecryptBlobFile function access to the encryption key in Azure Key Vault.
 
-1. Click the **Deploy to Azure** button found on the [DecryptBlobFile landing page](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile#deployment), which will open the provided Resource Manager template within the Azure portal.
+1. On the [DecryptBlobFile landing page](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Utils/DecryptBlobFile#deployment), select **Deploy to Azure** to open the Resource Manager template in the Azure portal.
 
-1. Choose the same subscription where your Azure Key Vault instance exists (this tutorial won't work if you select a different subscription).
+1. Choose the subscription where your Azure Key Vault instance exists. This tutorial doesn't work if you choose a different subscription.
 
 1. Select an existing resource group or create a new one. A dedicated resource group makes cleanup easier later.
 
-1. Select **Review + create**, make sure you agree to the terms, and then select **Create** to deploy the Azure Function.
+1. Select **Review + create**, agree to the terms, and then select **Create** to deploy the Azure Function.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/arm-template.png" alt-text="Screenshot of the ARM template page in Azure portal." border="true":::
 
 1. Wait for the deployment to finish.
 
-You should have an Azure Function app that contains the decryption logic and an Azure Storage resource that will store application data. In the next several steps, you'll give the app permissions to access the key vault and collect information that you'll need for the REST calls.
+You should have an Azure Function app that contains the decryption logic and an Azure Storage resource that will store application data. In the next steps, you give the app permissions to access the key vault and collect information that you'll need for the REST calls.
 
 ## Grant permissions in Azure Key Vault
 
-1. Navigate to your Azure Key Vault service in the Azure portal. [Create an access policy](/azure/key-vault/general/assign-access-policy-portal) in the Azure Key Vault that grants key access to the custom skill.
+1. Go to your Azure Key Vault service in the Azure portal. [Create an access policy](/azure/key-vault/general/assign-access-policy-portal) in the Azure Key Vault that grants key access to the custom skill.
 
-1. On the left pane, select **Access policies**, and then select **+ Create** to start the **Create an access policy** wizard.
+1. From the left pane, select **Access policies**, and then select **+ Create** to start the **Create an access policy** wizard.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/keyvault-access-policies.png" alt-text="Screenshot of the Access Policy command in the left pane." border="true":::
 
-1. On the **Permissions** page under **Configure from template**, select **Azure Data Lake Storage or Azure Storage**.
+1. On the **Permissions** page, under **Configure from template**, select **Azure Data Lake Storage or Azure Storage**.
 
 1. Select **Next**.
 
-1. On the **Principal** page, select the Azure Function instance that you deployed. You can search for it using the resource prefix that was used to create it in step 2, which has a default prefix value of **psdbf-function-app**.
+1. On the **Principal** page, select the Azure Function instance you deployed. You can search for it using its resource prefix, which has a default value of **psdbf-function-app**.
 
 1. Select **Next**.
 
 1. On **Review + create**, select **Create**.
 
 ## Collect app information
 
-1. Navigate to the **psdbf-function-app** function in the Azure portal, and make a note of the following properties you'll need for the REST calls:
+1. Go to the **psdbf-function-app** function in the Azure portal. Make a note of the following properties you'll need for the REST calls.
 
 1. Get the function URL, which can be found under **Essentials** on the main page for the function.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/function-uri.png" alt-text="Screenshot of the overview page and Essentials section of the Azure Function app." border="true":::
 
-1. Get the host key code, which can be found by navigating to **App keys**, clicking to show the **default** key, and copying the value.
+1. Get the host key code, which can be found by going to **App keys** and showing the **default** key, and copying the value.
 
     :::image type="content" source="media/indexing-encrypted-blob-files/function-host-key.png" alt-text="Screenshot of the App Keys page of the Azure Function app." border="true":::
 
-## Get an admin api-key and URL for Azure AI Search
+## Get an admin key and URL for Azure AI Search
+
+1. Sign in to the [Azure portal](https://portal.azure.com).
 
-1. Sign in to the [Azure portal](https://portal.azure.com), and in your search service **Overview** page, get the name of your search service. You can confirm your service name by reviewing the endpoint URL. If your endpoint URL were `https://mydemo.search.windows.net`, your service name would be `mydemo`.
+1. On your search service **Overview** page, get the name of your search service. You can confirm your service name by reviewing the endpoint URL. For example, if your endpoint URL is `https://mydemo.search.windows.net`, your service name is `mydemo`.
 
-2. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
+1. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
 
-All requests require an api-key in the header of every request sent to your service. A valid key establishes trust, on a per request basis, between the application sending the request and the service that handles it.
+An API key is required in the header of every request sent to your service. A valid key establishes trust, on a per-request basis, between the application sending the request and the service that handles it.
 
 ## Set up a REST client
 
-Create variables for endpoints and keys:
+Create the following variables for endpoints and keys.
 
 | Variable    | Where to get it |
 |-------------|-----------------|
 | `admin-key` | On the **Keys** page of the Azure AI Search service.  |
 | `search-service-name` | The name of the Azure AI Search service. The URL is `https://{{search-service-name}}.search.windows.net`. |
 | `storage-connection-string` | In the storage account, on the **Access Keys** tab, select **key1** > **Connection string**. |
 | `storage-container-name` | The name of the blob container that has the encrypted files to be indexed. |
-| `function-uri` |  In the Azure Function under **Essentials** on the main page. |
-| `function-code` | In the Azure Function, by navigating to **App keys**, clicking to show the **default** key, and copying the value. |
+| `function-uri` |  In the Azure Function, under **Essentials** on the main page. |
+| `function-code` | In the Azure Function, by going to **App keys**, showing the **default** key, and copying the value. |
 | `api-version` | Leave as **2020-06-30**. |
 | `datasource-name` | Leave as **encrypted-blobs-ds**. |
 | `index-name` | Leave as **encrypted-blobs-idx**. |
@@ -123,25 +125,25 @@ Create variables for endpoints and keys:
 
 ## Review and run each request
 
-Use HTTP requests to create the objects of an enrichment pipeline:
+Use the following HTTP requests to create the objects of an enrichment pipeline.
 
 + **PUT request to create the index**: This search index holds the data that Azure AI Search uses and returns.
 
-+ **POST request to create the data source**: This data source specifies the connection to your storage account containing the encrypted blob files. 
++ **POST request to create the data source**: This data source specifies the connection to your storage account containing the encrypted blob files.
 
-+ **PUT request to create the skillset**: The skillset specifies the custom skill definition for the Azure Function that will decrypt the blob file data, and a [DocumentExtractionSkill](cognitive-search-skill-document-extraction.md) to extract the text from each document after it has been decrypted.
++ **PUT request to create the skillset**: The skillset specifies the custom skill definition for the Azure Function that will decrypt the blob file data. It also specifies a [DocumentExtractionSkill](cognitive-search-skill-document-extraction.md) to extract the text from each document after it's decrypted.
 
 + **PUT request to create the indexer**: Running the indexer retrieves the blobs, applies the skillset, and indexes and stores the results. You must run this request last. The custom skill in the skillset invokes the decryption logic.
 
 ## Monitor indexing
 
-Indexing and enrichment commence as soon as you submit the Create Indexer request. Depending on how many documents are in your storage account, indexing can take a while. To find out whether the indexer is still running, send a **Get Indexer Status** request and review the response to learn whether the indexer is running, or to view error and warning information.  
+Indexing and enrichment commence as soon as you submit the Create Indexer request. Depending on how many documents are in your storage account, indexing can take a while. To find out whether the indexer is still running, send a **Get Indexer Status** request and review the response to learn whether the indexer is running or view error and warning information.  
 
-If you're using the Free tier, the following message is expected: `"Could not extract content or metadata from your document. Truncated extracted text to '32768' characters"`. This message appears because blob indexing on the Free tier has a [32K limit on character extraction](search-limits-quotas-capacity.md#indexer-limits). You won't see this message for this data set on higher tiers. 
+If you're using the Free tier, expect the following message: `"Could not extract content or metadata from your document. Truncated extracted text to '32768' characters"`. This message appears because blob indexing on the Free tier has a [32,000 limit on character extraction](search-limits-quotas-capacity.md#indexer-limits). You don't see this message for this data set on higher tiers.
 
 ## Search your content
 
-After indexer execution is finished, you can run some queries to verify that the data has been successfully decrypted and indexed. Navigate to your Azure AI Search service in the Azure portal, and use the [Search Explorer](search-explorer.md) to run queries over the indexed data.
+After indexer execution is finished, you can run queries to verify that the data is successfully decrypted and indexed. Go to your Azure AI Search service in the Azure portal and use the [Search Explorer](search-explorer.md) to run queries over the indexed data.
 
 ## Clean up resources
 
@@ -151,6 +153,6 @@ You can find and manage resources in the Azure portal, using the All resources o
 
 ## Next steps
 
-Now that you have successfully indexed encrypted files, you can [iterate on this pipeline by adding more skills](cognitive-search-defining-skillset.md). This will allow you to enrich and gain additional insights to your data.
+Now that you've indexed encrypted files, you can [iterate on this pipeline by adding more skills](cognitive-search-defining-skillset.md) to enrich and gain more insights into your data.
 
-If you're working with doubly encrypted data, you might want to investigate the index encryption features available in Azure AI Search. Although the indexer needs decrypted data for indexing purposes, once the index exists, it can be encrypted in a search index using a customer-managed key. This will ensure that your data is always encrypted when at rest. For more information, see [Configure customer-managed keys for data encryption in Azure AI Search](search-security-manage-encryption-keys.md).
+If you're working with doubly encrypted data, you might want to investigate the index encryption features available in Azure AI Search. Although the indexer needs decrypted data for indexing purposes, once the index exists, it can be encrypted in a search index using a customer-managed key. This ensures that your data is always encrypted when at rest. For more information, see [Configure customer-managed keys for data encryption in Azure AI Search](search-security-manage-encryption-keys.md).
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新索引加密 Blob 的教程"
}
```

### Explanation
在 `search-howto-index-encrypted-blobs.md` 文档中进行了小幅修改，主要内容包括：

1. **标题格式调整**：教程标题进行了格式的更新，将“Index encrypted blobs”更改为“Index Encrypted Blobs”，统一了大写字母的使用。

2. **日期更新**：文档的更新日期从 2025 年 2 月 24 日更新为 2025 年 3 月 28 日。

3. **内容清晰化**：对一些句子进行了表述上的改进，以提高可读性。例如，“索引器不能从以前使用客户管理密钥加密的 Blob 中提取内容”这句话的表述进行了简化，使其更易理解。

4. **步骤和要点的重新组织**：为确保用户能够清晰理解教程中的每个步骤，增加了列表格式的要点。引导用户逐步定义解密管道、存储输出的索引、执行管道以及使用全文搜索探索结果的操作。

5. **前提条件的增强**：增加了一段关于需拥有 Azure 账户和活动订阅的说明，并提供了创建免费账户的链接，以帮助新用户顺利开始。

6. **技术步骤的明确化**：在执行自定义技能部署和创建与 Azure Key Vault 关联的访问策略时，步骤之间的逻辑关系和所需操作均进行了清晰阐述，确保用户在操作时不迷失。

7. **对部分术语和操作的简化描述**：通过将某些长句拆分为简洁的多句，使得技术术语和流程更为简练易懂，提高了整体文档的可读性。

通过以上更新，该文档为用户提供了有关如何索引加密 Blobs 的指导，确保用户能够顺利地遵循教程完成相关操作。用户可以通过以下链接查看该指南的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-howto-index-encrypted-blobs.md)。

## articles/search/search-howto-managed-identities-data-sources.md{#item-edf98d}

<details>
<summary>Diff</summary>
````diff
@@ -17,9 +17,9 @@ ms.date: 11/22/2024
 # Configure a search service to connect using a managed identity in Azure AI Search
 
 > [!IMPORTANT]
-> User-assigned managed identity assignment is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true#identity) provides user-assigned managed identity assignment for Azure AI Search. Support for a *system-assigned* managed identity is generally available.
+> User-assigned managed identity assignment is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#identity) provides user-assigned managed identity assignment for Azure AI Search. Support for a *system-assigned* managed identity is generally available.
 
-You can use Microsoft Entra ID and role assignments for outbound connections from Azure AI Search to resources providing data, applied AI, or vectorization during indexing or queries. 
+You can use Microsoft Entra ID and role assignments for outbound connections from Azure AI Search to resources providing data, applied AI, or vectorization during indexing or queries.
 
 To use roles on an outbound connection, first configure your search service to use either a [system-assigned or user-assigned managed identity](/azure/active-directory/managed-identities-azure-resources/overview) as the security principal for your search service in a Microsoft Entra tenant. Once you have a managed identity, you can assign roles for authorized access. Managed identities and role assignments eliminate the need for passing secrets and credentials in a connection string or code.
 
@@ -131,7 +131,7 @@ For more information, see [Create or Update Service (Management REST API)](/rest
 ## Create a user-assigned managed identity
 
 > [!IMPORTANT]
-> Part of this scenario is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true#identity) provides user-assigned managed identity configuration for Azure AI Search.
+> Part of this scenario is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The [Management preview REST API](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#identity) provides user-assigned managed identity configuration for Azure AI Search.
 
 A user-assigned managed identity is a resource on Azure. You can create multiple user-assigned managed identities if you want more granularity in role assignments. For example, you might want separate identities for different applications and scenarios.
 
@@ -170,12 +170,12 @@ Associating a user-assigned managed identity is supported in the Azure portal, i
 
 ### [**REST API**](#tab/rest-user)
 
-You can use a preview Management REST API instead of the Azure portal to assign a user-assigned managed identity. Use API versions `2021-04-01-preview` or later. This example uses `2024-06-01-preview`.
+You can use a preview Management REST API instead of the Azure portal to assign a user-assigned managed identity. Use API versions `2021-04-01-preview` or later. This example uses `2025-05-01-preview`.
 
-1. Formulate a request to [UPDATE](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true#identity) a search service.
+1. Formulate a request to [UPDATE](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-05-01-preview&preserve-view=true#identity) a search service.
 
     ```http
-    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2024-06-01-preview
+    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/mysearchservice?api-version=2025-05-01-preview
     {
       "location": "[region]",
       "sku": {
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure AI Search 受管身份数据源配置指南"
}
```

### Explanation
在 `search-howto-managed-identities-data-sources.md` 文档中进行了小规模的修改，主要更新内容包括：

1. **REST API 版本更新**：多处将引用的 REST API 版本从 `2024-03-01-preview` 和 `2024-06-01-preview` 更新为 `2025-02-01-preview` 和 `2025-05-01-preview`，确保文档内容与最新的 API 版本一致。

2. **重要提示段落重述**：对有关用户分配受管身份的公开预览说明进行了小幅调整，虽然内容相同，但旨在提高可读性和一致性。

3. **内容表述调整**：在文本中关于使用 Microsoft Entra ID 和角色分配的描述保持一致，但进行了格式调整，使信息更加清晰，从而使用户更容易理解如何使用受管身份进行安全连接，而无需传递秘密和凭证。

4. **结构和格式优化**：在多个段落和列表中进行了格式的小调整，以便更好地组织信息，使文档更易于阅读。

通过这些更新，文档为用户提供了有关如何配置 Azure AI Search 使用受管身份连接数据源的最新和清晰的信息。用户可以通过以下链接查看该指南的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-howto-managed-identities-data-sources.md)。

## articles/search/search-howto-reindex.md{#item-46738a}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 12/09/2024
+ms.date: 03/21/2025
 ---
 
 # Update or rebuild an index in Azure AI Search
@@ -134,7 +134,7 @@ The following table explains the various per-document status codes that can be r
 
 If your client code frequently encounters a 207 response, one possible reason is that the system is under load. You can confirm this by checking the statusCode property for 503. If the statusCode is 503, we recommend throttling indexing requests. Otherwise, if indexing traffic doesn't subside, the system could start rejecting all requests with 503 errors.
 
-Status code 429 indicates that you have exceeded your quota on the number of documents per index. You must either create a new index or upgrade for higher capacity limits.
+Status code 429 indicates that you've exceeded your quota on the number of documents per index. You must either [upgrade for higher capacity limits](search-how-to-upgrade.md) or create a new index.
 
 > [!NOTE]
 > When you upload `DateTimeOffset` values with time zone information to your index, Azure AI Search normalizes these values to UTC. For example, 2024-01-13T14:03:00-08:00 is stored as 2024-01-13T22:03:00Z. If you need to store time zone information, add an extra column to your index for this data point.
@@ -228,7 +228,7 @@ Some modifications require an index drop and rebuild, replacing a current index
 | Assign an analyzer to a field | [Analyzers](search-analyzers.md) are defined in an index, assigned to fields, and then invoked during indexing to inform how tokens are created. You can add a new analyzer definition to an index at any time, but you can only *assign* an analyzer when the field is created. This is true for both the **analyzer** and **indexAnalyzer** properties. The **searchAnalyzer** property is an exception (you can assign this property to an existing field). |
 | Update or delete an analyzer definition in an index | You can't delete or change an existing analyzer configuration (analyzer, tokenizer, token filter, or char filter) in the index unless you rebuild the entire index. |
 | Add a field to a suggester | If a field already exists and you want to add it to a [Suggesters](index-add-suggesters.md) construct, rebuild the index. |
-| Switch tiers | In-place upgrades aren't supported. If you require more capacity, create a new service and rebuild your indexes from scratch. To help automate this process, you can use a code sample that backs up your index to a series of JSON files. You can then recreate the index in a search service you specify.|
+| Upgrade your service or tier | If you need more capacity, check if you can [upgrade your service](search-how-to-upgrade.md) or [switch to a higher service tier](search-capacity-planning.md#change-your-pricing-tier). If not, you must create a new service and rebuild your indexes from scratch. To help automate this process, you can use a code sample that backs up your index to a series of JSON files. You can then recreate the index in a search service you specify. |
 
 The order of operations is:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新重建索引文档"
}
```

### Explanation
在 `search-howto-reindex.md` 文档中进行了小幅修改，主要更新内容包括：

1. **日期更新**：文档中的日期从 2024 年 12 月 9 日更新为 2025 年 3 月 21 日，以反映最新的修改时间。

2. **状态码描述优化**：对状态码 429 的描述进行了改进，原文在提到超出索引文件数量配额的情况时，增加了一个明确的链接，指向 “升级以获得更高的容量限制” 的指南。这有助于用户更方便地访问相关信息并采取必要的措施。

3. **结构性内容调整**：在说明服务或层级升级的操作时，更明确地描述了如果需要更多容量，用户需要检查是否可以升级服务或切换到更高的服务层级；如果不能，则必须创建一个新服务并从头构建索引。这一调整进一步提升了指引的有效性和清晰度。

通过这些更新，文档为用户在使用 Azure AI Search 时提供了准确和易于理解的重建索引的指导，确保用户能够顺利理解并执行相关操作。用户可以通过以下链接查看该指南的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-howto-reindex.md)。

## articles/search/search-howto-run-reset-indexers.md{#item-fb10c8}

<details>
<summary>Diff</summary>
````diff
@@ -9,26 +9,28 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/19/2024
+ms.date: 03/19/2025
 ---
 
 # Run or reset indexers, skills, or documents
 
 In Azure AI Search, there are several ways to run an indexer:
 
-+ [Run immediately upon indexer creation](search-howto-create-indexers.md), assuming it's not created in "disabled" mode.
++ [Run immediately upon indexer creation](search-howto-create-indexers.md). This is the default unless you create the indexer in a "disabled" state.
 + [Run on a schedule](search-howto-schedule-indexers.md) to invoke execution at regular intervals.
 + Run on demand, with or without a "reset".
 
 This article explains how to run indexers on demand, with and without a reset. It also describes indexer execution, duration, and concurrency.
 
 ## How indexers connect to Azure resources
 
-Indexers are one of the few subsystems that make overt outbound calls to other Azure resources. In terms of Azure roles, indexers don't have separate identities: a connection from the search engine to another Azure resource is made using the [system or user-assigned managed identity](search-howto-managed-identities-data-sources.md) of a search service. If the indexer connects to an Azure resource on a virtual network, you should create a [shared private link](search-indexer-howto-access-private.md) for that connection. For more information about secure connections, see the [Security in Azure AI Search](search-security-overview.md).
+Indexers are one of the few subsystems that make overt outbound calls to other Azure resources. You can use keys or roles to authenticate the connection.
+
+In terms of Azure roles, indexers don't have separate identities: a connection from the search engine to another Azure resource is made using the [system or user-assigned managed identity](search-howto-managed-identities-data-sources.md) of a search service, plus a role assignment on the target Azure resource. If the indexer connects to an Azure resource on a virtual network, you should create a [shared private link](search-indexer-howto-access-private.md) for that connection.
 
 ## Indexer execution
 
-A search service runs one indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions). Every search service starts with one search unit, but each new partition or replica increases the search units of your service. You can check the search unit count in the Azure portal's Essential section of the **Overview** page. If you need concurrent processing, make sure your search units include sufficient replicas. Indexers don't run in the background, so you might detect more query throttling than usual if the service is under pressure.
+A search service runs one indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions). Every search service starts with one search unit, but each new partition or replica increases the search units of your service. You can check the search unit count in the Azure portal's Essential section of the **Overview** page. If you need concurrent processing, make sure your search units include sufficient replicas. Indexers don't run in the background, so you might experience more query throttling than usual if the service is under pressure.
 
 The following screenshot shows the number of search units, which determines how many indexers can run at once.
 
@@ -42,45 +44,46 @@ You can run multiple indexers at one time assuming sufficient capacity, but each
 
 An indexer job runs in a managed execution environment. Currently, there are two environments:
 
-+ A private execution environment runs on search clusters that are specific to your search service. If your search service is Standard2 or higher, you can [set the `executionEnvironment` parameter](search-how-to-create-indexers.md?tabs=indexer-rest#create-an-indexer) in the indexer definition to always run an indexer in the private execution environment. 
++ A private execution environment runs on search clusters that are specific to your search service.
 
 + A multitenant environment has content processors that are managed and secured by Microsoft at no extra cost. This environment is used to offload computationally intensive processing, leaving service-specific resources available for routine operations. Whenever possible, most skillsets execute in the multitenant environment. This is the default.
 
-  *Computationally intensive processing* refers to skillsets running on content processors and indexer jobs that process a high volume of documents, or documents of a large size. Non-skillset processing on the multitenant content processors is determined by hueristics and system information and isn't under customer control. S2 services and higher support pinning an indexer and skillset processing exclusively to your search clusters through the `executionEnvironment` parameter.
+  *Computationally intensive processing* refers to skillsets running on content processors and indexer jobs that process a high volume of documents, or documents of a large size. Non-skillset processing on the multitenant content processors is determined by heuristics and system information and isn't under customer control. 
+
+You can prevent usage of the multitenant environment on Standard2 or higher services by pinning an indexer and skillset processing exclusively to your search clusters. [Set the `executionEnvironment` parameter](search-how-to-create-indexers.md?tabs=indexer-rest#create-an-indexer) in the indexer definition to always run an indexer in the private execution environment.
 
-  > [!NOTE]
-  > [IP firewalls](search-indexer-securing-resources.md#network-access-and-indexer-execution-environments) block the multitenant environment, so if you have a firewall, create a rule that allows multitenant processing.
+[IP firewalls](search-indexer-securing-resources.md#setting-up-ip-ranges-for-indexer-execution) block the multitenant environment, so if you have a firewall, [create a rule](search-indexer-howto-access-ip-restricted.md#configure-ip-firewall-rules-to-allow-indexer-connections-from-azure-ai-search) that allows multitenant processor connections.
 
 Indexer limits vary for each environment:
 
 | Workload | Maximum duration | Maximum jobs | Execution environment |
 |----------|------------------|---------------------|-----------------------------|
-| Private execution | 24 hours | One indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions) <sup>1</sup>.  | Indexing doesn't run in the background. Instead, the search service will balance all indexing jobs against ongoing queries and object management actions (such as creating or updating indexes). When running indexers, you should expect to see [some query latency](search-performance-analysis.md#impact-of-indexing-on-queries) if indexing volumes are large. |
+| Private execution | 24 hours | One indexer job per [search unit](search-capacity-planning.md#concepts-search-units-replicas-partitions) <sup>1</sup>.  | Indexing doesn't run in the background. Instead, the search service balances all indexing jobs against ongoing queries and object management actions (such as creating or updating indexes). When running indexers, you should expect to see [some query latency](search-performance-analysis.md#impact-of-indexing-on-queries) if indexing volumes are large. |
 | Multitenant| 2 hours <sup>2</sup> | Indeterminate <sup>3</sup> | Because the content processing cluster is multitenant, content processors are added to meet demand. If you experience a delay in on-demand or scheduled execution, it's probably because the system is either adding processors or waiting for one to become available.|
 
 <sup>1</sup> Search units can be [flexible combinations](search-capacity-planning.md#partition-and-replica-combinations) of partitions and replicas, but indexer jobs aren't tied to one or the other. In other words, if you have 12 units, you can have 12 indexer jobs running concurrently in private execution, no matter how the search units are deployed.
 
-<sup>2</sup> If more than two hours are needed to process all of the data, [enable change detection](search-howto-create-indexers.md#change-detection-and-internal-state) and [schedule the indexer](search-howto-schedule-indexers.md) to run at 5 minute intervals to resume indexing quickly if it stops due to a timeout. See [Indexing a large data set](search-howto-large-index.md) for more strategies.
+<sup>2</sup> If more than two hours are needed to process all of the data, [enable change detection](search-howto-create-indexers.md#change-detection-and-internal-state) and [schedule the indexer](search-howto-schedule-indexers.md) to run at 5-minute intervals to resume indexing quickly if it stops due to a time out. See [Indexing a large data set](search-howto-large-index.md) for more strategies.
 
 <sup>3</sup> "Indeterminate" means that the limit isn't quantified by the number of jobs. Some workloads, such as skillset processing, can run in parallel, which could result in many jobs even though only one indexer is involved. Although the environment doesn't impose constraints, [indexer limits](search-limits-quotas-capacity.md#indexer-limits) for your search service still apply.
 
 ## Run without reset
 
-A [Run Indexer](/rest/api/searchservice/indexers/run) operation will detect and process only what it necessary to synchronize the search index with changes in the underlying data source. Incremental indexing starts by locating an internal high-water mark to find the last updated search document, which becomes the starting point for indexer execution over new and updated documents in the data source.
+A [Run Indexer](/rest/api/searchservice/indexers/run) operation detects and processes only what it necessary to synchronize the search index with changes in the underlying data source. Incremental indexing starts by locating an internal high-water mark to find the last updated search document, which becomes the starting point for indexer execution over new and updated documents in the data source.
 
-[Change detection](search-howto-create-indexers.md#change-detection-and-internal-state) is essential for determining what's new or updated in the data source. Indexers use the change detection capabilities of the underlying data source to determine what's new or updated in the data source. 
+[Change detection](search-howto-create-indexers.md#change-detection-and-internal-state) is essential for determining what's new or updated in the data source. Indexers use the change detection capabilities of the underlying data source to determine what's new or updated in the data source.
 
 + Azure Storage has built-in change detection through its LastModified property.
 
 + Other data sources, such as Azure SQL or Azure Cosmos DB, have to be configured for change detection before the indexer can read new and updated rows. 
 
-If the underlying content is unchanged, a run operation has no effect. In this case, indexer execution history will indicate `0\0` documents processed.
+If the underlying content is unchanged, a run operation has no effect. In this case, indexer execution history indicates `0\0` documents processed.
 
-You'll need to reset the indexer, as explained in the next section, to reprocess in full.
+You need to reset the indexer, as explained in the next section, to reprocess in full.
 
 ## Resetting indexers
 
-After the initial run, an indexer keeps track of which search documents have been indexed through an internal *high-water mark*. The marker is never exposed, but internally the indexer knows where it last stopped.
+After the initial run, an indexer keeps track of which search documents are indexed through an internal *high-water mark*. The marker is never exposed, but internally the indexer knows where it last stopped.
 
 If you need to rebuild all or part of an index, you can clear the indexer's high-water mark through a reset. Reset APIs are available at decreasing levels in the object hierarchy:
 
@@ -90,23 +93,26 @@ If you need to rebuild all or part of an index, you can clear the indexer's high
 
 After reset, follow with a Run command to reprocess new and existing documents. Orphaned search documents having no counterpart in the data source can't be removed through reset/run. If you need to delete documents, see [Documents - Index](/rest/api/searchservice/documents) instead.
 
+> [!NOTE]
+> Tables can't be empty. If you use TRUNCATE TABLE to clear rows, a reset and rerun of the indexer won't remove the corresponding search documents. To remove orphaned search documents, you must [index them with a delete action](search-howto-reindex.md#delete-orphan-documents).
+
 <a name="reset-indexers"></a>
 
 ## How to reset and run indexers
 
-Reset clears the high-water mark. All documents in the search index will be flagged for full overwrite, without inline updates or merging into existing content. For indexers with a skillset and [enrichment caching](cognitive-search-incremental-indexing-conceptual.md), resetting the index will also implicitly reset the skillset. 
+Reset clears the high-water mark. All documents in the search index are flagged for full overwrite, without inline updates or merging into existing content. For indexers with a skillset and [enrichment caching](cognitive-search-incremental-indexing-conceptual.md), resetting the index also implicitly resets the skillset. 
 
 The actual work occurs when you follow a reset with a Run command:
 
 + All new documents found the underlying source are added to the search index. 
-+ All documents that exist in both the data source and search index will be overwritten in the search index. 
-+ Any enriched content created from skillsets will be rebuilt. The enrichment cache, if one is enabled, is refreshed.
++ All documents that exist in both the data source and search index are overwritten in the search index. 
++ Any enriched content created from skillsets are rebuilt. The enrichment cache, if one is enabled, is refreshed.
 
-As previously noted, reset is a passive operation: you must follow up a Run request to rebuild the index. 
+As previously noted, reset is a passive operation: you must follow with a Run request to rebuild the index. 
 
 Reset/run operations apply to a search index or a knowledge store, to specific documents or projections, and to cached enrichments if a reset explicitly or implicitly includes skills.
 
-Reset also applies to create and update operations. It will not trigger deletion or clean up of orphaned documents in the search index. For more information about deleting documents, see [Documents - Index](/rest/api/searchservice/documents/).
+Reset also applies to create and update operations. It won't trigger deletion or clean up of orphaned documents in the search index. For more information about deleting documents, see [Documents - Index](/rest/api/searchservice/documents/).
 
 Once you reset an indexer, you can't undo the action.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新重置索引器操作文档"
}
```

### Explanation
在 `search-howto-run-reset-indexers.md` 文档中进行了较大幅度的修改，主要更新内容包括：

1. **日期更新**：文档的日期被更新为 2025 年 3 月 19 日，以反映最新的修改时间。

2. **索引器运行方式的清晰化**：强化了索引器的运行选项说明。特别指出了在创建索引器时的默认行为说明，即“默认情况下，将立即运行除非索引器以“禁用”状态创建”。

3. **安全连接方式更新**：说明了索引器如何连接到 Azure 资源的方式，现在允许使用密钥或角色进行身份验证，以便用户在进行连接时有更多的选择。

4. **执行环境说明增强**：增加了关于私有执行环境和多租户环境的详细描述，使用户更容易理解在不同环境下的索引器运行限制。此外，对 IP 防火墙对多租户环境的影响和配置进行了更清晰的描述。

5. **索引器运行和重置的操作细节**：对运行和重置索引器的过程进行了更加详细的描述，包括重置之后的运行行为，确保用户理解在重置后必须随之运行的必要性。

6. **文档内容流畅度提升**：对文本进行了全面的流畅度和结构性改进，确保信息传达更加清晰。

这些更新使得文档在指导用户如何在 Azure AI Search 中运行和重置索引器时更加全面且易于理解。用户可以通过以下链接查看该指南的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-howto-run-reset-indexers.md)。

## articles/search/search-indexer-how-to-access-private-sql.md{#item-1bd4cc}

<details>
<summary>Diff</summary>
````diff
@@ -87,7 +87,7 @@ For more information about connection properties, see [Create an Azure SQL Manag
    Because shared private link support for SQL managed instances is still in preview, you need a preview version of the management REST API. Use `2021-04-01-preview` or a later preview API version for this step. We recommend using the latest preview API version.
 
    ```azurecli
-   az rest --method put --uri https://management.azure.com/subscriptions/{{search-service-subscription-ID}}/resourceGroups/{{search service-resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}/sharedPrivateLinkResources/{{shared-private-link-name}}?api-version=2024-06-01-preview --body @create-pe.json
+   az rest --method put --uri https://management.azure.com/subscriptions/{{search-service-subscription-ID}}/resourceGroups/{{search service-resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}/sharedPrivateLinkResources/{{shared-private-link-name}}?api-version=2025-05-01-preview --body @create-pe.json
    ```
 
    Provide the subscription ID, resource group name, and service name of your Azure AI Search resource.
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新私有 SQL 访问文档中的 API 版本"
}
```

### Explanation
在 `search-indexer-how-to-access-private-sql.md` 文档中进行了小幅修改，主要更新内容包括：

1. **API 版本更新**：将文档中提到的用于创建共享私有链接的 Azure 管理 REST API 版本由 `2024-06-01-preview` 更新为 `2025-05-01-preview`。这一修改确保用户在使用该 API 时能够获得最新的功能和修复。

2. **内容一致性**：在 API 示例代码中，更新后的版本号确保了文档内容的准确性和一致性，同时反映了 Azure 服务的持续更新和改进。

此更新使得文档在指导用户如何使用 Azure AI Search 访问私有 SQL 时更加准确，确保用户采用的是符合最新标准的 API 版本。用户可以通过以下链接查看该指南的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-indexer-how-to-access-private-sql.md)。

## articles/search/search-indexer-howto-access-private.md{#item-73d30d}

<details>
<summary>Diff</summary>
````diff
@@ -63,7 +63,7 @@ While both scenarios have a dependency on Azure Private Link, they're independen
 
 When evaluating shared private links for your scenario, remember these constraints.
 
-+ Several of the resource types used in a shared private link are in preview. If you're connecting to a preview resource (Azure Database for MySQL or Azure SQL Managed Instance), use a preview version of the Management REST API to create the shared private link. These versions include `2020-08-01-preview`, `2021-04-01-preview`, `2024-03-01-preview`, and `2024-06-01-preview`. We recommend the latest preview API.
++ Several of the resource types used in a shared private link are in preview. If you're connecting to a preview resource (Azure Database for MySQL or Azure SQL Managed Instance), use a preview version of the Management REST API to create the shared private link. These versions include `2020-08-01-preview`, `2021-04-01-preview`, `2024-03-01-preview`, `2024-06-01-preview`, and `2025-02-01-preview`. We recommend the latest preview API.
 
 + Indexer execution must use the [private execution environment](search-howto-run-reset-indexers.md#indexer-execution-environment) that's specific to your search service. Private endpoint connections aren't supported from the multitenant content processing environment. The configuration setting for this requirement is covered in this article.
 
@@ -78,8 +78,8 @@ When evaluating shared private links for your scenario, remember these constrain
   | Workload | Tier requirements | Region requirements | Service creation requirements |
   |----------|-------------------|---------------------|---------------------|
   | Indexers without skillsets | Basic and higher | None | None |
-  | Skillsets with embedding skills ([integrated vectorization](vector-search-integrated-vectorization.md)) | Basic and higher | [High capacity regions](search-limits-quotas-capacity.md#partition-storage-gb) | [After April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date) |
-  | Skillsets using other [built-in](cognitive-search-predefined-skills.md) or custom skills | Standard 2 (S2) and higher | None | [After April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date) |
+  | Skillsets with embedding skills ([integrated vectorization](vector-search-integrated-vectorization.md)) | Basic and higher | [High capacity regions](search-limits-quotas-capacity.md#partition-storage-gb) | [After April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date) |
+  | Skillsets using other [built-in](cognitive-search-predefined-skills.md) or custom skills | Standard 2 (S2) and higher | None | [After April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date) |
 
 + Permissions on both Azure AI Search and the Azure resource:
 
@@ -88,9 +88,9 @@ When evaluating shared private links for your scenario, remember these constrain
   | Azure AI Search | `Microsoft.Search/searchServices/sharedPrivateLinkResources/write`<br> `Microsoft.Search/searchServices/sharedPrivateLinkResources/read`<br> `Microsoft.Search/searchServices/sharedPrivateLinkResources/operationStatuses/read` |
   | Other Azure resource | Permission to approve private endpoint connections. For example, on Azure Storage, you need `Microsoft.Storage/storageAccounts/privateEndpointConnectionsApproval/action`. |
 
-<!-- + For [integrated vectorization](vector-search-integrated-vectorization.md) only, outbound connections through shared private link are supported on all billable tiers, on services [created after April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date), in regions providing [higher capacity](search-limits-quotas-capacity.md#partition-storage-gb).  -->
+<!-- + For [integrated vectorization](vector-search-integrated-vectorization.md) only, outbound connections through shared private link are supported on all billable tiers, on services [created after April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date), in regions providing [higher capacity](search-limits-quotas-capacity.md#partition-storage-gb).  -->
 
-<!-- + For [AI enrichment](cognitive-search-concept-intro.md) and skillset processing, shared private link  that doesn't include an embedding skill and in services [created before April 3, 2024](vector-search-index-size.md#how-to-check-service-creation-date), Azure AI Search must be Standard 2 (S2) or higher. -->
+<!-- + For [AI enrichment](cognitive-search-concept-intro.md) and skillset processing, shared private link  that doesn't include an embedding skill and in services [created before April 3, 2024](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date), Azure AI Search must be Standard 2 (S2) or higher. -->
 
 <!-- + For all other use cases, that don't involve skillsets, Azure AI Search can be Basic or higher. -->
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新访问私有内容的文档"
}
```

### Explanation
在 `search-indexer-howto-access-private.md` 文档中进行了小幅修改，主要更新内容包括：

1. **API 版本更新**：文档中提到的可用用于创建共享私有链接的管理 REST API 版本得到更新，新增了 `2025-02-01-preview` 版本，以反映最新的可用选项，帮助用户在连接预览资源时选择更合适的 API。

2. **执行环境说明**：新增了一条关于索引器执行环境的说明，强调了索引器必须使用与其搜索服务相关的私有执行环境，并指出多租户内容处理环境不支持私有端点连接，增强了文档对环境配置要求的清晰度。

3. **表格中的链接更新**：在表格中，某些链接被更新以反映其最新的文档位置，从而帮助用户获取相关信息时更加准确。

这些更新确保文档在指导用户如何通过共享私有链接访问私有资源时内容准确、及时，并为使用 Azure AI Search 的用户提供了更多的信息和选择。用户可以通过以下链接查看该指南的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-indexer-howto-access-private.md)。

## articles/search/search-indexer-tutorial.md{#item-a3e3ff}

<details>
<summary>Diff</summary>
````diff
@@ -1,14 +1,14 @@
 ---
-title: C# tutorial indexing Azure SQL data
+title: 'C# Tutorial: Index Azure SQL Data'
 titleSuffix: Azure AI Search
-description: In this C# tutorial, connect to Azure SQL Database, extract searchable data, and load it into an Azure AI Search index.
+description: In this C# tutorial, you connect to Azure SQL Database, extract searchable data, and load it into an Azure AI Search index.
 
 manager: nitinme
 author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 03/11/2025
+ms.date: 03/28/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
@@ -17,104 +17,101 @@ ms.custom:
 
 # Tutorial: Index Azure SQL data using the .NET SDK
 
-Configure an [indexer](search-indexer-overview.md) to extract searchable data from Azure SQL Database, sending it to a search index in Azure AI Search. 
+Learn how to configure an [indexer](search-indexer-overview.md) to extract searchable data from Azure SQL Database and send it to a search index in Azure AI Search.
 
-This tutorial uses C# and the [Azure SDK for .NET](/dotnet/api/overview/azure/search) to perform the following tasks:
+In this tutorial, you use C# and the [Azure SDK for .NET](/dotnet/api/overview/azure/search) to:
 
 > [!div class="checklist"]
 > * Create a data source that connects to Azure SQL Database
 > * Create an indexer
 > * Run an indexer to load data into an index
 > * Query an index as a verification step
 
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Prerequisites
 
-* [Azure SQL Database](https://azure.microsoft.com/services/sql-database/) using SQL Server authentication
-* [Visual Studio](https://visualstudio.microsoft.com/downloads/)
-* [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) 
+* An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
+* [Azure SQL Database](https://azure.microsoft.com/services/sql-database/) using SQL Server authentication.
+* [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.
+* [Visual Studio](https://visualstudio.microsoft.com/downloads/).
 
 > [!NOTE]
-> You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ## Download files
 
 Source code for this tutorial is in the [DotNetHowToIndexer](https://github.com/Azure-Samples/search-dotnet-getting-started/tree/master/DotNetHowToIndexers) folder in the [Azure-Samples/search-dotnet-getting-started](https://github.com/Azure-Samples/search-dotnet-getting-started) GitHub repository.
 
-## 1 - Create services
+## Create services
 
-This tutorial uses Azure AI Search for indexing and queries, and Azure SQL Database as an external data source. If possible, create both services in the same region and resource group for proximity and manageability. In practice, Azure SQL Database can be in any region.
+This tutorial uses Azure AI Search for indexing and queries and Azure SQL Database as an external data source. If possible, create both services in the same region and resource group for proximity and manageability. In practice, Azure SQL Database can be in any region.
 
 ### Start with Azure SQL Database
 
-This tutorial provides *hotels.sql* file in the sample download to populate the database. Azure AI Search consumes flattened rowsets, such as one generated from a view or query. The SQL file in the sample solution creates and populates a single table.
-
-If you have an existing Azure SQL Database resource, you can add the hotels table to it, starting at the **Open query** step.
+This tutorial provides the *hotels.sql* file in the sample download to populate the database. Azure AI Search consumes flattened rowsets, such as one generated from a view or query. The SQL file in the sample solution creates and populates a single table.
 
-1. Create an Azure SQL database, using the instructions in [Quickstart: Create a single database](/azure/azure-sql/database/single-database-create-quickstart).
+If you have an existing Azure SQL Database resource, you can add the hotels table to it starting at the **Open query** step.
 
-   Server configuration for the database is important.
+1. [Create an Azure SQL database](/azure/azure-sql/database/single-database-create-quickstart). Server configuration for the database is important:
 
    * Choose the SQL Server authentication option that prompts you to specify a username and password. You need this for the ADO.NET connection string used by the indexer.
 
-   * Choose a public connection. It makes this tutorial easier to complete. Public isn't recommended for production and we recommend [deleting this resource](#clean-up-resources) at the end of the tutorial.
+   * Choose a public connection, which makes this tutorial easier to complete. Public isn't recommended for production, and we recommend [deleting this resource](#clean-up-resources) at the end of the tutorial.
 
    :::image type="content" source="media/search-indexer-tutorial/sql-server-config.png" alt-text="Screenshot of server configuration.":::
 
 1. In the Azure portal, go to the new resource.
 
-1. Add a firewall rule to allow access from your client, using the instructions in [Quickstart: Create a server-level firewall rule in Azure portal](/azure/azure-sql/database/firewall-create-server-level-portal-quickstart). You can run `ipconfig` from a command prompt to get your IP address. 
+1. [Add a firewall rule that allows access from your client](/azure/azure-sql/database/firewall-create-server-level-portal-quickstart). You can run `ipconfig` from a command prompt to get your IP address.
 
-1. Use the Query editor to load the sample data. On the navigation pane, select **Query editor (preview)** and enter the user name and password of server admin. 
+1. Use the Query editor to load the sample data. On the navigation pane, select **Query editor (preview)** and enter the username and password of the server admin.
 
-   If you get an access denied error, copy the client IP address from the error message, open the network security page for the server, and add an inbound rule that allows access from your client. 
+   If you get an access denied error, copy the client IP address from the error message, open the network security page for the server, and add an inbound rule that allows access from your client.
 
-1. In Query editor, select **Open query** and navigate to the location of *hotels.sql* file on your local computer. 
+1. In Query editor, select **Open query** and navigate to the location of *hotels.sql* file on your local computer.
 
 1. Select the file and select **Open**. The script should look similar to the following screenshot:
 
    :::image type="content" source="media/search-indexer-tutorial/sql-script.png" alt-text="Screenshot of SQL script in a Query Editor window." border="true":::
 
-1. Select **Run** to execute the query. In the Results pane, you should see a query succeeded message, for three rows.
+1. Select **Run** to execute the query. In the **Results** pane, you should see a query succeeded message for three rows.
 
 1. To return a rowset from this table, you can execute the following query as a verification step:
 
     ```sql
     SELECT * FROM Hotels
     ```
 
-1. Copy the ADO.NET connection string for the database. Under **Settings** > **Connection Strings**, copy the ADO.NET connection string, similar to the example below.
+1. Copy the ADO.NET connection string for the database. Under **Settings** > **Connection Strings**, copy the ADO.NET connection string, which should be similar to the following example:
 
     ```sql
     Server=tcp:<YOUR-DATABASE-NAME>.database.windows.net,1433;Initial Catalog=hotels-db;Persist Security Info=False;User ID=<YOUR-USER-NAME>;Password=<YOUR-PASSWORD>;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;
     ```
 
-You'll need this connection string in the next exercise, setting up your environment.
+You'll need this connection string to set up your environment in the next step.
 
 ### Azure AI Search
 
-The next component is Azure AI Search, which you can [create in the Azure portal](search-create-service-portal.md). You can use the Free tier to complete this walkthrough. 
+The next component is Azure AI Search, which you can [create in the Azure portal](search-create-service-portal.md). You can use the Free tier to complete this tutorial.
 
-### Get an admin api-key and URL for Azure AI Search
+### Get an admin key and URL for Azure AI Search
 
 API calls require the service URL and an access key. A search service is created with both, so if you added Azure AI Search to your subscription, follow these steps to get the necessary information:
 
-1. Sign in to the [Azure portal](https://portal.azure.com), and in your search service **Overview** page, get the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com). On your service **Overview** page, copy the endpoint URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
-1. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
+1. On **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
 
    :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of Azure portal pages showing the HTTP endpoint and access key location for a search service." border="false":::
 
-## 2 - Set up your environment
+## Set up your environment
 
-1. Start Visual Studio and open **DotNetHowToIndexers.sln**.
+1. Start Visual Studio and open *DotNetHowToIndexers.sln*.
 
-1. In Solution Explorer, open **appsettings.json** to provide connection information.
+1. In Solution Explorer, open *appsettings.json* to provide connection information.
 
-1. For `SearchServiceEndPoint`, if the full URL on the service overview page is "https://my-demo-service.search.windows.net", then the value to provide is the entire URL.
+1. For `SearchServiceEndPoint`, if the full URL on your service **Overview** page is `https://my-demo-service.search.windows.net`, provide the entire URL.
 
-1. For `AzureSqlConnectionString`, the string format is similar to this: `"Server=tcp:<your-database-name>.database.windows.net,1433;Initial Catalog=hotels-db;Persist Security Info=False;User ID=<your-user-name>;Password=<your-password>;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;"`
+1. For `AzureSqlConnectionString`, the string format is similar to `"Server=tcp:<your-database-name>.database.windows.net,1433;Initial Catalog=hotels-db;Persist Security Info=False;User ID=<your-user-name>;Password=<your-password>;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;"`.
 
     ```json
     {
@@ -124,19 +121,18 @@ API calls require the service URL and an access key. A search service is created
     }
     ```
 
-1. Replace the user password in the SQL connection string to a valid password. While the database and user names will copy over, the password must be entered manually.
-
-## 3 - Create the pipeline
+1. Replace the user password in the SQL connection string with a valid password. While the database and usernames will copy over, you must enter the password manually.
 
-Indexers require a data source object and an index. Relevant code is in two files:
+## Create the pipeline
 
-* **hotel.cs**, containing a schema that defines the index
+Indexers require a data source object and an index. The relevant code is in two files:
 
-* **Program.cs**, containing functions for creating and managing structures in your service
+* *hotel.cs* contains a schema that defines the index
+* *Program.cs* contains functions for creating and managing structures in your service
 
 ### In hotel.cs
 
-The index schema defines the fields collection, including attributes specifying allowed operations, such as whether a field is full-text searchable, filterable, or sortable as shown in the following field definition for HotelName. A [SearchableField](/dotnet/api/azure.search.documents.indexes.models.searchablefield) is full-text searchable by definition. Other attributes are assigned explicitly.
+The index schema defines the fields collection, including attributes specifying allowed operations, such as whether a field is full-text searchable, filterable, or sortable, as shown in the following field definition for `HotelName`. A [SearchableField](/dotnet/api/azure.search.documents.indexes.models.searchablefield) is, by definition, full-text searchable. Other attributes are explicitly assigned.
 
 ```csharp
 . . . 
@@ -146,11 +142,11 @@ public string HotelName { get; set; }
 . . .
 ```
 
-A schema can also include other elements, including scoring profiles for boosting a search score, custom analyzers, and other constructs. However, for our purposes, the schema is sparsely defined, consisting only of fields found in the sample datasets.
+A schema can also include other elements, such as scoring profiles for boosting a search score and custom analyzers. However, for this tutorial, the schema is sparsely defined, consisting only of fields found in the sample datasets.
 
 ### In Program.cs
 
-The main program includes logic for creating [an indexer client](/dotnet/api/azure.search.documents.indexes.models.searchindexer), an index, a data source, and an indexer. The code checks for and deletes existing resources of the same name, under the assumption that you might run this program multiple times.
+The main program includes logic for creating [an indexer client](/dotnet/api/azure.search.documents.indexes.models.searchindexer), an index, a data source, and an indexer. The code checks for and deletes existing resources of the same name, assuming that you might run this program multiple times.
 
 The data source object is configured with settings that are specific to Azure SQL Database resources, including [partial or incremental indexing](search-how-to-index-sql-database.md#CaptureChangedRows) for using the built-in [change detection features](/sql/relational-databases/track-changes/about-change-tracking-sql-server) of Azure SQL. The source demo hotels database in Azure SQL has a "soft delete" column named **IsDeleted**. When this column is set to true in the database, the indexer removes the corresponding document from the Azure AI Search index.
 
@@ -167,7 +163,7 @@ var dataSource =
 indexerClient.CreateOrUpdateDataSourceConnection(dataSource);
 ```
 
-An indexer object is platform-agnostic, where  configuration, scheduling, and invocation are the same regardless of the source. This example indexer includes a schedule, a reset option that clears indexer history, and calls a method to create and run the indexer immediately. To create or update an indexer, use [CreateOrUpdateIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.createorupdateindexerasync).
+An indexer object is platform agnostic, where configuration, scheduling, and invocation are the same regardless of the source. This example indexer includes a schedule and a reset option that clears the indexer history. It also calls a method to create and run the indexer immediately. To create or update an indexer, use [CreateOrUpdateIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.createorupdateindexerasync).
 
 ```csharp
 Console.WriteLine("Creating Azure SQL indexer...");
@@ -203,7 +199,7 @@ var indexer = new SearchIndexer("hotels-sql-idxr", dataSource.Name, searchIndex.
 await indexerClient.CreateOrUpdateIndexerAsync(indexer);
 ```
 
-Indexer runs are usually scheduled, but during development you might want to run the indexer immediately using [RunIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.runindexerasync).
+Indexer runs are usually scheduled, but during development, you might want to run the indexer immediately using [RunIndexerAsync](/dotnet/api/azure.search.documents.indexes.searchindexerclient.runindexerasync).
 
 ```csharp
 Console.WriteLine("Running Azure SQL indexer...");
@@ -218,35 +214,35 @@ catch (RequestFailedException ex) when (ex.Status == 429)
 }
 ```
 
-## 4 - Build the solution
+## Build the solution
 
-Press F5 to build and run the solution. The program executes in debug mode. A console window reports the status of each operation.
+Select **F5** to build and run the solution. The program executes in debug mode. A console window reports the status of each operation.
 
    :::image type="content" source="media/search-indexer-tutorial/console-output.png" alt-text="Screenshot showing the console output for the program." border="true":::
 
 Your code runs locally in Visual Studio, connecting to your search service on Azure, which in turn connects to Azure SQL Database and retrieves the dataset. With this many operations, there are several potential points of failure. If you get an error, check the following conditions first:
 
-* Search service connection information that you provide is the full URL. If you entered just the service name, operations stop at index creation, with a failure to connect error.
+* Search service connection information that you provide is the full URL. If you only entered the service name, operations stop at index creation, with a failure to connect error.
 
-* Database connection information in **appsettings.json**. It should be the ADO.NET connection string obtained from the Azure portal, modified to include a username and password that are valid for your database. The user account must have permission to retrieve data. Your local client IP address must be allowed inbound access through the firewall.
+* Database connection information in *appsettings.json*. It should be the ADO.NET connection string obtained from the Azure portal, modified to include a username and password that are valid for your database. The user account must have permission to retrieve data. Your local client IP address must be allowed inbound access through the firewall.
 
 * Resource limits. Recall that the Free tier has limits of three indexes, indexers, and data sources. A service at the maximum limit can't create new objects.
 
-## 5 - Search
+## Search
 
-Use Azure portal to verify object creation, and then use **Search explorer** to query the index.
+Use the Azure portal to verify object creation, and then use **Search explorer** to query the index.
 
-1. Sign in to the [Azure portal](https://portal.azure.com), and in your search service left pane, open each page in turn to verify the object is created. **Indexes**, **Indexers**, and **Data Sources** will have "hotels-sql-idx", "hotels-sql-indexer", and "hotels-sql-ds", respectively.
+1. Sign in to the [Azure portal](https://portal.azure.com) and go to your search service. From the left pane, open each page to verify the objects are created. **Indexes**, **Indexers**, and **Data Sources** should have **hotels-sql-idx**, **hotels-sql-indexer**, and **hotels-sql-ds**, respectively.
 
-1. On the Indexes tab, select the hotels-sql-idx index. On the hotels page, **Search explorer** is the first tab.
+1. On the **Indexes** tab, select the **hotels-sql-idx** index. On the hotels page, **Search explorer** is the first tab.
 
 1. Select **Search** to issue an empty query.
 
    The three entries in your index are returned as JSON documents. Search explorer returns documents in JSON so that you can view the entire structure.
 
    :::image type="content" source="media/search-indexer-tutorial/portal-search.png" alt-text="Screenshot of a Search Explorer query for the target index." border="true":::
 
-1. Next, [switch to **JSON View**](search-explorer.md#start-search-explorer) so that you can enter query parameters:
+1. [Switch to **JSON view**](search-explorer.md#start-search-explorer) so that you can enter query parameters.
 
    ```json
    {
@@ -255,9 +251,9 @@ Use Azure portal to verify object creation, and then use **Search explorer** to
    }
    ```
 
-   This query invokes full text search on the term `river`, and the result includes a count of the matching documents. Returning the count of matching documents is helpful in testing scenarios when you have a large index with thousands or millions of documents. In this case, only one document matches the query.
+   This query invokes full text search on the term `river`. The result includes a count of the matching documents. Returning the count of matching documents is helpful in testing scenarios where you have a large index with thousands or millions of documents. In this case, only one document matches the query.
 
-1. Lastly, enter parameters that limit search results to fields of interest: 
+1. Enter parameters that limit search results to fields of interest.
 
    ```json
    {
@@ -285,7 +281,7 @@ You can find and manage resources in the Azure portal, using the All resources o
 
 ## Next steps
 
-Now that you're familiar with the basics of SQL Database indexing, let's take a closer look at indexer configuration.
+Now that you're familiar with the basics of SQL Database indexing, take a closer look at indexer configuration:
 
 > [!div class="nextstepaction"]
 > [Configure a SQL Database indexer](search-how-to-index-sql-database.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 C# 教程以索引 Azure SQL 数据"
}
```

### Explanation
在 `search-indexer-tutorial.md` 文档中进行了小幅修改，主要更新内容包括：

1. **标题和描述更新**：标题从 “C# tutorial indexing Azure SQL data” 修改为 “C# Tutorial: Index Azure SQL Data”，同时对教程的描述进行了润色，使其更加清晰，强调了用户在教程中执行的操作。

2. **内容结构和语言改进**：多个段落内的句子结构得以简化和优化，以改善可读性和语法，包括将列表项目的陈述方式变为更主动的语态，这样用户能够更轻松地理解步骤。

3. **前置知识和准备工作**：增强了先决条件的描述，清晰地列出所需的资源和服务，为用户提供了明确的步骤来启动和配置 Azure SQL 数据库与 Azure AI Search 服务。

4. **增加注释和提示**：在关键步骤旁边增加了重要提示，提升了用户在设置过程中解决问题的能力，比如关于防火墙规则和资源使用限制的信息。

5. **进一步的说明**：在教程的最后部分，增加了下一步的链接，以引导用户更深入地了解索引器的配置。

这些修改使得文档内容更加准确和用户友好，帮助用户更顺利地完成 Azure SQL 数据的索引过程。用户可以通过以下链接查看该教程的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-indexer-tutorial.md)。

## articles/search/search-limits-quotas-capacity.md{#item-3b201a}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 03/11/2025
+ms.date: 04/09/2025
 ms.custom:
   - references_regions
   - build-2024
@@ -47,6 +47,7 @@ Maximum limits on storage, workloads, and quantities of indexes and other object
 | Maximum depth of complex fields |10 |10 |10 |10 |10 |10 |10 |10 |
 | Maximum [suggesters](/rest/api/searchservice/suggesters) per index |1 |1 |1 |1 |1 |1 |1 |1 |
 | Maximum [scoring profiles](/rest/api/searchservice/add-scoring-profiles-to-a-search-index) per index |100 |100 |100 |100 |100 |100 |100 |100 |
+| Maximum [semantic configurations](semantic-how-to-configure.md) per index |100 |100 |100 |100 |100 |100 |100 |100 |
 | Maximum functions per profile |8 |8 |8 |8 |8 |8 |8 |8 |
 | Maximum index size&nbsp;<sup>4</sup> | N/A | N/A | N/A | 1.88&nbsp;TB | 2.34&nbsp;TB | 100 GB| N/A | N/A |
 
@@ -56,7 +57,7 @@ Maximum limits on storage, workloads, and quantities of indexes and other object
 
 <sup>3</sup> An upper limit exists for elements because having a large number of them significantly increases the storage required for your index. An element of a complex collection is defined as a member of that collection. For example, assume a [Hotel document with a Rooms complex collection](search-howto-complex-data-types.md#complex-collection-limits), each room in the Rooms collection is considered an element. During indexing, the indexing engine can safely process a maximum of 3,000 elements across the document as a whole. [This limit](search-api-migration.md#upgrade-to-2019-05-06) was introduced in `api-version=2019-05-06` and applies to complex collections only, and not to string collections or to complex fields.
 
-<sup>4</sup> On most tiers, maximum index size is all available storage on your search service. For S2, S3, and S3 HD, the maximum size of any index is the number provided in the table. Applies to search services created after April 3, 2024.
+<sup>4</sup> For most tiers, the maximum index size is the total available storage on your search service. For S2, S3, and S3 HD services with multiple partitions, and therefore more storage, the maximum size of a single index is provided in the table. Applies to search services created after April 3, 2024.
 
 You might find some variation in maximum limits if your service happens to be provisioned on a more powerful cluster. The limits here represent the common denominator. Indexes built to the above specifications are portable across equivalent service tiers in any region.
 
@@ -81,12 +82,12 @@ When you index documents with vector fields, Azure AI Search constructs internal
 
 Vector limits vary by:
 
-+ [service creation date](vector-search-index-size.md#how-to-check-service-creation-date)
-+ [region](search-region-support.md)
++ [Service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date)
++ [Region](search-region-support.md)
 
-Higher vector limits from April 2024 onwards exist on *new search services* in regions providing the extra capacity, which is most of them.
+Higher vector limits from April 2024 onwards exist on *new search services* in regions providing the extra capacity, which is most of them. If you have an older service in a supported region, check if you can [upgrade your service](search-how-to-upgrade.md) to the higher vector limits.
 
-This table shows the progression of vector quota increases in GB over time. The quota is per partition, so if you scale a new Standard (S1) service to 6 partitions, total vector quota is 35 multiplied by 6.
+This table shows the progression of vector quota increases in GB over time. The quota is per partition, so if you scale a new Standard (S1) service to 6 partitions, the total vector quota is 35 multiplied by 6.
 
 | Service creation date |Basic | S1| S2 | S3/HD | L1 | L2 |
 |-----------------------|------|---|----|----|----|----|
@@ -150,9 +151,9 @@ Indexers can access other Azure resources [over private endpoints](search-indexe
 | Maximum private endpoints | N/A | 10 or 30 | 100 | 400 | 400 | N/A | 20 | 20 |
 | Maximum distinct resource types <sup>3</sup> | N/A | 4 | 7 | 15 | 15 | N/A | 4 | 4 |
 
-<sup>1</sup> AI enrichment and image analysis are computationally intensive and consume disproportionate amounts of available processing power. For this reason, private connections are disabled on lower tiers to ensure the performance and stability of the search service itself. On basic services, private connections to an Azure AI services multi-service resource are unsupported to preserve service stability. For the S1 tier, make sure the service was created with [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) after April 3, 2024. 
+<sup>1</sup> AI enrichment and image analysis are computationally intensive and consume disproportionate amounts of available processing power. For this reason, private connections are disabled on lower tiers to ensure the performance and stability of the search service itself. On Basic services, private connections to an Azure AI services multi-service resource are unsupported to preserve service stability. For the S1 tier, make sure the service was created with [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) after April 3, 2024.
 
-<sup>2</sup> Private connections to an embedding model are supported on basic and S1 high-capacity search services created after April 3, 2024, with the [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) for storage and computational processing. 
+<sup>2</sup> Private connections to an embedding model are supported on Basic and S1 high-capacity search services created after April 3, 2024, with the [higher limits](search-limits-quotas-capacity.md#partition-storage-gb) for storage and computational processing.
 
 <sup>3</sup> The number of distinct resource types are computed as the number of unique `groupId` values used across all shared private link resources for a given search service, irrespective of the status of the resource.
 
@@ -167,7 +168,7 @@ Maximum number of synonym maps varies by tier. Each rule can have up to 20 expan
 
 ## Index alias limits
 
-Maximum number of [index aliases](search-how-to-alias.md) varies by tier and [service creation date](vector-search-index-size.md#how-to-check-service-creation-date). In all tiers, if the service was created after October 2022 the maximum number of aliases is double the maximum number of indexes allowed. If the service was created before October 2022, the limit is the number of indexes allowed.
+Maximum number of [index aliases](search-how-to-alias.md) varies by tier and [service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date). In all tiers, if the service was created after October 2022 the maximum number of aliases is double the maximum number of indexes allowed. If the service was created before October 2022, the limit is the number of indexes allowed.
 
 | Service Creation Date | Free | Basic | S1 | S2 | S3 | S3-HD |L1 | L2 |
 |----------|------|-------|----|----|----|-------|----|----|
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新搜索限制、配额和容量文档"
}
```

### Explanation
在 `search-limits-quotas-capacity.md` 文档中进行了小幅修改，主要更新内容包括：

1. **发布日期更新**：文档的发布日期已从 `03/11/2025` 更改为 `04/09/2025`，以确保信息的时效性。

2. **新增项**：在索引的最大配置中，增加了 “最大语义配置” 的限制，这可以让用户更好地理解可以在索引中使用的语义配置的最大数量，增强了文档的全面性。

3. **表格内容的改善**：对表格中的语言进行了微调，使其更容易理解。例如，关于索引大小的描述进行了更准确的说明，强调了在 S2、S3 和 S3 HD 分区服务中，单个索引的最大大小。

4. **链接修改**：更新了一些内部链接，确保用户可以访问最新的文档。这些链接的修改帮助用户更轻松地找到相关资源，比如服务创建日期和如何检查服务更新的信息。

5. **内容表述的优化**：对一些句子的措辞进行了细微的调整，以提高可读性和理解度，特别是在描述私有连接和嵌入模型支持的部分。

这些更新确保文档内容在指导用户理解 Azure AI Search 的限制、配额和容量时更加准确和易于理解。用户可以通过以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-limits-quotas-capacity.md)。

## articles/search/search-manage-azure-cli.md{#item-7fdd08}

<details>
<summary>Diff</summary>
````diff
@@ -11,15 +11,10 @@ ms.custom:
   - devx-track-azurecli
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 03/21/2025
 ---
 
-# Manage your Azure AI Search service with the Azure CLI
-> [!div class="op_single_selector"]
-> * [Portal](search-manage.md)
-> * [PowerShell](search-manage-powershell.md)
-> * [Azure CLI](search-manage-azure-cli.md)
-> * [REST API](search-manage-rest.md)
+# Manage your Azure AI Search service using the Azure CLI
 
 You can run Azure CLI commands and scripts on Windows, macOS, Linux, or in Azure Cloud Shell to create and configure Azure AI Search.
 
@@ -37,7 +32,10 @@ Use the [**az search module**](/cli/azure/search) to perform the following tasks
 
 Occasionally, questions are asked about tasks *not* on the above list.
 
-You can't change a server name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service. 
+You can't change a service name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service.
+
+> [!NOTE]
+> The 2025-02-01-preview supports changing your pricing tier using the [Management REST APIs](search-manage-rest.md#upgrade-a-service) and the [Azure portal](search-capacity-planning.md#change-your-pricing-tier). Currently, you can only move up between Basic and Standard (S1, S2, and S3) tiers, such as going from Basic to S1.
 
 You can't use tools or APIs to transfer content, such as an index, from one service to another. Within a service, programmatic creation of content is through [Search Service REST API](/rest/api/searchservice/) or an SDK such as [Azure SDK for .NET](/dotnet/api/overview/azure/search.documents-readme). While there are no dedicated commands for content migration, you can write script that calls REST API or a client library to create and load indexes on a new service.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure CLI 管理 Azure AI 搜索服务文档"
}
```

### Explanation
在 `search-manage-azure-cli.md` 文档中进行了小幅修改，主要更新内容包括：

1. **发布日期更新**：文档的发布日期已从 `12/10/2024` 更新为 `03/21/2025`，确保用户获取最新的信息。

2. **标题修改**：将标题中的 "Manage your Azure AI Search service with the Azure CLI" 更改为 "Manage your Azure AI Search service using the Azure CLI"，以提高语句的流畅性与规范性。

3. **内容细节调整**：对有关无法更改服务名称、区域或定价层的描述进行了优化，确保信息的一致性和清晰度，避免用户产生误解。

4. **新增注意事项**：增加了一个重要注意事项，指出可以通过 Management REST APIs 和 Azure 门户来更改定价层，这个信息对用户的操作非常重要，并提供了相关文档的链接，以方便用户获取进一步的指导。

5. **次要语言改进**：对文档中一些句子的措辞进行了细微的调整，旨在提高可读性和流畅性，确保用户更好理解 Azure CLI 中的功能和限制。

这些更新使得文档内容更加准确、有条理，并且为用户提供了必要的指导和最新的功能信息。用户可以通过以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-manage-azure-cli.md)。

## articles/search/search-manage-powershell.md{#item-3c3485}

<details>
<summary>Diff</summary>
````diff
@@ -9,20 +9,15 @@ ms.author: haileytapia
 ms.service: azure-ai-search
 ms.devlang: powershell
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 03/21/2025
 ms.custom:
   - devx-track-azurepowershell
   - ignite-2023
 ---
 
-# Manage your Azure AI Search service with PowerShell
-> [!div class="op_single_selector"]
-> * [Portal](search-manage.md)
-> * [PowerShell](search-manage-powershell.md)
-> * [Azure CLI](search-manage-azure-cli.md)
-> * [REST API](search-manage-rest.md)
+# Manage your Azure AI Search service using PowerShell
 
-You can run PowerShell cmdlets and scripts on Windows, Linux, or in Azure Cloud Shell to create and configure Azure AI Search. 
+You can run PowerShell cmdlets and scripts on Windows, Linux, or in Azure Cloud Shell to create and configure Azure AI Search.
 
 Use the [**Az.Search** module](/powershell/module/az.search/) to perform the following tasks:
 
@@ -38,7 +33,10 @@ Use the [**Az.Search** module](/powershell/module/az.search/) to perform the fol
 
 Occasionally, questions are asked about tasks *not* on the above list.
 
-You can't change a server name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service. 
+You can't change a service name, region, or tier programmatically or in the Azure portal. Dedicated resources are allocated when a service is created. As such, changing the underlying hardware (location or node type) requires a new service.
+
+> [!NOTE]
+> The 2025-02-01-preview supports changing your pricing tier using the [Management REST APIs](search-manage-rest.md#upgrade-a-service) and the [Azure portal](search-capacity-planning.md#change-your-pricing-tier). Currently, you can only move up between Basic and Standard (S1, S2, and S3) tiers, such as going from Basic to S1.
 
 You can't use tools or APIs to transfer content, such as an index, from one service to another. Within a service, programmatic creation of content is through [Search Service REST API](/rest/api/searchservice/) or an SDK such as [Azure SDK for .NET](/dotnet/api/overview/azure/search.documents-readme). While there are no dedicated commands for content migration, you can write script that calls REST API or a client library to create and load indexes on a new service.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 PowerShell 管理 Azure AI 搜索服务文档"
}
```

### Explanation
在 `search-manage-powershell.md` 文档中进行了小幅修改，主要更新内容包括：

1. **发布日期更新**：文档的发布日期已从 `12/10/2024` 更改为 `03/21/2025`，以反映最新的时间信息。

2. **标题修改**：标题中的 "Manage your Azure AI Search service with PowerShell" 更改为 "Manage your Azure AI Search service using PowerShell"，更符合语言的习惯，增强了流畅性。

3. **内容条款调整**：对无法通过程序或 Azure 门户更改服务名称、区域或定价层的描述进行了语言上的优化，确保信息的一致性和清晰度。

4. **增加注意事项**：新增一个重要的提醒，指明通过 Management REST APIs 和 Azure 门户可以更改定价层，且提供了相关的文档链接。这一信息对用户操作非常重要，并能帮助用户利用最新功能。

5. **文本改进**：对文档中的一些句子的措辞进行了细微的调整，以提高可读性，使相关内容更加易于理解，尤其是在描述工具和 API 对于内容迁移的限制时。

这些更新使得文档内容更加准确、清晰，并为用户提供了最新的指导信息。用户可以通过以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-manage-powershell.md)。

## articles/search/search-manage-rest.md{#item-405ec7}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: Manage with REST
+title: Manage using REST
 titleSuffix: Azure AI Search
 description: Create and configure an Azure AI Search service with the Management REST API. The Management REST API is comprehensive in scope, with access to generally available and preview features.
 author: haileytap
@@ -8,23 +8,19 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 03/26/2025
 ---
 
-# Manage your Azure AI Search service with REST APIs
+# Manage your Azure AI Search service using REST APIs
 
-> [!div class="op_single_selector"]
-> * [Portal](search-manage.md)
-> * [PowerShell](search-manage-powershell.md)
-> * [Azure CLI](search-manage-azure-cli.md)
-> * [REST API](search-manage-rest.md)
-
-In this article, learn how to create and configure an Azure AI Search service using the [Management REST APIs](/rest/api/searchmanagement/). Only the Management REST APIs are guaranteed to provide early access to [preview features](/rest/api/searchmanagement/management-api-versions). 
+Learn how to create and configure an Azure AI Search service using the [Management REST APIs](/rest/api/searchmanagement/). Only the Management REST APIs are guaranteed to provide early access to [preview features](/rest/api/searchmanagement/management-api-versions).
 
 The Management REST API is available in stable and preview versions. Be sure to set a preview API version if you're accessing preview features.
 
 > [!div class="checklist"]
 > * [Create or update a service](#create-or-update-a-service)
+> * [Upgrade a service](#upgrade-a-service)
+> * [Change pricing tiers](#change-pricing-tiers)
 > * [Enable Azure role-based access control for data plane](#enable-rbac)
 > * [Enforce a customer-managed key policy](#enforce-cmk)
 > * [Disable semantic ranker](#disable-semantic-ranker)
@@ -33,49 +29,45 @@ The Management REST API is available in stable and preview versions. Be sure to
 > * [Regenerate an admin key](#regenerate-admin-api-keys)
 > * [List private endpoint connections](#list-private-endpoint-connections)
 > * [List search operations](#list-search-operations)
-> * [Delete a search services](#delete-a-search-service)
+> * [Delete a search service](#delete-a-search-service)
 
 All of the Management REST APIs have examples. If a task isn't covered in this article, see the [API reference](/rest/api/searchmanagement/) instead.
 
 ## Prerequisites
 
-* An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-search/).
+* An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
 * [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
-* [Azure CLI](/cli/azure/install-azure-cli) used to get an access token. You must be an owner or administrator in your Azure subscription.
-
-## Get an access token
+* [Azure CLI](/cli/azure/install-azure-cli) to get an access token, as described in the following steps. You must be an owner or administrator in your Azure subscription.
 
-Management REST API calls are authenticated through Microsoft Entra ID. You need to provide an access token on the request, along with permissions to create and configure a resource.
+   Management REST API calls are authenticated through Microsoft Entra ID. You must provide an access token on the request and permissions to create and configure a resource. In addition to the Azure CLI, you can use [Azure PowerShell to create an access token](/azure/azure-resource-manager/management/manage-resources-rest).
 
-You can use the [Azure CLI or Azure PowerShell to create an access token](/azure/azure-resource-manager/management/manage-resources-rest).
+   1. Open a command shell for Azure CLI.
 
-1. Open a command shell for Azure CLI.
+   1. Sign in to your Azure subscription. If you have multiple tenants or subscriptions, make sure you select the correct one.
 
-1. Sign in to your Azure subscription.
+       ```azurecli
+       az login
+       ```
 
-   ```azurecli
-   az login
-   ```
+   1. Get the tenant ID and subscription ID.
 
-1. Get the tenant ID and subscription ID. If you have multiple tenants or subscriptions, make sure you use the correct one.
+      ```azurecli
+      az account show
+      ```
 
-   ```azurecli
-   az account show
-   ````
+   1. Get an access token.
 
-1. Get an access token.
+      ```azurecli
+      az account get-access-token --query accessToken --output tsv
+      ```
 
-   ```azurecli
-   az account get-access-token --query accessToken --output tsv
-   ```
-
-You should have a tenant ID, subscription ID, and bearer token. You'll paste these values into the `.rest` or `.http` file that you create in the next step.
+      You should have a tenant ID, subscription ID, and bearer token. You'll paste these values into the `.rest` or `.http` file that you create in the next step.
 
 ## Set up Visual Studio Code
 
-If you're not familiar with the REST client for Visual Studio Code, this section includes setup so that you can complete the tasks in this quickstart.
+If you're not familiar with the REST client for Visual Studio Code, this section includes setup so that you can complete the tasks in this article.
 
 1. Start Visual Studio Code and select the **Extensions** tile.
 
@@ -129,7 +121,7 @@ If you're not familiar with the REST client for Visual Studio Code, this section
 
 ## Create or update a service
 
-Creates or updates a search service under the current subscription. This example uses variables for the search service name and region, which haven't been defined yet. Either provide the names directly, or add new variables to the collection.
+Creates or updates a search service under the current subscription. This example uses variables for the search service name and region, which haven't been defined yet. Either provide the names directly or add new variables to the collection.
 
 ```http
 ### Create a search service (provide an existing resource group)
@@ -152,6 +144,38 @@ PUT https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups
       }
 ```
 
+## Upgrade a service
+
+Some Azure AI Search capabilities are only available to new services. To avoid service recreation and bring these capabilities to an existing service, you can [upgrade your service](search-how-to-upgrade.md).
+
+```http
+### Upgrade a search service
+@resource-group = my-rg
+@search-service-name = my-search
+POST https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}/upgrade?api-version=2025-02-01-preview  HTTP/1.1
+     Content-type: application/json
+     Authorization: Bearer {{token}}
+```
+
+## Change pricing tiers
+
+If you need more <!-- or less-->capacity, you can [switch to a higher pricing tier](search-capacity-planning.md#change-your-pricing-tier). Currently, you can only move up between Basic and Standard (S1, S2, and S3) tiers. Use the `sku` property to specify the higher <!-- your new -->tier.
+
+```http
+### Change pricing tiers
+@resource-group = my-rg
+@search-service-name = my-search
+PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2025-02-01-preview HTTP/1.1
+     Content-type: application/json
+     Authorization: Bearer {{token}}
+
+    {
+        "sku": {
+            "name": "standard2"
+        }
+    }
+```
+
 ## Create an S3HD service
 
 To create an [S3HD](search-sku-tier.md#tier-descriptions) service, use a combination of `sku` and `hostingMode` properties. Set `sku` to `standard3` and "hostingMode" to `HighDensity`.
@@ -182,7 +206,7 @@ PUT https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups
 
 **Applies to:** Search Index Data Contributor, Search Index Data Reader, Search Service Contributor
 
-In this step, configure your search service to recognize an **authorization** header on data requests that provide an OAuth2 access token.
+Configure your search service to recognize an **authorization** header on data requests that provide an OAuth2 access token.
 
 To use role-based access control for data plane operations, set `authOptions` to `aadOrApiKey` and then send the request.
 
@@ -232,7 +256,7 @@ PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegrou
 [Semantic ranker is enabled](semantic-how-to-enable-disable.md) by default at the free plan that allows up to 1,000 requests per month at no charge. You can lock down the feature at the service level to prevent usage.
 
 ```http
-### disable semantic ranker
+### Disable semantic ranker
 PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2023-11-01 HTTP/1.1
      Content-type: application/json
      Authorization: Bearer {{token}}
@@ -251,7 +275,7 @@ PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegrou
 Azure AI Search [writes to external data sources](search-indexer-securing-resources.md) when updating a knowledge store, saving debug session state, or caching enrichments. The following example disables these workloads at the service level.
 
 ```http
-### disable-external-access
+### Disable external access
 PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2023-11-01 HTTP/1.1
      Content-type: application/json
      Authorization: Bearer {{token}}
@@ -266,7 +290,7 @@ PATCH https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegrou
 ## Delete a search service
 
 ```http
-### delete a search service
+### Delete a search service
 DELETE https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups/{{resource-group}}/providers/Microsoft.Search/searchServices/{{search-service-name}}?api-version=2023-11-01 HTTP/1.1
      Content-type: application/json
      Authorization: Bearer {{token}}
@@ -321,7 +345,7 @@ GET https://management.azure.com/subscriptions/{{subscriptionId}}/resourcegroups
 
 ## Next steps
 
-After a search service is configured, next steps include [create an index](search-how-to-create-search-index.md) or [query an index](search-query-overview.md) using the Azure portal, REST APIs, or an Azure SDK.
+After a search service is configured, your next steps include [creating an index](search-how-to-create-search-index.md) or [querying an index](search-query-overview.md) using the Azure portal, REST APIs, or an Azure SDK.
 
 * [Create an Azure AI Search index in the Azure portal](search-get-started-portal.md)
 * [Set up an indexer to load data from other services](search-indexer-overview.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 REST 管理 Azure AI 搜索服务文档"
}
```

### Explanation
在 `search-manage-rest.md` 文档中进行了重要的更新，主要增加和修改的内容如下：

1. **发布日期更新**：文档的发布日期已从 `12/10/2024` 更新为 `03/26/2025`，以确保信息的时效性。

2. **标题修改**：将标题中的 "Manage with REST" 更改为 "Manage using REST"，更为自然流畅。

3. **内容改善**：在文中添加了关于创建、升级和更改定价层等新功能的说明，细化了用户指引，使其更易于理解和使用。

4. **功能扩展说明**：引入了“升级服务”和“更改定价层”的段落，提供了相应的 HTTP 示例请求。这意味着用户现在可以通过 REST API 升级已有服务并更改其定价层，以便根据需求进行调整。

5. **API 参考链接**：新文档提供了更清晰的链接到 API 的参考，确保用户能轻松访问更多详细信息，从而进行更复杂的操作。

6. **格式调整**：对 REST 请求示例、注意事项和注释的格式进行了调整，增加了可读性和结构性，使用户更容易跟随指导完成操作。

7. **次要文本修正**：修正了一些措辞，以提升可读性和一致性，比如将不一致的标点符号和大小写进行了统一。

这些更新提升了文档的实用性和可读性，为用户提供了更清晰的操作步骤及更多的功能选项。用户可以通过以下链接访问该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-manage-rest.md)。

## articles/search/search-markdown-data-tutorial.md{#item-32ea2a}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index Markdown blobs'
+title: 'Tutorial: Index Markdown Blobs'
 titleSuffix: Azure AI Search
 description: Learn how to index and search Markdown in Azure blobs using Azure AI Search REST APIs.
 
@@ -9,40 +9,40 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: tutorial
-ms.date: 11/19/2024
+ms.date: 03/28/2025
 
 ---
 
 # Tutorial: Index nested Markdown blobs from Azure Storage using REST
 
 [!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
 
-Azure AI Search can index Markdown documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read Markdown data. 
+Azure AI Search can index Markdown documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read Markdown data.
 
-This tutorial shows you to index Markdown files indexed using the `oneToMany` Markdown parsing mode. It uses a REST client and the [Search REST APIs](/rest/api/searchservice/) to perform the following tasks:
+This tutorial shows you how to index Markdown files indexed using the `oneToMany` Markdown parsing mode. It uses a REST client and the [Search REST APIs](/rest/api/searchservice/) to:
 
 > [!div class="checklist"]
 > + Set up sample data and configure an `azureblob` data source
 > + Create an Azure AI Search index to contain searchable content
 > + Create and run an indexer to read the container and extract searchable content
 > + Search the index you just created
 
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Prerequisites
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ [Azure Storage](/azure/storage/common/storage-account-create)
++ [Azure Storage](/azure/storage/common/storage-account-create).
 
-+ [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing Azure AI Search resource](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription.
++ [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.
+
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
 > [!NOTE]
-> You can use the free service for this tutorial. A free search service limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ## Create a Markdown document
 
-Copy and paste the following Markdown into a file named `sample_markdown.md`. The sample data is a single Markdown file containing various Markdown elements. We chose one Markdown file to stay under the storage limits of the free tier.
+Copy and paste the following Markdown into a file named `sample_markdown.md`. The sample data is a single Markdown file containing various Markdown elements. We chose one Markdown file to stay under the storage limits of the Free tier.
 
 ````md
 # Project Documentation
@@ -193,7 +193,7 @@ Thank you for reviewing this example!
 
 ## Copy a search service URL and API key
 
-For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal. For alternative connection methods, see [managed identities](search-howto-managed-identities-data-sources.md).
+For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal. For alternative connection methods, see [Managed identities](search-howto-managed-identities-data-sources.md).
 
 1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the search service **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
@@ -205,7 +205,7 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 1. Start Visual Studio Code and create a new file.
 
-1. Provide values for variables used in the request: 
+1. Provide values for variables used in the request.
 
    ```http
    @baseUrl = PUT-YOUR-SEARCH-SERVICE-ENDPOINT-HERE
@@ -216,7 +216,7 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 1. Save the file using a `.rest` or `.http` file extension.
 
-See [Quickstart: Text search using REST](search-get-started-rest.md) if you need help with the REST client.
+For help with the REST client, see [Quickstart: Keyword search using REST](search-get-started-rest.md).
 
 ## Create a data source
 
@@ -293,19 +293,19 @@ You only need fields for the Markdown elements that the parser supports. These f
 
 - `content`: A string that contains the raw Markdown found in a specific location, based on the header metadata at that point in the document.
 
-- `sections`: An object that contains subfields for the header metadata up to the desired header level. For example, when `markdownHeaderDepth` is set to `h3`, contains string fields `h1`, `h2`, and `h3`. These fields are indexed by mirroring this structure in the index, or through field mappings in the format `/sections/h1`, `sections/h2`, etc. See index and indexer configurations in the following samples for in-context examples. The subfields contained are:
+- `sections`: An object that contains subfields for the header metadata up to the desired header level. For example, when `markdownHeaderDepth` is set to `h3`, contains string fields `h1`, `h2`, and `h3`. These fields are indexed by mirroring this structure in the index, or through field mappings in the format `/sections/h1`, `sections/h2`, etc. For in-context examples, see the index and indexer configurations in the following samples. The subfields contained are:
   - `h1` - A string containing the h1 header value. Empty string if not set at this point in the document.
   - (Optional) `h2`- A string containing the h2 header value. Empty string if not set at this point in the document.
   - (Optional) `h3`- A string containing the h3 header value. Empty string if not set at this point in the document.
   - (Optional) `h4`- A string containing the h4 header value. Empty string if not set at this point in the document.
   - (Optional) `h5`- A string containing the h5 header value. Empty string if not set at this point in the document.
   - (Optional) `h6`- A string containing the h6 header value. Empty string if not set at this point in the document.
 
-- `ordinal_position`: An integer value indicating the position of the section within the document hierarchy. This field is used for ordering the sections in their original sequence as they appear in the document, beginning with an ordinal position of 1 and incrementing sequentially for each content block. 
+- `ordinal_position`: An integer value that indicates the position of the section within the document hierarchy. This field is used for ordering the sections in their original sequence as they appear in the document, beginning with an ordinal position of 1 and incrementing sequentially for each content block.
 
-This implementation leverages [field mappings](search-indexer-field-mappings.md) in the indexer to map from the enriched content to the index. For more information on the parsed one-to-many document structure, see [index markdown blobs](search-how-to-index-markdown-blobs.md).
+This implementation uses [field mappings](search-indexer-field-mappings.md) in the indexer to map from the enriched content to the index. For more information about the parsed one-to-many document structure, see [Index Markdown blobs](search-how-to-index-markdown-blobs.md).
 
-This example provides samples of how to index data both with and without field mappings. In this case, we know that `h1` contains the title of the document, so we can map it to a field named `title`. We'll also be mapping the `h2` and `h3` fields to `h2_subheader` and `h3_subheader` respectively. The `content` and `ordinal_position` fields require no mapping because they are extracted from the Markdown directly into fields using those names. For an example of a full index schema that doesn't require field mappings, see the end of this section.
+This example provides samples of how to index data both with and without field mappings. In this case, we know that `h1` contains the title of the document, so we can map it to a field named `title`. We'll also be mapping the `h2` and `h3` fields to `h2_subheader` and `h3_subheader`, respectively. The `content` and `ordinal_position` fields require no mapping because they're extracted from the Markdown directly into fields using those names. For an example of a full index schema that doesn't require field mappings, see the end of this section.
 
 ```http
 ### Create an index
@@ -326,8 +326,10 @@ POST {{baseUrl}}/indexes?api-version=2024-11-01-preview  HTTP/1.1
     }
 ```
 
-###  Index schema in a configuration with no field mappings
-Field mappings allow you to manipulate and filter enriched content to fit into your desired index shape, but you may just want to take the enriched content directly. In that case, the schema would look like:
+### Index schema in a configuration with no field mappings
+
+Field mappings allow you to manipulate and filter enriched content to fit into your desired index shape. However, you might just want to take the enriched content directly. In that case, the schema would look like:
+
 ```http
 {
   "name": "sample-markdown-index",
@@ -347,9 +349,9 @@ Field mappings allow you to manipulate and filter enriched content to fit into y
 }
 ```
 
-To reiterate, we have subfields up to `h3` in the sections object because `markdownHeaderDepth` is set to `h3`. 
+To reiterate, we have subfields up to `h3` in the sections object because `markdownHeaderDepth` is set to `h3`.
 
-If you choose to use this schema, be sure to adjust later requests accordingly. This will require removing the field mappings from the indexer configuration and updating search queries to use the corresponding field names.
+If you use this schema, be sure to adjust later requests accordingly. This will require removing the field mappings from the indexer configuration and updating search queries to use the corresponding field names.
 
 ## Create and run an indexer
 
@@ -382,11 +384,11 @@ POST {{baseUrl}}/indexers?api-version=2024-11-01-preview  HTTP/1.1
     }
 ```
 
-**Key points**:
+Key points:
 
 + The indexer will only parse headers up to `h3`. Any lower-level headers (`h4`,`h5`,`h6`) will be treated as plain text and show up in the `content` field. This is why the index and field mappings only exist up to a depth of `h3`.
 
-+ The `content` and `ordinal_position` fields require no field mapping as they exist with those names in the enriched content.
++ The `content` and `ordinal_position` fields require no field mapping because they exist with those names in the enriched content.
 
 ## Run queries
 
@@ -404,7 +406,7 @@ POST {{baseUrl}}/indexes/sample-markdown-index/docs/search?api-version=2024-11-0
   }
 ```
 
-Send the request. This is an unspecified full text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
+Send the request. This is an unspecified full-text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
 
 ```json
 HTTP/1.1 200 OK
@@ -431,7 +433,7 @@ Connection: close
 
 ```
 
-Add a `search` parameter to search on a string. 
+Add a `search` parameter to search on a string.
 
 ```http
 ### Query the index
@@ -478,7 +480,8 @@ Connection: close
   ]
 }
 ```
-**Key points**:
+
+Key points:
 
 + Because the `markdownHeaderDepth` is set to `h3`, the `h4`, `h5`, and `h6` headers are treated as plaintext, so they appear in the `content` field.
 
@@ -528,14 +531,14 @@ Connection: close
 }
 ```
 
-For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case-sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
+For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
 
 > [!NOTE]
 > The `$filter` parameter only works on fields that were marked filterable at the creation of your index.
 
 ## Reset and rerun
 
-Indexers can be reset, clearing execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
+Indexers can be reset to clear execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
 
 ```http
 ### Reset the indexer
@@ -562,7 +565,8 @@ When you're working in your own subscription, at the end of a project, it's a go
 You can use the Azure portal to delete indexes, indexers, and data sources.
 
 ## Next steps
-Now that you're familiar with the basics of Azure Blob indexing, let's take a closer look at indexer configuration for Markdown blobs in Azure Storage.
+
+Now that you're familiar with the basics of Azure Blob indexing, take a closer look at indexer configuration for Markdown blobs in Azure Storage:
 
 > [!div class="nextstepaction"]
 > [Configure Markdown blob indexing](search-how-to-index-markdown-blobs.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Markdown 数据教程文档"
}
```

### Explanation
在 `search-markdown-data-tutorial.md` 文档中进行了小幅的更新，主要内容如下：

1. **发布日期更新**：文档的发布日期已从 `11/19/2024` 更新为 `03/28/2025`，确保信息的时效性。

2. **标题调整**：将标题中的 "Index Markdown blobs" 修改为 "Index Markdown Blobs" 以保持一致性和专业性。

3. **语言优化**：在多处对语言进行了细致调整，使其更加流畅和易于理解。例如，明确了在描述索引操作时使用“如何”一词，改进了句子的可读性。

4. **先决条件更新**：在先决条件部分，强调了订阅要求，将 "You must have an active subscription" 改为 "An Azure account with an active subscription"，使描述更清晰。同时对所需的其他工具和资源的说明进行了微调，提升了文档结构的逻辑性。

5. **内容增强**：新增了一些关于如何创建, 配置和运行索引器的详细说明，强调使用 REST API 的步骤和相关的示例。这有助于用户更好地理解操作流程。

6. **格式和可读性提升**：对代码块、列表和段落格式进行了调整，确保信息呈现的一致性，使得文档在视觉上更易于阅读。

7. **关键点总结调整**：对于教程中的关键点进行了重新排版和措辞，提升了总结的清晰度，增强了用户对信息的掌握。

这些更新提升了文档的准确性和可操作性，为用户提供了更全面的指导。用户可以通过以下链接访问该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-markdown-data-tutorial.md)。

## articles/search/search-performance-tips.md{#item-218e77}

<details>
<summary>Diff</summary>
````diff
@@ -6,12 +6,12 @@ author: mattgotteiner
 ms.author: magottei
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 12/10/2024
+ms.date: 03/21/2025
 ---
 
 # Tips for better performance in Azure AI Search
 
-This article is a collection of tips and best practices for boosting query and indexing performance for keyword search. Knowing which factors are most likely to impact search performance can help you avoid inefficiencies and get the most out of your search service. Some key factors include:
+This article is a collection of tips and best practices for boosting query and indexing performance for keyword search. Knowing which factors are most likely to affect search performance can help you avoid inefficiencies and get the most out of your search service. Some key factors include:
 
 + Index composition (schema and size)
 + Query design
@@ -22,7 +22,7 @@ This article is a collection of tips and best practices for boosting query and i
 
 ## Index size and schema
 
-Queries run faster on smaller indexes. This is partly a function of having fewer fields to scan, but it's also due to how the system caches content for future queries. After the first query, some content remains in memory where it's searched more efficiently. Because index size tends to grow over time, one best practice is to periodically revisit index composition, both schema and documents, to look for content reduction opportunities. However, if the index is right-sized, the only other calibration you can make is to increase capacity: either by [adding replicas](search-capacity-planning.md#adjust-capacity) or upgrading the service tier. The section ["Tip: Upgrade to a Standard S2 tier"](#tip-upgrade-to-a-standard-s2-tier) discusses the scale up versus scale out decision.
+Queries run faster on smaller indexes. This is partly a function of having fewer fields to scan, but it's also due to how the system caches content for future queries. After the first query, some content remains in memory where it's searched more efficiently. Because index size tends to grow over time, one best practice is to periodically revisit index composition, both schema and documents, to look for content reduction opportunities. However, if the index is right-sized, the only other calibration you can make is to increase capacity by [upgrading your service](search-how-to-upgrade.md), [adding replicas](search-capacity-planning.md#add-or-remove-partitions-and-replicas), or [switching to a higher tier](search-capacity-planning.md#change-your-pricing-tier). The section "[Tip: Switch to a Standard S2 tier](#tip-switch-to-a-standard-s2-tier)" discusses the scale up versus scale out decision.
 
 Schema complexity can also adversely affect indexing and query performance. Excessive field attribution builds in limitations and processing requirements. [Complex types](search-howto-complex-data-types.md) take longer to index and query. The next few sections explore each condition.
 
@@ -93,7 +93,7 @@ When query performance is slowing down in general, adding more replicas frequent
 
 One positive side-effect of adding partitions is that slower queries sometimes perform faster due to parallel computing. We've noted parallelization on low selectivity queries, such as queries that match many documents, or facets providing counts over a large number of documents. Since significant computation is required to score the relevancy of the documents, or to count the numbers of documents, adding extra partitions helps queries complete faster.  
 
-To add partitions, use [Azure portal](search-capacity-planning.md#adjust-capacity), [PowerShell](search-manage-powershell.md), [Azure CLI](search-manage-azure-cli.md), or a management SDK.
+To add partitions, use the [Azure portal](search-capacity-planning.md#add-or-remove-partitions-and-replicas), [PowerShell](search-manage-powershell.md), [Azure CLI](search-manage-azure-cli.md), or a management SDK.
 
 ## Service capacity
 
@@ -103,13 +103,13 @@ The tier of your search service and the number of replicas/partitions also have
 
 ### Tip: Create a new high capacity search service
 
-Basic and standard services created [in supported regions](search-limits-quotas-capacity.md#service-limits) after April 3, 2024 have more storage per partition than older services. Before upgrading to a higher tier and a higher billable rate, revisit the [tier service limits](search-limits-quotas-capacity.md#service-limits) to see if the same tier on a newer service gives you the necessary storage.
+Basic and Standard services created [in supported regions](search-limits-quotas-capacity.md#service-limits) after April 3, 2024 have more storage per partition than older services. If you have an older service, check if you can [upgrade your service](search-how-to-upgrade.md) to benefit from more capacity at the same billing rate. If an upgrade isn't available, review the [tier service limits](search-limits-quotas-capacity.md#service-limits) to see if the same tier on a newer service gives you the necessary storage.
 
-### Tip: Upgrade to a Standard S2 tier
+### Tip: Switch to a Standard S2 tier
 
 The Standard S1 search tier is often where customers start. A common pattern for S1 services is that indexes grow over time, which requires more partitions. More partitions lead to slower response times, so more replicas are added to handle the query load. As you can imagine, the cost of running an S1 service has now progressed to levels beyond the initial configuration.
 
-At this juncture, an important question to ask is whether it would be beneficial to move to a higher tier, as opposed to progressively increasing the number of partitions or replicas of the current service. 
+At this juncture, an important question to ask is whether it would be beneficial to [move to a higher tier](search-capacity-planning.md#change-your-pricing-tier), as opposed to progressively increasing the number of partitions or replicas of the current service.
 
 Consider the following topology as an example of a service that has taken on increasing levels of capacity:
 
@@ -133,7 +133,7 @@ However, if the administrator chose to move to a Standard S2 tier the topology w
 
 As this hypothetical scenario illustrates, you can have configurations on lower tiers that result in similar costs as if you had opted for a higher tier in the first place. However, higher tiers come with premium storage, which makes indexing faster. Higher tiers also have much more compute power, as well as extra memory. For the same costs, you could have more powerful infrastructure backing the same index.
 
-An important benefit of added memory is that more of the index can be cached, resulting in lower search latency, and a greater number of queries per second. With this extra power, the administrator may not need to even need to increase the replica count and could potentially pay less than by staying on the S1 service.
+An important benefit of added memory is that more of the index can be cached, resulting in lower search latency, and a greater number of queries per second. With this extra power, the administrator might not need to even need to increase the replica count and could potentially pay less than by staying on the S1 service.
 
 ### Tip: Consider alternatives to regular expression queries
 
@@ -146,5 +146,5 @@ Review these other articles related to service performance:
 + [Analyze performance](search-performance-analysis.md)
 + [Index large data sets in Azure AI Search](search-howto-large-index.md)
 + [Choose a service tier](search-sku-tier.md)
-+ [Plan or add capacity](search-capacity-planning.md#adjust-capacity)
++ [Plan or add capacity](search-capacity-planning.md#add-or-remove-partitions-and-replicas)
 + [Case Study: Use Cognitive Search to Support Complex AI Scenarios](https://techcommunity.microsoft.com/t5/azure-ai/case-study-effectively-using-cognitive-search-to-support-complex/ba-p/2804078)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "优化 Azure AI 搜索性能提示文档"
}
```

### Explanation
在 `search-performance-tips.md` 文档中进行了小幅的修改，主要包括以下内容：

1. **发布日期更新**：文档的发布日期已从 `12/10/2024` 更新为 `03/21/2025`，确保信息的时效性。

2. **措辞改进**：针对某些句子进行了小幅修改，以提高语句的流畅性和清晰度。例如，将 “影响” 替换为 “影响”，使其更符合中文表达的习惯。

3. **链接优化**：更新了一些文中引用的链接，使其指向最新的信息来源。例如，将 “add replicas” 更新为 “adding replicas” 确保语法一致性，并且修正了与服务升级相关的链接，使其更加明确并能直接引导用户获取所需的信息。

4. **内容明确性**：对涉及服务升级和分区的提示进行了重写，以加强语句的逻辑性和明确性。例如，“Tip: Upgrade to a Standard S2 tier” 改为 “Tip: Switch to a Standard S2 tier” ，让读者更清晰理解上下文。

5. **格式调整**：对部分内容的格式进行了优化，确保文档结构清晰，信息呈现更为一致和整洁，使得用户更易于阅读和理解。

6. **视觉一致性**：确保关键点和提示以一致的样式呈现，增强可读性，帮助读取时更容易找到关键信息。

这些更新增强了文档的整体可用性和准确性，为用户提供了更为优化的性能提示指导。用户可以通过以下链接访问该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-performance-tips.md)。

## articles/search/search-region-support.md{#item-25b0f1}

<details>
<summary>Diff</summary>
````diff
@@ -9,24 +9,25 @@ ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
 ms.custom: references_regions
-ms.date: 01/27/2025
+ms.date: 04/04/2025
 
 ---
 
 # Azure AI Search regions list
 
-This article identifies the cloud regions in which Azure AI Search is available. It also lists which premium features are available in each region. 
+This article identifies the cloud regions in which Azure AI Search is available. It also lists which premium features are available in each region.
 
 ## Features subject to regional availability
 
 Some features take a dependency on other Azure services or infrastructure that are subject to regional availability. If you need a specific feature, make sure it's available in the desired region.
 
 | Feature | Description | Availability |
 |---------|-------------|--------------|
-| [Extra capacity](search-limits-quotas-capacity.md#service-limits) | Higher capacity partitions became available in selected regions starting in April 2024 with a second wave following in May 2024. Currently, there are just a few regions that *don't* offer higher capacity partitions. If you're using an older search service, create a new search service to benefit from more capacity at the same billing rate. |  Regional support for extra capacity is noted in the footnotes of this article. <p>Check [service age](vector-search-index-size.md#how-to-check-service-creation-date) to see if your search service was created after high capacity partitions became available. <p>To check the capacity of an existing service, [find your search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) and select the **Properties** tab in the middle of the Overview page.|
-| [Availability zones](search-reliability.md#availability-zone-support) | Divides a region's data centers into distinct physical location groups, providing high-availability within the same geo. | Regional support is noted in this article. |
+| [Extra capacity](search-limits-quotas-capacity.md#service-limits) | Higher capacity partitions became available in select regions starting in April 2024, with a second wave following in May 2024. Currently, there are just a few regions that *don't* offer higher capacity partitions. If you have an older search service in a supported region, check if you can [upgrade your service](search-how-to-upgrade.md). Otherwise, create a new search service to benefit from more capacity at the same billing rate. |  Regional support for extra capacity is noted in the footnotes of this article. <p>Check your [service age](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date) to see if your search service was created after high capacity partitions became available. <p>To check the capacity of an existing service, [find your search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) and select the **Properties** tab in the middle of the **Overview** page.|
+| [Availability zones](search-reliability.md#availability-zone-support) | Divides a region's data centers into distinct physical location groups, providing high availability within the same geo. | Regional support is noted in this article. |
 | [Semantic ranker](semantic-search-overview.md) | Takes a dependency on Microsoft-hosted models in specific regions. | Regional support is noted in this article. |
-| [AI service integration](cognitive-search-concept-intro.md) | Refers to [built-in skills](cognitive-search-predefined-skills.md) that make internal calls to Azure AI for enrichment and transformation during indexing. Integration requires that Azure AI Search coexists with an [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) in the same physical region. You can bypass region requirements if you use [identity-based connections](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection), currently in public review. | Regional support is noted in this article. |
+| [Query rewrite](semantic-how-to-query-rewrite.md) | Takes a dependency on Microsoft-hosted models in specific regions. | Regional support is noted in this article. |
+| [AI service integration](cognitive-search-concept-intro.md) | Refers to [built-in skills](cognitive-search-predefined-skills.md) that make internal calls to Azure AI for enrichment and transformation during indexing. Integration requires that Azure AI Search coexists with an [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills) in the same physical region. You can bypass region requirements if you use [identity-based connections](cognitive-search-attach-cognitive-services.md#bill-through-a-keyless-connection), currently in public preview. | Regional support is noted in this article. |
 | [Azure OpenAI integration](vector-search-integrated-vectorization.md)  | Refers to the AzureOpenAIEmbedding skill and vectorizer that make internal calls to deployed embedding models on Azure OpenAI. | Check [Azure OpenAI model region availability](/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability) for the most current list of regions for each embedding and chat model. Specific Azure OpenAI models are in fewer regions, so check for model availability first, and then verify Azure AI Search is available in the same region.|
 | [Azure AI Foundry integration](vector-search-integrated-vectorization-ai-studio.md) | Refers to skills and vectorizers that make internal calls to the models hosted in the model catalog. | Check [Azure AI Foundry region availability](/azure/ai-foundry/reference/region-support) for the most current list of regions. |
 | [Azure AI Vision 4.0 multimodal APIs](search-get-started-portal-image-search.md) | Refers to the Azure AI Vision multimodal embeddings skill and vectorizer that call the multimodal embedding API. | Check the [Azure AI Vision region list](/azure/ai-services/computer-vision/overview-image-analysis#region-availability) first, and then verify Azure AI Search is available in the same region.|
@@ -39,97 +40,99 @@ AI service integration refers to internal connections to an Azure AI services mu
 
 ### Americas
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Brazil South​​ ​ | ✅ | ✅ | |  |
+| Brazil South​​ ​| ✅ |  | ✅ |  |
 | Canada Central​​ | ✅ | ✅ | ✅ |  |
-| Canada East​​ ​ |  | ✅ | |  |
-| ​Central US​​ | ✅ | ✅ | ✅ | |
+| Canada East​​ ​|  |  | ✅ |  |
+| ​Central US​​ | ✅ | ✅ | ✅ |  |
 | East US​ | ✅ | ✅ | ✅ |  |
-| East US 2 ​ | ✅ | ✅ | ✅ | |
-| Mexico Central | |  | ✅ | |
-| North Central US​ ​ | ✅ | ✅ | |  | 
-| South Central US​  | ✅ | ✅ | ✅ | |
-| West US​ ​ | ✅ | ✅ | |  |
-| West US 2​ ​ | ✅ | ✅ | ✅ | |
-| West US 3​ | ✅ | ✅ |✅ | |
-| West Central US​ ​ | ✅ | ✅ | | |
+| East US 2 ​ | ✅ | ✅ | ✅ |  |
+| Mexico Central |  | ✅ |  |  |
+| North Central US​ ​| ✅ |  | ✅ |  |
+| South Central US​ | ✅ | ✅ | ✅ |  |
+| West US​​ | ✅ |  | ✅ |  |
+| West US 2​ ​| ✅ | ✅ | ✅ |  |
+| West US 3​ | ✅ | ✅ | ✅ |  |
+| West Central US​ ​ | ✅ |  | ✅ |  |
 
 ### Europe
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| North Europe​​ | ✅ | ✅ | ✅ | S2, S3, L1, L2|
+| North Europe​ <sup>1</sup>​ | ✅ | ✅ | ✅ | ✅ |
 | West Europe​​ | ✅ | ✅ | ✅ |  |
-| France Central​​ | ✅ | ✅ | ✅ | |
-| Germany West Central​ ​| ✅ |  | ✅ | |
-| Italy North​​ |  |  | ✅ | |
-| Norway East​​ | ✅ |  | ✅ |  |
+| France Central​​ | ✅ | ✅ | ✅ |  |
+| Germany West Central​ ​| ✅ | ✅ |  |  |
+| Italy North​​ |  | ✅ |  |  |
+| Norway East​​ | ✅ | ✅ |  |  |
 | Poland Central​​ |  |  |  |  |
-| Spain Central <sup>1</sup> |  |  | ✅  |  |
-| Sweden Central​​ | ✅ |  | ✅ |  |
+| Spain Central <sup>2</sup> |  | ✅ |  |  |
+| Sweden Central​​ | ✅ | ✅ |  |  |
 | Switzerland North​ | ✅ | ✅ | ✅ |  |
 | Switzerland West​ | ✅ | ✅ | ✅ |  |
 | UK South​ | ✅ | ✅ | ✅ |  |
-| UK West​ ​|  | ✅ | |  |
+| UK West​ ​|  |  | ✅ |  |
 
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+<sup>1</sup> This region has capacity constraints on the following tiers: S2, S3, L1, and L2.
+
+<sup>2</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ### Middle East
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Israel Central​ <sup>1</sup> |  |  | ✅  |  |
-| Qatar Central​ <sup>1</sup> |  |  | ✅ | |
-| UAE North​​ | ✅ |  | ✅ |  |
+| Israel Central​ <sup>1</sup> |  | ✅ |  |  |
+| Qatar Central​ <sup>1</sup> |  | ✅ |  |  |
+| UAE North​​ | ✅ | ✅ |  |  |
 
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+<sup>1</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ### Africa
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| South Africa North​ | ✅ |  | ✅ |   |
+| South Africa North​ | ✅ | ✅ |  |  |
 
 ### Asia Pacific
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Australia East​ ​ | ✅ | ✅ | ✅ |   |
-| Australia Southeast​​​ |  | ✅ |  | |
+| Australia East​ ​| ✅ | ✅ | ✅ |  |
+| Australia Southeast​​​ |  |  | ✅ |  |
 | East Asia​ | ✅ | ✅ | ✅ |  |
-| Southeast Asia​ ​ ​ | ✅ | ✅ | ✅ |  |
+| Southeast Asia​​ | ✅ | ✅ | ✅ | ✅ |
 | Central India | ✅ | ✅ | ✅ |  |
-| Jio India West​ ​ | ✅ | ✅ |  |
-| South India <sup>1</sup> |  | | ✅ |
-| Japan East  | ✅ | ✅ | ✅ |
-| Japan West​ | ✅ | ✅ |  |
-| Korea Central | ✅ | ✅ | ✅ |
-| Korea South​ ​ |  | ✅ |  |
-| Taiwan North |  |  |   |  |
+| Jio India West​​ | ✅ |  | ✅ |  |
+| South India <sup>1</sup> |  | ✅ |  |  |
+| Japan East | ✅ | ✅ | ✅ |  |
+| Japan West​ | ✅ |  | ✅ |  |
+| Korea Central | ✅ | ✅ | ✅ |  |
+| Korea South​​ |  |  | ✅ |  |
+| Indonesia Central |  | ✅ |  |  |
 
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+<sup>1</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ## Azure Government regions
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| Arizona | ✅ | ✅  | | |
-| Texas |  | ✅ |  | |
-| Virginia | ✅ | ✅  | ✅ | |
+| Arizona | ✅ |  | ✅ |  |
+| Texas |  |  |  |  |
+| Virginia | ✅ | ✅ | ✅ |  |
 
 ## Azure operated by 21Vianet
 
-| Region | AI service integration | Semantic ranker | Availability zones | Capacity constrained |
+| Region | AI service integration | Availability zones | Semantic ranker | Query rewrite |
 |--|--|--|--|--|
-| China East |  |  |  |
-| China East 2 <sup>1</sup> | ✅  | | | |
-| China East 3 |  |  |  | |
-| China North |  |  | | |
-| China North 2 <sup>1</sup> |  |  | | |
-| China North 3 | | ✅ | ✅ | |
-
-<sup>1</sup> This region runs on older infrastructure that has lower storage limits per partition at every tier. Choose a different region if you want [higher limits](search-limits-quotas-capacity.md#service-limits).
+| China East |  |  |  |  |
+| China East 2 <sup>1</sup> | ✅ |  |  |  |
+| China East 3 |  |  |  |  |
+| China North |  |  |  |  |
+| China North 2 <sup>1</sup> |  |  |  |  |
+| China North 3 |  | ✅ | ✅ |  |
+
+<sup>1</sup> [Higher storage limits](search-limits-quotas-capacity.md#service-limits) aren't available in this region. If you want higher limits, choose a different region.
 
 ## See also
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure AI Search 区域支持文档"
}
```

### Explanation
对 `search-region-support.md` 文档进行了小幅更新，具体内容如下：

1. **发布日期更新**：文档的发布日期从 `01/27/2025` 更新为 `04/04/2025`，以确保用户获取到最新的信息。

2. **文本优化**：部分句子进行了语法和措辞的修改，以提升流畅性和可读性。例如，调整了对特定功能依赖于区域可用性的说明，使其更加清晰明确。

3. **功能表格更新**：对表格进行了显著更新，增加了对各个特性的描述和支持情况，包括：
   - 添加了新的功能如“查询重写”，并详细说明了其依赖于特定区域的Microsoft托管模型。
   - 更新了“额外容量”的描述，以便用户了解在不同区域的可用性及相关升级建议。

4. **区域支持表格内容改进**：区域支持表的列顺序进行调整，部分项目的排版和描述得到了清晰化，使得用户更容易查找所需的信息。例如，“语义排名器”和“查询重写”之间的关系进行了更清晰的排列。

5. **新增区域支持信息**：在“美洲”、“欧洲”、“中东”等地区的支持列表中，增加了对于不同功能的支持情况，使得用户可以更直观地了解各个区域的服务能力。

6. **信息一致性**：确保所有链接都是最新的，涉及外部参考的链接进行了更新，以保证读者能够顺利访问相关文档。

通过这些修改，文档变得更加准确和易于理解，为用户提供了额外的支持和指导。用户可以访问以下链接查看该文档的详细信息：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-region-support.md)。

## articles/search/search-reliability.md{#item-3e9b1a}

<details>
<summary>Diff</summary>
````diff
@@ -6,7 +6,7 @@ author: mattgotteiner
 ms.author: magottei
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 10/29/2024
+ms.date: 03/21/2025
 ms.custom:
   - subject-reliability
   - references_regions
@@ -21,13 +21,13 @@ Across Azure, [reliability](/azure/reliability/overview) means resiliency and av
 
 + Deploy multiple search services across different geographic regions. All search workloads are fully contained within a single service that runs in a single geographic region, but in a multi-service scenario, you have options for synchronizing content so that it's the same across all services. You can also set up a load balancing solution to redistribute requests or fail over if there's a service outage.
 
-For business continuity and recovery from disasters at a regional level, plan on a cross-regional topology, consisting of multiple search services having identical configuration and content. Your custom script or code provides the *fail over* mechanism to an alternate search service if one suddenly becomes unavailable.
+For business continuity and recovery from disasters at a regional level, plan on a cross-regional topology, consisting of multiple search services having identical configuration and content. Your custom script or code provides the *failover* mechanism to an alternate search service if one suddenly becomes unavailable.
 
 <a name="scale-for-availability"></a>
 
 ## High availability
 
-In Azure AI Search, replicas are copies of your index. A search service is commissioned with at least one replica, and can have up to 12 replicas. [Adding replicas](search-capacity-planning.md#adjust-capacity) allows Azure AI Search to do machine reboots and maintenance against one replica, while query execution continues on other replicas.
+In Azure AI Search, replicas are copies of your index. A search service is commissioned with at least one replica, and can have up to 12 replicas. [Adding replicas](search-capacity-planning.md#add-or-remove-partitions-and-replicas) allows Azure AI Search to do machine reboots and maintenance against one replica, while query execution continues on other replicas.
 
 For each individual search service, Microsoft guarantees at least 99.9% availability for configurations that meet these criteria:
 
@@ -45,7 +45,7 @@ No Service Level Agreement (SLA) is provided for the *Free* tier. For more infor
 
 [Availability zones](/azure/reliability/availability-zones-overview) are an Azure platform capability that divides a region's data centers into distinct physical location groups to provide high availability, within the same region. In Azure AI Search, individual replicas are the units for zone assignment. A search service runs within one region; its replicas run in different physical data centers (or zones) within that region.
 
-Availability zones are used when you add two or more replicas to your search service. Each replica is placed in a different availability zone within the region. If you have more replicas than available zones in the search service region, the replicas are distributed across zones as evenly as possible. There's no specific action on your part, except to [create a search service](search-create-service-portal.md) in a region that provides availability zones, and then to configure the service to [use multiple replicas](search-capacity-planning.md#adjust-capacity).
+Availability zones are used when you add two or more replicas to your search service. Each replica is placed in a different availability zone within the region. If you have more replicas than available zones in the search service region, the replicas are distributed across zones as evenly as possible. There's no specific action on your part, except to [create a search service](search-create-service-portal.md) in a region that provides availability zones, and then to configure the service to [use multiple replicas](search-capacity-planning.md#add-or-remove-partitions-and-replicas).
 
 ### Prerequisites
 
@@ -115,7 +115,7 @@ The goal of a geo-distributed set of search services is to have two or more inde
 You can implement this architecture by creating multiple services and designing a strategy for data synchronization. Optionally, you can include a resource like Azure Traffic Manager for routing requests. 
 
 > [!TIP]
-> For help in deploying multiple search services across multiple regions, see this [Bicep sample on GitHub](https://github.com/Azure-Samples/azure-search-multiple-regions) that deploys a fully configured, multi-regional search solution. The sample gives you two options for index synchronization, and request redirection using Traffic Manager.
+> For help with deploying multiple search services across multiple regions, see this [Bicep sample on GitHub](https://github.com/Azure-Samples/azure-search-multiple-regions) that deploys a fully configured, multi-regional search solution. The sample gives you two options for index synchronization, and request redirection using Traffic Manager.
 
 <a name="data-sync"></a>
 
@@ -166,7 +166,7 @@ Azure Traffic Manager is primarily used for routing network traffic across diffe
 Traffic Manager doesn't provide an endpoint for a direct connection to Azure AI Search, which means you can't put a search service directly behind Traffic Manager. Instead, the assumption is that requests flow to Traffic Manager, then to a search-enabled web client, and finally to a search service on the backend. The client and service are located in the same region. If one search service goes down, the search client starts failing, and Traffic Manager redirects to the remaining client.
 
 > [!NOTE]
-> If you are using Azure Load Balancer [health probes](/azure/load-balancer/load-balancer-custom-probe-overview) on a search service, you must use a HTTPS probe with `/ping` as the path.
+> If you are using Azure Load Balancer [health probes](/azure/load-balancer/load-balancer-custom-probe-overview) on a search service, you must use an HTTPS probe with `/ping` as the path.
 
 ![Diagram of search apps connecting through Azure Traffic Manager.][4]
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 Azure AI 搜索可靠性文档"
}
```

### Explanation
对 `search-reliability.md` 文档进行了小幅更新，主要修改内容包括：

1. **发布日期更新**：文档的发布日期由 `10/29/2024` 更新为 `03/21/2025`，以确保信息的时效性。

2. **措辞改进**：部分段落中某些术语进行了细微的调整，以提高文本的一致性和流畅性。特别是将“fail over”统一为“failover”，确保使用一致的术语表述。

3. **链接更新**：对一部分链接进行了更新。例如，将“[调整容量](search-capacity-planning.md#adjust-capacity)”更改为“[添加或删除分区和副本](search-capacity-planning.md#add-or-remove-partitions-and-replicas)”，让用户更容易理解操作内容。

4. **内容清晰度增强**：增强了关于多副本的描述，特别是在讨论高可用性时，强调了副本的分布和管理，确保用户能够清楚地理解如何利用可用区来提高服务的可靠性。

5. **说明性内容补充**：增补了一些操作提示，以帮助用户更好地理解具体的配置步骤，如创建支持可用区的搜索服务，以及如何配置多个副本。

这些更新旨在进一步优化文档的可读性、准确性和可操作性，使用户能够更好地理解Azure AI搜索服务的可靠性及其配置。用户可以通过以下链接访问文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-reliability.md)。

## articles/search/search-security-manage-encryption-keys.md{#item-db3487}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title:  Encrypt data using customer-managed keys
+title: Encrypt data using customer-managed keys
 titleSuffix: Azure AI Search
 description: Supplement server-side encryption in Azure AI Search using customer managed keys (CMK) or bring your own keys (BYOK) that you create and manage in Azure Key Vault.
 
@@ -8,7 +8,7 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: how-to
-ms.date: 03/03/2025
+ms.date: 04/07/2025
 ms.custom:
   - references_regions
   - ignite-2023
@@ -110,7 +110,7 @@ Enable the system assigned managed identity for your search service. It's a two-
 
 1. Give the identity a descriptive name.
 
-1. Next, assign the user-managed identity to the search service. This can be done using the latest preview [2024-06-01-preview](/rest/api/searchmanagement/management-api-versions) management API or the previous preview.
+1. Next, assign the user-managed identity to the search service. This can be done using the latest preview [2025-05-01-preview](/rest/api/searchmanagement/management-api-versions) management API or the previous preview.
 
     The identity property takes a type and one or more fully qualified user-assigned identities:
   
@@ -122,7 +122,7 @@ Enable the system assigned managed identity for your search service. It's a two-
     Example of how to assign a user-managed identity to a search service:
   
     ```http
-    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/[search service name]?api-version=2024-06-01-preview
+    PUT https://management.azure.com/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Search/searchServices/[search service name]?api-version=2025-05-01-preview
     Content-Type: application/json
 
     {
@@ -213,9 +213,9 @@ In the Azure portal, skillsets are defined in JSON view. Use the JSON shown in t
 
 1. Add a new object. In the object definition, select **Microsoft-managed encryption**.
 
-1. Select **Customer-managed keys** and use the pickers to select the vault, key, and version.
+1. Select **Customer-managed keys** and choose your subscription, vault, key, and version.
 
-:::image type="content" source="media/search-security-manage-encryption-keys/assign-key-vault-portal.png" alt-text="Screenshot of the encryption key page in the Azure portal.":::
+:::image type="content" source="media/search-security-manage-encryption-keys/assign-key-vault.png" alt-text="Screenshot of the encryption key page in the Azure portal.":::
 
 ### [**REST APIs**](#tab/rest)
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新用户管理加密密钥的管理文档"
}
```

### Explanation
对 `search-security-manage-encryption-keys.md` 文档进行了小幅更新，主要修改内容包含：

1. **标题和描述调整**：标题中的格式细微调整，确保一致性。描述内容保持不变。

2. **发布日期更新**：文档的发布日期由 `03/03/2025` 更新为 `04/07/2025`，以反映最新的发布日期。

3. **API版本更新**：在涉及用户管理的身份赋值说明中，将API版本从 `2024-06-01-preview` 更新为 `2025-05-01-preview`，以确保用户获取到最新的API信息。

4. **示例代码更新**：在示例HTTP请求中，更新了API版本对应的URL，使其反映最新的版本号，确保用户在使用时不会遇到过时的信息。

5. **措辞优化**：在描述选择加密密钥的步骤时，将“使用选择器选择”更改为“选择你的订阅、金库、密钥和版本”，使用户的操作指导更加明确和易于理解。

6. **图像更新**：更新了指向Azure门户中加密密钥页面截图的链接，以确保用户能够查看到最新的界面内容。

通过这些更新，文档的准确性、时效性和可操作性得到了提升，将帮助用户更有效地管理其加密密钥。用户可以访问以下链接查看该文档的详细信息：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-security-manage-encryption-keys.md)。

## articles/search/search-security-network-security-perimeter.md{#item-49c0d7}

<details>
<summary>Diff</summary>
````diff
@@ -26,7 +26,7 @@ This article explains how to join an Azure AI Search service to a [network secur
 * Block any data exfiltration from a search service to other services outside the perimeter.
 * Allow access to your search service using inbound and outbound access capabilities of the network security perimeter.
 
-You can add a search service to a network security perimeter in the Azure portal, as described in this article. Alternatively, you can use the [Azure Virtual Network Manager REST API](/rest/api/networkmanager/) to join a search service, and use the [Search Management REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) to view and synchronize the configuration settings.
+You can add a search service to a network security perimeter in the Azure portal, as described in this article. Alternatively, you can use the [Azure Virtual Network Manager REST API](/rest/api/networkmanager/) to join a search service, and use the [Search Management REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2025-05-01-preview&preserve-view=true) to view and synchronize the configuration settings.
 
 ## Limitations and considerations
 
@@ -270,9 +270,9 @@ In order to test your connection through network security perimeter, you need ac
 
 ## View and manage network security perimeter configuration
 
-You can use the [Network Security Perimeter Configuration REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) to review and reconcile perimeter configurations.
+You can use the [Network Security Perimeter Configuration REST APIs](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2025-05-01preview&preserve-view=true) to review and reconcile perimeter configurations.
 
-Be sure to use preview API version `2024-06-01-preview`. [Learn how to call the Management REST APIs](search-manage-rest.md).
+Be sure to use preview API version `2024-06-01-preview` or a later preview. [Learn how to call the Management REST APIs](search-manage-rest.md).
 
 ## See also
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新网络安全边界配置文档"
}
```

### Explanation
对 `search-security-network-security-perimeter.md` 文档进行了小幅更新，主要修改内容包括：

1. **API版本更新**：在描述如何将搜索服务添加到网络安全边界时，将引用的Search Management REST API版本从 `2024-06-01-preview` 更新为 `2025-05-01-preview`，以确保提供最新的API信息。

2. **API链接调整**：在使用网络安全边界配置REST API的部分，更新了文档中提及的API链接，确保用户链接到最新的API版本。

3. **措辞优化**：在提示使用API版本时，将指示更改为“确保使用预览API版本 `2024-06-01-preview` 或更高版本”，以反映允许使用更高版本API的灵活性。

这些小的修改旨在提升文档的准确性和时效性，确保用户在配置网络安全边界时获取最新的信息和指导。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-security-network-security-perimeter.md)。

## articles/search/search-security-overview.md{#item-6b3f1e}

<details>
<summary>Diff</summary>
````diff
@@ -237,7 +237,7 @@ CMK support was rolled out in two phases. If you created your search service dur
 
 + The second rollout on May 13, 2021 added encryption for temporary disks and extended CMK encryption to [all supported regions](search-region-support.md).
 
-  If you're using CMK from a service created during the first rollout and you also want CMK encryption over temporary disks, you need to create a new search service in your region of choice and redeploy your content. To determine your service creation date, see [How to check service creation date](vector-search-index-size.md#how-to-check-service-creation-date).
+  If you're using CMK from a service created during the first rollout and you also want CMK encryption over temporary disks, you need to create a new search service in your region of choice and redeploy your content. To determine your service creation date, see [How to check service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date).
 
 Enabling CMK encryption will increase index size and degrade query performance. Based on observations to date, you can expect to see an increase of 30-60 percent in query times, although actual performance will vary depending on the index definition and types of queries. Because of the negative performance impact, we recommend that you only enable this feature on indexes that really require it. For more information, see [Configure customer-managed encryption keys in Azure AI Search](search-security-manage-encryption-keys.md).
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新客户管理加密密钥文档链接"
}
```

### Explanation
对 `search-security-overview.md` 文档进行了小幅更新，主要修改内容包括：

1. **内容补充**：新增了一条关于2021年5月13日第二阶段推出的内容，说明该阶段增加了对临时磁盘的加密并扩展了客户管理加密密钥（CMK）加密到所有支持的区域。

2. **链接更新**：在描述使用CMK的搜索服务创建日期的段落中，将参考链接从 `vector-search-index-size.md#how-to-check-service-creation-date` 更新为 `search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date`，以确保用户能够访问到最新的服务创建日期检查指南。

这些修改意在提升文档的清晰度和准确性，使用户能够获取最新的产品信息和操作指导。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-security-overview.md)。

## articles/search/search-semi-structured-data.md{#item-d3388d}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index semi-structured data in JSON blobs'
+title: 'Tutorial: Index Semi-Structured Data in JSON Blobs'
 titleSuffix: Azure AI Search
 description: Learn how to index and search semi-structured Azure JSON blobs using Azure AI Search REST APIs.
 
@@ -10,42 +10,42 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 12/10/2024
+ms.date: 03/28/2025
 
 ---
 
 # Tutorial: Index nested JSON blobs from Azure Storage using REST
 
-Azure AI Search can index JSON documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read semi-structured data. Semi-structured data contains tags or markings which separate content within the data. It splits the difference between unstructured data, which must be fully indexed, and formally structured data that adheres to a data model, such as a relational database schema that can be indexed on a per-field basis.
+Azure AI Search can index JSON documents and arrays in Azure Blob Storage using an [indexer](search-indexer-overview.md) that knows how to read semi-structured data. Semi-structured data contains tags or markings that separate content within the data. It splits the difference between unstructured data, which must be fully indexed, and formally structured data that adheres to a data model, such as a relational database schema that can be indexed on a per-field basis.
 
-This tutorial shows you to index nested JSON arrays. It uses a REST client and the [Search REST APIs](/rest/api/searchservice/) to perform the following tasks:
+This tutorial shows you how to index nested JSON arrays, using a REST client and the [Search REST APIs](/rest/api/searchservice/) to:
 
 > [!div class="checklist"]
 > + Set up sample data and configure an `azureblob` data source
 > + Create an Azure AI Search index to contain searchable content
 > + Create and run an indexer to read the container and extract searchable content
 > + Search the index you just created
 
-If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
-
 ## Prerequisites
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ [Azure Storage](/azure/storage/common/storage-account-create)
++ [Azure Storage](/azure/storage/common/storage-account-create).
 
-+ [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing Azure AI Search resource](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription.
++ [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription.
+
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
 > [!NOTE]
-> You can use the free service for this tutorial. A free search service limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
+> You can use a free search service for this tutorial. The Free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before you start, make sure you have room on your service to accept the new resources.
 
 ### Download files
 
 Download a zip file of the sample data repository and extract the contents. [Learn how](https://docs.github.com/get-started/start-your-journey/downloading-files-from-github).
 
 + [ny-philharmonic-free](https://github.com/Azure-Samples/azure-search-sample-data)
 
-Sample data is a single JSON file containing a JSON array and 1,521 nested JSON elements. Sample data originates from [NY Philharmonic Performance History](https://www.kaggle.com/datasets/nyphil/perf-history) on Kaggle. We chose one JSON file to stay under the storage limits of the free tier.
+The sample data is a single JSON file that contains a JSON array and 1,521 nested JSON elements. The data originates from the [NY Philharmonic Performance History](https://www.kaggle.com/datasets/nyphil/perf-history) on Kaggle. We chose one JSON file to stay under the storage limits of the Free tier.
 
 Here's the first nested JSON in the file. The remainder of the file includes 1,520 other instances of concert performances.
 
@@ -90,7 +90,7 @@ Here's the first nested JSON in the file. The remainder of the file includes 1,5
 
 ### Upload sample data to Azure Storage
 
-1. In Azure Storage, create a new container and name it *ny-philharmonic-free*.
+1. In Azure Storage, create a new container named **ny-philharmonic-free**.
 
 1. [Upload the sample data files](/azure/storage/blobs/storage-quickstart-blobs-portal).
 
@@ -106,7 +106,7 @@ Here's the first nested JSON in the file. The remainder of the file includes 1,5
 
 ### Copy a search service URL and API key
 
-For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal.
+For this tutorial, connections to Azure AI Search require an endpoint and an API key. You can get these values from the Azure portal. For alternative connection methods, see [Managed identities](search-howto-managed-identities-data-sources.md).
 
 1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the search service **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
@@ -116,9 +116,9 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 ## Set up your REST file
 
-1. Start Visual Studio Code and create a new file
+1. Start Visual Studio Code and create a new file.
 
-1. Provide values for variables used in the request: 
+1. Provide values for variables used in the request.
 
    ```http
    @baseUrl = PUT-YOUR-SEARCH-SERVICE-ENDPOINT-HERE
@@ -129,7 +129,7 @@ For this tutorial, connections to Azure AI Search require an endpoint and an API
 
 1. Save the file using a `.rest` or `.http` file extension.
 
-See [Quickstart: Text search using REST](search-get-started-rest.md) if you need help with the REST client.
+For help with the REST client, see [Quickstart: Keyword search using REST](search-get-started-rest.md).
 
 ## Create a data source
 
@@ -199,7 +199,7 @@ Connection: close
 
 [Create Index (REST)](/rest/api/searchservice/indexes/create) creates a search index on your search service. An index specifies all the parameters and their attributes.
 
-For nested JSON, the index fields must be identical to the source fields. Currently, Azure AI Search doesn't support field mappings to nested JSON. For this reason, field names and data types must match completely. The following index aligns to the JSON elements in the raw content.
+For nested JSON, the index fields must be identical to the source fields. Currently, Azure AI Search doesn't support field mappings to nested JSON, so field names and data types must match completely. The following index aligns to the JSON elements in the raw content.
 
 ```http
 ### Create an index
@@ -235,7 +235,7 @@ POST {{baseUrl}}/indexes?api-version=2024-07-01  HTTP/1.1
     }
 ```
 
-**Key points**:
+Key points:
 
 + You can't use [field mappings](search-indexer-field-mappings.md) to reconcile differences in field names or data types. This index schema is designed to mirror the raw content.
 
@@ -268,7 +268,7 @@ POST {{baseUrl}}/indexers?api-version=2024-07-01  HTTP/1.1
     }
 ```
 
-**Key points**:
+Key points:
 
 + The raw content file contains a JSON array (`"programs"`) with 1,526 nested JSON structures. Set `parsingMode` to `jsonArray` to tell the indexer that each blob contains a  JSON array. Because the nested JSON starts one level down, set `documentRoot` to `/programs`.
 
@@ -290,7 +290,7 @@ POST {{baseUrl}}/indexes/ny-philharmonic-index/docs/search?api-version=2024-07-0
   }
 ```
 
-Send the request. This is an unspecified full text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
+Send the request. This is an unspecified full-text search query that returns all of the fields marked as retrievable in the index, along with a document count. The response should look like:
 
 ```json
 HTTP/1.1 200 OK
@@ -321,7 +321,7 @@ Connection: close
 }
 ```
 
-Add a `search` parameter to search on a string. Add a `select` parameter to limit the results to fewer fields. Add a `filter` to further narrow the search.
+Add a `search` parameter to search on a string, a `select` parameter to limit the results to fewer fields, and a `filter` to further narrow the search.
 
 ```http
 ### Query the index
@@ -339,14 +339,14 @@ POST {{baseUrl}}/indexes/ny-philharmonic-index/docs/search?api-version=2024-07-0
 
 Two documents are returned in the response.
 
-For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case-sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
+For filters, you can also use Logical operators (and, or, not) and comparison operators (eq, ne, gt, lt, ge, le). String comparisons are case -sensitive. For more information and examples, see [Create a query](search-query-simple-examples.md).
 
 > [!NOTE]
-> The `$filter` parameter only works on fields that were marked filterable at the creation of your index.
+> The `$filter` parameter only works on fields that were marked filterable during index creation.
 
 ## Reset and rerun
 
-Indexers can be reset, clearing execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
+Indexers can be reset to clear execution history, which allows a full rerun. The following GET requests are for reset, followed by rerun.
 
 ```http
 ### Reset the indexer
@@ -374,7 +374,7 @@ You can use the Azure portal to delete indexes, indexers, and data sources.
 
 ## Next steps
 
-Now that you're familiar with the basics of Azure Blob indexing, let's take a closer look at indexer configuration for JSON blobs in Azure Storage.
+Now that you're familiar with the basics of Azure Blob indexing, take a closer look at indexer configuration for JSON blobs in Azure Storage:
 
 > [!div class="nextstepaction"]
 > [Configure JSON blob indexing](search-howto-index-json-blobs.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新半结构化数据索引教程"
}
```

### Explanation
对 `search-semi-structured-data.md` 文档进行了小幅更新，主要修改内容包括：

1. **标题格式修正**：将教程标题中的“semi-structured”修改为“Semi-Structured”，统一为标题格式的书写规则。

2. **日期更新**：将文档的最后更新日期从 `12/10/2024` 修改为 `03/28/2025`，以反映最新的编辑时间。

3. **句子结构优化**：对多个句子和指示进行了轻微的语言调整，以提高可读性。例如，明确说明如何使用REST客户端索引嵌套的JSON数组，语句更加流畅。

4. **链接规范化**：更新了一些链接的描述和撰写方式，使其更加一致和清晰。例如，更新了如何创建和查找Azure AI Search服务的描述，增加了对Active Subscription的说明。

5. **代码示例和说明的调整**：对一些代码段和说明进行了重新组织和格式化，例如在有关创建数据源和查询索引的部分，调整了表达方式以提高可读性，确保用户能够更清晰地理解步骤。

这些修改旨在提升文档的整体可用性和清晰度，使其更易于用户理解和操作。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-semi-structured-data.md)。

## articles/search/search-sku-manage-costs.md{#item-6e0122}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: conceptual
-ms.date: 02/25/2025
+ms.date: 03/21/2025
 ---
 
 # Plan and manage costs of an Azure AI Search service
@@ -22,7 +22,7 @@ As a first step, estimate your baseline costs by using the Azure pricing calcula
 Azure provides built-in cost management that cuts across service boundaries to provide inclusive cost monitoring and the ability to set budgets and define alerts. The costs of running a search service will vary depending on capacity and which features you use. After you create your search service, optimize capacity so that you pay only for what you need. 
 
 > [!NOTE]
-> Higher capacity partitions are available at the same billing rate on newer services created after April and May 2024. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits) for partition size upgrades.
+> Higher capacity partitions are available at the same billing rate on newer services created after April and May 2024. For more information about partition size upgrades, see [Service limits](search-limits-quotas-capacity.md#service-limits).
 
 <a name="billable-events"></a>
 
@@ -76,7 +76,7 @@ Follow these guidelines to minimize costs of an Azure AI Search solution.
 
 1. [Scale up](search-capacity-planning.md) for resource-intensive operations like indexing, and then readjust downwards for regular query workloads. If there are predictable patterns to your workloads, you might be able to synchronize scale up to coincide with the expected volume (you would need to write code to automate this).
 
-   When estimating the cost of a search solution, keep in mind that pricing and capacity aren't linear (doubling capacity more than doubles the cost on the same tier). Also, at some point, switching up to a higher tier can give you better and faster performance at roughly the same price point. For more information and an example, see [Upgrade to a Standard S2 tier](search-performance-tips.md#tip-upgrade-to-a-standard-s2-tier).
+   When estimating the cost of a search solution, keep in mind that pricing and capacity aren't linear (doubling capacity more than doubles the cost on the same tier). Also, at some point, switching up to a higher tier can give you better and faster performance at roughly the same price point. For more information and an example, see [Switch to a Standard S2 tier](search-performance-tips.md#tip-switch-to-a-standard-s2-tier).
 
 1. Consider [Azure Web App](/azure/app-service/overview) for your front-end application so that requests and responses stay within the data center boundary.
 
@@ -100,7 +100,7 @@ Search runs as a continuous service. Dedicated resources are always operational,
 
 **Can I change the billing rate (tier) of an existing search service?**
 
-In-place upgrade or downgrade isn't supported. Changing a service tier requires provisioning a new service at the desired tier.
+Existing services can be switched between Basic and Standard (S1, S2, and S3) tiers. Currently, you can only switch from a lower tier to a higher tier, such as going from Basic to S1. For more information, see [Change your pricing tier](search-capacity-planning.md#change-your-pricing-tier).
 
 ## Next steps
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新Azure AI搜索服务成本管理文档"
}
```

### Explanation
对 `search-sku-manage-costs.md` 文档进行了小幅更新，主要修改内容包括：

1. **日期变更**：将文档的最后更新日期从 `02/25/2025` 修改为 `03/21/2025`，以反映最新的编辑时间。

2. **内容细化**：
   - 调整了关于高容量分区的说明，使表述更加清晰，明确指出有关分区大小升级的信息。
   - 将提及的内容从“有关分区大小升级的信息”链接到“服务限制”文档，以便用户更清楚地了解相关细节。

3. **术语修正**：将文中关于服务层级的称呼从“升级到标准 S2 层”修改为“切换到标准 S2 层”，使得对话更加准确，避免误解。

4. **服务层级说明**：在讨论现有服务的计费层级变更时，增加了更具体的信息，解释了现有服务如何在基本层与标准层（S1, S2, S3）之间切换，强调了只允许从较低的层级切换到较高的层级。

这些调整的目的是优化文档的准确性和易读性，使用户能更好地理解如何管理Azure AI搜索服务的成本。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-sku-manage-costs.md)。

## articles/search/search-sku-tier.md{#item-7686b8}

<details>
<summary>Diff</summary>
````diff
@@ -8,26 +8,27 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: conceptual
-ms.date: 01/15/2025
+ms.date: 03/21/2025
 
 ---
 
 # Choose a service tier for Azure AI Search
 
-Part of [creating a search service](search-create-service-portal.md) is choosing a pricing tier (or SKU) that's fixed for the lifetime of the service. In the Azure portal, tier is specified in the **Select Pricing Tier** page when you create the service. In PowerShell or Azure CLI, the tier is specified through the **`-Sku`** parameter.
+Part of [creating a search service](search-create-service-portal.md) is choosing a pricing tier (or SKU). In the Azure portal, tier is specified in the **Select Pricing Tier** page when you create the service. In PowerShell or Azure CLI, the tier is specified through the `-Sku` parameter.
 
-The tier determines:
+The tier determines the:
 
-+ Maximum number of indexes and other objects allowed on the service
-+ Size and speed of partitions (physical storage)
-+ Billable rate as a fixed monthly cost, but also an incremental cost if you add capacity
++ Maximum number of indexes and other objects allowed on the service.
++ Size and speed of partitions (physical storage).
++ Billable rate as a fixed monthly cost, but also an incremental cost if you add capacity.
++ Workload characteristics. Some tiers are optimized for specific workloads.
 
 In a few instances, the tier you choose determines the availability of [premium features](#feature-availability-by-tier).
 
 Billing rates are shown in the Azure portal's **Select Pricing Tier** page. You can check the [pricing page](https://azure.microsoft.com/pricing/details/search/) for regional rates and review [Plan and manage costs](search-sku-manage-costs.md) to learn more about the billing model.
 
 > [!NOTE]
-> Search services created after April 3, 2024 have larger partitions and higher vector quotas at almost every tier. For more information, see [service limits](search-limits-quotas-capacity.md#service-limits).
+> Search services created after April 3, 2024 have larger partitions and higher vector quotas at almost every tier. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits).
 
 ## Tier descriptions
 
@@ -63,7 +64,7 @@ Currently, several regions are capacity-constrained for specific tiers and can't
 
 ## Feature availability by tier
 
-Most features are available on all tiers, including the free tier. In a few cases, the tier determines the availability of a feature. The following table describes the constraints.
+Most features are available on all tiers, including the Free tier. In a few cases, the tier determines the availability of a feature. The following table describes the constraints.
 
 | Feature | Tier considerations |
 |---------|---------------------|
@@ -88,7 +89,7 @@ Tiers determine the  maximum storage of the service itself, plus the maximum num
 Tier pricing includes details about per-partition storage that ranges from 15 GB for Basic, up to 2 TB for Storage Optimized (L2) tiers. Other hardware characteristics, such as speed of operations, latency, and transfer rates, aren't published, but tiers that are designed for specific solution architectures are built on hardware that has the features to support those scenarios. For more information about partitions, see [Estimate and manage capacity](search-capacity-planning.md) and [Reliability in Azure AI Search](search-reliability.md).
 
 > [!NOTE]
-> Higher capacity partitions became available in selected regions starting in April 2024. A second wave of higher capacity partitions released in May 2024. If you're using an older search service, consider creating a new search service to benefit from more capacity at the same billing rate. For more information, see [Service limits](search-limits-quotas-capacity.md#service-limits). To check the age of your search service, see [How to check service creation date](vector-search-index-size.md#how-to-check-service-creation-date).
+> Higher capacity partitions became available in select regions in April 2024. A second wave of higher capacity partitions was released in May 2024. If you have an older search service, you might be able to [upgrade your service](search-how-to-upgrade.md) to benefit from more capacity at the same billing rate.
 
 ## Billing rates
 
@@ -102,22 +103,30 @@ The following example provides an illustration. Assume a hypothetical billing ra
 
 This billing model is based on the concept of applying the billing rate to the number *search units* (SU) used by a search service. All services are initially provisioned at one SU, but you can increase the SUs by adding either partitions or replicas to handle larger workloads. For more information, see [How to estimate costs of a search service](search-sku-manage-costs.md).
 
-## Tier upgrade or downgrade
+## Tier changes
 
-There's no built-in support to upgrade or downgrade tiers. If you want to switch to a different tier, the approach is:
+Services can be switched between Basic and Standard (S1, S2, and S3) tiers. Currently, you can only switch from a lower tier to a higher tier, such as going from Basic to S1. Your region also can't have capacity constraints on the higher tier. For more information, see [Change your pricing tier](search-capacity-planning.md#change-your-pricing-tier).
 
-+ Create a new search service at the new tier.
+If you want to switch to a lower tier or to a different tier than those previously listed, the approach is:
 
-+ Deploy your search content onto the new service. [Follow this checklist](search-howto-move-across-regions.md#prepare-and-move) to make sure you have all of the content.
+1. Create a new search service at the new tier.
 
-+ Delete the old search service once you're sure it's no longer needed.
+1. Deploy your search content onto the new service. [Follow this checklist](search-howto-move-across-regions.md#prepare-and-move) to make sure you have all of the content.
 
-For large indexes that you don't want to rebuild from scratch, consider using one of the backup and restore samples to move them:[backup and restore sample (C#)](https://github.com/Azure-Samples/azure-search-dotnet-utilities/blob/main/index-backup-restore/README.md), [backup and restore sample (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/index-backup-restore/azure-search-backup-and-restore.ipynb), [larget index backup and restore (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/resumable-index-backup-restore/backup-and-restore.ipynb).
+1. Delete the old search service when you're sure it's no longer needed.
+
+For large indexes that you don't want to rebuild from scratch, consider using one of the backup and restore samples to move them:
+
++ [Backup and restore sample (C#)](https://github.com/Azure-Samples/azure-search-dotnet-utilities/blob/main/index-backup-restore/README.md)
++ [Backup and restore sample (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/index-backup-restore/azure-search-backup-and-restore.ipynb)
++ [Largest index backup and restore (Python)](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/utilities/resumable-index-backup-restore/backup-and-restore.ipynb)
 
 ## Next steps
 
-The best way to choose a pricing tier is to start with a least-cost tier, and then allow experience and testing inform your decision to keep the service or create a new one at a higher tier. For next steps, we recommend that you create a search service at a tier that can accommodate the level of testing you propose to do, and then review the following guidance for recommendations on estimating cost and capacity.
+The best way to choose a pricing tier is to start with a least-cost tier, and then allow experience and testing to inform your decision to keep the service or switch to a higher tier.
+
+For next steps, we recommend that you create a search service at a tier that can accommodate the level of testing you propose to do, and then review the following guidance on estimating cost and capacity:
 
 + [Create a search service](search-create-service-portal.md)
 + [Estimate costs](search-sku-manage-costs.md)
-+ [Estimate capacity](search-sku-manage-costs.md)
++ [Estimate capacity](search-capacity-planning.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新Azure AI搜索服务SKU选择文档"
}
```

### Explanation
对 `search-sku-tier.md` 文档进行了小幅更新，主要修改内容包括：

1. **日期更新**：将文档的最后更新日期从 `01/15/2025` 修改为 `03/21/2025`，确保读者获取到最新的信息。

2. **内容精简与清晰化**：简化了关于选择定价层（SKU）的描述，提高了段落的清晰度。例如，将部分信息重新组织，使其更加直观和易懂。
   - 例如，修改了有关定价层决定项的描述，分点列出以增强可读性。

3. **精确术语调整**：对文中多处术语的表述进行调整，如“可用性”更改为“可用性”以保持一致性，并明确了“免费层”的定义。

4. **许可证变更说明**：
   - 更改了服务从基本层到标准层（S1，S2，S3）之间的转换步骤，并强调只能从较低的层级切换到较高的层级，同时更新了相应的链接和说明，以帮助用户更好地理解这一过程。
   
5. **备份和还原样例添加**：增加了备份和恢复的样例链接，为用户提供了迁移和管理大型索引的更详细指导。

这些变更旨在提高文档的准确性、可读性和实用性，帮助用户更好地理解Azure AI Search服务的定价策略和SKU选择的相关细节。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-sku-tier.md)。

## articles/search/search-synapseml-cognitive-services.md{#item-dcc36f}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: Index at scale (Spark)'
+title: 'Tutorial: Index at Scale (Spark)'
 titleSuffix: Azure AI Search
 description: Search big data from Apache Spark that's been transformed by SynapseML. Load invoices into data frames, apply machine learning, and then send output to a generated search index.
 
@@ -10,52 +10,52 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 01/30/2025
+ms.date: 03/28/2025
 ---
 
 # Tutorial: Index large data from Apache Spark using SynapseML and Azure AI Search
 
-In this Azure AI Search tutorial, learn how to index and query large data loaded from a Spark cluster. Set up a Jupyter Notebook that performs the following actions:
+In this Azure AI Search tutorial, you learn how to index and query large data loaded from a Spark cluster. You set up a Jupyter Notebook to:
 
 > [!div class="checklist"]
 > + Load various forms (invoices) into a data frame in an Apache Spark session
-> + Analyze them to determine their features
+> + Analyze the forms to determine their features
 > + Assemble the resulting output into a tabular data structure
 > + Write the output to a search index hosted in Azure AI Search
 > + Explore and query over the content you created
 
-This tutorial takes a dependency on [SynapseML](https://microsoft.github.io/SynapseML/), an open source library that supports massively parallel machine learning over big data. In SynapseML, search indexing and machine learning are exposed through *transformers* that perform specialized tasks. Transformers tap into a wide range of AI capabilities. In this exercise, use the **AzureSearchWriter** APIs for analysis and AI enrichment.
+This tutorial takes a dependency on [SynapseML](https://microsoft.github.io/SynapseML/), an open-source library that supports massively parallel machine learning over big data. In SynapseML, search indexing and machine learning are exposed through *transformers* that perform specialized tasks. Transformers tap into a wide range of AI capabilities. In this exercise, you use the **AzureSearchWriter** APIs for analysis and AI enrichment.
 
 Although Azure AI Search has native [AI enrichment](cognitive-search-concept-intro.md), this tutorial shows you how to access AI capabilities outside of Azure AI Search. By using SynapseML instead of indexers or skills, you're not subject to data limits or other constraints associated with those objects.
 
 > [!TIP]
-> Watch a short video of this demo at [https://www.youtube.com/watch?v=iXnBLwp7f88](https://www.youtube.com/watch?v=iXnBLwp7f88). The video expands on this tutorial with more steps and visuals.
+> Watch a [short video of this demo](https://www.youtube.com/watch?v=iXnBLwp7f88). The video expands on this tutorial with more steps and visuals.
 
 ## Prerequisites
 
 You need the `synapseml` library and several Azure resources. If possible, use the same subscription and region for your Azure resources and put everything into one resource group for simple cleanup later. The following links are for portal installs. The sample data is imported from a public site.
 
 + [SynapseML package](https://microsoft.github.io/SynapseML/docs/Get%20Started/Install%20SynapseML/#python) <sup>1</sup>
-+ [Azure AI Search](search-create-service-portal.md) (any tier), with an **API Kind** of `AIServices` <sup>2</sup> 
-+ [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) (any tier) <sup>3</sup>
-+ [Azure Databricks](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal) (any tier) with Apache Spark 3.3.0 runtime<sup>4</sup>
++ [Azure AI Search](search-create-service-portal.md) (any tier) <sup>2</sup> 
++ [Azure AI services multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) (any tier) with an **API Kind** of `AIServices` <sup>3</sup>
++ [Azure Databricks](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal) (any tier) with Apache Spark 3.3.0 runtime <sup>4</sup>
 
 <sup>1</sup> This link resolves to a tutorial for loading the package.
 
-<sup>2</sup> You can use the free search tier to index the sample data, but [choose a higher tier](search-sku-tier.md) if your data volumes are large. For billable tiers, provide the [search API key](search-security-api-keys.md#find-existing-keys) in the [Set up dependencies](#step-2-set-up-dependencies) step further on.
+<sup>2</sup> You can use the Free tier to index the sample data, but [choose a higher tier](search-sku-tier.md) if your data volumes are large. For billable tiers, provide the [search API key](search-security-api-keys.md#find-existing-keys) in the [Set up dependencies](#set-up-dependencies) step further on.
 
-<sup>3</sup> This tutorial uses Azure AI Document Intelligence and Azure AI Translator. In the instructions that follow, provide a [multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) key and the region. The same key works for both services. **It's important that you use an Azure AI services multi-service account of API kind of `AIServices` for this tutorial**. You can check the API kind in the Azure portal on the Overview section of your Azure AI services multi-service account page. For more information about API kind, see [Attach an Azure AI services multi-service resource in Azure AI Search](cognitive-search-attach-cognitive-services.md).
+<sup>3</sup> This tutorial uses Azure AI Document Intelligence and Azure AI Translator. In the instructions that follow, provide a [multi-service account](/azure/ai-services/multi-service-resource#azure-ai-services-resource-for-azure-ai-search-skills?pivots=azportal) key and the region. The same key works for both services. **For this tutorial, it's important that you use an Azure AI services multi-service account with an API kind of `AIServices`**. You can check the API kind in the Azure portal on the Overview section of your Azure AI services multi-service account page. For more information about API kind, see [Attach an Azure AI services multi-service resource in Azure AI Search](cognitive-search-attach-cognitive-services.md).
 
 <sup>4</sup> In this tutorial, Azure Databricks provides the Spark computing platform. We used the [portal instructions](/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal) to set up the cluster and workspace.
 
 > [!NOTE]
-> All of the above Azure resources support security features in the Microsoft Identity platform. For simplicity, this tutorial assumes key-based authentication, using endpoints and keys copied from the Azure portal pages of each service. If you implement this workflow in a production environment, or share the solution with others, remember to replace hard-coded keys with integrated security or encrypted keys.
+> The preceding Azure resources support security features in the Microsoft Identity platform. For simplicity, this tutorial assumes key-based authentication, using endpoints and keys copied from the Azure portal pages of each service. If you implement this workflow in a production environment or share the solution with others, remember to replace hard-coded keys with integrated security or encrypted keys.
 
-## Step 1: Create a Spark cluster and notebook
+## Create a Spark cluster and notebook
 
-In this section, create a cluster, install the `synapseml` library, and create a notebook to run the code.
+In this section, you create a cluster, install the `synapseml` library, and create a notebook to run the code.
 
-1. In Azure portal, find your Azure Databricks workspace and select **Launch workspace**.
+1. In the Azure portal, find your Azure Databricks workspace and select **Launch workspace**.
 
 1. On the left menu, select **Compute**.
 
@@ -67,7 +67,7 @@ In this section, create a cluster, install the `synapseml` library, and create a
 
    :::image type="content" source="media/search-synapseml-cognitive-services/cluster-green-dot.png" alt-text="Screenshot of a Data Bricks compute page with a green dot by the cluster name.":::
 
-1. Install the `synapseml` library after the cluster is created:
+1. After the cluster is created, install the `synapseml` library:
 
    1. Select **Libraries** from the tabs at the top of the cluster's page.
 
@@ -77,7 +77,7 @@ In this section, create a cluster, install the `synapseml` library, and create a
 
    1. Select **Maven**.
 
-   1. In Coordinates, search for or type `com.microsoft.azure:synapseml_2.12:1.0.9`
+   1. In **Coordinates**, search for `com.microsoft.azure:synapseml_2.12:1.0.9`.
 
    1. Select **Install**.
 
@@ -89,17 +89,17 @@ In this section, create a cluster, install the `synapseml` library, and create a
 
 1. Give the notebook a name, select **Python** as the default language, and select the cluster that has the `synapseml` library.
 
-1. Create seven consecutive cells. You use these to paste in code in the following sections.
+1. Create seven consecutive cells. In the following sections, you paste code in these cells.
 
    :::image type="content" source="media/search-synapseml-cognitive-services/create-seven-cells.png" alt-text="Screenshot of the notebook with placeholder cells." border="true":::
 
-## Step 2: Set up dependencies
+## Set up dependencies
 
-Paste the following code into the first cell of your notebook. 
+Paste the following code into the first cell of your notebook.
 
-Replace the placeholders with endpoints and access keys for each resource. Provide a name for a new search index that's created for you. No other modifications are required, so run the code when you're ready.
+Replace the placeholders with endpoints and access keys for each resource. Provide a name for a new search index to be created for you. No other modifications are required, so run the code when you're ready.
 
-This code imports multiple packages and sets up access to the Azure resources used in this workflow.
+This code imports multiple packages and sets up access to the Azure resources used in this tutorial.
 
 ```python
 import os
@@ -115,11 +115,11 @@ search_key = "placeholder-search-service-admin-api-key"
 search_index = "placeholder-for-new-search-index-name"
 ```
 
-## Step 3: Load data into Spark
+## Load data into Spark
 
 Paste the following code into the second cell. No modifications are required, so run the code when you're ready.
 
-This code loads a few external files from an Azure storage account. The files are various invoices, and they're read into a data frame.
+This code loads a few external files from an Azure storage account. The files are various invoices that are read into a data frame.
 
 ```python
 def blob_to_url(blob):
@@ -141,7 +141,7 @@ df2 = (spark.read.format("binaryFile")
 display(df2)
 ```
 
-## Step 4: Add document intelligence
+## Add document intelligence
 
 Paste the following code into the third cell. No modifications are required, so run the code when you're ready.
 
@@ -163,15 +163,15 @@ analyzed_df = (AnalyzeInvoices()
 display(analyzed_df)
 ```
 
-The output from this step should look similar to the next screenshot. Notice how the forms analysis is packed into a densely structured column, which is difficult to work with. The next transformation resolves this issue by parsing the column into rows and columns.
+The output should look similar to the following screenshot. Notice how the forms analysis is packed into a densely structured column, which is difficult to work with. The next transformation resolves this issue by parsing the column into rows and columns.
 
 :::image type="content" source="media/search-synapseml-cognitive-services/analyze-forms-output.png" alt-text="Screenshot of the AnalyzeInvoices output." border="true":::
 
-## Step 5: Restructure document intelligence output
+## Restructure document intelligence output
 
 Paste the following code into the fourth cell and run it. No modifications are required.
 
-This code loads [FormOntologyLearner](https://mmlspark.blob.core.windows.net/docs/0.10.0/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.FormOntologyTransformer), a transformer that analyzes the output of Document Intelligence transformers and infers a tabular data structure. The output of AnalyzeInvoices is dynamic and varies based on the features detected in your content. Furthermore, the transformer consolidates output into a single column. Because the output is dynamic and consolidated, it's difficult to use in downstream transformations that require more structure.
+This code loads [FormOntologyLearner](https://mmlspark.blob.core.windows.net/docs/0.10.0/pyspark/synapse.ml.cognitive.html#module-synapse.ml.cognitive.FormOntologyTransformer), a transformer that analyzes the output of Document Intelligence transformers and infers a tabular data structure. The output of AnalyzeInvoices is dynamic and varies based on the features detected in your content. Furthermore, the transformer consolidates the output into a single column. Because the output is dynamic and consolidated, it's difficult to use in downstream transformations that require more structure.
 
 FormOntologyLearner extends the utility of the AnalyzeInvoices transformer by looking for patterns that can be used to create a tabular data structure. Organizing the output into multiple columns and rows makes the content consumable in other transformers, like AzureSearchWriter.
 
@@ -193,11 +193,11 @@ Notice how this transformation recasts the nested fields into a table, which ena
 
 :::image type="content" source="media/search-synapseml-cognitive-services/form-ontology-learner-output.png" alt-text="Screenshot of the FormOntologyLearner output." border="true":::
 
-## Step 6: Add translations
+## Add translations
 
 Paste the following code into the fifth cell. No modifications are required, so run the code when you're ready.
 
-This code loads [Translate](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#translator-sample), a transformer that calls the Azure AI Translator service in Azure AI services. The original text, which is in English in the "Description" column, is machine-translated into various languages. All of the output is consolidated into "output.translations" array.
+This code loads [Translate](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#translator-sample), a transformer that calls the Azure AI Translator service in Azure AI services. The original text, which is in English in the "Description" column, is machine-translated into various languages. All of the output is consolidated into the "output.translations" array.
 
 ```python
 from synapse.ml.cognitive import Translate
@@ -223,11 +223,11 @@ display(translated_df)
 > 
 > :::image type="content" source="media/search-synapseml-cognitive-services/translated-strings.png" alt-text="Screenshot of table output, showing the Translations column." border="true":::
 
-## Step 7: Add a search index with AzureSearchWriter
+## Add a search index with AzureSearchWriter
 
-Paste the following code in the sixth cell and then run it. No modifications are required.
+Paste the following code in the sixth cell and run it. No modifications are required.
 
-This code loads [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample). It consumes a tabular dataset and infers a search index schema that defines one field for each column. Because the translations structure is an array, it's articulated in the index as a complex collection with subfields for each language translation. The generated index has a document key and use the default values for fields created using the [Create Index REST API](/rest/api/searchservice/indexes/create).
+This code loads [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample). It consumes a tabular dataset and infers a search index schema that defines one field for each column. Because the translations structure is an array, it's articulated in the index as a complex collection with subfields for each language translation. The generated index has a document key and uses the default values for fields created using the [Create Index REST API](/rest/api/searchservice/indexes/create).
 
 ```python
 from synapse.ml.cognitive import *
@@ -243,21 +243,21 @@ from synapse.ml.cognitive import *
     ))
 ```
 
-You can check the search service pages in Azure portal to explore the index definition created by AzureSearchWriter.
+To explore the index definition created by AzureSearchWriter, check the search service pages in the Azure portal.
 
 > [!NOTE]
-> If you can't use default search index, you can provide an external custom definition in JSON, passing its URI as a string in the "indexJson" property. Generate the default index first so that you know which fields to specify, and then follow with customized properties if you need specific analyzers, for example.
+> If you can't use the default search index, you can provide an external custom definition in JSON, passing its URI as a string in the "indexJson" property. Generate the default index first so that you know which fields to specify, and then follow with customized properties if you need specific analyzers, for example.
 
-## Step 8: Query the index
+## Query the index
 
-Paste the following code into the seventh cell and then run it. No modifications are required, except that you might want to vary the syntax or try more examples to further explore your content:
+Paste the following code into the seventh cell and run it. No modifications are required, except that you might want to vary the syntax or try more examples to further explore your content:
 
 + [Query syntax](query-simple-syntax.md)
 + [Query examples](search-query-simple-examples.md)
 
 There's no transformer or module that issues queries. This cell is a simple call to the [Search Documents REST API](/rest/api/searchservice/documents/search-post). 
 
-This particular example is searching for the word "door" (`"search": "door"`). It also returns a "count" of the number of matching documents, and selects just the contents of the "Description' and "Translations" fields for the results. If you want to see the full list of fields, remove the "select" parameter.
+This particular example is searching for the word "door" (`"search": "door"`). It also returns a "count" of the number of matching documents and selects just the contents of the "Description' and "Translations" fields for the results. If you want to see the full list of fields, remove the "select" parameter.
 
 ```python
 import requests
@@ -278,7 +278,7 @@ You can find and manage resources in the Azure portal, using the **All resources
 
 ## Next steps
 
-In this tutorial, you learned about the [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample) transformer in SynapseML, which is a new way of creating and loading search indexes in Azure AI Search. The transformer takes structured JSON as an input. The FormOntologyLearner can provide the necessary structure for output produced by the Document Intelligence transformers in SynapseML.
+In this tutorial, you learned about the [AzureSearchWriter](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/AI%20Services/Overview/#azure-cognitive-search-sample) transformer in SynapseML, which is a new way of creating and loading search indexes in Azure AI Search. The transformer takes structured JSON as an input. FormOntologyLearner can provide the necessary structure for output produced by the Document Intelligence transformers in SynapseML.
 
 As a next step, review the other SynapseML tutorials that produce transformed content you might want to explore through Azure AI Search:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新SynapseML与Azure AI Search的指南"
}
```

### Explanation
对 `search-synapseml-cognitive-services.md` 文档进行了小幅更新，主要修改内容包括：

1. **标题与日期更新**：
   - 将标题中的 "Index at scale (Spark)" 修改为 "Index at Scale (Spark)"，使其格式一致。
   - 更新文档日期，从 `01/30/2025` 修改为 `03/28/2025`，反映最新修改时间。

2. **语句优化**：对多个段落进行语句结构的优化，提升了文本的流畅性和可读性，例如将“分析它们以确定其特征”精简为“分析表单以确定其特征”。

3. **附加说明**：在关于 SynapseML 的依赖描述中，增加了 "open-source" 一词，强调该库的开放特性。

4. **步骤和代码说明**：
   - 各章节的标题进行了一些小的修改，以提高标题的直观性。例如，将“第1步”改为“创建Spark集群和Notebook” 以简化语句。
   - 对代码单元和说明进行了优化，使其更加清晰易懂，减少冗余并明确每一步的要求，例如在说明中强调无须修改的代码块。

5. **链接与说明**：
   - 许多代码注释和说明文本中的不必要的“动词”被简化，以简化对用户的指导。
   - 保持关键链接的完整性，确保引导用户可以顺利访问。

这些更新的目的是提升文档的可读性和应用效果，使用户在使用 SynapseML 和 Azure AI Search 索引大数据时能获得更好的指导。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsearch-synapseml-cognitive-services.md)。

## articles/search/semantic-how-to-configure.md{#item-7a92a6}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 12/10/2024
+ms.date: 04/04/2025
 ---
 
 # Configure semantic ranker and return captions in search results
@@ -44,6 +44,8 @@ You can specify a semantic configuration on new or existing indexes, using any o
 
 A *semantic configuration* is a section in your index that establishes field inputs for semantic ranking. You can add or update a semantic configuration at any time, no rebuild necessary. If you create multiple configurations, you can specify a default. At query time, specify a semantic configuration on a [query request](semantic-how-to-query-request.md), or leave it blank to use the default.
 
+You can create up to 100 semantic configurations in a single index.
+
 A semantic configuration has a name and the following properties:
 
 | Property | Characteristics |
@@ -158,6 +160,60 @@ SearchIndex searchIndex = new(indexName)
 
 ---
 
+## Opt in for prerelease semantic ranking models
+
+[!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
+
+Starting in [2025-03-01-preview REST APIs](/rest/api/searchservice/operation-groups?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and in Azure SDKs that provide the property, you can optionally configure an index to use prerelease semantic ranking models if one is deployed in your region. There's no mechanism for knowing if a prerelease is available, or if it was used on specific query. For this reason, we recommend that you use this property in test environments, and only if you're interested in trying out the very latest semantic ranking models.
+
+The configuration property is `"flightingOptIn": true`, and it's set in the semantic configuration section of an index. The property is null or false by default. You can set it true on a create or update request at any time, and it affects semantic queries moving forward, assuming the query stipulates a semantic configuration that includes the property.
+
+```rest
+PUT https://myservice.search.windows.net/indexes('hotels')?allowIndexDowntime=False&api-version=2025-03-01-preview
+
+{
+  "name": "hotels",
+  "fields": [ ],
+  "scoringProfiles": [ ],
+  "defaultScoringProfile": "geo",
+  "suggesters": [ ],
+  "analyzers": [ ],
+  "corsOptions": { },
+  "encryptionKey": { },
+  "similarity": { },
+  "semantic": {
+    "configurations": [
+      {
+        "name": "semanticHotels",
+        "prioritizedFields": {
+          "titleField": {
+            "fieldName": "hotelName"
+          },
+        "prioritizedContentFields": [
+            {
+              "fieldName": "description"
+            },
+            {
+              "fieldName": "description_fr"
+            }
+          ],
+        "prioritizedKeywordsFields": [
+            {
+              "fieldName": "tags"
+            },
+            {
+              "fieldName": "category"
+            }
+          ],
+        "flightingOptIn": true
+        }
+      }
+    ]
+  },
+  "vectorSearch": {  }
+}
+```
+
 ## Next steps
 
 Test your semantic configuration by running a semantic query.
````
</details>

### Summary

```json
{
    "modification_type": "new feature",
    "modification_title": "添加对预发布语义排名模型的支持"
}
```

### Explanation
对 `semantic-how-to-configure.md` 文档进行了重要的修改，介绍了新的预发布语义排名模型配置选项。主要的修改包括：

1. **日期更新**：将文档更新日期从 `12/10/2024` 修改为 `04/04/2025`，确保读者获取到最新的信息。

2. **新增语义配置说明**：
   - 文档中明确指出，单个索引现在可以创建最多 100 个语义配置，帮助用户更清楚地理解这一限制。

3. **预发布语义排名模型的配置**：
   - 引入了可选择的预发布语义排名模型的配置选项。用户可以根据需要在 2025-03-01-preview REST APIs 和相关 Azure SDK 中配置这一功能，但建议在测试环境中使用。
   - 具体分析了如何设置这一属性，使用 `"flightingOptIn": true` 选项，以便在索引的语义配置中启用预发布模型的使用。

4. **REST API 示例代码**：
   - 提供了详细的 REST API 示例代码，以帮助用户如何在创建或更新请求中配置索引并将 `"flightingOptIn"` 属性设为 `true`。这个示例代码展示了如何构建请求体来更新一个索引，通过实际的代码片段来指导用户实现这一配置。

这些更改旨在扩展 Azure AI Search 的功能，使用户能够试用最新的语义排名模型，同时确保文档内容清晰且易于理解。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsemantic-how-to-configure.md)。

## articles/search/semantic-how-to-enable-disable.md{#item-71ac1e}

<details>
<summary>Diff</summary>
````diff
@@ -10,12 +10,12 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: how-to
-ms.date: 01/28/2025
+ms.date: 03/31/2025
 ---
 
 # Enable or disable semantic ranker
 
-Semantic ranker is a premium feature billed by usage. By default, semantic ranker is turned on when you create a new billable search service, but anyone with *Contributor* permissions can disable it or change the billing plan. If you don't want anyone to use the feature, you can [disable it service-wide using the management REST API](#disable-semantic-ranker-using-the-rest-api).
+Semantic ranker is a premium feature billed by usage. By default, semantic ranker is enabled on a new billable search service and it's configured for the free plan, but anyone with *Contributor* permissions can disable it or change the billing plan. If you don't want anyone to use the feature, you can [disable it service-wide using the management REST API](#disable-semantic-ranker-using-the-rest-api).
 
 ## Check availability
 
@@ -45,7 +45,7 @@ To enable semantic ranker using the REST API, you can use the [Create or Update
 
 Management REST API calls are authenticated through Microsoft Entra ID. For instructions on how to authenticate, see [Manage your Azure AI Search service with REST APIs](search-manage-rest.md).
 
-* Management REST API version 2023-11-01 provides the configuration property.
+* Management REST API version 2023-11-01 or later provides the configuration property.
 
 * *Owner* or *Contributor* permissions are required to enable or disable features. 
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新语义排名器启用与禁用的说明"
}
```

### Explanation
对 `semantic-how-to-enable-disable.md` 文档进行了小幅更新，调整了部分措辞以确保信息的准确性和清晰度，主要内容如下：

1. **日期更新**：将文档的更新日期从 `01/28/2025` 修改为 `03/31/2025`，确保展示的是最新的文档信息。

2. **启用语义排名器的默认状态说明**：将关于默认状态的描述由“关闭”

更改为“启用”，并指出新创建的可计费搜索服务将默认配置为免费计划。这一调整使得用户会更清晰地了解到语义排名器的默认行为以及其计费方式。

3. **REST API版本描述的调整**：将“Management REST API version 2023-11-01 provides the configuration property”更改为“Management REST API version 2023-11-01 or later provides the configuration property”，强调了支持该配置的REST API版本范围，从而提高了文档的完整性和准确性。

这些修改的目的是提高文档的可读性和准确性，确保用户在启用与禁用语义排名器的过程中能获得可靠的信息。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsemantic-how-to-enable-disable.md)。

## articles/search/semantic-how-to-query-request.md{#item-85530d}

<details>
<summary>Diff</summary>
````diff
@@ -11,7 +11,7 @@ ms.custom:
   - ignite-2023
   - ignite-2024
 ms.topic: how-to
-ms.date: 02/18/2025
+ms.date: 03/31/2025
 ---
 
 # Add semantic ranking to queries in Azure AI Search
@@ -46,7 +46,7 @@ You can use any of the following tools and SDKs to build a query that uses seman
 
 A few query capabilities bypass relevance scoring, which makes them incompatible with semantic ranking. If your query logic includes the following features, you can't semantically rank your results:
 
-+ A query with `search=*` or an empty search string, such as pure filter-only query, won't work because there's nothing to measure semantic relevance against and so the search scores are zero. The query must provide terms or phrases that can be evaluated during processing.
++ A query with `search=*` or an empty search string, such as pure filter-only query, won't work because there's nothing to measure semantic relevance against and so the search scores are zero. The query must provide terms or phrases that can be evaluated during processing, and that produces search documents that are scored for relevance. Scored results are inputs to the semantic ranker.
 
 + Sorting (orderBy clauses) on specific fields overrides search scores and a semantic score. Given that the semantic score is supposed to provide the ranking, adding an orderby clause results in an HTTP 400 error if you apply semantic ranking over ordered results.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新语义排名查询请求的说明"
}
```

### Explanation
对 `semantic-how-to-query-request.md` 文档进行了小幅更新，以增强对语义排名查询请求的说明，主要内容如下：

1. **日期更新**：将文档的更新日期从 `02/18/2025` 修改为 `03/31/2025`，确保文档反映最新的发布信息。

2. **查询功能的详细说明**：
   - 对关于空查询字符串的描述进行了扩展，指出除了原有的理由外，查询必须提供可以在处理时进行评估的术语或短语，并且该查询应产生被评分的搜索文档。这一细化强调了语义排名器需要对查询结果进行评分，而不仅仅是执行匹配。
   
3. **排序（orderBy）条款的说明**： 
   - 新增了关于排序条款的注意事项，强调在应用排序后会覆盖搜索分数和语义分数，如果在有序结果上应用语义排名则会导致 HTTP 400 错误。这一点非常关键，因为它提醒用户在使用语义排名时需要谨慎处理排序条件。

这些修改的目的是改善文档的清晰度和用户理解，确保用户在创建语义排名查询时能够清晰地了解各项功能和限制。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsemantic-how-to-query-request.md)。

## articles/search/semantic-how-to-query-rewrite.md{#item-3e168f}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.custom:
   - ignite-2024
   - references_regions
 ms.topic: how-to
-ms.date: 11/19/2024
+ms.date: 03/31/2025
 ---
 
 # Rewrite queries with semantic ranker in Azure AI Search (Preview)
@@ -21,7 +21,8 @@ Query rewriting is the process of transforming a user's query into a more effect
 
 Query rewriting improves results from [semantic ranking](search-get-started-semantic.md) by correcting typos or spelling errors in user queries, and expanding queries with synonyms.
 
-Search with query rewriting works like this: 
+Search with query rewriting works like this:
+
 - The user query is sent via the `search` property in the request.
 - The search service sends the search query (or a variation of it) to a generative model that generates alternative queries.
 - The search service uses the original query and the rewritten queries to retrieve search results.
@@ -33,31 +34,25 @@ Query rewriting is an optional feature. Without query rewriting, the search serv
 
 ## Prerequisites
 
-+ A search service, Basic tier or higher.
-
-> [!NOTE]
-> Query rewriting is currently available in the North Europe, and Southeast Asia regions.
-
-+ Your search service must have [semantic ranker enabled](semantic-how-to-enable-disable.md). Review [semantic ranking](semantic-search-overview.md) if you need an introduction to the feature. 
+- A search service, Basic tier or higher, in **North Europe** or **Southeast Asia**.
 
-> [!IMPORTANT]
-> Semantic ranker is currently required for query rewriting.
+- [Semantic ranker must be enabled](semantic-how-to-enable-disable.md). It's enabled by default on newer search services. Review [semantic ranking](semantic-search-overview.md) if you need an introduction to the feature. 
 
-+ An existing search index with a [semantic configuration](semantic-how-to-configure.md) and rich text content. The examples in this guide use the [hotels-sample-index](search-get-started-portal.md) sample data to demonstrate query rewriting. You can use your own data and index to test query rewriting.
+- An existing search index with a [semantic configuration](semantic-how-to-configure.md) and rich text content. The examples in this guide use the [hotels-sample-index](search-get-started-portal.md) sample data to demonstrate query rewriting. You can use your own data and index to test query rewriting.
 
-+ You need a web client that supports REST API requests. The examples in this guide were tested with [Visual Studio Code](https://code.visualstudio.com/download) with the [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) extension. 
+- To follow the instructions in this article, you need a web client that supports REST API requests. The examples in this guide were tested with [Visual Studio Code](https://code.visualstudio.com/download) with the [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) extension. 
 
 > [!TIP]
 > Content that includes explanations or definitions work best for semantic ranking. 
 
 ## Make a search request with query rewrites
 
-In this REST API example, we use [Search Documents](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&branch=searchindex202411&preserve-view=true) to formulate the request. For more information about the request and response properties, see the [API reference documentation](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&branch=searchindex202411&preserve-view=true).
+In this REST API example, use [Search Documents (preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2025-03-01-preview&branch=searchindex202503&preserve-view=true) to formulate the request.
 
 1. Paste the following request into a web client as a template. 
 
     ```http
-    POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2024-11-01-preview
+    POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2025-03-01-preview
     {
         "search": "newer hotel near the water with a great restaurant",
         "semanticConfiguration":"en-semantic-config",
@@ -69,24 +64,24 @@ In this REST API example, we use [Search Documents](/rest/api/searchservice/docu
     }
     ```
 
-    - You replace `search-service-name` with your search service name.
-    - You replace `hotels-sample-index` with your index name if it's different. 
-    - We set "search" to a full text search query. The search property is required for query rewriting, unless you specify [vector queries](#vector-queries-with-query-rewrite). If you specify vector queries, then the "search" text must match the `"text"` property of the `"vectorQueries"` object. Your search string can support either the [simple syntax](query-simple-syntax.md) or [full Lucene syntax](query-lucene-syntax.md).
-    - We set "semanticConfiguration" to a [predefined semantic configuration](semantic-how-to-configure.md) embedded in your index.
-    - We set "queryType" to "semantic". We either need to set "queryType" to "semantic" or include a nonempty "semanticQuery" property in the request. [Semantic ranking](semantic-search-overview.md) is required for query rewriting.
-    - We set "queryRewrites" to "generative|count-5" to get up to five query rewrites. You can set the count to any value between 1 and 10. 
-    - Since we requested query rewrites by setting the "queryRewrites" property, we must set "queryLanguage" to the search text language. The Search service uses the same language for the query rewrites. In this example, we use "en-US". The supported locales are: 
+    - Replace `search-service-name` with your search service name.
+    - Replace `hotels-sample-index` with your index name if it's different. 
+    - Set "search" to a full text search query. The search property is required for query rewriting, unless you specify [vector queries](#vector-queries-with-query-rewrite). If you specify vector queries, then the "search" text must match the `"text"` property of the `"vectorQueries"` object. Your search string can support either the [simple syntax](query-simple-syntax.md) or [full Lucene syntax](query-lucene-syntax.md).
+    - Set "semanticConfiguration" to a [predefined semantic configuration](semantic-how-to-configure.md) embedded in your index.
+    - Set "queryType" to "semantic". You either need to set "queryType" to "semantic" or include a nonempty "semanticQuery" property in the request. [Semantic ranking](semantic-search-overview.md) is required for query rewriting.
+    - Set "queryRewrites" to "generative|count-5" to get up to five query rewrites. You can set the count to any value between 1 and 10. 
+    - Since you requested query rewrites by setting the "queryRewrites" property, you must set "queryLanguage" to the search text language. The search service uses the same language for the query rewrites. In this example, you use "en-US". The supported locales are: 
         `en-AU`, `en-CA`, `en-GB`, `en-IN`, `en-US`, `ar-EG`, `ar-JO`, `ar-KW`, `ar-MA`, `ar-SA`, `bg-BG`, `bn-IN`, `ca-ES`, `cs-CZ`, `da-DK`, `de-DE`, `el-GR`, `es-ES`, `es-MX`, `et-EE`, `eu-ES`, `fa-AE`, `fi-FI`, `fr-CA`, `fr-FR`, `ga-IE`, `gl-ES`, `gu-IN`, `he-IL`, `hi-IN`, `hr-BA`, `hr-HR`, `hu-HU`, `hy-AM`, `id-ID`, `is-IS`, `it-IT`, `ja-JP`, `kn-IN`, `ko-KR`, `lt-LT`, `lv-LV`, `ml-IN`, `mr-IN`, `ms-BN`, `ms-MY`, `nb-NO`, `nl-BE`, `nl-NL`, `no-NO`, `pa-IN`, `pl-PL`, `pt-BR`, `pt-PT`, `ro-RO`, `ru-RU`, `sk-SK`, `sl-SL`, `sr-BA`, `sr-ME`, `sr-RS`, `sv-SE`, `ta-IN`, `te-IN`, `th-TH`, `tr-TR`, `uk-UA`, `ur-PK`, `vi-VN`, `zh-CN`, `zh-TW`.
-    - We set "debug" to "queryRewrites" to get the query rewrites in the response. 
+    - Set "debug" to "queryRewrites" to get the query rewrites in the response. 
   
       > [!TIP]
       > Only set `"debug": "queryRewrites"` for testing purposes. For better performance, don't use debug in production.
 
-    - We set "top" to 1 to return only the top search result. 
-    
+    - Set "top" to 1 to return only the top search result. 
+  
 1. Send the request to execute the query and return results.
 
-Next, we evaluate the search results with the query rewrites.
+Next, you evaluate the search results with the query rewrites.
 
 ## Evaluate the response
 
@@ -145,8 +140,9 @@ Here's an example of a response that includes query rewrites:
 ```
 
 Here are some key points to note:
-- Because we set the "debug" property to "queryRewrites" for testing, the response includes a `@search.debug` object with the text input query and query rewrites. 
-- Because we set the "queryRewrites" property to "generative|count-5", the response includes up to five query rewrites.
+
+- Because you set the "debug" property to "queryRewrites" for testing, the response includes a `@search.debug` object with the text input query and query rewrites. 
+- Because you set the "queryRewrites" property to "generative|count-5", the response includes up to five query rewrites.
 - The `"inputQuery"` value is the query sent to the generative model for query rewriting. The input query isn't always the same as the user's `"search"` query.
 
 Here's an example of a response without query rewrites. 
@@ -201,13 +197,13 @@ Here's an example of a response without query rewrites.
 
 You can include vector queries in your search request to combine keyword search and vector search into a single request and a unified response.
 
-Here's an example of a query that includes a vector query with query rewrites. We modify a [previous example](#make-a-search-request-with-query-rewrites) to include a vector query.
+Here's an example of a query that includes a vector query with query rewrites. Modify a [previous example](#make-a-search-request-with-query-rewrites) to include a vector query.
 
-- We add a "vectorQueries" object to the request. This object includes a vector query with the "kind" set to "text". 
+- Add a "vectorQueries" object to the request. This object includes a vector query with the "kind" set to "text". 
 - The "text" value is the same as the "search" value. These values must be identical for query rewriting to work.
 
 ```http
-POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2024-11-01-preview
+POST https://[search-service-name].search.windows.net/indexes/hotels-sample-index/docs/search?api-version=2025-03-01-preview
 {
     "search": "newer hotel near the water with a great restaurant",
     "vectorQueries": [
@@ -256,6 +252,7 @@ You might observe that the debug (test) response includes an empty array for the
 ```
 
 In the preceding example:
+
 - The response includes a `@search.semanticPartialResponseReason` property with a value of "Transient". This message means that at least one of the queries failed to complete. 
 - The response also includes a `@search.semanticQueryRewriteResultType` property with a value of "OriginalQueryOnly". This message means that the query rewrites are unavailable. Only the original query is used to retrieve search results.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新语义查询重写的说明"
}
```

### Explanation
对 `semantic-how-to-query-rewrite.md` 文档进行了小幅修改，以提升对语义查询重写功能的准确性和清晰度，主要内容如下：

1. **日期更新**：将文档的更新日期从 `11/19/2024` 修改为 `03/31/2025`，确保文档反映最新的时间信息。

2. **查询重写说明的增强**：
   - 对查询重写的基本概念进行了细化，增加了对如何通过用户输入的查询生成替代查询的描述。
   - 修改了有关查询重写所需的搜索服务的描述，使其更清晰，强调该功能需要基础层或更高级别的搜索服务，并且仅在**北欧**或**东南亚**地区提供。

3. **预备条件的简单化**：
   - 将有关语义排名器的要求进一步简化，指出其必须启用，并强调它在新创建的搜索服务中默认启用。
   - 预备条件中的其他内容也简化为更流畅的语言，使得说明更易读。

4. **API示例的更新**：
   - 针对 REST API 示例中的 URL 进行了更新，以反映最新的 API 版本。
   - 对示例请求的详细说明进行了简化，清楚地指出每个参数的设置和使用。

5. **响应示例的调整**：
   - 在响应示例的说明中，适当调整了措辞，确保用户理解测试响应和调试设置的影响。

这些修改旨在提升文档的可读性和用户理解，确保用户能够有效利用语义查询重写功能。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsemantic-how-to-query-rewrite.md)。

## articles/search/semantic-search-overview.md{#item-b7497b}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: concept-article
-ms.date: 02/18/2025
+ms.date: 03/31/2025
 ---
 
 # Semantic ranking in Azure AI Search
@@ -24,7 +24,7 @@ Semantic ranker is a premium feature, billed by usage. We recommend this article
 
 ## What is semantic ranking?
 
-Semantic ranker calls LLMs at query time. LLms are used to improve the quality of an initial [BM25-ranked](index-similarity-and-scoring.md) or [RRF-ranked](hybrid-search-ranking.md) search result for text-based queries, the text portion of vector queries, and hybrid queries. When you enable it on your search service, semantic ranking extends the query execution pipeline in three ways:
+Semantic ranker calls LLMs at query time. LLms are used to improve the quality of an initial [BM25-ranked](index-similarity-and-scoring.md) or [RRF-ranked](hybrid-search-ranking.md) search result for text-based queries, the text portion of vector queries, and hybrid queries. Semantic ranking extends the query execution pipeline in three ways:
 
 * First, it always adds secondary ranking over an initial result set that was scored using BM25 or Reciprocal Rank Fusion (RRF). This secondary ranking uses multi-lingual, deep learning models adapted from Microsoft Bing to promote the most semantically relevant results.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新语义排名概述的细节"
}
```

### Explanation
对 `semantic-search-overview.md` 文档进行了小幅修改，以提高对语义排名功能的说明质量，主要修改内容如下：

1. **日期更新**：将文档的更新日期从 `02/18/2025` 修改为 `03/31/2025`，确保文档准确反映最新的时间信息。

2. **语义排名功能描述的调整**：
   - 对描述语义排名器调用大型语言模型（LLMs）的方式进行了微调，以提升语句的流畅性和易读性。这一部分强调了 LLMs 在查询执行过程中的角色，以及它们如何改善基于文本的查询和混合查询的初步搜索结果。

3. **查询执行管道的说明**：
   - 移除了额外的句子结构，简化了对语义排名扩展查询执行管道方式的描述，使用户能够更清晰地理解三个扩展方式的作用。

这些修改主要目的是提升文档的可读性和用户理解，使得用户对于 Azure AI Search 的语义排名功能有更精准的掌握。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fsemantic-search-overview.md)。

## articles/search/toc.yml{#item-c4768f}

<details>
<summary>Diff</summary>
````diff
@@ -201,6 +201,8 @@ items:
       href: search-region-support.md
     - name: Choose a tier
       href: search-sku-tier.md
+    - name: Upgrade a service
+      href: search-how-to-upgrade.md
     - name: Service limits
       href: search-limits-quotas-capacity.md
     - name: Plan and manage capacity
@@ -215,11 +217,11 @@ items:
       href: search-modeling-multitenant-saas-applications.md
     - name: Manage
       items:
-      - name: Manage with PowerShell
+      - name: Manage using PowerShell
         href: search-manage-powershell.md
-      - name: Manage with Azure CLI
+      - name: Manage using Azure CLI
         href: search-manage-azure-cli.md
-      - name: Manage with REST
+      - name: Manage using REST
         href: search-manage-rest.md
       - name: Move service across regions
         href: search-howto-move-across-regions.md
@@ -399,19 +401,21 @@ items:
         href: index-add-language-analyzers.md
       - name: Add a custom analyzer
         href: index-add-custom-analyzers.md
-    - name: Filters
+    - name: Filters and facets
       items:
       - name: Filters in text queries
         displayName: query
         href: search-filters.md
-      - name: Add faceted navigation
-        href: search-faceted-navigation.md
       - name: Understand collection filters
         href: search-query-understand-collection-filters.md
       - name: Troubleshoot collection filters
         href: search-query-troubleshoot-collection-filters.md
       - name: Normalize text for filters
         href: search-normalizers.md
+      - name: Add faceted navigation
+        href: search-faceted-navigation.md
+      - name: Faceted navigation examples
+        href: search-faceted-navigation-examples.md
     - name: Search results
       items:
       - name: Page, sort, and shape results
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新搜索文档目录"
}
```

### Explanation
对 `toc.yml` 文件进行了小幅修改，以增强搜索文档的目录结构和内容，具体内容如下：

1. **新条目添加**：在文档中新增了“升级服务”的条目，提供了对该主题的链接，方便用户访问有关如何升级搜索服务的指南。

2. **条目名称修改**：将多个条目中的“管理”相关的条目名称进行了优化，从“Manage with PowerShell”、“Manage with Azure CLI”和“Manage with REST”更改为“Manage using PowerShell”、“Manage using Azure CLI”和“Manage using REST”，使其语句更流畅。

3. **条目扩展**：
   - 将“Filters”的标题扩展为“Filters and facets”，并新增了“Faceted navigation examples”的子条目，为用户提供更多有关领域的学习资源。
   - 在“Filters and facets”部分，重新排列了一些条目的位置，以增强逻辑性和可导航性。

这些修改旨在提升用户对搜索文档的访问体验，使其能够更方便地找到与服务管理和导航相关的指南。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Ftoc.yml)。

## articles/search/tutorial-create-custom-analyzer.md{#item-ad5520}

<details>
<summary>Diff</summary>
````diff
@@ -1,5 +1,5 @@
 ---
-title: 'Tutorial: create a custom analyzer'
+title: 'Tutorial: Create a Custom Analyzer'
 titleSuffix: Azure AI Search
 description: Learn how to build a custom analyzer to improve the quality of search results in Azure AI Search.
 author: gmndrg
@@ -8,14 +8,14 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: tutorial
-ms.date: 01/17/2025
+ms.date: 03/28/2025
 ---
 
 # Tutorial: Create a custom analyzer for phone numbers
 
-In search solutions, strings that have complex patterns or special characters can be a challenge to work with because the [default analyzer](search-analyzers.md) strips out or misinterprets meaningful parts of a pattern, resulting in a poor search experience when users can't find the information they expected. Phone numbers are a classic example of strings that are hard to analyze. They come in various formats, and they include special characters that the default analyzer ignores. 
+In search solutions, strings that have complex patterns or special characters can be challenging to work with because the [default analyzer](search-analyzers.md) strips out or misinterprets meaningful parts of a pattern. This results in a poor search experience where users can't find the information they expect. Phone numbers are a classic example of strings that are difficult to analyze. They come in various formats and include special characters that the default analyzer ignores.
 
-With phone numbers as its subject, this tutorial takes a close look at the problems of patterned data, and shows you to solve that problem using a [custom analyzer](index-add-custom-analyzers.md). The approach outlined here can be used as-is for phone numbers, or adapted for fields having the same characteristics (patterned, with special characters), such as URLs, emails, postal codes, and dates.
+With phone numbers as its subject, this tutorial shows you how to solve patterned data problems using a [custom analyzer](index-add-custom-analyzers.md). This approach can be used as is for phone numbers or adapted for fields with the same characteristics (patterned with special characters), such as URLs, emails, postal codes, and dates.
 
 In this tutorial, you use a REST client and the [Azure AI Search REST APIs](/rest/api/searchservice/) to:
 
@@ -27,33 +27,33 @@ In this tutorial, you use a REST client and the [Azure AI Search REST APIs](/res
 
 ## Prerequisites
 
-The following services and tools are required for this tutorial.
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
 
-+ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
++ [Azure AI Search](search-what-is-azure-search.md). [Create a service](search-create-service-portal.md) or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. For this tutorial, you can use a free service.
 
-+ [Azure AI Search](search-what-is-azure-search.md). [Create](search-create-service-portal.md) or [find an existing Azure AI Search resource](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription. You can use a free service for this quickstart. 
++ [Visual Studio Code](https://code.visualstudio.com/download) with a [REST client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client).
 
 ### Download files
 
-Source code for this tutorial is the [custom-analyzer.rest](https://github.com/Azure-Samples/azure-search-rest-samples/tree/main/custom-analyzers/custom-analyzer.rest) file in the [Azure-Samples/azure-search-rest-samples](https://github.com/Azure-Samples/azure-search-rest-samples) GitHub repository.
+Source code for this tutorial is in the [custom-analyzer.rest](https://github.com/Azure-Samples/azure-search-rest-samples/tree/main/custom-analyzers/custom-analyzer.rest) file in the [Azure-Samples/azure-search-rest-samples](https://github.com/Azure-Samples/azure-search-rest-samples) GitHub repository.
 
-### Copy a key and URL
+### Copy an admin key and URL
 
 The REST calls in this tutorial require a search service endpoint and an admin API key. You can get these values from the Azure portal.
 
-1. Sign in to the [Azure portal](https://portal.azure.com), navigate to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com), go to the **Overview** page, and copy the URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
 1. Under **Settings** > **Keys**, copy an admin key. Admin keys are used to add, modify, and delete objects. There are two interchangeable admin keys. Copy either one.
 
    :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of the URL and API keys in the Azure portal.":::
 
-A valid API key establishes trust, on a per request basis, between the application sending the request and the search service handling it.
+A valid API key establishes trust, on a per-request basis, between the application sending the request and the search service handling it.
 
 ## Create an initial index
 
 1. Open a new text file in Visual Studio Code.
 
-1. Set variables to the search endpoint and the API key you collected in the previous step.
+1. Set variables to the search endpoint and the API key you collected in the previous section.
 
    ```http
    @baseUrl = PUT-YOUR-SEARCH-SERVICE-URL-HERE
@@ -62,7 +62,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
 
 1. Save the file with a `.rest` file extension.
 
-1. Paste in the following example to create a small index called `phone-numbers-index` with two fields: `id` and `phone_number`. We haven't defined an analyzer yet, so the `standard.lucene` analyzer is used by default.
+1. Paste the following example to create a small index called `phone-numbers-index` with two fields: `id` and `phone_number`. You haven't defined an analyzer yet, so the `standard.lucene` analyzer is used by default.
 
     ```http
     ### Create a new index
@@ -94,7 +94,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       }
     ```
 
-1. Select **Send request**. You should have an `HTTP/1.1 201 Created` response and the response body should include the JSON representation of the index schema.
+1. Select **Send request**. You should have an `HTTP/1.1 201 Created` response, and the response body should include the JSON representation of the index schema.
 
 1. Load data into the index, using documents that contain various phone number formats. This is your test data.
 
@@ -150,7 +150,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       }
     ```
 
-1. Let's try a few queries similar to what a user might type. A user could search for `(425) 555-0100` in any number of formats and still expect results to be returned. Start by searching `(425) 555-0100`:
+1. Try queries similar to what a user might type. For example, a user might search for `(425) 555-0100` in any number of formats and still expect results to be returned. Start by searching `(425) 555-0100`.
 
     ```http  
     ### Search for a phone number
@@ -159,7 +159,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       api-key: {{apiKey}}
     ```
 
-    The query returns **three out of four expected results**, but also returns **two unexpected results**:
+    The query returns three out of four expected results but also returns two unexpected results.
 
     ```json
     {
@@ -188,7 +188,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
     }
     ```
 
-1. Let's try again without any formatting: `4255550100`.
+1. Try again without any formatting: `4255550100`.
 
    ```http  
     ### Search for a phone number
@@ -197,7 +197,7 @@ A valid API key establishes trust, on a per request basis, between the applicati
       api-key: {{apiKey}}
     ```
 
-   This query does even worse, returning only **one of four correct matches**.
+   This query does even worse, returning only one of four correct matches.
 
     ```json
     {
@@ -210,48 +210,48 @@ A valid API key establishes trust, on a per request basis, between the applicati
     }
     ```
 
-If you find these results confusing, you're not alone. In the next section, let's dig into why we're getting these results.
+If you find these results confusing, you're not alone. The next section explains why you're getting these results.
 
 <a name="how-analyzers-work"></a>
 
 ## Review how analyzers work
 
-To understand these search results, we need to understand what the analyzer is doing. From there, we can test the default analyzer using the [Analyze API](/rest/api/searchservice/indexes/analyze), providing a foundation for designing an analyzer that better meets our needs.
+To understand these search results, you must understand what the analyzer is doing. From there, you can test the default analyzer using the [Analyze API](/rest/api/searchservice/indexes/analyze), providing a foundation for designing an analyzer that better meets your needs.
 
-An [analyzer](search-analyzers.md) is a component of the [full text search engine](search-lucene-query-architecture.md) responsible for processing text in query strings and indexed documents. Different analyzers manipulate text in different ways depending on the scenario. For this scenario, we need to build an analyzer tailored to phone numbers.
+An [analyzer](search-analyzers.md) is a component of the [full-text search engine](search-lucene-query-architecture.md) responsible for processing text in query strings and indexed documents. Different analyzers manipulate text in different ways depending on the scenario. For this scenario, we need to build an analyzer tailored to phone numbers.
 
 Analyzers consist of three components:
 
 + [**Character filters**](#CharFilters) that remove or replace individual characters from the input text.
-+ A [**Tokenizer**](#Tokenizers) that breaks the input text into tokens, which become keys in the search index.
++ A [**tokenizer**](#Tokenizers) that breaks the input text into tokens, which become keys in the search index.
 + [**Token filters**](#TokenFilters) that manipulate the tokens produced by the tokenizer.
 
-In the following diagram, you can see how these three components work together to tokenize a sentence:
+The following diagram shows how these three components work together to tokenize a sentence.
 
   :::image type="content" source="media/tutorial-create-custom-analyzer/analyzers-explained.png" alt-text="Diagram of Analyzer process to tokenize a sentence":::
 
-These tokens are then stored in an inverted index, which allows for fast, full-text searches.  An inverted index enables full-text search by mapping all unique terms extracted during lexical analysis to the documents in which they occur. You can see an example in the next diagram:
+These tokens are then stored in an inverted index, which allows for fast, full-text searches. An inverted index enables full-text search by mapping all unique terms extracted during lexical analysis to the documents in which they occur. You can see an example in the following diagram:
 
   :::image type="content" source="media/tutorial-create-custom-analyzer/inverted-index-explained.png" alt-text="Example inverted index":::
 
 All of search comes down to searching for the terms stored in the inverted index. When a user issues a query:
 
 1. The query is parsed and the query terms are analyzed.
-1. The inverted index is then scanned for documents with matching terms.
-1. Finally, the retrieved documents are ranked by the [scoring algorithm](index-ranking-similarity.md).
+1. The inverted index is scanned for documents with matching terms.
+1. The [scoring algorithm](index-ranking-similarity.md) ranks the retrieved documents.
 
   :::image type="content" source="media/tutorial-create-custom-analyzer/query-architecture-explained.png" alt-text="Diagram of Analyzer process ranking similarity":::
 
-If the query terms don't match the terms in your inverted index, results aren't returned. To learn more about how queries work, see this article on [full text search](search-lucene-query-architecture.md).
+If the query terms don't match the terms in your inverted index, results aren't returned. To learn more about how queries work, see [Full-text search in Azure AI Search](search-lucene-query-architecture.md).
 
 > [!Note]
-> [Partial term queries](search-query-partial-matching.md) are an important exception to this rule. These queries (prefix query, wildcard query, regex query) bypass the lexical analysis process unlike regular term queries. Partial terms are only lowercased before being matched against terms in the index. If an analyzer isn't configured to support these types of queries, you'll often receive unexpected results because matching terms don't exist in the index.
+> [Partial term queries](search-query-partial-matching.md) are an important exception to this rule. Unlike regular term queries, these queries (prefix query, wildcard query, and regex query) bypass the lexical analysis process. Partial terms are only lowercased before being matched against terms in the index. If an analyzer isn't configured to support these types of queries, you often receive unexpected results because matching terms don't exist in the index.
 
 ## Test analyzers using the Analyze API
 
 Azure AI Search provides an [Analyze API](/rest/api/searchservice/indexes/analyze) that allows you to test analyzers to understand how they process text.
 
-The Analyze API is called using the following request:
+Call the Analyze API using the following request:
 
 ```http
 POST {{baseUrl}}/indexes/phone-numbers-index/analyze?api-version=2024-07-01  HTTP/1.1
@@ -264,7 +264,7 @@ POST {{baseUrl}}/indexes/phone-numbers-index/analyze?api-version=2024-07-01  HTT
   }
 ```
 
-The API returns the tokens extracted from the text, using the analyzer you specified. The standard Lucene analyzer splits the phone number into three separate tokens:
+The API returns the tokens extracted from the text, using the analyzer you specified. The standard Lucene analyzer splits the phone number into three separate tokens.
 
 ```json
 {
@@ -315,23 +315,23 @@ Response:
 }
 ```
 
-Keep in mind that both query terms and the indexed documents undergo analysis. Thinking back to the search results from the previous step, we can start to see why those results were returned.
+Keep in mind that both query terms and the indexed documents undergo analysis. Thinking back to the search results from the previous step, you can start to see why those results are returned.
 
-In the first query, unexpected phone numbers were returned because one of their tokens, `555`, matched one of the terms we searched. In the second query, only the one number was returned because it was the only record that had a token matching `4255550100`.
+In the first query, unexpected phone numbers are returned because one of their tokens, `555`, matched one of the terms you searched. In the second query, only the one number is returned because it's the only record that has a token matching `4255550100`.
 
 ## Build a custom analyzer
 
-Now that we understand the results we're seeing, let's build a custom analyzer to improve the tokenization logic.
+Now that you understand the results you're seeing, build a custom analyzer to improve the tokenization logic.
 
-The goal is to provide intuitive search against phone numbers no matter what format the query or indexed string is in. To achieve this outcome, we'll specify a [character filter](#CharFilters), a [tokenizer](#Tokenizers), and a [token filter](#TokenFilters).
+The goal is to provide intuitive search against phone numbers no matter what format the query or indexed string is in. To achieve this outcome, specify a [character filter](#CharFilters), a [tokenizer](#Tokenizers), and a [token filter](#TokenFilters).
 
 <a name="CharFilters"></a>
 
 ### Character filters
 
-Character filters are used to process text before it's fed into the tokenizer. Common uses of character filters include filtering out HTML elements or replacing special characters.
+Character filters process text before it's fed into the tokenizer. Common uses of character filters are filtering out HTML elements and replacing special characters.
 
-For phone numbers, we want to remove whitespace and special characters because not all phone number formats contain the same special characters and spaces.
+For phone numbers, you want to remove whitespace and special characters because not all phone number formats contain the same special characters and spaces.
 
 ```json
 "charFilters": [
@@ -363,9 +363,9 @@ The filter removes `-` `(` `)` `+` `.` and spaces from the input.
 
 Tokenizers split text into tokens and discard some characters, such as punctuation, along the way. In many cases, the goal of tokenization is to split a sentence into individual words.
 
-For this scenario, we'll use a keyword tokenizer, `keyword_v2`, because we want to capture the phone number as a single term. Note that this isn't the only way to solve this problem. See the [Alternate approaches](#Alternate) section below.
+For this scenario, use a keyword tokenizer, `keyword_v2`, to capture the phone number as a single term. This isn't the only way to solve this problem, as explained in the [Alternate approaches](#Alternate) section.
 
-Keyword tokenizers always output the same text it was given as a single term.
+Keyword tokenizers always output the same text they're given as a single term.
 
 |Input|Output|  
 |-|-|  
@@ -376,9 +376,9 @@ Keyword tokenizers always output the same text it was given as a single term.
 
 ### Token filters
 
-Token filters will filter out or modify the tokens generated by the tokenizer. One common use of a token filter is to lowercase all characters using a lowercase token filter. Another common use is filtering out [stopwords](reference-stopwords.md) such as `the`, `and`, or `is`.
+Token filters modify or filter out the tokens generated by the tokenizer. One common use of a token filter is to lowercase all characters using a lowercase token filter. Another common use is filtering out [stopwords](reference-stopwords.md), such as `the`, `and`, or `is`.
 
-While we don't need to use either of those filters for this scenario, we'll use an nGram token filter to allow for partial searches of phone numbers.
+While you don't need to use either of those filters for this scenario, use an nGram token filter to allow for partial searches of phone numbers.
 
 ```json
 "tokenFilters": [
@@ -395,9 +395,9 @@ While we don't need to use either of those filters for this scenario, we'll use
 
 The [nGram_v2 token filter](https://lucene.apache.org/core/6_6_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html) splits tokens into n-grams of a given size based on the `minGram` and `maxGram` parameters.
 
-For the phone analyzer, we set `minGram` to `3` because that is the shortest substring we expect users to search. `maxGram` is set to `20` to ensure that all phone numbers, even with extensions, will fit into a single n-gram.
+For the phone analyzer, `minGram` is set to `3` because that's the shortest substring users are expected to search. `maxGram` is set to `20` to ensure that all phone numbers, even with extensions, fit into a single n-gram.
 
- The unfortunate side effect of n-grams is that some false positives will be returned. We'll fix this in a later step by building out a separate analyzer for searches that doesn't include the n-gram token filter.
+The unfortunate side effect of n-grams is that some false positives are returned. You fix this in a later step by building out a separate analyzer for searches that doesn't include the n-gram token filter.
 
 |Input|Output|  
 |-|-|  
@@ -406,7 +406,7 @@ For the phone analyzer, we set `minGram` to `3` because that is the shortest sub
 
 ### Analyzer
 
-With our character filters, tokenizer, and token filters in place, we're ready to define our analyzer.
+With the character filters, tokenizer, and token filters in place, you're ready to define the analyzer.
 
 ```json
 "analyzers": [
@@ -424,26 +424,26 @@ With our character filters, tokenizer, and token filters in place, we're ready t
 ]
 ```
 
-From the Analyze API, given the following inputs, outputs from the custom analyzer are shown in the following table.
+From the Analyze API, given the following inputs, outputs from the custom analyzer are as follows:
 
 |Input|Output|  
 |-|-|  
 |`12345`|`[123, 1234, 12345, 234, 2345, 345]`|  
 |`(321) 555-0199`|`[321, 3215, 32155, 321555, 3215550, 32155501, 321555019, 3215550199, 215, 2155, 21555, 215550, ... ]`|
 
-All of the tokens in the output column exist in the index. If our query includes any of those terms, the phone number is returned.
+All of the tokens in the output column exist in the index. If your query includes any of those terms, the phone number is returned.
 
 ## Rebuild using the new analyzer
 
-1. Delete the current index:
+1. Delete the current index.
 
    ```http
     ### Delete the index
     DELETE {{baseUrl}}/indexes/phone-numbers-index?api-version=2024-07-01 HTTP/1.1
         api-key: {{apiKey}}
     ```
 
-1. Recreate the index using the new analyzer. This index schema adds a custom analyzer definition, and a custom analyzer assignment on the phone number field.
+1. Recreate the index using the new analyzer. This index schema adds a custom analyzer definition and a custom analyzer assignment on the phone number field.
 
     ```http
     ### Create a new index
@@ -513,7 +513,7 @@ All of the tokens in the output column exist in the index. If our query includes
 
 ### Test the custom analyzer
 
-After recreating the index, you can now test out the analyzer using the following request:
+After you recreate the index, test the analyzer using the following request:
 
 ```http
 POST {{baseUrl}}/indexes/tutorial-first-analyzer/analyze?api-version=2024-07-01  HTTP/1.1
@@ -526,7 +526,7 @@ POST {{baseUrl}}/indexes/tutorial-first-analyzer/analyze?api-version=2024-07-01
   }
 ```
 
-You should now see the collection of tokens resulting from the phone number:
+You should now see the collection of tokens resulting from the phone number.
 
 ```json
 {
@@ -558,9 +558,9 @@ You should now see the collection of tokens resulting from the phone number:
 
 ## Revise the custom analyzer to handle false positives
 
-After making some sample queries against the index with the custom analyzer, you'll find that recall has improved and all matching phone numbers are now returned. However, the n-gram token filter causes some false positives to be returned as well. This is a common side effect of an n-gram token filter.
+After using the custom analyzer to make sample queries against the index, you should see that recall has improved and all matching phone numbers are now returned. However, the n-gram token filter also causes some false positives to be returned. This is a common side effect of an n-gram token filter.
 
-To prevent false positives, we'll create a separate analyzer for querying. This analyzer is identical to the previous one, except that it **omits** the `custom_ngram_filter`.
+To prevent false positives, create a separate analyzer for querying. This analyzer is identical to the previous one, except that it omits the `custom_ngram_filter`.
 
 ```json
     {
@@ -574,7 +574,7 @@ To prevent false positives, we'll create a separate analyzer for querying. This
     }
 ```
 
-In the index definition, we then specify both an `indexAnalyzer` and a `searchAnalyzer`.
+In the index definition, specify both an `indexAnalyzer` and a `searchAnalyzer`.
 
 ```json
     {
@@ -589,25 +589,25 @@ In the index definition, we then specify both an `indexAnalyzer` and a `searchAn
     }
 ```
 
-With this change, you're all set. Here are your next steps: 
+With this change, you're all set. Here are your next steps:
 
-1. Delete the index. 
+1. Delete the index.
 
-1. Recreate the index after adding the new custom analyzer (`phone_analyzer-search`) and assigning that analyzer to the `phone-number` field's `searchAnalyzer` property.
+1. Recreate the index after you add the new custom analyzer (`phone_analyzer-search`) and assign that analyzer to the `phone-number` field's `searchAnalyzer` property.
 
 1. Reload the data.
 
-1. Retest the queries to verify the search works as expected. If you're using the sample file, this step creates the third index named `phone-number-index-3`.
+1. Retest the queries to verify that the search works as expected. If you're using the sample file, this step creates the third index named `phone-number-index-3`.
 
 <a name="Alternate"></a>
 
 ## Alternate approaches
 
 The analyzer described in the previous section is designed to maximize the flexibility for search. However, it does so at the cost of storing many potentially unimportant terms in the index.
 
-The following example shows an alternative analyzer that's more efficient in tokenization, but has drawbacks. 
+The following example shows an alternative analyzer that's more efficient in tokenization, but it has drawbacks.
 
-Given an input of `14255550100`, the analyzer can't logically chunk the phone number. For example, it can't separate the country code, `1`, from the area code, `425`. This discrepancy would lead to the phone number not being returned if a user didn't include a country code in their search.
+Given an input of `14255550100`, the analyzer can't logically chunk the phone number. For example, it can't separate the country code, `1`, from the area code, `425`. This discrepancy leads to the phone number not being returned if a user doesn't include a country code in their search.
 
 ```json
 "analyzers": [
@@ -638,7 +638,7 @@ Given an input of `14255550100`, the analyzer can't logically chunk the phone nu
 ]
 ```
 
-You can see in the following example that the phone number is split into the chunks you would normally expect a user to be searching for.
+In the following example, the phone number is split into the chunks you normally expect a user to be search for.
 
 |Input|Output|  
 |-|-|  
@@ -648,7 +648,7 @@ Depending on your requirements, this might be a more efficient approach to the p
 
 ## Takeaways
 
-This tutorial demonstrated the process for building and testing a custom analyzer. You created an index, indexed data, and then queried against the index to see what search results were returned. From there, you used the Analyze API to see the lexical analysis process in action.
+This tutorial demonstrated the process of building and testing a custom analyzer. You created an index, indexed data, and then queried against the index to see what search results were returned. From there, you used the Analyze API to see the lexical analysis process in action.
 
 While the analyzer defined in this tutorial offers an easy solution for searching against phone numbers, this same process can be used to build a custom analyzer for any scenario that shares similar characteristics.
 
@@ -660,7 +660,7 @@ You can find and manage resources in the Azure portal, using the All resources o
 
 ## Next steps
 
-Now that you're familiar with how to create a custom analyzer, let's take a look at all of the different filters, tokenizers, and analyzers available to you to build a rich search experience.
+Now that you know how to create a custom analyzer, take a look at all of the different filters, tokenizers, and analyzers available for building a rich search experience:
 
 > [!div class="nextstepaction"]
-> [Custom Analyzers in Azure AI Search](index-add-custom-analyzers.md)
+> [Custom analyzers in Azure AI Search](index-add-custom-analyzers.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新自定义分析器创建教程"
}
```

### Explanation
对 `tutorial-create-custom-analyzer.md` 文档进行了全面的修改，以增强内容的清晰度和准确性。主要修改点包括：

1. **标题格式调整**：将标题从小写变更为大写，增强了标题的专业性。

2. **日期更新**：将文档更新日期从 `01/17/2025` 修改为 `03/28/2025`，确保信息时效性。

3. **语言优化**：优化了多处语句，使得文档的可读性和表达更加流畅。例如，简化了复杂句子，增加了连贯性。

4. **内容重组**：
   - 增加了有关自定义分析器在电话号处理中的具体应用细节，提高了用户理解复杂数据格式处理的能力。
   - 在“前提条件”部分重新排列了工具和服务的介绍顺序，更符合用户需求。

5. **示例代码修改**：调整了示例代码块的描述，使其更易于理解，命令步骤的表述也进行了简化，便于读者更好地跟随。

6. **分析器定义和测试内容完善**：在构建自定义分析器部分，增加了对字符过滤器、标记器和标记过滤器的具体介绍，帮助用户明白这些组件如何协同工作。

这些修改旨在改善文档的用户体验，使用户能够更有效地创建和使用自定义分析器。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Ftutorial-create-custom-analyzer.md)。

## articles/search/tutorial-multiple-data-sources.md{#item-71558f}

<details>
<summary>Diff</summary>
````diff
@@ -7,7 +7,7 @@ author: haileytap
 ms.author: haileytapia
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 03/10/2025
+ms.date: 03/28/2025
 ms.custom:
   - devx-track-csharp
   - devx-track-dotnet
@@ -49,7 +49,7 @@ A finished version of the code in this tutorial can be found in the following pr
 > [!NOTE]
 > You can use a free search service for this tutorial. The free tier limits you to three indexes, three indexers, and three data sources. This tutorial creates one of each. Before starting, make sure you have room on your service to accept the new resources.
 
-## 1 - Create services
+## Create services
 
 This tutorial uses Azure AI Search for indexing and queries, Azure Cosmos DB for the first data set, and Azure Blob Storage for the second data set.
 
@@ -99,7 +99,7 @@ This sample uses two small sets of data describing seven fictional hotels. One s
 
 The third component is Azure AI Search, which you can [create in the Azure portal](search-create-service-portal.md) or [find an existing search service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your Azure resources.
 
-### Copy an admin API key and URL for Azure AI Search
+### Copy an admin key and URL for Azure AI Search
 
 To authenticate to your search service, you need the service URL and an access key.
 
@@ -111,7 +111,7 @@ To authenticate to your search service, you need the service URL and an access k
 
 Having a valid key establishes trust, on a per-request basis, between the application sending the request and the service handling it.
 
-## 2 - Set up your environment
+## Set up your environment
 
 1. Open Visual Studio.
 
@@ -140,7 +140,7 @@ The first two entries are the URL and admin keys of a search service. Use the fu
 
 The next entries specify account names and connection string information for the Azure Blob Storage and Azure Cosmos DB data sources.
 
-## 3 - Map key fields
+## Map key fields
 
 Merging content requires that both data streams are targeting the same documents in the search index.
 
@@ -155,7 +155,7 @@ Azure AI Search indexers can use field mappings to rename and even reformat data
 > [!NOTE]
 > In most cases, autogenerated document keys, such as those created by default by some indexers, don't make good document keys for combined indexes. In general, use a meaningful, unique key value that already exists in your data sources or can be easily added.
 
-## 4 - Explore the code
+## Explore the code
 
 When the data and configuration settings are in place, the sample program in **/v11/AzureSearchMultipleDataSources.sln** should be ready to build and run.
 
@@ -351,7 +351,7 @@ Because the index is already populated with hotel data from the Azure Cosmos DB
 > [!NOTE]
 > If you have the same non-key fields in both of your data sources, and the data in those fields doesn't match, the index contains the values from whichever indexer ran most recently. In our example, both data sources contain a **HotelName** field. If for some reason the data in this field is different, for documents with the same key value, the **HotelName** data from the most recently indexed data source is the value stored in the index.
 
-## 5 - Search
+## Search
 
 After running the program, you can explore the populated search index using the [**Search explorer**](search-explorer.md) in the Azure portal.
 
@@ -377,7 +377,7 @@ You can find and manage resources in the Azure portal using the All resources or
 
 ## Next step
 
-Now that you're familiar with the concept of ingesting data from multiple sources, take a closer look at indexer configuration, starting with Azure Cosmos DB:
+Now that you're familiar with ingesting data from multiple sources, take a closer look at indexer configuration, starting with Azure Cosmos DB:
 
 > [!div class="nextstepaction"]
 > [Configure an Azure Cosmos DB for NoSQL indexer](search-howto-index-cosmosdb.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新多个数据源教程"
}
```

### Explanation
对 `tutorial-multiple-data-sources.md` 文件进行了小幅修改，具体修改如下：

1. **更新日期**：将文档的更新日期从 `03/10/2025` 修改为 `03/28/2025`，确保信息的准确性。

2. **标题修改**：多个章节的标题被简化，例如将“1 - Create services”修改为“Create services”，使其更为简洁明了，增强标题的一致性。

3. **语言优化**：进行了多处语句的优化，提高了内容的清晰度和可读性。例如，从“Copy an admin API key and URL for Azure AI Search”更改为“Copy an admin key and URL for Azure AI Search”，简化了表达。

4. **结构调整**：部分章节标题的更改使文章结构更为流畅，提升了读者的理解速度。例如，从“2 - Set up your environment”简化为“Set up your environment”。

5. **小节间的语气一致性**：调整了一些小节的描述以保持语气的一致性，例如确保每个步骤的描述保持主动语态，增强了指导性。

这些修改旨在提高文档的可读性和专业性，使开发者在处理多个数据源时能更轻松地理解和实施教程的步骤。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Ftutorial-multiple-data-sources.md)。

## articles/search/tutorial-optimize-indexing-push-api.md{#item-ef0e96}

<details>
<summary>Diff</summary>
````diff
@@ -1,24 +1,24 @@
 ---
-title: 'C# tutorial: Optimize indexing by using the push API'
+title: 'C# Tutorial: Optimize Indexing Using the Push API'
 titleSuffix: Azure AI Search
-description: Learn how to efficiently index data by using Azure AI Search's push API. This tutorial and sample code are in C#.
+description: Learn how to efficiently index data using Azure AI Search's push API. This tutorial and sample code are in C#.
 author: gmndrg
 ms.author: gimondra
 ms.service: azure-ai-search
 ms.topic: tutorial
-ms.date: 10/14/2024
+ms.date: 03/28/2025
 ms.custom:
   - devx-track-csharp
   - ignite-2023
 ---
 
-# Tutorial: Optimize indexing by using the push API
+# Tutorial: Optimize indexing using the push API
 
-Azure AI Search supports [two basic approaches](search-what-is-data-import.md) for importing data into a search index: *push* your data into the index programmatically, or *pull* in the data by pointing an [Azure AI Search indexer](search-indexer-overview.md) at a supported data source.
+Azure AI Search supports [two basic approaches](search-what-is-data-import.md) for importing data into a search index: *pushing* your data into the index programmatically, or *pulling* in your data by pointing an [Azure AI Search indexer](search-indexer-overview.md) to a supported data source.
 
-This tutorial explains how to efficiently index data using the [push model](search-what-is-data-import.md#pushing-data-to-an-index) by batching requests and using an exponential backoff retry strategy. You can [download and run the sample application](https://github.com/Azure-Samples/azure-search-dotnet-scale/tree/main/optimize-data-indexing). This article explains the key aspects of the application and what factors to consider when indexing data.
+This tutorial explains how to efficiently index data using the [push model](search-what-is-data-import.md#pushing-data-to-an-index) by batching requests and using an exponential backoff retry strategy. You can [download and run the sample application](https://github.com/Azure-Samples/azure-search-dotnet-scale/tree/main/optimize-data-indexing). This tutorial also explains the key aspects of the application and what factors to consider when indexing data.
 
-This tutorial uses C# and the [Azure.Search.Documents library](/dotnet/api/overview/azure/search) from the Azure SDK for .NET to perform the following tasks:
+In this tutorial, you use C# and the [Azure.Search.Documents library](/dotnet/api/overview/azure/search) from the Azure SDK for .NET to:
 
 > [!div class="checklist"]
 > * Create an index
@@ -29,11 +29,8 @@ This tutorial uses C# and the [Azure.Search.Documents library](/dotnet/api/overv
 
 ## Prerequisites
 
-The following services and tools are required for this tutorial.
-
-+ An Azure subscription. If you don't have one, you can [create a free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
-
-+ [Visual Studio](https://visualstudio.microsoft.com/downloads/), any edition. Sample code and instructions were tested on the free Community edition.
++ An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F).
++ [Visual Studio](https://visualstudio.microsoft.com/downloads/).
 
 <a name="get-service-info"></a>
 
@@ -43,7 +40,7 @@ Source code for this tutorial is in the [optimize-data-indexing/v11](https://git
 
 ## Key considerations
 
-Factors that affect indexing speeds are listed next. To learn more, see [Index large data sets](search-howto-large-index.md).
+The following factors affect indexing speeds. For more information, see [Index large data sets](search-howto-large-index.md).
 
 + **Service tier and number of partitions/replicas**: Adding partitions or upgrading your tier increases indexing speeds.
 + **Index schema complexity**: Adding fields and field properties lowers indexing speeds. Smaller indexes are faster to index.
@@ -52,21 +49,21 @@ Factors that affect indexing speeds are listed next. To learn more, see [Index l
 + **Retry strategy**: An exponential backoff retry strategy is a best practice for optimum indexing.
 + **Network data transfer speeds**: Data transfer speeds can be a limiting factor. Index data from within your Azure environment to increase data transfer speeds.
 
-## Step 1: Create an Azure AI Search service
+## Create an Azure AI Search service
 
-To complete this tutorial, you need an Azure AI Search service, which you can [create in the Azure portal](search-create-service-portal.md), or [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription. We recommend using the same tier you plan to use in production so that you can accurately test and optimize indexing speeds.
+This tutorial requires an Azure AI Search service, which you can [create in the Azure portal](search-create-service-portal.md). You can also [find an existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) in your current subscription. To accurately test and optimize indexing speeds, we recommend using the same tier you plan to use in production.
 
 ### Get an admin key and URL for Azure AI Search
 
 This tutorial uses key-based authentication. Copy an admin API key to paste into the *appsettings.json* file.
 
-1. Sign in to the [Azure portal](https://portal.azure.com). Get the endpoint URL from your search service **Overview** page. An example endpoint might look like `https://mydemo.search.windows.net`.
+1. Sign in to the [Azure portal](https://portal.azure.com). On your service **Overview** page, copy the endpoint URL. An example endpoint might look like `https://mydemo.search.windows.net`.
 
-1. In **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
+1. On **Settings** > **Keys**, get an admin key for full rights on the service. There are two interchangeable admin keys, provided for business continuity in case you need to roll one over. You can use either the primary or secondary key on requests for adding, modifying, and deleting objects.
 
     :::image type="content" source="media/search-get-started-rest/get-url-key.png" alt-text="Screenshot of the HTTP endpoint and API key locations.":::
 
-## Step 2: Set up your environment
+## Set up your environment
 
 1. Start Visual Studio and open *OptimizeDataIndexing.sln*.
 
@@ -80,11 +77,11 @@ This tutorial uses key-based authentication. Copy an admin API key to paste into
 }
 ```
 
-## Step 3: Explore the code
+## Explore the code
 
-Once you update *appsettings.json*, the sample program in *OptimizeDataIndexing.sln* should be ready to build and run.
+After you update *appsettings.json*, the sample program in *OptimizeDataIndexing.sln* should be ready to build and run.
 
-This code is derived from the C# section of [Quickstart: Full text search using the Azure SDKs](search-get-started-text.md). You can find more detailed information on the basics of working with the .NET SDK in that article.
+This code is derived from the C# section of [Quickstart: Full text search using the Azure SDKs](search-get-started-text.md), which provides detailed information about the basics of working with the .NET SDK.
 
 This simple C#/.NET console app performs the following tasks:
 
@@ -105,9 +102,9 @@ This simple C#/.NET console app performs the following tasks:
 
 This sample program uses the Azure SDK for .NET to define and create an Azure AI Search index. It takes advantage of the `FieldBuilder` class to generate an index structure from a C# data model class.
 
-The data model is defined by the `Hotel` class, which also contains references to the `Address` class. The FieldBuilder drills down through multiple class definitions to generate a complex data structure for the index. Metadata tags are used to define the attributes of each field, such as whether it's searchable or sortable.
+The data model is defined by the `Hotel` class, which also contains references to the `Address` class. `FieldBuilder` drills down through multiple class definitions to generate a complex data structure for the index. Metadata tags are used to define the attributes of each field, such as whether it's searchable or sortable.
 
-The following snippets from the *Hotel.cs* file show how a single field, and a reference to another data model class, can be specified.
+The following snippets from the *Hotel.cs* file specify a single field and a reference to another data model class.
 
 ```csharp
 . . .
@@ -135,9 +132,9 @@ private static async Task CreateIndexAsync(string indexName, SearchIndexClient i
 
 ### Generate data
 
-A simple class is implemented in the *DataGenerator.cs* file to generate data for testing. The sole purpose of this class is to make it easy to generate a large number of documents with a unique ID for indexing.
+A simple class is implemented in the *DataGenerator.cs* file to generate data for testing. The purpose of this class is to make it easy to generate a large number of documents with a unique ID for indexing.
 
-To get a list of 100,000 hotels with unique IDs, run the following lines of code:
+To get a list of 100,000 hotels with unique IDs, run the following code:
 
 ```csharp
 long numDocuments = 100000;
@@ -147,23 +144,24 @@ List<Hotel> hotels = dg.GetHotels(numDocuments, "large");
 
 There are two sizes of hotels available for testing in this sample: *small* and *large*.
 
-The schema of your index has an effect on indexing speeds. For this reason, it makes sense to convert this class to generate data that best matches your intended index schema after you run through this tutorial.
+The schema of your index affects indexing speeds. After you complete this tutorial, consider converting this class to generate data that best matches your intended index schema.
 
-## Step 4: Test batch sizes
+## Test batch sizes
 
-Azure AI Search supports the following APIs to load single or multiple documents into an index:
+To load single or multiple documents into an index, Azure AI Search supports the following APIs:
 
 + [Documents - Index (REST API)](/rest/api/searchservice/documents)
-+ [IndexDocumentsAction class](/dotnet/api/azure.search.documents.models.indexdocumentsaction) or [IndexDocumentsBatch class](/dotnet/api/azure.search.documents.models.indexdocumentsbatch)
++ [IndexDocumentsAction class](/dotnet/api/azure.search.documents.models.indexdocumentsaction)
++ [IndexDocumentsBatch class](/dotnet/api/azure.search.documents.models.indexdocumentsbatch)
 
-Indexing documents in batches significantly improves indexing performance. These batches can be up to 1,000 documents, or up to about 16 MB per batch.
+Indexing documents in batches significantly improves indexing performance. These batches can be up to 1,000 documents or up to about 16 MB per batch.
 
 Determining the optimal batch size for your data is a key component of optimizing indexing speeds. The two primary factors influencing the optimal batch size are:
 
 + The schema of your index
 + The size of your data
 
-Because the optimal batch size is dependent on your index and your data, the best approach is to test different batch sizes to determine what results in the fastest indexing speeds for your scenario.
+Because the optimal batch size depends on your index and your data, the best approach is to test different batch sizes to determine what results in the fastest indexing speeds for your scenario.
 
 The following function demonstrates a simple approach to testing batch sizes.
 
@@ -203,7 +201,7 @@ public static async Task TestBatchSizesAsync(SearchClient searchClient, int min
 }
 ```
 
-Because not all documents are the same size (although they are in this sample), we estimate the size of the data we're sending to the search service. You can do this by using the following function that first converts the object to json and then determines its size in bytes. This technique allows us to determine which batch sizes are most efficient in terms of MB/s indexing speeds.
+Because not all documents are the same size (although they are in this sample), we estimate the size of the data we're sending to the search service. You can do this by using the following function that first converts the object to JSON and then determines its size in bytes. This technique allows us to determine which batch sizes are most efficient in terms of MB/s indexing speeds.
 
 ```csharp
 // Returns size of object in MB
@@ -232,26 +230,26 @@ The function requires a `SearchClient` plus the number of tries you'd like to te
 await TestBatchSizesAsync(searchClient, numTries: 3);
 ```
 
-When you run the function, you should see an output in your console like the following example:
+When you run the function, you should see an output in your console similar to the following example:
 
 :::image type="content" source="media/tutorial-optimize-data-indexing/test-batch-sizes.png" alt-text="Screenshot of the output of test batch size function.":::
 
-Identify which batch size is most efficient and then use that batch size in the next step of the tutorial. You might see a plateau in MB/s across different batch sizes.
+Identify which batch size is most efficient and use that batch size in the next step of this tutorial. You might see a plateau in MB/s across different batch sizes.
 
-## Step 5: Index the data
+## Index the data
 
 Now that you identified the batch size you intend to use, the next step is to begin to index the data. To index data efficiently, this sample:
 
-+ uses multiple threads/workers
-+ implements an exponential backoff retry strategy
++ Uses multiple threads/workers
++ Implements an exponential backoff retry strategy
 
 Uncomment lines 41 through 49, and then rerun the program. On this run, the sample generates and sends batches of documents, up to 100,000 if you run the code without changing the parameters.
 
 ### Use multiple threads/workers
 
-To take full advantage of Azure AI Search's indexing speeds, use multiple threads to send batch indexing requests concurrently to the service.  
+To take advantage of Azure AI Search's indexing speeds, use multiple threads to send batch indexing requests concurrently to the service.  
 
-Several of the key considerations previously mentioned can affect the optimal number of threads. You can modify this sample and test with different thread counts to determine the optimal thread count for your scenario. However, as long as you have several threads running concurrently, you should be able to take advantage of most of the efficiency gains.
+Several of the [key considerations](#key-considerations) can affect the optimal number of threads. You can modify this sample and test with different thread counts to determine the optimal thread count for your scenario. However, as long as you have several threads running concurrently, you should be able to take advantage of most of the efficiency gains.
 
 As you ramp up the requests hitting the search service, you might encounter [HTTP status codes](/rest/api/searchservice/http-status-codes) indicating the request didn't fully succeed. During indexing, two common HTTP status codes are:
 
@@ -260,11 +258,11 @@ As you ramp up the requests hitting the search service, you might encounter [HTT
 
 ### Implement an exponential backoff retry strategy
 
-If a failure happens, requests should be retried using an [exponential backoff retry strategy](/dotnet/architecture/microservices/implement-resilient-applications/implement-retries-exponential-backoff).
+If a failure happens, you should retry requests using an [exponential backoff retry strategy](/dotnet/architecture/microservices/implement-resilient-applications/implement-retries-exponential-backoff).
 
-Azure AI Search's .NET SDK automatically retries 503s and other failed requests but you should implement your own logic to retry 207s. Open-source tools such as [Polly](https://github.com/App-vNext/Polly) can be useful in a retry strategy.
+Azure AI Search's .NET SDK automatically retries 503s and other failed requests, but you should implement your own logic to retry 207s. Open-source tools like [Polly](https://github.com/App-vNext/Polly) can be useful in a retry strategy.
 
-In this sample, we implement our own exponential backoff retry strategy. We start by defining some variables including the `maxRetryAttempts` and the initial `delay` for a failed request:
+In this sample, we implement our own exponential backoff retry strategy. We start by defining some variables, including the `maxRetryAttempts` and the initial `delay` for a failed request.
 
 ```csharp
 // Create batch of documents for indexing
@@ -279,9 +277,9 @@ TimeSpan delay = delay = TimeSpan.FromSeconds(2);
 int maxRetryAttempts = 5;
 ```
 
-The results of the indexing operation are stored in the variable `IndexDocumentResult result`. This variable is important because it allows you to check if any documents in the batch failed, as shown in the following example. If there's a partial failure, a new batch is created based on the failed documents' ID.
+The results of the indexing operation are stored in the variable `IndexDocumentResult result`. This variable allows you to check if documents in the batch failed, as shown in the following example. If there's a partial failure, a new batch is created based on the failed documents' ID.
 
-`RequestFailedException` exceptions should also be caught as they indicate the request failed completely and should also be retried.
+`RequestFailedException` exceptions should also be caught, as they indicate the request failed completely, and retried.
 
 ```csharp
 // Implement exponential backoff
@@ -339,56 +337,56 @@ do
 
 From here, wrap the exponential backoff code into a function so it can be easily called.
 
-Another function is then created to manage the active threads. For simplicity, that function isn't included here but can be found in *ExponentialBackoff.cs*. The function can be called with the following command where `hotels` is the data we want to upload, `1000` is the batch size, and `8` is the number of concurrent threads:
+Another function is then created to manage the active threads. For simplicity, that function isn't included here but can be found in *ExponentialBackoff.cs*. You can call the function using the following command, where `hotels` is the data we want to upload, `1000` is the batch size, and `8` is the number of concurrent threads.
 
 ```csharp
 await ExponentialBackoff.IndexData(indexClient, hotels, 1000, 8);
 ```
 
-When you run the function, you should see an output:
+When you run the function, you should see an output similar to the following example:
 
 :::image type="content" source="media/tutorial-optimize-data-indexing/index-data-start.png" alt-text="Screenshot that shows the output of an index data function.":::
 
-When a batch of documents fails, an error is printed out indicating the failure and that the batch is being retried:
+When a batch of documents fails, an error is printed indicating the failure and that the batch is being retried.
 
 ```
 [Batch starting at doc 6000 had partial failure]
 [Retrying 560 failed documents]
 ```
 
-After the function is finished running, you can verify that all of the documents were added to the index.
+After the function finishes running, you can verify that all of the documents were added to the index.
 
-## Step 6: Explore the index
+## Explore the index
 
-You can explore the populated search index after the program has run either programmatically or by using the [Search explorer](search-explorer.md) in the Azure portal.
+After the program finishes running, you can explore the populated search index either programmatically or using the [Search explorer](search-explorer.md) in the Azure portal.
 
 ### Programatically
 
-There are two main options for checking the number of documents in an index: the [Count Documents API](/rest/api/searchservice/documents/count) and the [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics). Both paths require time to process so don't be alarmed if the number of documents returned is initially lower than you expect.
+There are two main options for checking the number of documents in an index: the [Count Documents API](/rest/api/searchservice/documents/count) and the [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics). Both paths require time to process, so don't be alarmed if the number of documents returned is initially lower than you expect.
 
 #### Count Documents
 
-The Count Documents operation retrieves a count of the number of documents in a search index:
+The Count Documents operation retrieves a count of the number of documents in a search index.
 
 ```csharp
 long indexDocCount = await searchClient.GetDocumentCountAsync();
 ```
 
 #### Get Index Statistics
 
-The Get Index Statistics operation returns a document count for the current index, plus storage usage. Index statistics take longer than document count to update.
+The Get Index Statistics operation returns a document count for the current index, plus storage usage. Index statistics take longer to update than document count.
 
 ```csharp
 var indexStats = await indexClient.GetIndexStatisticsAsync(indexName);
 ```
 
 ### Azure portal
 
-In the Azure portal, from the left pane, and find the **optimize-indexing** index in the **Indexes** list.
+In the Azure portal, from the left pane, find the **optimize-indexing** index in the **Indexes** list.
 
 :::image type="content" source="media/tutorial-optimize-data-indexing/portal-output.png" alt-text="Screenshow that shows a list of Azure AI Search indexes.":::
 
-The *Document Count* and *Storage Size* are based on [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics) and can take several minutes to update.
+The **Document Count** and **Storage Size** are based on the [Get Index Statistics API](/rest/api/searchservice/indexes/get-statistics) and can take several minutes to update.
 
 ## Reset and rerun
 
@@ -406,7 +404,7 @@ You can find and manage resources in the Azure portal, using the **All resources
 
 ## Next step
 
-To learn more about indexing large amounts data, try the following tutorial.
+To learn more about indexing large amounts data, try the following tutorial:
 
 > [!div class="nextstepaction"]
 > [Tutorial: Index large data from Apache Spark using SynapseML and Azure AI Search](search-synapseml-cognitive-services.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新优化索引的推送 API 教程"
}
```

### Explanation
对 `tutorial-optimize-indexing-push-api.md` 文档进行了多处修改，具体如下：

1. **标题和描述更新**：将标题格式化为更统一的风格，将“C# tutorial: Optimize indexing by using the push API”改为“C# Tutorial: Optimize Indexing Using the Push API”。同时，简化了描述中的语句，使其更加流畅。

2. **日期修改**：更新文档日期从 `10/14/2024` 改为 `03/28/2025`，确保内容保持最新。

3. **语言优化**：多处文本的措辞得到了改进，例如将“pushing your data into the index programmatically”修改为“pushing your data into the index”，简化了表述，提升了可读性。

4. **内容细化**：在关键考虑因素部分，对句子进行了调整，将原来的“Factors that affect indexing speeds are listed next”修改为“The following factors affect indexing speeds”，使表述更加直接。

5. **步骤标题调整**：将步骤的序号去除，使得各步骤标题更具一致性和简洁性，例如将“Step 1: Create an Azure AI Search service”修改为“Create an Azure AI Search service”。

6. **示例代码的清晰度**：对代码段的前后描述进行了改进，使得代码如何工作和其背景信息变得更为清晰。

7. **逻辑结构优化**：在教程中增加了关于并发请求和重试策略的建议，以提高索引效率，并提供了清晰的处理失败请求的策略。

这些调整旨在提高文档的易读性和专业性，帮助开发者更高效地使用推送 API 进行数据索引。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Ftutorial-optimize-indexing-push-api.md)。

## articles/search/tutorial-rag-build-solution-pipeline.md{#item-25ce01}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: tutorial
-ms.date: 01/09/2025
+ms.date: 03/24/2025
 ---
 
 # Tutorial: Build an indexing pipeline for RAG on Azure AI Search
@@ -29,7 +29,7 @@ In this tutorial, you:
 If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.
 
 > [!TIP]
-> You can use the [Import and vectorize data wizard](search-import-data-portal.md) to create your pipeline. Try some quickstarts: [Image search](search-get-started-portal-image-search.md) and [Vector search](search-get-started-portal-import-vectors.md).
+> You can use the [Import and vectorize data wizard](search-import-data-portal.md) to create your pipeline. Try some quickstarts [Image search](search-get-started-portal-image-search.md) or [Vector search](search-get-started-portal-import-vectors.md), to learn more about the pipeline and its moving parts.
 
 ## Prerequisites
 
@@ -115,15 +115,15 @@ print(f"{result.name} created")
 
 ## Create a data source connection
 
-In this step, set up the sample data and a connection to Azure Blob Storage. The indexer retrieves PDFs from a container. You create the container and upload files in this step.
+In this step, set up the sample data and a connection from Azure AI Search to Azure Blob Storage. The indexer retrieves PDFs from a container. You create the container and upload files in this step.
 
 The original ebook is large, over 100 pages and 35 MB in size. We broke it up into smaller PDFs, one per page of text, to stay under the [document limit for indexers](search-limits-quotas-capacity.md#indexer-limits) of 16 MB per API call and also the [AI enrichment data limits](search-limits-quotas-capacity.md#data-limits-ai-enrichment). For simplicity, we omit image vectorization for this exercise.
 
 1. Sign in to the [Azure portal](https://portal.azure.com) and find your Azure Storage account.
 
 1. Create a container and upload the PDFs from [earth_book_2019_text_pages](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/nasa-e-book/earth_book_2019_text_pages).
 
-1. Make sure Azure AI Search has [**Storage Blob Data Reader** permissions](/azure/role-based-access-control/role-assignments-portal) on the resource.
+1. Make sure your [Azure AI Search managed identity](search-howto-managed-identities-data-sources.md) has a [**Storage Blob Data Reader**](/azure/role-based-access-control/role-assignments-portal) role assignment on Azure Storage.
 
 1. Next, in Visual Studio Code, define an indexer data source that provides connection information during indexing.
 
@@ -148,15 +148,15 @@ The original ebook is large, over 100 pages and 35 MB in size. We broke it up in
     print(f"Data source '{data_source.name}' created or updated")
     ```
 
-If you set up a managed identity for Azure AI Search for the connection, the connection string includes a `ResourceId=` suffix. It should look similar to the following example: `"ResourceId=/subscriptions/FAKE-SUBCRIPTION=ID/resourceGroups/FAKE-RESOURCE-GROUP/providers/Microsoft.Storage/storageAccounts/FAKE-ACCOUNT;"`
+If you set up a [managed identity for an Azure AI Search connection to Azure Storage](search-howto-managed-identities-storage.md), the data source connection string includes a `ResourceId=` suffix. It should look similar to the following example: `"ResourceId=/subscriptions/FAKE-SUBSCRIPTION-ID/resourceGroups/FAKE-RESOURCE-GROUP/providers/Microsoft.Storage/storageAccounts/FAKE-ACCOUNT;"`
 
 ## Create a skillset
 
 Skills are the basis for integrated data chunking and vectorization. At a minimum, you want a Text Split skill to chunk your content, and an embedding skill that create vector representations of your chunked content.
 
 In this skillset, an extra skill is used to create structured data in the index. The [Entity Recognition skill](cognitive-search-skill-entity-recognition-v3.md) is used to identify locations, which can range from proper names to generic references, such as "ocean" or "mountain". Having structured data gives you more options for creating interesting queries and boosting relevance.
 
-The AZURE_AI_MULTISERVICE_KEY is needed even if you're using role-based access control. Azure AI Search uses the key for billing purposes and it's required unless your workloads stay under the free limit. You can also a keyless connection if you're using the most recent preview API or beta packages. For more information, see [Attach an Azure AI services multi-service resource to a skillset](cognitive-search-attach-cognitive-services.md).
+The AZURE_AI_MULTISERVICE_KEY is needed even if you're using role-based access control. Azure AI Search uses the key for billing purposes and it's required unless your workloads stay under the free limit. You can also set up a keyless connection if you're using the most recent preview API or beta packages. For more information, see [Attach an Azure AI services multi-service resource to a skillset](cognitive-search-attach-cognitive-services.md).
 
 ```python
 from azure.search.documents.indexes.models import (
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新 RAG 建立解决方案管道教程"
}
```

### Explanation
对 `tutorial-rag-build-solution-pipeline.md` 文档进行了小幅修改，具体修改内容包括：

1. **日期更新**：将文档日期从 `01/09/2025` 修改为 `03/24/2025`，确保信息的时效性。

2. **语言优化**：在教程中的提示框内，原来的句子“Try some quickstarts: [Image search](search-get-started-portal-image-search.md) and [Vector search](search-get-started-portal-import-vectors.md)”被更改为“Try some quickstarts [Image search](search-get-started-portal-image-search.md) or [Vector search](search-get-started-portal-import-vectors.md), to learn more about the pipeline and its moving parts.”，这使得建议更清晰并且语气更加流畅。

3. **连接描述明确化**：在创建数据源连接的说明中，原文中的“从 Azure Blob Storage”修改为“从 Azure AI Search 到 Azure Blob Storage”，提供了更明确的上下文。

4. **优化权限描述**：将关于权限的描述更新为“Make sure your [Azure AI Search managed identity](search-howto-managed-identities-data-sources.md) has a [**Storage Blob Data Reader**](/azure/role-based-access-control/role-assignments-portal) role assignment on Azure Storage.”，使得与托管身份的相关链接和描述更加一致。

5. **连接字符串说明**：在说明连接字符串格式的段落中，调整了“设置 Azure AI Search 的托管身份的连接”部分，增加并明确了与 Azure 存储的连接。

6. **技能集的描述优化**：对技能集部分的措辞进行了润色，使文本更具流畅性，加深了对技能使用的理解。

这些修改旨在提高文档的可读性和专业性，帮助用户更好地理解如何在 Azure AI Search 中建立索引管道。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Ftutorial-rag-build-solution-pipeline.md)。

## articles/search/vector-search-how-to-chunk-documents.md{#item-b79133}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: conceptual
-ms.date: 03/11/2025
+ms.date: 03/31/2025
 ---
 
 # Chunk large documents for vector search solutions in Azure AI Search
@@ -20,7 +20,9 @@ We recommend [integrated vectorization](vector-search-integrated-vectorization.m
 
 ## Common chunking techniques
 
-Chunking is only required if the source documents are too large for the maximum input size imposed by models. Here are some common chunking techniques, associated with built-in features if you use [indexers](search-indexer-overview.md) and [skills](cognitive-search-working-with-skillsets.md).
+Chunking is only required if the source documents are too large for the maximum input size imposed by models, but it's also beneficial if content is poorly represented as a single vector. Consider a wiki page that covers a lot of varied sub-topics. The entire page might be small enough to meet model input requirements, but you might get better results if you chunk at a finer grain.
+
+Here are some common chunking techniques, associated with built-in features if you use [indexers](search-indexer-overview.md) and [skills](cognitive-search-working-with-skillsets.md).
 
 | Approach | Usage | Built-in functionality |
 |----------|-------|-----------------|
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新向量搜索文档分块教程"
}
```

### Explanation
对 `vector-search-how-to-chunk-documents.md` 文档进行了小幅修改，主要包括以下内容：

1. **日期更新**：将文档日期从 `03/11/2025` 修改为 `03/31/2025`，确保信息的时效性。

2. **增强的内容描述**：在关于分块技术的讨论中，原文阐述了分块的必要性，进行了扩展。添加了新的内容，说明分块不仅仅是在源文档超出模型输入大小限制的情况下是必要的，还强调了当内容作为单个向量表现不佳时，进行分块的好处，例如对包含多个子主题的维基页面的处理。

3. **结构优化**：在介绍分块技术的段落中，调整了内容的安排，使段落结构更加清晰，首先解释了分块的需求，然后列出常见的分块技巧及其与内置功能的关联，增加了文本的流畅性和逻辑性。

这些修改旨在提高关于如何在 Azure AI Search 中进行文档分块的指导性和信息的全面性。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fvector-search-how-to-chunk-documents.md)。

## articles/search/vector-search-how-to-quantization.md{#item-744f48}

<details>
<summary>Diff</summary>
````diff
@@ -9,12 +9,12 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 11/19/2024
+ms.date: 03/31/2025
 ---
 
 # Compress vectors using scalar or binary quantization
 
-Azure AI Search supports scalar and binary quantization for reducing the size of vectors in a search index. Quantization is recommended for reducing vector size because it lowers both memory and disk storage consumption for float16 and float32 embeddings. To offset the effects of lossy compression, you can add oversampling and rescoring over uncompressed vectors.
+Azure AI Search supports scalar and binary quantization for reducing the size of vectors in a search index. Quantization is recommended because it reduces both memory and disk storage for float16 and float32 embeddings. To offset the effects of lossy compression, you can add oversampling and rescoring.
 
 To use built-in quantization, follow these steps:
 
@@ -26,15 +26,15 @@ To use built-in quantization, follow these steps:
 > - Create a new vector profile that uses the named configuration
 > - Create a new vector field having the new vector profile
 > - Load the index with float32 or float16 data that's quantized during indexing with the configuration you defined
-> - Optionally, [query quantized data](#query-a-quantized-vector-field-using-oversampling) using the oversampling parameter if you want to override the default
+> - Optionally, [query quantized data](#query-a-quantized-vector-field-using-oversampling) using the oversampling parameter. If the vector field doesn't specify oversampling in its definition, you can add it at query time.
 
 ## Prerequisites
 
-- [Vector fields in a search index](vector-search-how-to-create-index.md) with a `vectorSearch` configuration, using the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (eKNN) algorithms and a new vector profile.
+- [Vector fields in a search index](vector-search-how-to-create-index.md), with a `vectorSearch` configuration specifying either the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (eKNN) algorithm, and a new vector profile.
 
 ## Supported quantization techniques
 
-Quantization applies to vector fields receiving float-type vectors. In the examples in this article, the field's data type is `Collection(Edm.Single)` for incoming float32 embeddings, but float16 is also supported. When the vectors are received on a field with compression configured, the engine automatically performs quantization to reduce the footprint of the vector data in memory and on disk.
+Quantization applies to vector fields receiving float-type vectors. In the examples in this article, the field's data type is `Collection(Edm.Single)` for incoming float32 embeddings, but float16 is also supported. When the vectors are received on a field with compression configured, the engine performs quantization to reduce the footprint of the vector data in memory and on disk.
 
 Two types of quantization are supported:
 
@@ -43,15 +43,38 @@ Two types of quantization are supported:
 - Binary quantization converts floats into binary bits, which takes up 1 bit. This results in up to 28 times reduced vector index size.
 
 >[!Note]
-> While free services support quantization, they may not demonstrate the full storage savings due to the limited storage quota.
+> While free services support quantization, they don't demonstrate the full storage savings due to the limited storage quota.
+
+## Recommended rescoring techniques
+
+Rescoring is a technique used to offset information loss due to vector compression. It uses oversampling to pick up extra vectors, and supplemental information to rescore initial results found by the query. Supplemental information is either uncompressed original full-precision vectors - or for binary quantization only - you have the option of rescoring using the binary quantized document candidates against the query vector. Rescoring options are specified in the index, but you can invoke rescoring at query time if the index supports it.
+
+API versions determine which rescoring behavior is operational for your code. The most recent preview API supports a new rescoring approach for binary quantization. Indexes created with `2025-03-01-preview` can use the new rescoring behaviors.
+
+| API version | Quantization type | Rescoring properties |
+|-------------|-------------------|------------------|
+| [2024-07-01](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-07-01&preserve-view=true) | Scalar and binary quantization, on vector indexes built using Hierarchical Navigable Small World (HNSW) graphs for similarity search | `rerankWithOriginalVectors` |
+| [2024-11-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) | Scalar and binary quantization on HNSW graphs | `rescoringOptions.enableRescoring` and `rescoreStorageMethod.preserveOriginals` |
+| [2025-03-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true) | Binary quantization on HNSW graphs | Previous parameter combinations are still supported but binary quantization can now be rescored if original embeddings are deleted: `rescoringOptions.enableRescoring` and `rescoringOptions.rescoreStorageMethod=discardOriginals` |
+
+Only HNSW graphs allow rescoring. Exhaustive K Nearest Neighbors (eKNN) doesn't support rescoring.
+
+<!-- - In version 2024-11-01-preview, set `rescoringOptions.enableRescoring` and `rescoreStorageMethod.preserveOriginals`
+- In version 2025-03-01-preview, set `rescoringOptions.enableRescoring` and `rescoringOptions.rescoreStorageMethod=preserveOriginals` for scalar or binary quantization, or `rescoringOptions.enableRescoring` and `rescoringOptions.rescoreStorageMethod=discardOriginals` for binary quantization only -->
+
+The generalized process for rescoring is:
+
+1. The vector query executes over compressed vector fields.
+1. The vector query returns the top k oversampled candidates.
+1. Oversampled k candidates are rescored using either the uncompressed original vectors, or the dot product of binary quantization. 1. After rescoring, results are adjusted so that more relevant matches appear first.
 
 ## Add "compressions" to a search index
 
-The following example shows a partial index definition with a fields collection that includes a vector field, and a `vectorSearch.compressions` section.
+This section explains how to specify a `vectorsSearch.compressions` section in the index. The following example shows a partial index definition with a fields collection that includes a vector field.
 
-It includes both `scalarQuantization` or `binaryQuantization`. You can specify as many compression configurations as you need, and then assign the ones you want to a vector profile.
+The compression example includes both `scalarQuantization` or `binaryQuantization`. You can specify as many compression configurations as you need, and then assign the ones you want to a vector profile.
 
-Syntax for `vectorSearch.Compressions` varies between stable and preview REST APIs, with the preview adding new options for storage optimization, plus changes to existing syntax. Backwards compatibility is preserved through internal API mappings, but you should adopt the new syntax in code that targets 2024-11-01-preview and future versions.
+Syntax for `vectorSearch.Compressions` varies between stable and preview REST APIs, with the preview adding more options for storage optimization, plus changes to existing syntax. Backwards compatibility is preserved through internal API mappings, but we recommend adopting the newer properties in code that targets 2024-11-01-preview and future versions.
 
 ### [**2024-07-01**](#tab/2024-07-01)
 
@@ -68,23 +91,109 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-07-01
     { "name": "vectorContent", "type": "Collection(Edm.Single)", "retrievable": false, "searchable": true, "dimensions": 1536,"vectorSearchProfile": "vector-profile-1"},
   ],
   "vectorSearch": {
-        "profiles": [ ],
-        "algorithms": [ ],
+    "profiles": [ 
+      {
+          "name": "vector-profile-1",
+          "algorithm": "use-hnsw",
+          "compression": "use-scalar"
+      }
+    ],
+    "algorithms": [ 
+      {
+        "name": "use-hnsw",
+        "kind": "hnsw",
+        "hnswParameters": { },
+        "exhaustiveKnnParameters": null
+      }
+    ],
+    "compressions": [
+      {
+        "name": "use-scalar",
+        "kind": "scalarQuantization",
+        "scalarQuantizationParameters": {
+          "quantizedDataType": "int8"
+        },
+        "rerankWithOriginalVectors": true,
+        "defaultOversampling": 10
+      },
+      {
+        "name": "use-binary",
+        "kind": "binaryQuantization",
+        "rerankWithOriginalVectors": true,
+        "defaultOversampling": 10
+      }
+    ]
+  }
+}
+```
+
+**Key points**:
+
+- `kind` must be set to `scalarQuantization` or `binaryQuantization`.
+
+- `rerankWithOriginalVectors` uses the original uncompressed vectors to recalculate similarity and rerank the top results returned by the initial search query. The uncompressed vectors exist in the search index even if `stored` is false. This property is optional. Default is true.
+
+- `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
+
+- `quantizedDataType` is optional and applies to scalar quantization only. If you add it, it must be set to `int8`. This is the only primitive data type supported for scalar quantization at this time. Default is `int8`.
+
+### [**2024-11-01-preview**](#tab/2024-11-01-preview)
+
+Use the [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) REST API to configure compression settings.
+
+Changes in this version include new `rescoringOptions` that replace `rerankWithOriginalVectors`, and extend the API with more storage options. Notice that `defaultOversampling` is now a property of `rescoringOptions`.
+
+Rescoring options are used to mitigate the effects of lossy comprehension. You can set `rescoringOptions` for scalar or binary quantization.
+
+```http
+POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-preview
+
+{
+  "name": "my-index",
+  "fields": [
+    { "name": "Id", "type": "Edm.String", "key": true, "retrievable": true, "searchable": true, "filterable": true },
+    { "name": "content", "type": "Edm.String", "retrievable": true, "searchable": true },
+    { "name": "vectorContent", "type": "Collection(Edm.Single)", "retrievable": false, "searchable": true, "dimensions": 1536,"vectorSearchProfile": "vector-profile-1"},
+  ],
+  "vectorSearch": {
+        "profiles": [ 
+          {
+              "name": "vector-profile-1",
+              "algorithm": "use-hnsw",
+              "compression": "use-scalar"
+          }
+        ],
+        "algorithms": [ 
+          {
+            "name": "use-hnsw",
+            "kind": "hnsw",
+            "hnswParameters": { },
+            "exhaustiveKnnParameters": null
+          }
+        ],
         "compressions": [
           {
             "name": "use-scalar",
             "kind": "scalarQuantization",
+            "rescoringOptions": {
+                "enableRescoring": true,
+                "defaultOversampling": 10,
+                "rescoreStorageMethod": "preserveOriginals"
+            },
             "scalarQuantizationParameters": {
               "quantizedDataType": "int8"
             },
-            "rerankWithOriginalVectors": true,
-            "defaultOversampling": 10
+            "truncationDimension": 1024
           },
           {
             "name": "use-binary",
             "kind": "binaryQuantization",
-            "rerankWithOriginalVectors": true,
-            "defaultOversampling": 10
+            "rescoringOptions": {
+                "enableRescoring": true,
+                "defaultOversampling": 10,
+                "rescoreStorageMethod": "preserveOriginals"
+            },
+            "truncationDimension": 1024
           }
         ]
     }
@@ -95,22 +204,28 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-07-01
 
 - `kind` must be set to `scalarQuantization` or `binaryQuantization`.
 
-- `rerankWithOriginalVectors` uses the original uncompressed vectors to recalculate similarity and rerank the top results returned by the initial search query. The uncompressed vectors exist in the search index even if `stored` is false. This property is optional. Default is true.
+- `rescoringOptions` are a collection of properties used to offset lossy compression by rescoring query results using the original full-precision vectors that exist prior to quantization. For rescoring to work, you must have the vector instance that provides this content. Setting `rescoreStorageMethod` to `discardOriginals` prevents you from using `enableRescoring` or `defaultOversampling`. For more information about vector storage, see [Eliminate optional vector instances from storage](vector-search-how-to-storage-options.md).
+
+- `"rescoreStorageMethod": "preserveOriginals"` is the API equivalent of `"rerankWithOriginalVectors": true`. Rescoring vector search results with the original full-precision vectors can result in adjustments to search score and rankings, promoting the more relevant matches as determined by the rescoring step.
 
 - `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
 
 - `quantizedDataType` is optional and applies to scalar quantization only. If you add it, it must be set to `int8`. This is the only primitive data type supported for scalar quantization at this time. Default is `int8`.
 
-### [**2024-11-01-preview**](#tab/2024-11-01-preview)
+- `truncationDimension` is a preview feature that taps inherent capabilities of the text-embedding-3 models to "encode information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks" (see [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)). You can use truncated dimensions with or without rescoring options. For more information about how this feature is implemented in Azure AI Search, see [Truncate dimensions using MRL compression](vector-search-how-to-truncate-dimensions.md).
 
-Use the [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) REST API to configure compression settings.
+### [**2025-03-01-preview**](#tab/2025-03-01-preview)
 
-Changes in this version include new `rescoringOptions` that replace `rerankWithOriginalVectors`, and extend the API with more storage options. Notice that `defaultOversampling` is now a property of `rescoringOptions`.
+Use the [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2025-031-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true) REST API to configure compression settings.
 
-Rescoring options are used to mitigate the effects of lossy comprehension. You can set `rescoringOptions` for scalar or binary quantization.
+Changes in this version include new guidance for *binary quantization*. If you set `enableRescoring` to true, you can set `rescoreStorageMethod` to `discardOriginals` to further reduce storage, without reducing quality. 
+
+Azure AI Search supports a lossy rescoring option on the binary quantized document vectors, which helps close the quality gap between no rescoring and full-precision rescoring when using `binaryQuantization`.
+
+For scalar quantization, there are no rescoring changes in this preview.
 
 ```http
-POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-preview
+POST https://[servicename].search.windows.net/indexes?api-version=2025-03-01-preview
 
 {
   "name": "my-index",
@@ -120,8 +235,21 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-pre
     { "name": "vectorContent", "type": "Collection(Edm.Single)", "retrievable": false, "searchable": true, "dimensions": 1536,"vectorSearchProfile": "vector-profile-1"},
   ],
   "vectorSearch": {
-        "profiles": [ ],
-        "algorithms": [ ],
+        "profiles": [ 
+          {
+              "name": "vector-profile-1",
+              "algorithm": "use-hnsw",
+              "compression": "use-binary"
+          }
+        ],
+        "algorithms": [ 
+          {
+            "name": "use-hnsw",
+            "kind": "hnsw",
+            "hnswParameters": { },
+            "exhaustiveKnnParameters": null
+          }
+        ],
         "compressions": [
           {
             "name": "use-scalar",
@@ -142,7 +270,7 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-pre
             "rescoringOptions": {
                 "enableRescoring": true,
                 "defaultOversampling": 10,
-                "rescoreStorageMethod": "preserveOriginals"
+                "rescoreStorageMethod": "discardOriginals"
             },
             "truncationDimension": 1024
           }
@@ -155,21 +283,21 @@ POST https://[servicename].search.windows.net/indexes?api-version=2024-11-01-pre
 
 - `kind` must be set to `scalarQuantization` or `binaryQuantization`.
 
-- `rescoringOptions` are a collection of properties used to offset lossy compression by rescoring query results using the original full-precision vectors that exist prior to quantization. For rescoring to work, you must have the vector instance that provides this content. Setting `rescoreStorageMethod` to `discardOriginals` prevents you from using `enableRescoring` or `defaultOversampling`. For more information about vector storage, see [Eliminate optional vector instances from storage](vector-search-how-to-storage-options.md).
+- `rescoringOptions` are a collection of properties used to offset lossy compression by rescoring query results using the original full-precision vectors that exist prior to quantization.
 
-- `"rescoreStorageMethod": "preserveOriginals"` is the API equivalent of `"rerankWithOriginalVectors": true`. Rescoring vector search results with the original full-precision vectors can result in adjustments to search score and rankings, promoting the more relevant matches as determined by the rescoring step.
+- `enableRescoring` rescores the initial results obtained by query execution over compressed data. For scalar quantization, rescoring uses uncompressed vectors to produce more relevant results and takes a dependency on `preserveOriginals`. For binary quantization, rescoring is the same as scalar quantization if you preserve originals, but you can also discard originals and still get rescoring. In this scenario, rescoring is calculated by the dot product of the full precision query and binary quantized data in the index.  
 
-- `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
+- `"rescoreStorageMethod": "discardOriginals"` removes original vectors. These aren't needed for binary quantization.
 
-- `quantizedDataType` is optional and applies to scalar quantization only. If you add it, it must be set to `int8`. This is the only primitive data type supported for scalar quantization at this time. Default is `int8`.
+- `defaultOversampling` considers a broader set of potential results to offset the reduction in information from quantization. The formula for potential results consists of the `k` in the query, with an oversampling multiplier. For example, if the query specifies a `k` of 5, and oversampling is 20, then the query effectively requests 100 documents for use in reranking, using the original uncompressed vector for that purpose. Only the top `k` reranked results are returned. This property is optional. Default is 4.
 
 - `truncationDimension` is a preview feature that taps inherent capabilities of the text-embedding-3 models to "encode information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks" (see [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)). You can use truncated dimensions with or without rescoring options. For more information about how this feature is implemented in Azure AI Search, see [Truncate dimensions using MRL compression](vector-search-how-to-truncate-dimensions.md).
 
 ---
 
 ## Add the vector search algorithm
 
-You can use HNSW algorithm or exhaustive KNN in the 2024-11-01-preview REST API. For the stable version, use HNSW only.
+You can use HNSW algorithm or exhaustive KNN in the 2024-11-01-preview REST API or later. For the stable version, use HNSW only. If you want rescoring, you must choose HNSW.
 
    ```json
    "vectorSearch": {
@@ -240,15 +368,15 @@ Scalar quantization reduces the resolution of each number within each vector emb
 
 Each component of the vector is mapped to the closest representative value within this set of quantization levels in a process akin to rounding a real number to the nearest integer. In the quantized 8-bit vector, the identifier number stands in place of the original value. After quantization, each vector is represented by an array of identifiers for the bins to which its components belong. These quantized vectors require much fewer bits to store compared to the original vector, thus reducing storage requirements and memory footprint.
 
-## How  binary quantization works in Azure AI Search
+## How binary quantization works in Azure AI Search
 
 Binary quantization compresses high-dimensional vectors by representing each component as a single bit, either 0 or 1. This method drastically reduces the memory footprint and accelerates vector comparison operations, which are crucial for search and retrieval tasks. Benchmark tests show up to 96% reduction in vector index size.
 
-It's particularly effective for embeddings with dimensions greater than 1024. For smaller dimensions, we recommend testing the quality of binary quantization, or trying scalar instead. Additionally, we’ve found BQ performs very well when embeddings are centered around zero. Most popular embedding models such as OpenAI, Cohere, and Mistral are centered around zero.
+It's particularly effective for embeddings with dimensions greater than 1024. For smaller dimensions, we recommend testing the quality of binary quantization, or trying scalar instead. Additionally, we’ve found binary quantization performs very well when embeddings are centered around zero. Most popular embedding models such as OpenAI, Cohere, and Mistral are centered around zero.
 
 ## Query a quantized vector field using oversampling
 
-Query syntax for a compressed or quantized vector field is the same as for noncompressed vector fields, unless you want to override parameters associated with oversampling or rescoring with original vectors.
+Query syntax for a compressed or quantized vector field is the same as for noncompressed vector fields, unless you want to override parameters associated with oversampling and rescoring. You can add an o`versampling` parameter to invoke oversampling and rescoring at query time.
 
 ### [**2024-07-01**](#tab/query-2024-07-01)
 
@@ -302,22 +430,34 @@ POST https://[service-name].search.windows.net/indexes/demo-index/docs/search?ap
 
 **Key points**:
 
-- Applies to vector fields that undergo vector compression, per the vector profile assignment.
+- Oversampling applies to vector fields that undergo vector compression, per the vector profile assignment.
 
-- Overrides the `defaultOversampling` value or introduces oversampling at query time, even if the index's compression configuration didn't specify oversampling or reranking options.
+- Oversampling in the query overrides the `defaultOversampling` value in the index, or invokes oversampling and rescoring at query time, even if the index's compression configuration didn't specify oversampling or reranking options.
 
----
+### [**2025-03-01-preview**](#tab/query-2025-03-01-preview)
+
+The latest preview API is identical to the previous preview API in terms of `vectorQueries` specification. As with the previous version, we recommend oversampling as mitigation for lossy compression.
+
+```http
+POST https://[service-name].search.windows.net/indexes/demo-index/docs/search?api-version=2025-03-01-preview
+
+{    
+    "vectorQueries": [
+        {    
+            "kind": "vector",    
+            "vector": [8, 2, 3, 4, 3, 5, 2, 1],    
+            "fields": "myvector",
+            "oversampling": 12.0,
+            "k": 5   
+        }
+  ]    
+}
+```
 
-<!-- 
-RESCORE WITH ORIGINAL VECTORS -- NEEDS AN H2 or H3
-It's used to rescore search results obtained used compressed vectors.
+**Key points**:
+
+- Oversampling applies to vector fields that undergo vector compression, per the vector profile assignment.
 
-Rescore with original vectors
-After the initial query, rescore results using uncompressed vectors
- 
-For "enableRescoring", we provide true or false options. if it's true, the query will first retrieve using compressed vectors, then rescore results using uncompressed vectors.
+- Oversampling in the query overrides the `defaultOversampling` value in the index, or invokes oversampling and rescoring at query time, even if the index's compression configuration didn't specify oversampling or reranking options.
 
-Step one: Vector query executes using the compressed vectors.
-Step two: Query returns the top oversampling k-matches.
-Step three: Oversampling k-matches are rescored using the uncompressed vectors, adjusting the scores and ranking so that more relevant matches appear first.
- -->
+---
````
</details>

### Summary

```json
{
    "modification_type": "major update",
    "modification_title": "更新量化教程以支持新功能和更详细的说明"
}
```

### Explanation
对 `vector-search-how-to-quantization.md` 文档进行了重大更新，主要改动包括：

1. **日期更新**：将文档的发布日期从 `11/19/2024` 修改为 `03/31/2025`，确保信息的时效性。

2. **内容扩展和精简**：增加了大量新内容和修订，以更好地解释量化过程的各个方面，特别是关于如何使用二进制和标量量化来压缩向量以及这些技术的优缺点。

3. **新的重评分技术**：新增了关于重评分的章节，介绍如何通过使用原始全精度向量来补偿因量化造成的信息损失。并且介绍了在查询阶段如何进行重评分，包括重评分选项和不同的API版本支持的行为。

4. **更新的语法说明**：提供了新的API版本和语法，用于指定在索引中添加“compressions”部分的方式，突出标量和二进制量化的配置选项。

5. **更新的技术细节**：细化了量化过程中的每个步骤，说明如何定义索引、配置压缩方式，以及在查询时如何应用超采样。

6. **量化技术支持**：加入了关于支持的量化技术的详细描述，并更新了有关如何在 Azure AI Search 中应用这些技术的指导。

7. **示例代码更新**：增加了示例代码以展示如何配置压缩和进行查询，改进了示例的可读性。

这些更改旨在增强文档的全面性和实用性，使用户能够更好地理解如何在 Azure AI Search 中实现向量量化，优化存储和检索过程。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fvector-search-how-to-quantization.md)。

## articles/search/vector-search-how-to-storage-options.md{#item-ee1680}

<details>
<summary>Diff</summary>
````diff
@@ -9,41 +9,44 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 11/19/2024
+ms.date: 03/31/2025
 ---
 
 # Eliminate optional vector instances from storage
 
 Azure AI Search stores multiple copies of vector fields that are used in specific workloads. If you don't need to support a specific behavior, like returning raw vectors in a query response, you can set properties in the index that omit storage for that workload.
 
+Removing storage is irreversible and requires reindexing if you want it back.
+
 ## Prerequisites
 
-- [Vector fields in a search index](vector-search-how-to-create-index.md) with a `vectorSearch` configuration, using the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (KNN) algorithms and a new vector profile.
+- [Vector fields in a search index](vector-search-how-to-create-index.md), with a `vectorSearch` configuration specifying either the Hierarchical Navigable Small Worlds (HNSW) or exhaustive K-nearest neighbor (KNN) algorithm, and a new vector profile.
 
 ## How vector fields are stored
 
-For every vector field, there could be three copies of the vectors, each serving a different purpose:
+For every vector field, there are up to three copies of the vectors, each serving a different purpose:
 
 | Instance | Usage | Controlled using |
-|----------|-------|------------|
-| Source vectors which store the JSON that was received during document indexing (JSON data) | Used for incremental data refresh with `merge` or `mergeOrUpload` during document indexing. Also used if you want "retrievable" vectors returned in the query response. | `stored` property on vector fields |
-| Original full-precision vectors (binary data) | In existing indexes, these are used for internal index operations and for exhaustive KNN search. For vectors using compression, it's also used for rescoring (if enabled) on an oversampled candidate set of results from ANN search on vector fields using [scalar or binary quantization](vector-search-how-to-quantization.md) compression. | `rescoringOptions.rescoreStorageMethod` property in `vectorSearch.compressions`. For *uncompressed* vector fields on indexes created with `2024-11-01-Preview` API versions and later, this will be omitted by default with no impact on search activities nor quality. |
-| Vectors in the [HNSW graph for Approximate Nearest Neighbors (ANN) search](vector-search-overview.md) (HNSW graph) | Used for ANN query execution. Consists of either full-precision vectors (when no compression is applied) or quantized vectors (when compression is applied) | Only applies to HNSW. These data structures are required for efficient ANN search. |
+|----------|-------|------------------|
+| Source vectors received during document indexing (JSON data) | Used for incremental data refresh with `merge` or `mergeOrUpload` indexing action. Also used to return "retrievable" vectors in the query response. | `stored` property on vector fields |
+| Original full-precision vectors (binary data) | Used for internal index operations and for exhaustive KNN search in older API versions. For compressed vectors, it's also used for `preserveOriginals` rescoring on an oversampled candidate set of results from ANN search. This applies to vector fields that undergo [scalar or binary quantization](vector-search-how-to-quantization.md). | `rescoringOptions.rescoreStorageMethod` property in `vectorSearch.compressions`. |
+| Vectors in the [HNSW graph for Approximate Nearest Neighbors (ANN) search](vector-search-overview.md) (HNSW graph) or vectors for exhaustive K Nearest Neighbors (eKNN index) | Used for query execution. Consists of either full-precision vectors (when no compression is applied) or quantized vectors. | Essential. There are no parameters for removing this instance. |
+
+You can set properties that permanently discard the first two instances (JSON data and binary data) from vector storage, but not the last instance.
 
-You can set properties that permanently discard the first two instances (JSON data and binary data) from vector storage.
+To offset lossy compression for HNSW, you can keep the second instance (binary data) for rescoring purposes to improve ANN search quality. For eKNN, only scalar quantization is supported, and rescoring isn't an option. In newer API versions like the latest preview, the second instance isn't kept for eKNN because the third instance provides full-precision vectors in an eKNN index.
 
-The last instance (HNSW graph) is required for ANN vector query execution. If any compression techniques such as [scalar or binary quantization](vector-search-how-to-quantization.md) are used, they are applied to this set of data. If you want to offset lossy compression, you should keep the second instance (binary data) for rescoring purposes to improve ANN search quality.
+### Indexes created with 2024-11-01-preview or later API versions
 
-### Indexes created on or after 2024-11-01-preview API version
-For indexes created with the 2024-11-01-preview API version with uncompressed vector fields, the second and third instances (binary data and HNSW graph) are combined as part of our cost reduction investments, reducing overall storage. The same index created with the 2024-11-01-preview API is functionally equivalent but uses less storage compared to identical indexes created with earlier API versions. Physical data structures are established on a Create Index request, so you must delete and recreate the index to realize the storage reductions.
+For indexes created with the 2024-11-01-preview or a later API with uncompressed vector fields, the second and third instances (binary data and HNSW graph) are combined as part of our cost reduction investments, reducing overall storage. A newer generation index with consolidated vectors is functionally equivalent to older indexes, but uses less storage. Physical data structures are established on a Create Index request, so you must delete and recreate the index to realize the storage reductions.
 
-If you choose to use [vector compression](vector-search-how-to-configure-compression-storage.md), we compress (quantize) the in-memory portion of the vector index. Since memory is often a primary constraint for vector indexes, this allows storing more vectors within the same search service. However, lossy compression results in some information loss, which can impact search quality.
+If you choose [vector compression](vector-search-how-to-configure-compression-storage.md), AI Search compresses (quantizes) the in-memory portion of the vector index. Since memory is often a primary constraint for vector indexes, this practice allows you to store more vectors within the same search service. However, lossy compression equates to less information in the index, which can affect search quality.
 
-To mitigate this, enabling "rescoring" and "oversampling" helps maintain accuracy. This retrieves a larger set of candidate documents from the compressed index and then recomputes similarity scores using the original vectors, which must be retained in storage. As a result, while quantization reduces memory usage (vector index size usage), it slightly increases storage requirements since both compressed and original vectors are stored. The additional storage is approximately equal to the size of the compressed index.
+To mitigate the loss in information, you can [enable "rescoring" and "oversampling" options](vector-search-how-to-quantization.md#recommended-rescoring-techniques) to help maintain quality. The effect is retrieval of a larger set of candidate documents from the compressed index, with recomputation of similarity scores using the original vectors or the dot product. For rescoring to work, original vectors must be retained in storage for certain scenarios. As a result, while quantization reduces memory usage (vector index size usage), it slightly increases storage requirements since both compressed and original vectors are stored. The extra storage is approximately equal to the size of the compressed index.
 
-## Set the `stored` property
+## Remove source vectors (JSON data)
 
-The `stored` property is a boolean property on a vector field definition that determines whether storage is allocated for retrievable vector field content (the source instance). The `stored` property is true by default. If you don't need raw vector content in a query response, you can save up to 50 percent storage per field by changing `stored` to false.
+The `stored` property is a boolean property on a vector field definition that determines whether storage is allocated for retrievable vector field content obtained during indexing (the source instance). The `stored` property is true by default. If you don't need raw vector content in a query response, you can save up to 50 percent storage per field by changing `stored` to false.
 
 Considerations for setting `stored` to false:
 
@@ -52,7 +55,7 @@ Considerations for setting `stored` to false:
 - However, if your indexing strategy includes [partial document updates](search-howto-reindex.md#update-content), such as "merge" or "mergeOrUpload" on an existing document, setting `stored=false` prevents content updates to those fields during the merge. On each "merge" or "mergeOrUpload" operation to a search document, you must provide the vector fields in its entirety, along with the nonvector fields that you're updating, or the vector is dropped.
 
 > [!IMPORTANT]
-> Setting the `stored=false` attribution is irreversible. This property can only be set when you create the index and is only allowed on vector fields. Updating an existing index with new vector fields cannot set this property to `false`. If you want retrievable vector content later, you must drop and rebuild the index, or create and load a new field that has the new attribution.
+> Setting the `stored=false` attribution is irreversible. This property can only be set when you create the index and is only allowed on vector fields. Updating an existing index with new vector fields can't set this property to `false`. If you want retrievable vector content later, you must drop and rebuild the index, or create and load a new field that has the new attribution.
 
 For new vector fields in a search index, set `stored` to false to permanently remove retrievable storage for the vector field. The following example shows a vector field definition with the `stored` property.
 
@@ -86,31 +89,44 @@ PUT https://[service-name].search.windows.net/indexes/demo-index?api-version=202
 
 - Defaults are `stored` set to true and `retrievable` set to false. In a default configuration, a retrievable copy is stored, but it's not automatically returned in results. When `stored` is true, you can toggle `retrievable` between true and false at any time without having to rebuild an index. When `stored` is false, `retrievable` must be false and can't be changed.
 
-## Set the `rescoreStorageMethod` property
+## Remove full-precision vectors (binary data)
 
 [!INCLUDE [Feature preview](./includes/previews/preview-generic.md)]
 
-The `rescoreStorageMethod` property controls the storage of full-precision vectors when compression is used.
+When you compress vectors using either scalar or binary quantization, query execution is over the quantized vectors. In this case, you only need the original full-precision vectors (binary data) if you want to rescore.
 
-For *uncompressed* vector fields on indexes created with `2024-11-01-Preview` API versions and later, this will be omitted by default with no impact on search activities nor quality. For existing vector fields created prior to this API version, there is no in-place ability to remove this copy of data.
+If you use newer preview APIs *and* binary quantization, you can safely discard full-precision vectors because rescoring strategies now use the dot product of a binary embedding, which produces high quality search results, without having to reference full-precision vectors in the index.
 
-On a vector compression, the `rescoreStorageMethod` property is set to `preserveOriginals` by default, which retains full-precision vectors for[oversampling and rescoring capabilities](vector-search-how-to-quantization.md#add-compressions-to-a-search-index) to reduce the effect of lossy compression on the HNSW graph. If you don't use these capabilities, you can reduce vector storage by setting `rescoreStorageMethod` to `discardOriginals`.
+The `rescoreStorageMethod` property controls whether full-precision vectors are stored. The guidance for whether to retain full-precision vectors is:
 
-> [!IMPORTANT]
-> Setting the `rescoreStorageMethod` property is irreversible and will have different levels of search quality loss depending on the compression method. This can be set on indexes created with `2024-11-01-Preview` or later, either during index creation or adding new vector fields.
+- For scalar quantization, preserve original full-precision vectors in the index because they're required for rescore.
+- For binary quantization, preserve original full-precision vectors for the highest quality of rescoring, or discard full-precision vectors (requires 2025-03-01-preview) if you want to rescore based on the dot product of the binary embeddings.
+
+Vector storage strategies have been evolving over the last several releases. Index creation date and API version determine your storage options. 
+
+| API version | Applies to | Remove full-precision vectors |
+|-------------|-------------------------------|
+| 2024-07-01 and earlier | Not applicable. | There's no mechanism for removing full-precision vectors. |
+| 2024-11-01-preview | Binary embeddings | Use `rescoreStorageMethod.discardOriginals` to remove full-precision vectors, but doing so prevents rescoring. `enableRescoring` must be false if originals are gone.|
+| 2025-03-01-preview | Binary embeddings | Use `rescoreStorageMethod.discardOriginals` to remove full-precision vectors in the index while still retaining rescore options. In this preview, rescoring is possible because the technique changed. The dot product of the binary embeddings is used on the rescore, producing high quality search results equivalent to or better than earlier techniques based on full-precision vectors. |
 
-If you intend to use scalar or binary quantization, we recommend retaining `rescoreStorageMethod` set to `preserveOriginals` to maximize search quality.
+Notice that scalar isn't listed in the table. If you use scalar quantization, you must retain original full-precision vectors if you want to rescore.
+
+In `vectorSearch.compressions`, the `rescoreStorageMethod` property is set to `preserveOriginals` by default, which retains full-precision vectors for [oversampling and rescoring capabilities](vector-search-how-to-quantization.md#add-compressions-to-a-search-index) to reduce the effect of lossy compression on the HNSW graph. If you don't need full-precision vectors, you can reduce vector storage by setting `rescoreStorageMethod` to `discardOriginals`.
+
+> [!IMPORTANT]
+> Setting the `rescoreStorageMethod` property is irreversible and can adversely affect search quality, although the degree depends on the compression method and any mitigations you apply.
 
 To set this property:
 
-1. Use [Create Index](/rest/api/searchservice/indexes/create?view=rest-searchservice-2024-11-01-preview&preserve-view=true) or [Create or Update Index 2024-11-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true) REST APIs, or an Azure SDK beta package providing the feature.
+1. Use [Create Index (preview)](/rest/api/searchservice/indexes/create?view=rest-searchservice-2025-03-01-preview&preserve-view=true) or [Create or Update Index (preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview&preserve-view=true) REST APIs, or an Azure SDK beta package providing the feature.
 
 1. Add a `vectorSearch` section to your index with profiles, algorithms, and compressions.
 
-1. Under compressions, add `rescoringOptions` with `enableRescoring` set to true, `defaultOversampling` set to a positive integer, and `rescoreStorageMethod` set to `preserveOriginals`.
+1. Under `vectorSearch.compressions`, add `rescoringOptions` with `enableRescoring` set to true, `defaultOversampling` set to a positive integer, and `rescoreStorageMethod` set to `discardOriginals` for binary quantization and `preserveOriginals` for scalar quantization.
 
     ```http
-    PUT https://[service-name].search.windows.net/indexes/demo-index?api-version=2024-11-01-preview
+    PUT https://[service-name].search.windows.net/indexes/demo-index?api-version=2025-03-01-preview
     
     {
         "name": "demo-index",
@@ -119,9 +135,14 @@ To set this property:
         "vectorSearch": {
             "profiles": [
                 {
-                "name": "myVectorProfile",
+                "name": "myVectorProfile-1",
                 "algorithm": "myHnsw",
                 "compression": "myScalarQuantization"
+                },
+                {
+                "name": "myVectorProfile-2",
+                "algorithm": "myHnsw",
+                "compression": "myBinaryQuantization"
                 }
             ],
             "algorithms": [
@@ -150,6 +171,16 @@ To set this property:
                         "quantizedDataType": "int8"
                     },
                     "truncationDimension": null
+                },
+                {
+                    "name": "myBinaryQuantization",
+                    "kind": "binaryQuantization",
+                    "rescoringOptions": {
+                        "enableRescoring": true,
+                        "defaultOversampling": 10,
+                        "rescoreStorageMethod": "discardOriginals"
+                    },
+                    "truncationDimension": null
                 }
             ]
         }
````
</details>

### Summary

```json
{
    "modification_type": "major update",
    "modification_title": "更新存储选项文档以增强清晰度和功能"
}
```

### Explanation
对 `vector-search-how-to-storage-options.md` 文档进行了重大更新，主要包括以下几点：

1. **日期更新**：将文档的发布日期从 `11/19/2024` 修改为 `03/31/2025`，确保信息的时效性。

2. **内容精简与重组**：简化了关于向量存储实例的描述，明确表述每种实例的用途，并重新组织了段落结构，提升了文档的可读性和逻辑性。

3. **不可逆操作警示**：增加了关于删除存储实例的不可逆性说明，强调一旦删除某些数据实例，若后续需要再复原，需重新索引。

4. **更新技术细节**：详细描述了在不同 API 版本（如 2024-11-01-preview 和 2025-03-01-preview）下的向量存储策略，以及哪些情况下可以安全地移除全精度向量。

5. **提升信息保留的建议**：针对量化过程中如何保持向量质量，提供了优化建议，包括支持使用原始全精度向量进行重评分以弥补压缩带来的信息损失。

6. **示例代码修改**：更新了代码示例，以反映最新 API 版本变更，增加了对于不同压缩方法如何设置 `rescoreStorageMethod` 属性的新指导。

这些更改旨在提高文档的完整性和实用性，以帮助用户更好地管理 Azure AI Search 中的向量存储选项。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fvector-search-how-to-storage-options.md)。

## articles/search/vector-search-index-size.md{#item-bb2846}

<details>
<summary>Diff</summary>
````diff
@@ -10,7 +10,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 01/09/2025
+ms.date: 03/20/2025
 ---
 
 # Vector index size and staying under limits
@@ -24,7 +24,7 @@ For each vector field, Azure AI Search constructs an internal vector index using
 > [Vector optimization techniques](vector-search-how-to-configure-compression-storage.md) are now generally available. Use capabilities like narrow data types, scalar and binary quantization, and elimination of redundant storage to reduce your vector quota and storage quota consumption.
 
 > [!NOTE]
-> Not all algorithms consumes vector index size quota. Vector quotas are established based on memory requirements of approximate nearest neighbor search. Vector fields created with the Hierarchical Navigable Small World (HNSW) algorithm need to reside in memory during query execution because of the random-access nature of graph-based traversals. Vector fields using exhaustive KNN algorithm are loaded into memory dynamically in pages during query execution, and as a result do not consume vector quota.
+> Not all algorithms consume vector index size quota. Vector quotas are established based on memory requirements of approximate nearest neighbor search. Vector fields created with the Hierarchical Navigable Small World (HNSW) algorithm need to reside in memory during query execution because of the random-access nature of graph-based traversals. Vector fields using exhaustive KNN algorithm are loaded into memory dynamically in pages during query execution, and as a result do not consume vector quota.
 
 ## Key points about quota and vector index size
 
@@ -38,29 +38,11 @@ For each vector field, Azure AI Search constructs an internal vector index using
 
 If you aren't sure what your search service limits are, here are two ways to get that information:
 
-+ In the Azure portal, in the search service **Overview** page, both the **Properties** tab and **Usage** tab show partition size and storage, and also vector quota and vector index size.
++ In the Azure portal, on the search service **Overview** page, both the **Properties** tab and **Usage** tab show partition size and storage, and also vector quota and vector index size.
 
-+ In the Azure portal, in the **Scale** page, you can review the number and size of partitions.
++ In the Azure portal, on the **Scale** page, you can review the number and size of partitions.
 
-## How to check service creation date
-
-Newer services created after April 3, 2024 offer five to ten times more vector storage as older ones at the same tier billing rate. If your service is older, consider creating a new service and migrating your content.
-
-1. In Azure portal, open the resource group that contains your search service.
-
-1. On the leftmost pane, under **Settings**, select **Deployments**.
-
-1. Locate your search service deployment. If there are many deployments, use the filter to look for "search".
-
-1. Select the deployment. If you have more than one, click through to see if it resolves to your search service.
-
-    :::image type="content" source="media/vector-search-index-size/resource-group-deployments.png" lightbox="media/vector-search-index-size/resource-group-deployments.png" alt-text="Screenshot of a filtered deployments list.":::
-
-1. Expand deployment details. You should see *Created* and the creation date.
-
-   :::image type="content" source="media/vector-search-index-size/deployment-details.png" lightbox="media/vector-search-index-size/deployment-details.png" alt-text="Screenshot of the deployment details showing creation date.":::
-
-1. Now that you know the age of your search service, review the vector quota limits based on service creation: [Vector index size limits](search-limits-quotas-capacity.md#vector-index-size-limits).
+Your vector limit varies depending on your [service creation date](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date).
 
 ## How to get vector index size
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新向量索引大小文档以修正措辞和增加信息"
}
```

### Explanation
对 `vector-search-index-size.md` 文档进行了小幅更新，主要包括以下几点：

1. **日期更新**：将文档的发布日期从 `01/09/2025` 修改为 `03/20/2025`，确保文档的时效性和相关性。

2. **措辞修正**：对一些句子的措辞进行了微调，主要是将“消耗”更改为“消费”，以提高表述的准确性。

3. **信息增强**：在检查服务创建日期的信息部分中，去掉了一些冗余的步骤描述，集中强调用户可以通过服务创建日期来了解其向量限制，以避免过于复杂的指引。也增加了一句话，明确指出向量限制会根据服务创建日期而有所不同。

4. **内容整合**：精简了一些不必要的内容，使得文档更加简洁流畅，易于理解。

这些更新旨在提高文档的专业性和可操作性，使用户更容易获取和理解 Azure AI Search 中向量索引的相关信息。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fvector-search-index-size.md)。

## articles/search/vector-search-overview.md{#item-56e5fa}

<details>
<summary>Diff</summary>
````diff
@@ -67,7 +67,7 @@ Azure AI Search supports [hybrid scenarios](hybrid-search-overview.md) that run
 
 Vector search is available as part of all Azure AI Search tiers in all regions at no extra charge.
 
-Newer services created after April 3, 2024 support [higher quotas for vector indexes](vector-search-index-size.md).
+Newer services created after April 3, 2024 support [higher quotas for vector indexes](vector-search-index-size.md). If you have an older service, you might be able to [upgrade your service](search-how-to-upgrade.md) for higher vector quotas.
 
 Vector search is available in:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "增加关于服务升级以提高向量配额的说明"
}
```

### Explanation
对 `vector-search-overview.md` 文档进行了小幅更新，主要包括以下内容：

1. **说明增强**：在关于新服务支持更高向量索引配额的句子后增加了信息，指出如果用户使用的是旧服务，可能能够通过升级服务来获取更高的向量配额。这为用户提供了额外的选择和信息，帮助他们更好地管理其 Azure AI Search 服务。

2. **措辞简化**：文档中对新服务配额的表述进行了微调，使其更为清晰易懂，同时突出了升级服务这一选项的重要性。

此次更新旨在增强文档的实用性和可操作性，使用户理解其服务升级的可能性，以便更好地利用 Azure AI Search 的功能。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fvector-search-overview.md)。

## articles/search/vector-store.md{#item-db9b8c}

<details>
<summary>Diff</summary>
````diff
@@ -9,7 +9,7 @@ ms.service: azure-ai-search
 ms.custom:
   - ignite-2023
 ms.topic: concept-article
-ms.date: 03/11/2025
+ms.date: 03/21/2025
 ---
 
 # Vector storage in Azure AI Search
@@ -160,11 +160,11 @@ The following screenshot shows an S1 service configured with one partition and o
 
 :::image type="content" source="media/vector-search-overview/usage-tiles-storage-vector-index.png" alt-text="Screenshot of usage tiles showing storage, vector index, and index count.":::
 
-Vector index limits and estimations are covered in [another article](vector-search-index-size.md), but two points to emphasize up front is that maximum storage varies by service tier, and also by when the search service was created. Newer same-tier services have significantly more capacity for vector indexes. For these reasons, take the following actions:
+Vector index limits and estimations are covered in [another article](vector-search-index-size.md), but two points to emphasize are that maximum storage varies by service tier and by when the search service was created. Newer same-tier services have significantly more capacity for vector indexes. For these reasons, take the following actions:
 
-+ [Check the deployment date of your search service](vector-search-index-size.md#how-to-check-service-creation-date). If it was created before April 3, 2024, consider creating a new search service for greater capacity.
++ [Check the deployment date of your search service](search-how-to-upgrade.md#check-your-service-creation-or-upgrade-date). If it was created before April 3, 2024, you might be able to [upgrade your service](search-how-to-upgrade.md) for greater capacity.
 
-+ [Choose a scalable tier](search-sku-tier.md) if you anticipate fluctuations in vector storage requirements. The Basic tier is fixed at one partition on older search services. Consider Standard 1 (S1) and above for more flexibility and faster performance, or create a new search service that uses higher limits and more partitions at every nillable tier.
++ [Choose a scalable tier](search-sku-tier.md) if you anticipate fluctuations in vector storage requirements. The Basic tier is fixed at one partition on older search services. Consider Standard 1 (S1) and above for more flexibility and faster performance. In the 2025-02-01-preview, you can also [switch from a lower tier to a higher tier](search-capacity-planning.md#change-your-pricing-tier).
 
 ## Basic operations and interaction
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "更新向量存储文档以提供更多服务升级信息"
}
```

### Explanation
对 `vector-store.md` 文档进行了小幅更新，主要包括以下变化：

1. **发布日期更新**：将文档的发布日期从 `03/11/2025` 修改为 `03/21/2025`，确保文档反映最新的信息。

2. **措辞调整**：在关于向量索引限制和估算的描述中，调整了一些措辞，使其更为简洁流畅。例如，去掉了“要强调的前两点”，“强调”一词的重复使用得到了简化。

3. **信息更新**：在提到如何检查搜索服务创建日期时，增加了通过升级服务以获得更大容量的可能性。这一更新向用户明确了他们可以考虑选择升级而不是单纯创建新服务的方法。

4. **新功能说明**：在最后一段中，添加了关于在2025年02月01日预览版中可以如何在不同定价层之间切换的信息。这为用户提供了更多的灵活性和选择以满足其存储需求的波动。

通过这些更新，文档的可读性和实用性得到了改善，帮助用户更好地理解 Azure AI 搜索中的向量存储选项及其配置。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fvector-store.md)。

## articles/search/whats-new.md{#item-fa71b4}

<details>
<summary>Diff</summary>
````diff
@@ -7,156 +7,98 @@ author: HeidiSteen
 ms.author: heidist
 ms.service: azure-ai-search
 ms.topic: overview
-ms.date: 02/28/2025
+ms.date: 03/31/2025
 ms.custom:
   - references_regions
   - ignite-2024
 ---
 
 # What's new in Azure AI Search
 
-**Azure Cognitive Search is now Azure AI Search**. Learn about the latest updates to Azure AI Search functionality, docs, and samples.
+Learn about the latest updates to Azure AI Search functionality, docs, and samples.
 
 > [!NOTE]
 > Preview features are announced here, but we also maintain a [preview features list](search-api-preview.md) so you can find them in one place.
 
-## February 2025
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [**Customer-managed keys support for Managed HSM**](search-security-manage-encryption-keys.md) | Security | Use either Azure Key Vault or Azure Key Vault Managed HSM (Hardware Security Module) to store customer-managed keys for extra encryption of sensitive content. |
-
-## December 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [**RAG chat with Azure AI Search + Python**](https://azure.github.io/ai-app-templates/repo/azure-samples/azure-search-openai-demo/) | Template | An AI application template for building a RAG solution using Azure AI Search and Python. |
-
-## November 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [**Network security perimeter**](search-security-network-security-perimeter.md) | Security | Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. The Azure portal and the Management REST APIs in the [2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) can be used to view and reconcile network security perimeter configurations. |
-| [**Shared private link support for Azure AI service connections**](search-indexer-howto-access-private.md) | Security  | Connections to Azure AI for built-in skills processing can now be private using a shared private link on the connection. |
-| [**Rescoring options for compressed vectors**](/azure/search/vector-search-how-to-quantization?tabs=2024-11-01-preview%2Cquery-2024-07-01#add-compressions-to-a-search-index) | Relevance | You can set options to rescore with original vectors instead of compressed vectors. Applies to HNSW and exhaustive KNN vector algorithms, using binary and scalar compression. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**Store fewer vector instances**](vector-search-how-to-storage-options.md) | vector search | In vector compression scenarios, you can omit storage of full precision vectors if you don't need them for rescoring. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**Query rewrite in the semantic reranker**](semantic-how-to-query-rewrite.md) | Relevance | You can set options on a semantic query to rewrite the query input into a revised or expanded query that generates more relevant results from the L2 ranker. Available in the [Search Documents (2024-11-01-preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature.|
-| [**New semantic ranker models**](semantic-search-overview.md) | Relevance | Semantic ranker runs with improved models in all supported regions. There is no change to APIs or the Azure portal experience. |
-| [**Document Layout skill**](cognitive-search-skill-document-intelligence-layout.md) | Applied AI (skills) | A new skill used to analyze a document for structure and provide [structure-aware (paragraph) chunking](search-how-to-semantic-chunking.md). This skill calls Document Intelligence and uses the Document Intelligence layout model. Available in selected regions through the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature.|
-| [**Keyless billing for Azure AI skills processing**](cognitive-search-attach-cognitive-services.md) | Applied AI (skills) | You can now use a managed identity and roles for a keyless connection to Azure AI services for built-in skills processing. This capability removes restrictions for having both search and AI services in the same region. Available in the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**Markdown parsing mode**](search-how-to-index-markdown-blobs.md) | Indexer data source |  With this parsing mode, indexers can generate one-to-one or one-to-many search documents from Markdown files in Azure Storage and OneLake. Available in the [Create or Update Indexer (2024-11-01-preview)](/rest/api/searchservice/indexers/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
-| [**2024-11-01-preview**](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-11-01-preview&preserve-view=true) | API | Preview release of REST APIs for query rewrite, Document Layout skill, keyless billing for skills processing, Markdown parsing mode, and rescoring options for compressed vectors. |
-| [**Portal support for structured data**](search-get-started-portal-import-vectors.md) | Feature | The **Import and vectorize data** wizard now supports Azure SQL, Azure Cosmos DB, and Azure Table Storage.|
-
-## October 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Lower the dimension requirements for MRL-trained text embedding models on Azure OpenAI](vector-search-how-to-truncate-dimensions.md) | Feature | Text-embedding-3-small and Text-embedding-3-large are trained using Matryoshka Representation Learning (MRL). This allows you to truncate the embedding vectors to fewer dimensions, and adjust the balance between vector index size usage and retrieval quality. A new `truncationDimension` in the [2024-09-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true) enables access to MRL compression in text embedding models. This can only be configured for new vector fields. |
-| [Unpack `@search.score` to view subscores in hybrid search results](hybrid-search-ranking.md#unpack-a-search-score-into-subscores-preview) | Feature | You can investigate Reciprocal Rank Fusion (RRF) ranked results by viewing the individual query subscores of the final merged and scored result. A new `debug` property unpacks the search score. `QueryResultDocumentSubscores`, `QueryResultDocumentRerankerInput`, and `QueryResultDocumentSemanticField` provide the extra detail. These definitions are available in the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
-| [Target filters in a hybrid search to just the vector queries](hybrid-search-how-to-query.md#hybrid-search-with-filters-targeting-vector-subqueries-preview) | Feature | A filter on a hybrid query involves all subqueries on the request, regardless of type. You can override the global filter to scope the filter to a specific subquery. The new `filterOverride` parameter is available on hybrid queries using the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
-| [Text Split skill (token chunking)](cognitive-search-skill-textsplit.md) | Applied AI (skills) | This skill has new parameters that improve data chunking for embedding models. A new `unit` parameter lets you specify token chunking. You can now chunk by token length, setting the length to a value that makes sense for your embedding model. You can also specify the tokenizer and any tokens that shouldn't be split during data chunking. The new `unit` parameter and query subscore definitions are found in the [2024-09-01-preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
-| [2024-09-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-09-01-preview&preserve-view=true) | API | Preview release of REST APIs for truncated dimensions in text-embedding-3 models, targeted vector filtering for hybrid queries, RRF subscore details for debugging, and token chunking for Text Split skill.|
-| [Portal support for customer-managed key encryption (CMK)](search-security-manage-encryption-keys.md#step-4-encrypt-content) | Feature | When you create new objects in the Azure portal, you can now specify CMK-encryption and select an Azure Key Vault to provide the key. |
-
-## August 2024
+## March 2025
 
 | Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
 |-----------------------------|------|--------------|
-| [Debug Session improvements](cognitive-search-debug-session.md) | feature | There are two important improvements. First, you can now debug integrated vectorization and data chunking workloads. Second, Debug Sessions is redesigned for a more streamlined presentation of skills and mappings. You can select an object in the flow, and view or edit its details in a side panel. The previous tabbed layout is fully replaced with more context-sensitive information on the page. |
-| [2024-07-01](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-07-01&preserve-view=true) | API | Stable release of REST APIs for generally available vector data types, vector compression, and integrated vectorization during indexing and queries. |
-| [Integrated vectorization](vector-search-integrated-vectorization.md) | Feature | Announcing general availability. Skills-driven data chunking and embedding during indexing. |
-| [Vectorizers](vector-search-how-to-configure-vectorizer.md) | Feature  | Announcing general availability. Text-to-vector conversion during query execution. Both [Azure OpenAI vectorizer](vector-search-vectorizer-azure-open-ai.md) and [custom Web API vectorizer](vector-search-vectorizer-custom-web-api.md) are generally available. |
-| [AzureOpenAIEmbedding skill](cognitive-search-skill-azure-openai-embedding.md) | Feature | Announcing general availability. A skill type that calls an Azure OpenAI embedding model to generate embeddings during indexing.  |
-| [Index projections](index-projections-concept-intro.md) | Feature | Announcing general availability. A component of a skillset definition that defines the shape of a secondary index, supporting a one-to-many index pattern, where content from an enrichment pipeline can target multiple indexes. |
-| [Binary and Scalar quantization](vector-search-how-to-quantization.md)  | Feature | Announcing general availability. Compress vector index size in memory and on disk using built-in quantization. |
-| [Narrow data types](vector-search-how-to-assign-narrow-data-types.md) | Feature  | Announcing general availability. Assign a smaller data type on vector fields, assuming incoming data is of that data type. |
-| [Import and vectorize data wizard](search-get-started-portal-import-vectors.md) | Azure portal | Announcing general availability. A wizard that creates a full indexing pipeline that includes data chunking and vectorization. The wizard creates all necessary objects and configurations. This release adds wizard support for Azure Data Lake in Azure Storage.|
-| [stored property](vector-search-how-to-storage-options.md) | Feature  | Announcing general availability. Boolean that reduces storage of vector indexes by *not* storing retrievable vectors. |
-| [vectorQueries.Weight property](vector-search-how-to-query.md#vector-weighting) | Feature  | Announcing general availability. Specify the relative weight of each vector query in a search operation. |
-
-## July 2024
+| [Service upgrade (preview)](search-how-to-upgrade.md) | Service | Upgrade your search service to higher storage limits in your region. With a one-time upgrade, you no longer need to recreate your service. Available in [Upgrade Service (2025-02-01-preview)](/rest/api/searchmanagement/services/upgrade?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true) and the Azure portal. |
+| [Pricing tier change (preview)](search-capacity-planning.md#change-your-pricing-tier) | Service | Change the [pricing tier](search-sku-tier.md) of your search service. This provides flexibility to scale storage, increase request throughput, and decrease latency based on your needs. In this preview, you can only change between Basic and Standard (S1, S2, and S3) tiers. Available in [Update Service (2025-02-01-preview)](/rest/api/searchmanagement/services/update?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true#searchupdateservicewithsku) and the Azure portal. |
+| [Facet hierarchies, aggregations, and facet filters (preview)](search-faceted-navigation-examples.md) | Queries | New facet query parameters support nested facets. For numeric facetable fields, you can sum the values of each field. You can also specify filters on a facet to add inclusion or exclusion criteria. Available in [Search Documents (2025-03-01-preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2025-03-01-preview&preserve-view=true) and the Azure portal.|
+| [Rescore vector queries over binary quantization using full precision vectors (preview)](vector-search-how-to-quantization.md#recommended-rescoring-techniques) | Queries | For vector indexes that contain binary quantization, you can rescore query results using a full precision vector query. The query engine uses the dot product of the binary embeddings and the vector query for rescoring, which improves the quality of search results.  Set `enableRescoring` and `discardOriginals` to use this feature, and call the latest preview API version on the request.|
+| [Semantic ranker pre-release models (preview)](semantic-how-to-configure.md#opt-in-for-prerelease-semantic-ranking-models) | Index | Opt in to use pre-release semantic ranker models if one happens to be available in your region. Available in [Create or Update Index (2025-03-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2025-03-01-preview#semanticconfiguration&preserve-view=true).|
+| [Search Service REST 2025-03-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2025-03-01-preview&preserve-view=true) | REST | Public preview release of REST APIs for data plane operations. Adds support for multi-vector embeddings, hierarchical facets, facet aggregation, and facet filters. |
+| [Search Management 2025-02-01-preview](/rest/api/searchmanagement/management-api-versions?view=rest-searchmanagement-2025-02-01-preview&preserve-view=true) | REST | Public review release of REST APIs for control plane operations. Adds support for in-place upgrade to higher capacity partitions, in-place upgrade to higher tiers, and Azure Confidential Compute. |
 
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Chat with your data](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator) | Accelerator| A solution accelerator for the RAG pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to create conversational search experiences. The code with sample data is available for use case scenarios such as financial advisor and contract review and summarization.|
-| [Conversational Knowledge Mining](https://github.com/microsoft/Customer-Service-Conversational-Insights-with-Azure-OpenAI-Services) | Accelerator| A solution accelerator built on top of Azure AI Search, Azure Speech and Azure OpenAI services that allows customers to extract actionable insights from post-contact center conversations. |
-| [Build your own copilot](https://github.com/microsoft/Build-your-own-AI-Assistant-Solution-Accelerator) | Accelerator| Create your own custom copilot solution that empowers [Client Advisor](https://github.com/microsoft/Build-your-own-copilot-Solution-Accelerator/blob/main/ClientAdvisor/README.md) to harness the power of generative AI across both structured and unstructured data. Help our customers to optimize daily tasks and foster better interactions with more clients.  |
-
-## June 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Image search in the Azure portal](search-get-started-portal-image-search.md) | Feature | Search explorer now supports image search. In a vector index that has vectorized image content, you can drop images into Search Explorer to query for a match.
-
-## May 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Higher capacity and more vector quota at every tier (same billing rate)](search-limits-quotas-capacity.md#service-limits) | Infrastructure | For most regions, partition sizes are now even larger for Standard 2 (S2), Standard 3 (S3), and Standard 3 High Density (S3 HD) for services created after April 3, 2024. To get the larger partitions, create a new service in a [region that provides newer infrastructure](search-region-support.md). <br><br>Storage Optimized tiers (L1 and L2) also have more capacity. L1 and L2 customers must create a new service to benefit from the higher capacity. There's no in-place upgrade at this time. <br><br>Extra capacity is now available in [more regions](search-limits-quotas-capacity.md#service-limits): Germany North​, Germany West Central​, South Africa North​, Switzerland West​, and Azure Government (Texas, Arizona, and Virginia).|
-| [OneLake integration (preview)](search-how-to-index-onelake-files.md) | Feature | New indexer for OneLake files and OneLake shortcuts. If you use Microsoft Fabric and OneLake for data access to Amazon Web Services (AWS) and Google data sources, use this indexer to import external data into a search index. This indexer is available through the Azure portal, the [2024-05-01-preview REST API](/rest/api/searchservice/data-sources/create-or-update?view=rest-searchservice-2024-05-01-preview&preserve-view=true), and Azure SDK beta packages. |
-| [Vector relevance](vector-search-how-to-query.md) <br>[hybrid query relevance](hybrid-search-how-to-query.md) | Feature | Four enhancements improve vector and hybrid search relevance. <br><br>First, you can now set thresholds on vector search results to exclude low-scoring results. <br><br>Second, changes in the query architecture apply scoring profiles at the end of the query pipeline for every query type. Document boosting is a common scoring profile, and it now works as expected on vector and hybrid queries.<br><br>Third, you can set [`MaxTextRecallSize` and `countAndFacetMode`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) in hybrid queries to control the quantity of BM25-ranked search results that flow into the hybrid ranking model. <br><br>Fourth, for vector and hybrid search, you can weight a vector query to have boost or diminish its importance in a multiquery request. |
-| [Binary vectors support](/rest/api/searchservice/supported-data-types) | Feature | `Collection(Edm.Byte)` is a new supported data type. This data type opens up integration with the [Cohere v3 binary embedding models](https://cohere.com/blog/int8-binary-embeddings) and custom binary quantization. Narrow data types lower the cost of large vector datasets. See [Index binary data for vector search](vector-search-how-to-index-binary-data.md) for more information.| 
-| [Azure AI Vision multimodal embeddings skill (preview)](cognitive-search-skill-vision-vectorize.md) | Skill | New skill that's bound to the [multimodal embeddings API of Azure AI Vision](/azure/ai-services/computer-vision/concept-image-retrieval). You can generate embeddings for text or images during indexing. This skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true).|
-| [Azure AI Vision vectorizer (preview)](vector-search-vectorizer-ai-services-vision.md) | Vectorizer | New vectorizer connects to an Azure AI Vision resource using the [multimodal embeddings API](/azure/ai-services/computer-vision/concept-image-retrieval) to generate embeddings at query time. This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
-| [Azure AI Foundry model catalog vectorizer (preview)](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) | Vectorizer | New vectorizer connects to an embedding model deployed from the [Azure AI Foundry model catalog](/azure/ai-foundry/how-to/model-catalog-overview). This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). <br><br>[**How to implement integrated vectorization using models from Azure AI Foundry**](vector-search-integrated-vectorization-ai-studio.md).|
-| [AzureOpenAIEmbedding skill (preview) supports more models on Azure OpenAI](cognitive-search-skill-azure-openai-embedding.md) | Skill | Now supports text-embedding-3-large and text-embedding-3-small, along with text-embedding-ada-002 from the previous update. New `dimensions` and `modelName` properties make it possible to specify the various embedding models on Azure OpenAI. Previously, the dimensions limits were fixed at 1,536 dimensions, applicable to text-embedding-ada-002 only. The updated skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true).|
-| Azure portal updates | Portal | [Import and vectorize data wizard](search-get-started-portal-import-vectors.md) now supports OneLake indexers as a data source. For embeddings, it also supports connections to Azure AI Vision multimodal, Azure AI Foundry model catalog, and more embedding models on Azure OpenAI. <br><br>When adding a field to an index, you can choose a [binary data type](vector-search-how-to-index-binary-data.md). <br><br>[Search explorer](search-explorer.md) now defaults to 2024-05-01-preview and supports the new preview features for vector and hybrid queries.  |
-| [2024-05-01-preview](/rest/api/searchservice/search-service-api-versions#2024-05-01-preview) | API | New preview version of the Search REST APIs provides new skills and vectorizers, new binary data type, OneLake files indexer, and new query parameters for more relevant results. See [Upgrade REST APIs](search-api-migration.md) if you have existing code written against the 2023-07-01-preview and need to migrate to this version.|
-| Azure SDK beta packages | API | Review the changelogs of the following Azure SDK beta packages for new feature support: [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/CHANGELOG.md), [Azure SDK for .NET](https://github.com/Azure/azure-sdk-for-net/blob/Azure.Search.Documents_11.6.0-beta.4/sdk/search/Azure.Search.Documents/CHANGELOG.md), [Azure SDK for Java](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/search/azure-search-documents/CHANGELOG.md) |
-| [Python code samples](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/readme.md)  | Samples | New end-to-end samples demonstrate [integration with Cohere Embed v3](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/community-integration/cohere/azure-search-cohere-embed-v3-sample.ipynb), [integration with OneLake and cloud data platforms on Google and AWS](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/e2e-demos/azure-ai-search-e2e-build-demo.ipynb), and [integration with Azure AI Vision multimodal APIs](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/embeddings/multimodal-embeddings/multimodal-embeddings.ipynb). |
-
-## April 2024
-
-| Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
-|-----------------------------|------|--------------|
-| [Security update addressing information disclosure](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2024-29063) | API | GET responses [no longer return connection strings or keys](search-api-migration.md#breaking-changes-for-client-code-that-reads-connection-information). Applies to GET Skillset, GET Index, and GET Indexer. This change helps protect your Azure assets integrated with AI Search from unauthorized access. |
-| [More storage on Basic and Standard tiers](search-limits-quotas-capacity.md#service-limits) | Infrastructure |  Basic now supports up to three partitions and three replicas. Basic and Standard (S1, S2, S3) tiers have significantly more storage per partition, at the same per-partition billing rate. Extra capacity is subject to [regional availability](search-limits-quotas-capacity.md#service-limits) and applies to new search services created after April 3, 2024. Currently, there's no in-place upgrade, so you must create a new search service to get the extra storage. |
-| [More quota for vectors](search-limits-quotas-capacity.md#vector-index-size-limits) | Infrastructure | Vector quotas are also higher on new services created after April 3, 2024 in selected regions. |
-| [Vector quantization, narrow vector data types, and a new `stored` property (preview)](vector-search-how-to-configure-compression-storage.md) | Feature | Collectively, these three features add vector compression and smarter storage options. First, *scalar quantization* reduces vector index size in memory and on disk. Second, [narrow data types](/rest/api/searchservice/supported-data-types) reduce per-field storage by storing smaller values. Third, you can use `stored` to opt-out of storing the extra copy of a vector that's used only for search results. If you don't need vectors in a query response, you can set `stored` to false to save on space. |
-| [2024-03-01-preview Search REST API](/rest/api/searchservice/search-service-api-versions#2024-03-01-preview) | API | New preview version of the Search REST APIs for the new data types, vector compression properties, and vector storage options. |
-| [2024-03-01-preview Management REST API](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true) | API | New preview version of the Management REST APIs for control plane operations.  |
-| [2023-07-01-preview deprecation announcement](/rest/api/searchservice/search-service-api-versions#2023-07-01-preview) | API | Deprecation announced on April 8, 2024. It becomes unsupported on July 8, 2024. This was the first REST API that offered vector search support. Newer API versions have a different vector configuration. You should [migrate to a newer version](search-api-migration.md) as soon as possible. |
-
-## February 2024
+## February 2025
 
 | Item&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type |  Description |
 |-----------------------------|------|--------------|
-| New dimension limits | Feature | For vector fields, maximum dimension limits are now `3072`, up from `2048`. |
+| [Customer-managed keys support for Managed HSM](search-security-manage-encryption-keys.md) | Security | Use either Azure Key Vault or Azure Key Vault Managed HSM (Hardware Security Module) to store customer-managed keys for extra encryption of sensitive content. |
 
-## 2023 announcements
+## 2024 announcements
 
 | Month | Type | Announcement |
 |-------|------|-------------|
-| November | Feature | [**Vector search, generally available**](vector-search-overview.md). The previous restriction on customer-managed keys (CMK) is now lifted. [Prefiltering](vector-search-how-to-query.md) and [exhaustive K-nearest neighbor algorithm](vector-search-ranking.md) are also now generally available. |
-| November | Feature | [**Semantic ranker, generally available**](semantic-search-overview.md)|
-| November | Feature | [**Integrated vectorization (preview)**](vector-search-integrated-vectorization.md) adds data chunking and text-to-vector conversions during indexing, and also adds text-to-vector conversions at query time. |
-| November | Feature | [**Import and vectorize data wizard (preview)**](search-get-started-portal-import-vectors.md) automates data chunking and vectorization. It targets the [2023-10-01-Preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-10-01-preview&preserve-view=true) REST API. | 
-| November | Feature | [**Index projections (preview)**](index-projections-concept-intro.md) defines the shape of a secondary index, used for a one-to-many index pattern, where content from an enrichment pipeline can target multiple indexes. | 
-| November | API | [**2023-11-01 Search REST API**](/rest/api/searchservice/search-service-api-versions#2023-11-01) is stable version of the Search REST APIs for [vector search](vector-search-overview.md) and [semantic ranking](semantic-how-to-query-request.md). See [Upgrade REST APIs](search-api-migration.md) for migration steps to generally available features.|
-| November | API | [**2023-11-01 Management REST API**](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2023-11-01&preserve-view=true) adds APIs that [enable or disable semantic ranker](/rest/api/searchmanagement/services/create-or-update#searchsemanticsearch). |
-| November | Skill | [**Azure OpenAI Embedding skill (preview)**](cognitive-search-skill-azure-openai-embedding.md) connects to a deployed embedding model on your Azure OpenAI resource to generate embeddings during skillset execution.|
-| November | Skill | [**Text Split skill (preview)**](cognitive-search-skill-textsplit.md) updated in [2023-10-01-Preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-10-01-preview&preserve-view=true) to support native data chunking. |
-| November | Video | [**How vector search and semantic ranking improve your GPT prompts**](https://www.youtube.com/watch?v=Xwx1DJ0OqCk) explains how hybrid retrieval gives you optimal grounding data for generating useful AI responses and enables search over both concepts and keywords. |
-| November | Sample | [**Role-based access control in Generative AI applications**](https://techcommunity.microsoft.com/t5/azure-ai-services-blog/access-control-in-generative-ai-applications-with-azure/ba-p/3956408) explains how to use Microsoft Entra ID and Microsoft Graph API to roll out granular user permissions on chunked content in your index. |
-| October | Sample  | [**"Chat with your data" solution accelerator**](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator). End-to-end RAG pattern that uses Azure AI Search as a retriever. It provides indexing, data chunking, and orchestration. |
-| October | Feature | [**Exhaustive  K-Nearest Neighbors (KNN)**](vector-search-overview.md#eknn) scoring algorithm for similarity search in vector space. Available in the 2023-10-01-Preview REST API only. |
-| October | Feature | [**Prefilters in vector search**](vector-search-how-to-query.md) evaluate filter criteria before query execution, reducing the amount of content that needs to be searched. Available in the 2023-10-01-Preview REST API only, through a new `vectorFilterMode` property on the query that can be set to `preFilter` (default) or `postFilter`, depending on your requirements. |
-| October | API | [**2023-10-01-Preview Search REST API**](/rest/api/searchservice/search-service-api-versions#2023-10-01-Preview), breaking changes the definition for [vector fields](vector-search-how-to-create-index.md) and [vector queries](vector-search-how-to-query.md).|
-| August | Feature | [**Enhanced semantic ranking**](semantic-search-overview.md).  Upgraded models are rolling out for semantic reranking, and availability is extended to more regions. Maximum unique token counts doubled from 128 to 256.|
-| July | Sample | [**Vector demo (Azure SDK for JavaScript)**](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-javascript/readme.md). Uses Node.js and the **@azure/search-documents 12.0.0-beta.2** library to generate embeddings, create and load an index, and run several vector queries. |
-| July | Sample | [**Vector demo (Azure SDK for .NET)**](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-dotnet/DotNetVectorDemo/readme.md).  Uses the **Azure.Search.Documents 11.5.0-beta.3** library to generate embeddings, create and load an index, and run several vector queries. You can also try [this sample](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/search/Azure.Search.Documents/samples/Sample07_VectorSearch.md) from the Azure SDK team.|
-| July | Sample | [**Vector demo (Azure SDK for Python)**](https://github.com/Azure/azure-search-vector-samples/tree/main/demo-python) Uses the latest beta release of the **azure.search.documents** to generate embeddings, create and load an index, and run several vector queries. Visit the [azure-search-vector-samples/demo-python](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python) repo for more vector search demos. |
-| June | Feature | [**Vector search public preview**](vector-search-overview.md). |
-| June | Feature | [**Semantic search availability**](semantic-search-overview.md), available on the Basic tier.|
-| June | API | [**2023-07-01-Preview Search REST API**](/rest/api/searchservice/index-preview). Support for vector search. |
-| May | Feature | [**Azure RBAC (role-based access control, generally available)**](search-security-rbac.md). |
-| May | API | [**2022-09-01 Management REST API**](/rest/api/searchmanagement), with support for configuring search to use Azure roles. The **Az.Search** module of Azure PowerShell and **Az search** module of the Azure CLI are updated to support search service authentication options. You can also use the [**Terraform provider**](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/search_service) to configure authentication options (see this [Terraform quickstart](search-get-started-terraform.md) for details). | 
-| April | Sample |  [**Multi-region deployment of Azure AI Search for business continuity and disaster recovery**](https://github.com/Azure-Samples/azure-search-multiple-regions). Deployment scripts that fully configure a multi-regional solution for Azure AI Search, with options for synchronizing content and request redirection if an endpoint fails. |
-| March |  Sample | [**ChatGPT + Enterprise data with Azure OpenAI and Azure AI Search (GitHub)**](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/README.md). Python code and a template for combining Azure AI Search with the large language models in OpenAI. For background, see this Tech Community blog post: [Revolutionize your Enterprise Data with ChatGPT](https://techcommunity.microsoft.com/t5/ai-applied-ai-blog/revolutionize-your-enterprise-data-with-chatgpt-next-gen-apps-w/ba-p/3762087). <br><br>Key points: <br><br>Use Azure AI Search to consolidate and index searchable content.</br> <br>Query the index for initial search results.</br> <br>Assemble prompts from those results and send to the gpt-35-turbo (preview) model in Azure OpenAI.</br> <br>Return a cross-document answer and provide citations and transparency in your customer-facing app so that users can assess the response.</br> |
+| December | Template | [RAG chat with Azure AI Search + Python](https://azure.github.io/ai-app-templates/repo/azure-samples/azure-search-openai-demo/). An AI application template for building a RAG solution using Azure AI Search and Python. |
+| November | Security | [Network security perimeter](search-security-network-security-perimeter.md).  Join a search service to a [network security perimeter](/azure/private-link/network-security-perimeter-concepts) to control network access to your search service. The Azure portal and the Management REST APIs in the [2024-06-01-preview](/rest/api/searchmanagement/network-security-perimeter-configurations?view=rest-searchmanagement-2024-06-01-preview&preserve-view=true) can be used to view and reconcile network security perimeter configurations. |
+| November | Security | [Shared private link support for Azure AI service connections](search-indexer-howto-access-private.md). Connections to Azure AI for built-in skills processing can now be private using a shared private link on the connection. |
+| November | Relevance | [Rescoring options for compressed vectors](/azure/search/vector-search-how-to-quantization?tabs=2024-11-01-preview%2Cquery-2024-07-01#add-compressions-to-a-search-index). You can set options to rescore with original vectors instead of compressed vectors. Applies to HNSW and exhaustive KNN vector algorithms, using binary and scalar compression. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Vector search | [Store fewer vector instances](vector-search-how-to-storage-options.md). In vector compression scenarios, you can omit storage of full precision vectors if you don't need them for rescoring. Available in the [Create or Update Index (2024-11-01-preview)](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Relevance | [Query rewrite in the semantic reranker](semantic-how-to-query-rewrite.md). You can set options on a semantic query to rewrite the query input into a revised or expanded query that generates more relevant results from the L2 ranker. Available in the [Search Documents (2024-11-01-preview)](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature.|
+| November | Relevance | [New semantic ranker models](semantic-search-overview.md). Semantic ranker runs with improved models in all supported regions. There's no change to APIs or the Azure portal experience. |
+| November | Applied AI (skills) | [Document Layout skill](cognitive-search-skill-document-intelligence-layout.md). A new skill used to analyze a document for structure and provide [structure-aware (paragraph) chunking](search-how-to-semantic-chunking.md). This skill calls Document Intelligence and uses the Document Intelligence layout model. Available in selected regions through the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Applied AI (skills) | [Keyless billing for Azure AI skills processing](cognitive-search-attach-cognitive-services.md). You can now use a managed identity and roles for a keyless connection to Azure AI services for built-in skills processing. This capability removes restrictions for having both search and AI services in the same region. Available in the [Create or Update Skillset (2024-11-01-preview)](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | Indexer data source | [Markdown parsing mode](search-how-to-index-markdown-blobs.md). With this parsing mode, indexers can generate one-to-one or one-to-many search documents from Markdown files in Azure Storage and OneLake. Available in the [Create or Update Indexer (2024-11-01-preview)](/rest/api/searchservice/indexers/create-or-update?view=rest-searchservice-2024-11-01-preview&preserve-view=true), the Azure portal, and in the Azure SDK beta packages that provide this feature. |
+| November | API | [2024-11-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-11-01-preview&preserve-view=true). Preview release of REST APIs for query rewrite, Document Layout skill, keyless billing for skills processing, Markdown parsing mode, and rescoring options for compressed vectors. |
+| November | Feature | [Portal support for structured data](search-get-started-portal-import-vectors.md). The **Import and vectorize data** wizard now supports Azure SQL, Azure Cosmos DB, and Azure Table Storage. |
+| October | Feature | [Lower the dimension requirements for MRL-trained text embedding models on Azure OpenAI](vector-search-how-to-truncate-dimensions.md). Text-embedding-3-small and Text-embedding-3-large are trained using Matryoshka Representation Learning (MRL). This allows you to truncate the embedding vectors to fewer dimensions, and adjust the balance between vector index size usage and retrieval quality. A new `truncationDimension` in the [2024-09-01-preview](/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true) enables access to MRL compression in text embedding models. This can only be configured for new vector fields. |
+| October | Feature | [Unpack `@search.score` to view subscores in hybrid search results](hybrid-search-ranking.md#unpack-a-search-score-into-subscores-preview). You can investigate Reciprocal Rank Fusion (RRF) ranked results by viewing the individual query subscores of the final merged and scored result. A new `debug` property unpacks the search score. `QueryResultDocumentSubscores`, `QueryResultDocumentRerankerInput`, and `QueryResultDocumentSemanticField` provide the extra detail. These definitions are available in the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
+| October | Feature | [Target filters in a hybrid search to just the vector queries](hybrid-search-how-to-query.md#hybrid-search-with-filters-targeting-vector-subqueries-preview). A filter on a hybrid query involves all subqueries on the request, regardless of type. You can override the global filter to scope the filter to a specific subquery. The new `filterOverride` parameter is available on hybrid queries using the [2024-09-01-preview](/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
+| October | Applied AI (skills) | [Text Split skill (token chunking)](cognitive-search-skill-textsplit.md). This skill has new parameters that improve data chunking for embedding models. A new `unit` parameter lets you specify token chunking. You can now chunk by token length, setting the length to a value that makes sense for your embedding model. You can also specify the tokenizer and any tokens that shouldn't be split during data chunking. The new `unit` parameter and query subscore definitions are found in the [2024-09-01-preview](/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2024-09-01-preview&preserve-view=true). |
+| October | API | [2024-09-01-preview](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-09-01-preview&preserve-view=true). Preview release of REST APIs for truncated dimensions in text-embedding-3 models, targeted vector filtering for hybrid queries, RRF subscore details for debugging, and token chunking for Text Split skill.|
+| October | Feature | [Portal support for customer-managed key encryption (CMK)](search-security-manage-encryption-keys.md#step-4-encrypt-content). When you create new objects in the Azure portal, you can now specify CMK-encryption and select an Azure Key Vault to provide the key. |
+| August | Feature | [Debug Session improvements](cognitive-search-debug-session.md). There are two important improvements. First, you can now debug integrated vectorization and data chunking workloads. Second, Debug Sessions is redesigned for a more streamlined presentation of skills and mappings. You can select an object in the flow, and view or edit its details in a side panel. The previous tabbed layout is fully replaced with more context-sensitive information on the page. |
+| August | API | [2024-07-01](/rest/api/searchservice/search-service-api-versions?view=rest-searchservice-2024-07-01&preserve-view=true). Stable release of REST APIs for generally available vector data types, vector compression, and integrated vectorization during indexing and queries. |
+| August | Feature | [Integrated vectorization](vector-search-integrated-vectorization.md), Announcing general availability. Skills-driven data chunking and embedding during indexing. |
+| August | Feature | [Vectorizers](vector-search-how-to-configure-vectorizer.md). Announcing general availability. Text-to-vector conversion during query execution. Both [Azure OpenAI vectorizer](vector-search-vectorizer-azure-open-ai.md) and [custom Web API vectorizer](vector-search-vectorizer-custom-web-api.md) are generally available. |
+| August | Feature | [AzureOpenAIEmbedding skill](cognitive-search-skill-azure-openai-embedding.md). Announcing general availability. A skill type that calls an Azure OpenAI embedding model to generate embeddings during indexing. |
+| August | Feature | [Index projections](index-projections-concept-intro.md). Announcing general availability. A component of a skillset definition that defines the shape of a secondary index, supporting a one-to-many index pattern, where content from an enrichment pipeline can target multiple indexes. |
+| August | Feature | [Binary and Scalar quantization](vector-search-how-to-quantization.md). Announcing general availability. Compress vector index size in memory and on disk using built-in quantization. |
+| August | Feature | [Narrow data types](vector-search-how-to-assign-narrow-data-types.md). Announcing general availability. Assign a smaller data type on vector fields, assuming incoming data is of that data type. |
+| August | Feature | [Import and vectorize data wizard](search-get-started-portal-import-vectors.md). Announcing general availability. A wizard that creates a full indexing pipeline that includes data chunking and vectorization. The wizard creates all necessary objects and configurations. This release adds wizard support for Azure Data Lake in Azure Storage. |
+| August | Feature | [stored property](vector-search-how-to-storage-options.md). Announcing general availability. Boolean that reduces storage of vector indexes by *not* storing retrievable vectors. |
+| August | Feature | [vectorQueries.Weight property](vector-search-how-to-query.md#vector-weighting). Announcing general availability. Specify the relative weight of each vector query in a search operation. |
+| July | Accelerator | [Chat with your data](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator). A solution accelerator for the RAG pattern running in Azure, using Azure AI Search for retrieval and Azure OpenAI large language models to create conversational search experiences. The code with sample data is available for use case scenarios such as financial advisor and contract review and summarization. |
+| July | Accelerator | [Conversational Knowledge Mining](https://github.com/microsoft/Customer-Service-Conversational-Insights-with-Azure-OpenAI-Services). A solution accelerator built on top of Azure AI Search, Azure Speech and Azure OpenAI services that allows customers to extract actionable insights from post-contact center conversations. |
+| July | Accelerator | [Build your own copilot](https://github.com/microsoft/Build-your-own-AI-Assistant-Solution-Accelerator). Create your own custom copilot solution that empowers [Client Advisor](https://github.com/microsoft/Build-your-own-copilot-Solution-Accelerator/blob/main/ClientAdvisor/README.md) to harness the power of generative AI across both structured and unstructured data. Help our customers to optimize daily tasks and foster better interactions with more clients. |
+| June | Feature | [Image search in the Azure portal](search-get-started-portal-image-search.md). Search explorer now supports image search. In a vector index that contains vectorized image content, you can drop images into Search Explorer to query for a match. |
+| May | Service limits| [Higher capacity and more vector quota at every tier (same billing rate)](search-limits-quotas-capacity.md#service-limits). For most regions, partition sizes are now even larger for Standard 2 (S2), Standard 3 (S3), and Standard 3 High Density (S3 HD) for services created after April 3, 2024. To get the larger partitions, create a new service in a [region that provides newer infrastructure](search-region-support.md). <br><br>Storage Optimized tiers (L1 and L2) also have more capacity. L1 and L2 customers must create a new service to benefit from the higher capacity. There's no in-place upgrade at this time. <br><br>Extra capacity is now available in [more regions](search-limits-quotas-capacity.md#service-limits): Germany North​, Germany West Central​, South Africa North​, Switzerland West​, and Azure Government (Texas, Arizona, and Virginia). |
+| May | Feature | [OneLake integration (preview)](search-how-to-index-onelake-files.md). New indexer for OneLake files and OneLake shortcuts. If you use Microsoft Fabric and OneLake for data access to Amazon Web Services (AWS) and Google data sources, use this indexer to import external data into a search index. This indexer is available through the Azure portal, the [2024-05-01-preview REST API](/rest/api/searchservice/data-sources/create-or-update?view=rest-searchservice-2024-05-01-preview&preserve-view=true), and Azure SDK beta packages. |
+| May | Feature | [Vector relevance](vector-search-how-to-query.md) <br>[hybrid query relevance](hybrid-search-how-to-query.md). Four enhancements improve vector and hybrid search relevance. <br><br>First, you can now set thresholds on vector search results to exclude low-scoring results. <br><br>Second, changes in the query architecture apply scoring profiles at the end of the query pipeline for every query type. Document boosting is a common scoring profile, and it now works as expected on vector and hybrid queries.<br><br>Third, you can set [`MaxTextRecallSize` and `countAndFacetMode`](hybrid-search-how-to-query.md#set-maxtextrecallsize-and-countandfacetmode) in hybrid queries to control the quantity of BM25-ranked search results that flow into the hybrid ranking model. <br><br>Fourth, for vector and hybrid search, you can weight a vector query to have boost or diminish its importance in a multiquery request. |
+| May | Feature | [Binary vectors support](/rest/api/searchservice/supported-data-types). `Collection(Edm.Byte)` is a new supported data type. This data type opens up integration with the [Cohere v3 binary embedding models](https://cohere.com/blog/int8-binary-embeddings) and custom binary quantization. Narrow data types lower the cost of large vector datasets. See [Index binary data for vector search](vector-search-how-to-index-binary-data.md) for more information. |
+| May | Skill | [Azure AI Vision multimodal embeddings skill (preview)](cognitive-search-skill-vision-vectorize.md). New skill that's bound to the [multimodal embeddings API of Azure AI Vision](/azure/ai-services/computer-vision/concept-image-retrieval). You can generate embeddings for text or images during indexing. This skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
+| May | Vectorizer | [Azure AI Vision vectorizer (preview)](vector-search-vectorizer-ai-services-vision.md). New vectorizer connects to an Azure AI Vision resource using the [multimodal embeddings API](/azure/ai-services/computer-vision/concept-image-retrieval) to generate embeddings at query time. This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
+| May | Vectorizer | [Azure AI Foundry model catalog vectorizer (preview)](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md). New vectorizer connects to an embedding model deployed from the [Azure AI Foundry model catalog](/azure/ai-foundry/how-to/model-catalog-overview). This vectorizer is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). <br><br>[**How to implement integrated vectorization using models from Azure AI Foundry**](vector-search-integrated-vectorization-ai-studio.md).|
+| May | Skill | [AzureOpenAIEmbedding skill (preview) supports more models on Azure OpenAI](cognitive-search-skill-azure-openai-embedding.md). Now supports text-embedding-3-large and text-embedding-3-small, along with text-embedding-ada-002 from the previous update. New `dimensions` and `modelName` properties make it possible to specify the various embedding models on Azure OpenAI. Previously, the dimensions limits were fixed at 1,536 dimensions, applicable to text-embedding-ada-002 only. The updated skill is available through the Azure portal and the [2024-05-01-preview REST API](/rest/api/searchservice/operation-groups?view=rest-searchservice-2024-05-01-preview&preserve-view=true). |
+| May | Portal | [Import and vectorize data wizard](search-get-started-portal-import-vectors.md) now supports OneLake indexers as a data source. For embeddings, it also supports connections to Azure AI Vision multimodal, Azure AI Foundry model catalog, and more embedding models on Azure OpenAI. <br><br>When adding a field to an index, you can choose a [binary data type](vector-search-how-to-index-binary-data.md). <br><br>[Search explorer](search-explorer.md) now defaults to 2024-05-01-preview and supports the new preview features for vector and hybrid queries. |
+| May | API | [2024-05-01-preview](/rest/api/searchservice/search-service-api-versions#2024-05-01-preview). New preview version of the Search REST APIs provides new skills and vectorizers, new binary data type, OneLake files indexer, and new query parameters for more relevant results. See [Upgrade REST APIs](search-api-migration.md) if you have existing code written against the 2023-07-01-preview and need to migrate to this version. |
+| May | API | Azure SDK beta packages. Review the changelogs of the following Azure SDK beta packages for new feature support: [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/CHANGELOG.md), [Azure SDK for .NET](https://github.com/Azure/azure-sdk-for-net/blob/Azure.Search.Documents_11.6.0-beta.4/sdk/search/Azure.Search.Documents/CHANGELOG.md), [Azure SDK for Java](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/search/azure-search-documents/CHANGELOG.md) |
+| May | Samples | [Python code samples](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/readme.md). New end-to-end samples demonstrate [integration with Cohere Embed v3](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/community-integration/cohere/azure-search-cohere-embed-v3-sample.ipynb), [integration with OneLake and cloud data platforms on Google and AWS](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/e2e-demos/azure-ai-search-e2e-build-demo.ipynb), and [integration with Azure AI Vision multimodal APIs](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/embeddings/multimodal-embeddings/multimodal-embeddings.ipynb). |
+| April | API | [Security update addressing information disclosure](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2024-29063). GET responses [no longer return connection strings or keys](search-api-migration.md#breaking-changes-for-client-code-that-reads-connection-information). Applies to GET Skillset, GET Index, and GET Indexer. This change helps protect your Azure assets integrated with AI Search from unauthorized access. |
+| April | API | [2024-03-01-preview Search REST API](/rest/api/searchservice/search-service-api-versions#2024-03-01-preview) |
+| April | API | [2024-03-01-preview Management REST API](/rest/api/searchmanagement/operation-groups?view=rest-searchmanagement-2024-03-01-preview&preserve-view=true) |
+| April | API | [2023-07-01-preview deprecation announcement](/rest/api/searchservice/search-service-api-versions#2023-07-01-preview). This version is no longer supported as of July 8, 2024. Newer API versions have a different vector configuration. You should [migrate to a newer version](search-api-migration.md) as soon as possible. |
+| April | Service limits | [Basic and Standard tiers](search-limits-quotas-capacity.md#service-limits) offer more storage per partition, at the same per-partition billing rate. Extra capacity is subject to [regional availability](search-limits-quotas-capacity.md#service-limits) and applies to new search services created after April 3, 2024. Basic now supports up to three partitions and three replicas. |
+| April | Service limits | [Vector quotas are higher](search-limits-quotas-capacity.md#vector-index-size-limits) on new services created after April 3, 2024 in selected regions. |
+| April | Feature | [Vector quantization, narrow vector data types, and a new `stored` property (preview)](vector-search-how-to-configure-compression-storage.md). Collectively, these three features minimize storage and costs.|
+| February | Feature | New dimension limits on vector fields. Maximum dimension limits are now `3072`, up from `2048`.|
 
 ## Previous year's announcements
 
++ [2023 announcements](/previous-versions/azure/search/search-whats-new-2023)
 + [2022 announcements](/previous-versions/azure/search/search-whats-new-2022)
 + [2021 announcements](/previous-versions/azure/search/search-whats-new-2021)
 + [2020 announcements](/previous-versions/azure/search/search-whats-new-2020)
````
</details>

### Summary

```json
{
    "modification_type": "breaking change",
    "modification_title": "重构Whats New文档以提升信息准确性和可用性"
}
```

### Explanation
对 `whats-new.md` 文档进行了重构，主要包括以下变化：

1. **内容精简与更新**：原有文档中的部分内容被删除，取而代之的是更新的信息。特别是更新了日期至 `03/31/2025`，并调整了文档结构，使其更具可读性。

2. **新增功能概述**：在新的文档中，新增了关于Azure AI搜索功能的最新更新，包括安全性、相关性、和兼容性等方面的改进。例如，新增了对客户管理密钥的支持、对压缩向量的重评分选项、以及新的语义排序模型等内容。

3. **时间段分类和重新组织**：文档重新组织了内容的时间节点，将信息按时间段重新分类，并补充了有关2025年3月的新功能和服务，以及2024年和以前的公告，突出了新的特性和功能发布。

4. **链接合并与更新**：对文中链接进行了整合与更新，使得用户能够更加方便地访问相关的功能介绍和API文档。尤其是在涉及新的服务与更新时，提供了明确的文档链接，以避免信息查找的困难。

此次更新对用户来说是一次全面的文档更新，既提供了最新的信息，又改善了查阅体验，使用户能够更轻松地获取到Azure AI搜索中的新特性和功能。用户可以访问以下链接查看该文档的详细内容：[查看文档](https://github.com/MicrosoftDocs/azure-ai-docs/blob/a9279bd8eb8b1f3c8122b2403c1109b293f096cc/articles%2Fsearch%2Fwhats-new.md)。


